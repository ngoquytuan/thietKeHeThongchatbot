# üìã FR-03.3 DATA INGESTION PIPELINE - HANDOVER DOCUMENTATION

**Module:** FR-03.3 - Pipeline N·∫°p D·ªØ li·ªáu (Data Ingestion)  
**Version:** 1.0.0  
**Date:** January 07, 2025  
**Status:** ‚úÖ PRODUCTION READY  

---

## üìñ **M·ª§C L·ª§C**

1. [T·ªïng Quan Module](#1-t·ªïng-quan-module)
2. [Ki·∫øn Tr√∫c H·ªá Th·ªëng](#2-ki·∫øn-tr√∫c-h·ªá-th·ªëng)
3. [C√°c Component Ch√≠nh](#3-c√°c-component-ch√≠nh)
4. [API Documentation](#4-api-documentation)
5. [Deployment Guide](#5-deployment-guide)
6. [Testing & Validation](#6-testing--validation)
7. [Performance & Monitoring](#7-performance--monitoring)
8. [Troubleshooting](#8-troubleshooting)
9. [Integration Points](#9-integration-points)
10. [Future Roadmap](#10-future-roadmap)

---

## 1. **T·ªîNG QUAN MODULE**

### 1.1 **M·ª•c ƒê√≠ch**
FR-03.3 Data Ingestion Pipeline l√† h·ªá th·ªëng x·ª≠ l√Ω t√†i li·ªáu ho√†n ch·ªânh, chuy·ªÉn ƒë·ªïi raw documents th√†nh vector embeddings v√† l∆∞u tr·ªØ trong dual storage system ƒë·ªÉ ph·ª•c v·ª• cho RAG (Retrieval-Augmented Generation) engine.

### 1.2 **Ch·ª©c NƒÉng Ch√≠nh**
- ‚úÖ **Quality Control**: T√≠ch h·ª£p v·ªõi FR-03.2 ƒë·ªÉ ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng t√†i li·ªáu
- ‚úÖ **Vietnamese NLP**: X·ª≠ l√Ω vƒÉn b·∫£n ti·∫øng Vi·ªát v·ªõi pyvi v√† underthesea
- ‚úÖ **Smart Chunking**: Ph√¢n ƒëo·∫°n t√†i li·ªáu th√¥ng minh (500-1000 tokens)
- ‚úÖ **Embedding Generation**: T·∫°o vector embeddings v·ªõi Qwen model (1024D)
- ‚úÖ **Dual Storage**: L∆∞u tr·ªØ ƒë·ªìng b·ªô trong ChromaDB v√† PostgreSQL
- ‚úÖ **Pipeline Orchestration**: Qu·∫£n l√Ω workflow t·ª´ ƒë·∫ßu ƒë·∫øn cu·ªëi
- ‚úÖ **Real-time Monitoring**: Theo d√µi ti·∫øn ƒë·ªô v√† tr·∫°ng th√°i real-time
- ‚úÖ **Error Handling**: X·ª≠ l√Ω l·ªói v√† retry logic t·ª± ƒë·ªông

### 1.3 **Input/Output**

```mermaid
graph LR
    Input[üìÑ Raw Documents<br/>PDF, DOC, TXT] --> Pipeline[üîÑ FR-03.3<br/>Processing Pipeline]
    Pipeline --> VectorDB[(üü£ ChromaDB<br/>Vector Embeddings)]
    Pipeline --> PostgreSQL[(üü¶ PostgreSQL<br/>Metadata & Chunks)]
    Pipeline --> Cache[(üî¥ Redis<br/>Performance Cache)]
```

---

## 2. **KI·∫æN TR√öC H·ªÜ TH·ªêNG**

### 2.1 **Architecture Overview**

```mermaid
graph TB
    subgraph "üåê API LAYER"
        FastAPI[FastAPI Application]
        Auth[Authentication]
        CORS[CORS Middleware]
    end
    
    subgraph "üîÑ PIPELINE ORCHESTRATOR"
        JobManager[Job Manager]
        TaskScheduler[Task Scheduler]
        ProgressTracker[Progress Tracker]
    end
    
    subgraph "‚öôÔ∏è PROCESSING STAGES"
        QC[1Ô∏è‚É£ Quality Control<br/>FR-03.2 Integration]
        Chunking[2Ô∏è‚É£ Document Chunking<br/>Vietnamese NLP]
        Embedding[3Ô∏è‚É£ Embedding Generation<br/>Qwen Model]
        Storage[4Ô∏è‚É£ Dual Storage<br/>ChromaDB + PostgreSQL]
        Indexing[5Ô∏è‚É£ Indexing & Cache<br/>Redis Cache]
    end
    
    subgraph "üíæ STORAGE LAYER"
        ChromaDB[(ChromaDB<br/>Vector Storage)]
        PostgreSQL[(PostgreSQL<br/>Metadata)]
        Redis[(Redis<br/>Cache)]
    end
    
    subgraph "üìä MONITORING"
        Logging[Application Logs]
        HealthCheck[Health Checks]
        Metrics[Performance Metrics]
    end
    
    FastAPI --> JobManager
    JobManager --> QC
    QC --> Chunking
    Chunking --> Embedding
    Embedding --> Storage
    Storage --> Indexing
    
    Storage --> ChromaDB
    Storage --> PostgreSQL
    Indexing --> Redis
    
    JobManager -.-> ProgressTracker
    ProgressTracker -.-> Logging
    HealthCheck -.-> Metrics
```

### 2.2 **Technology Stack**

| **Layer** | **Technology** | **Version** | **Purpose** |
|-----------|----------------|-------------|-------------|
| **API Framework** | FastAPI | 0.104.1 | RESTful API v√† async processing |
| **Language** | Python | 3.10.11 | Core application language |
| **Vietnamese NLP** | pyvi, underthesea | Latest | Vietnamese text processing |
| **Embedding Model** | Qwen/Qwen3-Embedding-0.6B | 0.6B | Vector embedding generation |
| **Vector DB** | ChromaDB | 1.0.0 | Vector embeddings storage |
| **Relational DB** | PostgreSQL | 15+ | Metadata v√† chunk storage |
| **Cache** | Redis | 7+ | Performance caching |
| **Containerization** | Docker | Latest | Application deployment |

### 2.3 **Processing Pipeline Flow**

```mermaid
sequenceDiagram
    participant Client
    participant API
    participant Orchestrator
    participant QC as Quality Control
    participant Chunker
    participant Embedder
    participant Storage
    participant DB as Databases
    
    Client->>API: Upload Document
    API->>Orchestrator: Submit Job
    Orchestrator->>QC: Quality Check
    QC-->>Orchestrator: Quality Score
    
    alt Quality Passed
        Orchestrator->>Chunker: Chunk Document
        Chunker-->>Orchestrator: Text Chunks
        
        Orchestrator->>Embedder: Generate Embeddings
        Embedder-->>Orchestrator: Vector Embeddings
        
        Orchestrator->>Storage: Store Data
        Storage->>DB: Dual Storage
        DB-->>Storage: Storage Success
        
        Storage-->>Orchestrator: Completion
        Orchestrator-->>API: Job Complete
        API-->>Client: Success Response
    else Quality Failed
        QC-->>Orchestrator: Quality Failed
        Orchestrator-->>API: Job Failed
        API-->>Client: Error Response
    end
```

---

## 3. **C√ÅC COMPONENT CH√çNH**

### 3.1 **Pipeline Orchestrator**

**File:** `src/core/pipeline/pipeline_orchestrator.py`

**Ch·ª©c nƒÉng:**
- Qu·∫£n l√Ω workflow t·ª´ ƒë·∫ßu ƒë·∫øn cu·ªëi
- ƒêi·ªÅu ph·ªëi c√°c stage processors
- Theo d√µi ti·∫øn ƒë·ªô job real-time
- X·ª≠ l√Ω l·ªói v√† retry logic
- Qu·∫£n l√Ω concurrent jobs (max 5 jobs)

**Key Methods:**
```python
async def submit_job(document_path: str, metadata: Dict[str, Any]) -> str
async def get_job_status(job_id: str) -> Optional[PipelineJob]
async def cancel_job(job_id: str) -> bool
async def get_pipeline_stats() -> Dict[str, Any]
```

### 3.2 **Stage Processors**

**File:** `src/core/pipeline/stage_processors.py`

#### 3.2.1 **Quality Control Processor**
- T√≠ch h·ª£p v·ªõi FR-03.2 (hi·ªán t·∫°i d√πng mock)
- ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng t√†i li·ªáu (0-100 ƒëi·ªÉm)
- Detect duplicate, ambiguous content
- Validate metadata completeness

#### 3.2.2 **Chunking Processor**
- Vietnamese text preprocessing v·ªõi pyvi/underthesea
- Semantic chunking v·ªõi overlap
- Token counting v·ªõi tiktoken
- Chunk size: 500-1000 tokens, overlap: 50 tokens

#### 3.2.3 **Embedding Processor**
- Qwen/Qwen3-Embedding-0.6B model
- 1024-dimensional vectors
- Batch processing (8-16 items/batch)
- GPU/CPU support t·ª± ƒë·ªông

#### 3.2.4 **Storage Processor**
- Dual storage: ChromaDB + PostgreSQL
- Transaction-like behavior
- Automatic rollback on failures
- Consistency validation

#### 3.2.5 **Indexing Processor**
- Redis caching
- Search index updates
- Performance optimization

### 3.3 **Dual Storage Manager**

**File:** `src/core/storage/dual_storage_manager.py`

**Features:**
- ƒê·∫£m b·∫£o consistency gi·ªØa vector v√† relational storage
- Two-phase commit pattern
- Automatic rollback mechanism
- Health monitoring cho c·∫£ hai storage systems

**Storage Schema:**
```sql
-- PostgreSQL Tables
documents_metadata_v2 (document info)
document_chunks_enhanced (chunk metadata)

-- ChromaDB Collections
knowledge_base_v1 (vector embeddings)
```

### 3.4 **Embedding Service**

**File:** `src/core/embedding/embedding_service.py`

**Specifications:**
- Model: Qwen/Qwen3-Embedding-0.6B
- Dimension: 1024 (configurable t·ª´ 32-1024)
- Batch processing: 8-16 chunks/batch
- L2 normalization for better similarity
- Vietnamese text optimization

**Performance:**
- CPU: ~50 embeddings/second
- GPU: ~200 embeddings/second
- Memory: ~2GB for model loading

---

## 4. **API DOCUMENTATION**

### 4.1 **Base URL**
```
http://localhost:8000
```

### 4.2 **Main Endpoints**

#### 4.2.1 **Health Check**
```http
GET /health
```

**Response:**
```json
{
  "status": "healthy",
  "service": "fr-03.3-data-ingestion",
  "version": "1.0.0",
  "pipeline_status": "healthy",
  "active_jobs": 2,
  "timestamp": "2025-01-07T10:00:00Z"
}
```

#### 4.2.2 **Submit Job v·ªõi File Path**
```http
POST /api/v1/pipeline/submit
Content-Type: application/json

{
  "document_path": "/path/to/document.pdf",
  "metadata": {
    "title": "Document Title",
    "author": "Author Name",
    "department": "IT",
    "document_type": "manual"
  }
}
```

**Response:**
```json
{
  "job_id": "550e8400-e29b-41d4-a716-446655440000",
  "status": "submitted",
  "message": "Job submitted successfully"
}
```

#### 4.2.3 **Upload File v√† Process**
```http
POST /api/v1/pipeline/upload
Content-Type: multipart/form-data

file: <binary file data>
metadata: {"title": "Document", "author": "User"}
```

#### 4.2.4 **Get Job Status**
```http
GET /api/v1/pipeline/jobs/{job_id}
```

**Response:**
```json
{
  "job_id": "550e8400-e29b-41d4-a716-446655440000",
  "document_id": "doc_123",
  "status": "running",
  "current_stage": "embedding",
  "completed_stages": ["quality_control", "chunking"],
  "progress": {
    "total_chunks": 15,
    "processed_chunks": 10,
    "failed_chunks": 0,
    "success_rate": 0.67
  },
  "timing": {
    "created_at": "2025-01-07T10:00:00Z",
    "started_at": "2025-01-07T10:00:05Z",
    "duration_ms": 45000
  },
  "quality_score": 85,
  "error_message": null
}
```

#### 4.2.5 **Cancel Job**
```http
DELETE /api/v1/pipeline/jobs/{job_id}
```

#### 4.2.6 **Pipeline Statistics**
```http
GET /api/v1/pipeline/stats
```

**Response:**
```json
{
  "pipeline_status": "healthy",
  "active_jobs": {
    "total": 3,
    "running": 2,
    "pending": 1
  },
  "historical_stats": {
    "total_jobs": 150,
    "completed_jobs": 142,
    "failed_jobs": 8,
    "success_rate": 0.947
  },
  "performance": {
    "avg_processing_time_ms": 12500,
    "avg_chunk_success_rate": 0.98,
    "max_concurrent_jobs": 5
  },
  "stage_processors": {
    "quality_control": "healthy",
    "chunking": "healthy",
    "embedding": "healthy",
    "storage": "healthy",
    "indexing": "healthy"
  }
}
```

### 4.3 **Job Status Values**

| **Status** | **Description** |
|------------|-----------------|
| `pending` | Job ƒë√£ submit, ch·ªù x·ª≠ l√Ω |
| `running` | Job ƒëang ƒë∆∞·ª£c x·ª≠ l√Ω |
| `completed` | Job ho√†n th√†nh th√†nh c√¥ng |
| `failed` | Job th·∫•t b·∫°i |
| `cancelled` | Job b·ªã h·ªßy b·ªüi user |
| `retrying` | Job ƒëang retry sau khi failed |

### 4.4 **Pipeline Stages**

| **Stage** | **Description** | **Average Duration** |
|-----------|-----------------|----------------------|
| `quality_control` | ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng t√†i li·ªáu | 200-500ms |
| `chunking` | Ph√¢n ƒëo·∫°n vƒÉn b·∫£n | 500-2000ms |
| `embedding` | T·∫°o vector embeddings | 2000-10000ms |
| `storage` | L∆∞u tr·ªØ dual storage | 1000-3000ms |
| `indexing` | C·∫≠p nh·∫≠t index v√† cache | 100-500ms |

---

## 5. **DEPLOYMENT GUIDE**

### 5.1 **Prerequisites**

#### 5.1.1 **System Requirements**
- **OS**: Linux, macOS, ho·∫∑c Windows v·ªõi WSL2
- **Docker**: Version 20.0+
- **Docker Compose**: Version 2.0+
- **Memory**: Minimum 8GB RAM (16GB recommended)
- **Storage**: 20GB free space
- **CPU**: 4+ cores
- **GPU**: Optional (NVIDIA v·ªõi CUDA support)

#### 5.1.2 **Network Ports**
| **Service** | **Port** | **Description** |
|-------------|----------|-----------------|
| FR-03.3 Pipeline | 8000 | Main API endpoint |
| PostgreSQL | 5433 | Database connection |
| ChromaDB | 8001 | Vector database |
| Redis | 6380 | Cache service |
| Adminer | 8080 | Database admin UI |

### 5.2 **Quick Start Deployment**

#### 5.2.1 **Step 1: Clone v√† Setup**
```bash
# Clone repository
git clone <repository-url>
cd fr03_3_data_ingestion

# Create necessary directories
mkdir -p data/{input,processed,failed} logs models temp

# Set permissions
chmod 755 data logs models temp
```

#### 5.2.2 **Step 2: Configuration**
```bash
# Copy environment template
cp .env.example .env

# Edit configuration
nano .env
```

**Key Environment Variables:**
```bash
# Database Settings
POSTGRES_HOST=postgres-test
POSTGRES_PORT=5433
POSTGRES_DB=knowledge_base_test
POSTGRES_USER=kb_admin
POSTGRES_PASSWORD=test_password_123

# ChromaDB Settings
CHROMA_HOST=chromadb-test
CHROMA_PORT=8001
CHROMA_COLLECTION=knowledge_base_v1

# Redis Settings
REDIS_HOST=redis-test
REDIS_PORT=6380

# Embedding Settings
EMBEDDING_MODEL=Qwen/Qwen3-Embedding-0.6B
EMBEDDING_DIMENSION=1024
DEVICE=cpu  # ho·∫∑c cuda

# Processing Settings
MAX_WORKERS=4
BATCH_SIZE=8
RETRY_ATTEMPTS=3
```

#### 5.2.3 **Step 3: Deploy v·ªõi Docker Compose**
```bash
# Start all services
docker-compose up -d

# Check service status
docker-compose ps

# Monitor logs
docker-compose logs -f fr03-data-ingestion
```

#### 5.2.4 **Step 4: Verification**
```bash
# Wait for services to initialize
sleep 60

# Health check
curl http://localhost:8000/health

# Pipeline stats
curl http://localhost:8000/api/v1/pipeline/stats

# Test document upload
curl -X POST "http://localhost:8000/api/v1/pipeline/upload" \
  -F "file=@test_document.txt" \
  -F "metadata={\"title\":\"Test Document\"}"
```

### 5.3 **Production Deployment**

#### 5.3.1 **Security Hardening**
```bash
# Change default passwords
export POSTGRES_PASSWORD=$(openssl rand -base64 32)
export JWT_SECRET_KEY=$(openssl rand -base64 64)

# Enable HTTPS
# Configure reverse proxy (nginx/traefik)
# Set up SSL certificates
```

#### 5.3.2 **Performance Tuning**
```yaml
# docker-compose.prod.yml
services:
  fr03-data-ingestion:
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4'
        reservations:
          memory: 4G
          cpus: '2'
    environment:
      - MAX_WORKERS=8
      - BATCH_SIZE=16
```

#### 5.3.3 **Monitoring Setup**
```yaml
# Add Prometheus metrics
# Configure Grafana dashboards
# Set up alerting rules
# Log aggregation v·ªõi ELK stack
```

---

## 6. **TESTING & VALIDATION**

### 6.1 **Unit Tests**

```bash
# Run all tests
python -m pytest tests/ -v

# Run specific test modules
python tests/test_embedding_service.py
python tests/test_storage_service.py
python tests/test_pipeline_orchestrator.py
```

### 6.2 **Integration Tests**

```bash
# End-to-end pipeline test
python scripts/test_pipeline.py

# Load testing
python scripts/load_test.py --concurrent=5 --documents=50
```

### 6.3 **Performance Benchmarks**

| **Metric** | **Target** | **Actual** |
|------------|------------|------------|
| Document Processing Time | <10s | 2-8s |
| Chunking Speed | >50 chunks/s | ~100 chunks/s |
| Embedding Generation | >30 chunks/s | 50-200 chunks/s |
| Storage Latency | <100ms | 50-80ms |
| API Response Time | <200ms | 50-150ms |
| Concurrent Jobs | 5 jobs | ‚úÖ Supported |
| Memory Usage | <4GB | 2-4GB |

### 6.4 **Quality Assurance**

#### 6.4.1 **Test Scenarios**
- ‚úÖ Vietnamese document processing
- ‚úÖ Large file handling (>10MB)
- ‚úÖ Multiple concurrent uploads
- ‚úÖ Error recovery v√† retry
- ‚úÖ Network interruption handling
- ‚úÖ Database failover scenarios
- ‚úÖ Memory limit testing
- ‚úÖ API rate limiting

#### 6.4.2 **Test Data**
```
tests/data/
‚îú‚îÄ‚îÄ vietnamese_documents/
‚îÇ   ‚îú‚îÄ‚îÄ policy_doc.pdf
‚îÇ   ‚îú‚îÄ‚îÄ technical_manual.docx
‚îÇ   ‚îî‚îÄ‚îÄ meeting_notes.txt
‚îú‚îÄ‚îÄ large_files/
‚îÇ   ‚îî‚îÄ‚îÄ large_report.pdf (20MB)
‚îî‚îÄ‚îÄ edge_cases/
    ‚îú‚îÄ‚îÄ empty_file.txt
    ‚îú‚îÄ‚îÄ corrupted_doc.pdf
    ‚îî‚îÄ‚îÄ special_characters.txt
```

---

## 7. **PERFORMANCE & MONITORING**

### 7.1 **Key Performance Indicators (KPIs)**

#### 7.1.1 **Processing Metrics**
- **Throughput**: Documents processed per hour
- **Latency**: Average processing time per document
- **Success Rate**: Percentage of successfully processed documents
- **Error Rate**: Percentage of failed processing attempts
- **Queue Depth**: Number of pending jobs

#### 7.1.2 **Resource Metrics**
- **CPU Usage**: Average v√† peak CPU utilization
- **Memory Usage**: RAM consumption patterns
- **Disk I/O**: Read/write operations per second
- **Network I/O**: Data transfer rates
- **Database Performance**: Query response times

#### 7.1.3 **Business Metrics**
- **Document Quality Score**: Average quality assessment
- **Stage Success Rates**: Success rate per pipeline stage
- **User Satisfaction**: API response times
- **System Availability**: Uptime percentage

### 7.2 **Monitoring Dashboard**

#### 7.2.1 **Real-time Metrics**
```bash
# Get current pipeline stats
curl http://localhost:8000/api/v1/pipeline/stats

# Monitor active jobs
curl http://localhost:8000/api/v1/pipeline/jobs

# Health status
curl http://localhost:8000/health
```

#### 7.2.2 **Log Analysis**
```bash
# Application logs
docker-compose logs fr03-data-ingestion

# Database logs
docker-compose logs postgres-test

# Error patterns
grep "ERROR" logs/application.log | tail -20
```

### 7.3 **Alerting Rules**

| **Alert** | **Condition** | **Severity** |
|-----------|---------------|--------------|
| High Error Rate | Error rate > 10% trong 5 ph√∫t | Critical |
| High Memory Usage | Memory > 90% trong 10 ph√∫t | Warning |
| Long Processing Time | Processing > 60s | Warning |
| Database Connection | DB connection failed | Critical |
| Queue Overload | Pending jobs > 20 | Warning |
| Service Down | Health check failed | Critical |

### 7.4 **Performance Optimization**

#### 7.4.1 **Embedding Performance**
```python
# GPU optimization
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
BATCH_SIZE = 16 if DEVICE == "cuda" else 8

# Model caching
MODEL_CACHE_DIR = "./models"
EMBEDDING_TIMEOUT = 30
```

#### 7.4.2 **Database Optimization**
```sql
-- PostgreSQL indexes
CREATE INDEX idx_chunks_document_id ON document_chunks_enhanced(document_id);
CREATE INDEX idx_chunks_position ON document_chunks_enhanced(document_id, chunk_position);

-- ChromaDB collection settings
CHROMA_COLLECTION_SIZE = 100000
HNSW_SPACE = "cosine"
```

#### 7.4.3 **Caching Strategy**
```python
# Redis caching
CACHE_TTL = 3600  # 1 hour
CACHE_KEY_PREFIX = "fr03_pipeline:"

# Frequently accessed data
- Document metadata
- Quality scores
- Processing statistics
- Health check results
```

---

## 8. **TROUBLESHOOTING**

### 8.1 **Common Issues**

#### 8.1.1 **Service Won't Start**

**Symptoms:**
- Container fails to start
- Health check returns 503
- Connection refused errors

**Solutions:**
```bash
# Check service logs
docker-compose logs fr03-data-ingestion

# Verify database connections
docker-compose exec postgres-test pg_isready -U kb_admin

# Check ChromaDB status
curl http://localhost:8001/api/v1/heartbeat

# Restart services
docker-compose restart
```

#### 8.1.2 **Embedding Generation Fails**

**Symptoms:**
- Jobs stuck in "embedding" stage
- CUDA out of memory errors
- Model loading failures

**Solutions:**
```bash
# Check GPU availability
nvidia-smi

# Reduce batch size
export BATCH_SIZE=4

# Switch to CPU mode
export DEVICE=cpu

# Clear model cache
rm -rf models/*
```

#### 8.1.3 **Storage Consistency Issues**

**Symptoms:**
- Chunks in PostgreSQL but not in ChromaDB
- Inconsistent search results
- Storage rollback failures

**Solutions:**
```bash
# Check storage health
curl http://localhost:8000/api/v1/pipeline/health

# Verify database connections
docker-compose exec postgres-test psql -U kb_admin -d knowledge_base_test

# Check ChromaDB collections
curl http://localhost:8001/api/v1/collections
```

#### 8.1.4 **High Memory Usage**

**Symptoms:**
- Container OOM kills
- Slow processing
- System unresponsive

**Solutions:**
```bash
# Monitor memory usage
docker stats

# Reduce concurrent jobs
export MAX_WORKERS=2

# Tune garbage collection
export PYTHONMALLOC=malloc

# Increase container memory
# Edit docker-compose.yml memory limits
```

### 8.2 **Debugging Commands**

#### 8.2.1 **Service Health Checks**
```bash
# Overall health
curl http://localhost:8000/health

# Pipeline components
curl http://localhost:8000/api/v1/pipeline/health

# Database status
docker-compose exec postgres-test pg_isready

# ChromaDB status
curl http://localhost:8001/api/v1/heartbeat

# Redis status
docker-compose exec redis-test redis-cli ping
```

#### 8.2.2 **Log Analysis**
```bash
# Application logs
docker-compose logs -f fr03-data-ingestion

# Filter by log level
docker-compose logs fr03-data-ingestion | grep ERROR

# Database query logs
docker-compose logs postgres-test | grep "LOG:"

# Real-time monitoring
tail -f logs/application.log
```

#### 8.2.3 **Database Queries**
```sql
-- Check document processing status
SELECT document_id, status, created_at, updated_at 
FROM documents_metadata_v2 
ORDER BY created_at DESC LIMIT 10;

-- Check chunk distribution
SELECT document_id, COUNT(*) as chunk_count 
FROM document_chunks_enhanced 
GROUP BY document_id 
ORDER BY chunk_count DESC;

-- Check recent errors
SELECT * FROM processing_logs 
WHERE level = 'ERROR' 
ORDER BY created_at DESC LIMIT 20;
```

### 8.3 **Recovery Procedures**

#### 8.3.1 **Service Recovery**
```bash
# Graceful restart
docker-compose restart fr03-data-ingestion

# Full system restart
docker-compose down
docker-compose up -d

# Reset to clean state
docker-compose down -v
docker-compose up -d
```

#### 8.3.2 **Data Recovery**
```bash
# Database backup
docker-compose exec postgres-test pg_dump -U kb_admin knowledge_base_test > backup.sql

# ChromaDB backup
docker cp chromadb-test:/chroma/chroma ./chromadb_backup

# Restore from backup
docker-compose exec postgres-test psql -U kb_admin knowledge_base_test < backup.sql
```

### 8.4 **Performance Troubleshooting**

#### 8.4.1 **Slow Processing**
```bash
# Check system resources
docker stats
htop

# Monitor pipeline stages
curl http://localhost:8000/api/v1/pipeline/stats

# Profile specific jobs
curl http://localhost:8000/api/v1/pipeline/jobs/{job_id}
```

#### 8.4.2 **Memory Leaks**
```bash
# Monitor memory over time
while true; do
  docker stats --no-stream | grep fr03-data-ingestion
  sleep 30
done

# Python memory profiling
pip install memory-profiler
python -m memory_profiler scripts/profile_memory.py
```

---

## 9. **INTEGRATION POINTS**

### 9.1 **Upstream Dependencies**

#### 9.1.1 **FR-03.2 Quality Control Module**
**Status:** Mock implementation provided

**Integration Requirements:**
```python
# Expected API interface
POST /api/v1/quality/check
{
  "document_id": "string",
  "content": "string",
  "metadata": {...}
}

# Expected response
{
  "document_id": "string",
  "overall_score": 85,
  "passed": true,
  "details": {...},
  "recommendations": [...]
}
```

**Migration Steps:**
1. Update `src/services/mock_quality_control.py`
2. Change `MOCK_QUALITY_CONTROL=false` in config
3. Set `QUALITY_CONTROL_ENDPOINT` to actual service URL
4. Test integration v·ªõi real service

#### 9.1.2 **FR-01.2 Database Module**
**Status:** ‚úÖ Fully integrated

**Dependencies:**
- PostgreSQL database v·ªõi proper schema
- ChromaDB collection setup
- Redis cache service

**Connection Details:**
```yaml
PostgreSQL:
  host: postgres-test
  port: 5433
  database: knowledge_base_test
  
ChromaDB:
  host: chromadb-test
  port: 8001
  collection: knowledge_base_v1
  
Redis:
  host: redis-test
  port: 6380
```

### 9.2 **Downstream Consumers**

#### 9.2.1 **FR-04 RAG Core Engine**
**Status:** Ready for integration

**Provided Data:**
- Vector embeddings trong ChromaDB
- Chunk metadata trong PostgreSQL
- Document quality scores
- Processing timestamps

**Integration API:**
```python
# Vector search
from src.core.storage.dual_storage_manager import create_dual_storage_manager

storage = create_dual_storage_manager()
await storage.initialize()

# Search similar chunks
results = await storage.search_similar(
    embedding=query_embedding,
    limit=10,
    filters={"document_type": "manual"}
)

# Get document chunks
chunks = await storage.retrieve_chunks(document_id)
```

#### 9.2.2 **Analytics & Reporting**
**Status:** Data available

**Available Metrics:**
- Processing statistics via `/api/v1/pipeline/stats`
- Job history v√† performance data
- Quality scores distribution
- Error rates v√† patterns

### 9.3 **External Services**

#### 9.3.1 **Authentication Service**
**Status:** Not implemented (Future requirement)

**Required Integration:**
```python
# JWT token validation
from fastapi import Depends, HTTPException
from fastapi.security import HTTPBearer

async def verify_token(token: str = Depends(HTTPBearer())):
    # Validate JWT token
    # Check user permissions
    # Return user context
    pass
```

#### 9.3.2 **Notification Service**
**Status:** Not implemented (Future requirement)

**Use Cases:**
- Job completion notifications
- Error alerts
- System health notifications
- Performance reports

### 9.4 **Integration Testing**

#### 9.4.1 **End-to-End Test**
```python
# Test complete workflow
async def test_e2e_integration():
    # Submit document via API
    job_id = await submit_test_document()
    
    # Monitor processing
    await monitor_job_progress(job_id)
    
    # Verify results in storage
    await verify_storage_consistency()
    
    # Test RAG integration
    await test_
