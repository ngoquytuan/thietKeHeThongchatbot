N·∫øu ƒë√£ ƒë·ªß th√¥ng tin h√£y gi√∫p t√¥i vi·∫øt t√†i li·ªáu thi·∫øt k·∫ø module d∆∞·ªõi ƒë√¢y.
T√†i li·ªáu n√†y cung c·∫•p roadmap ƒë·∫ßy ƒë·ªß ƒë·ªÉ implement FR-04.1 m√† kh√¥ng c·∫ßn code c·ª• th·ªÉ nh∆∞ng 
c√≥ c√°c b∆∞·ªõc l√†m c·ª• th·ªÉ nh∆∞ chu·∫©n b·ªã m√¥i tr∆∞·ªùng PC th·∫ø n√†o, ph·∫ßn m·ªÅm g√¨?
 ƒë·ªÉ Team k·ªπ thu·∫≠t c√≥ th·ªÉ follow step-by-step guide n√†y ƒë·ªÉ thi·∫øt k·∫ø t·ªëi ∆∞u cho h·ªá th·ªëng chatbot
 v√† c√≥ mermaidchart lu·ªìng c√¥ng vi·ªác, ƒëi·ªÉm quay ƒë·∫ßu khi b·ªã fail.

### 3.4 Module RAG Core Engine (FR-04)

**FR-04.1 - Retrieval (Truy xu·∫•t)**
- Semantic search v·ªõi ƒë·ªô ch√≠nh x√°c t·ªëi thi·ªÉu 80%
- Hybrid search (k·∫øt h·ª£p semantic v√† keyword)
- Filtering theo access level c·ªßa user
- Tr·∫£ v·ªÅ top-K documents c√≥ li√™n quan (K configurable, default=5)
---
# T√ÄI LI·ªÜU THI·∫æT K·∫æ MODULE RAG CORE ENGINE
## FR-04.1 - RETRIEVAL (TRUY XU·∫§T) IMPLEMENTATION GUIDE

---

**Phi√™n b·∫£n:** 1.0  
**Ng√†y:** 01/09/2025  
**Module:** FR-04.1 - Retrieval (Truy xu·∫•t)  
**ƒê·ªëi t∆∞·ª£ng:** Team K·ªπ thu·∫≠t  

---

## 1. T·ªîNG QUAN MODULE

### 1.1 M·ª•c ti√™u
X√¢y d·ª±ng h·ªá th·ªëng truy xu·∫•t t√†i li·ªáu th√¥ng minh v·ªõi kh·∫£ nƒÉng:
- **Semantic Search**: ƒê·ªô ch√≠nh x√°c ‚â•80%
- **Hybrid Search**: K·∫øt h·ª£p semantic + keyword search
- **Access Control**: L·ªçc theo quy·ªÅn truy c·∫≠p ng∆∞·ªùi d√πng
- **Configurable Results**: Top-K documents (default K=5)

### 1.2 Ki·∫øn tr√∫c t·ªïng quan
```mermaid
graph LR
    Query[User Query] --> QueryProcessor[Query Processor]
    QueryProcessor --> SemanticEngine[Semantic Search Engine]
    QueryProcessor --> KeywordEngine[Keyword Search Engine]
    SemanticEngine --> HybridRanker[Hybrid Ranker]
    KeywordEngine --> HybridRanker
    HybridRanker --> AccessFilter[Access Control Filter]
    AccessFilter --> TopKSelector[Top-K Selector]
    TopKSelector --> Results[Final Results]
```

---

## 2. CHU·∫®N B·ªä M√îI TR∆Ø·ªúNG PH√ÅT TRI·ªÇN

### 2.1 Y√™u c·∫ßu h·ªá th·ªëng
```bash
# Hardware Requirements
CPU: ‚â•8 cores (Intel i7/AMD Ryzen 7 ho·∫∑c t∆∞∆°ng ƒë∆∞∆°ng)
RAM: ‚â•32GB (khuy·∫øn ngh·ªã 64GB cho embedding operations)
Storage: ‚â•500GB SSD NVMe
GPU: Optional - NVIDIA GTX 1080 ho·∫∑c cao h∆°n (ƒë·ªÉ ch·∫°y local embedding models)

# Operating System
Ubuntu 22.04 LTS / CentOS 8 / Windows 11 Pro
Docker Desktop 4.0+ v·ªõi WSL2 (n·∫øu d√πng Windows)
```

### 2.2 C√†i ƒë·∫∑t m√¥i tr∆∞·ªùng c∆° b·∫£n

#### **B∆∞·ªõc 1: C√†i ƒë·∫∑t Python v√† dependencies**
```bash
# C√†i ƒë·∫∑t Python 3.11
sudo apt update
sudo apt install python3.11 python3.11-venv python3.11-dev
python3.11 -m pip install --upgrade pip

# T·∫°o virtual environment
python3.11 -m venv rag_env
source rag_env/bin/activate

# C√†i ƒë·∫∑t core libraries
pip install -r requirements.txt
```

#### **requirements.txt**
```txt
# Core Libraries
fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.5.0
python-multipart==0.0.6

# Vector Search & Embeddings
chromadb==0.4.18
sentence-transformers==2.2.2
transformers==4.35.2
torch==2.1.1
faiss-cpu==1.7.4  # ho·∫∑c faiss-gpu n·∫øu c√≥ GPU

# Full-text Search
elasticsearch==8.11.0
whoosh==2.7.4  # lightweight alternative

# Database & Caching
asyncpg==0.29.0
redis==5.0.1
sqlalchemy[asyncio]==2.0.23

# Utilities
numpy==1.24.3
pandas==2.0.3
scikit-learn==1.3.2
nltk==3.8.1
spacy==3.7.2

# Testing & Development
pytest==7.4.3
pytest-asyncio==0.21.1
black==23.11.0
flake8==6.1.0
```

#### **B∆∞·ªõc 2: C√†i ƒë·∫∑t Docker v√† Database services**
```bash
# C√†i ƒë·∫∑t Docker
sudo apt install docker.io docker-compose
sudo usermod -aG docker $USER

# Kh·ªüi t·∫°o services b·∫±ng Docker Compose
# T·∫°o file docker-compose.yml (xem ph·∫ßn ph·ª• l·ª•c)
docker-compose up -d postgres redis elasticsearch
```

#### **B∆∞·ªõc 3: C√†i ƒë·∫∑t c√°c c√¥ng c·ª• ph√°t tri·ªÉn**
```bash
# VSCode v·ªõi Python extensions
# Ho·∫∑c PyCharm Professional

# Git v√† version control
git config --global user.name "Your Name"
git config --global user.email "your.email@company.com"

# Postman ho·∫∑c curl ƒë·ªÉ test APIs
# DBeaver ƒë·ªÉ qu·∫£n l√Ω database
```

---

## 3. IMPLEMENTATION ROADMAP

### 3.1 Workflow t·ªïng th·ªÉ

```mermaid
graph TD
    Start([üöÄ Start Implementation]) --> Setup[üì¶ Setup Environment]
    
    Setup --> Phase1[üî§ Phase 1: Text Preprocessing]
    Phase1 --> TestPhase1{‚úÖ Test Phase 1}
    TestPhase1 -->|‚ùå Fail| Debug1[üêõ Debug Text Processing]
    Debug1 --> Phase1
    TestPhase1 -->|‚úÖ Pass| Phase2[üî¢ Phase 2: Embedding Engine]
    
    Phase2 --> TestPhase2{‚úÖ Test Phase 2}
    TestPhase2 -->|‚ùå Fail| Debug2[üêõ Debug Embedding]
    Debug2 --> Phase2
    TestPhase2 -->|‚úÖ Pass| Phase3[üîç Phase 3: Vector Search]
    
    Phase3 --> TestPhase3{‚úÖ Test Phase 3}
    TestPhase3 -->|‚ùå Fail| Debug3[üêõ Debug Vector Search]
    Debug3 --> Phase3
    TestPhase3 -->|‚úÖ Pass| Phase4[üîé Phase 4: Keyword Search]
    
    Phase4 --> TestPhase4{‚úÖ Test Phase 4}
    TestPhase4 -->|‚ùå Fail| Debug4[üêõ Debug Keyword Search]
    Debug4 --> Phase4
    TestPhase4 -->|‚úÖ Pass| Phase5[‚öñÔ∏è Phase 5: Hybrid Ranking]
    
    Phase5 --> TestPhase5{‚úÖ Test Phase 5}
    TestPhase5 -->|‚ùå Fail| Debug5[üêõ Debug Hybrid Ranking]
    Debug5 --> Phase5
    TestPhase5 -->|‚úÖ Pass| Phase6[üõ°Ô∏è Phase 6: Access Control]
    
    Phase6 --> TestPhase6{‚úÖ Test Phase 6}
    TestPhase6 -->|‚ùå Fail| Debug6[üêõ Debug Access Control]
    Debug6 --> Phase6
    TestPhase6 -->|‚úÖ Pass| Phase7[üéØ Phase 7: API Integration]
    
    Phase7 --> TestPhase7{‚úÖ Test Phase 7}
    TestPhase7 -->|‚ùå Fail| Debug7[üêõ Debug API]
    Debug7 --> Phase7
    TestPhase7 -->|‚úÖ Pass| FinalTest[üèÅ End-to-End Testing]
    
    FinalTest --> Deploy([üöÄ Ready for Deployment])
    
    %% Styling
    classDef startEnd fill:#4caf50,stroke:#2e7d32,stroke-width:2px,color:#fff
    classDef phase fill:#2196f3,stroke:#1565c0,stroke-width:2px,color:#fff
    classDef test fill:#ff9800,stroke:#ef6c00,stroke-width:2px,color:#fff
    classDef debug fill:#f44336,stroke:#c62828,stroke-width:2px,color:#fff
    
    class Start,Deploy startEnd
    class Phase1,Phase2,Phase3,Phase4,Phase5,Phase6,Phase7 phase
    class TestPhase1,TestPhase2,TestPhase3,TestPhase4,TestPhase5,TestPhase6,TestPhase7,FinalTest test
    class Debug1,Debug2,Debug3,Debug4,Debug5,Debug6,Debug7 debug
```

---

## 4. CHI TI·∫æT TRI·ªÇN KHAI T·ª™NG PHASE

### **PHASE 1: üî§ TEXT PREPROCESSING ENGINE**

#### **M·ª•c ti√™u:** Chu·∫©n h√≥a v√† x·ª≠ l√Ω query ƒë·∫ßu v√†o
#### **Th·ªùi gian:** 2-3 ng√†y
#### **Output:** Query ƒë∆∞·ª£c l√†m s·∫°ch v√† t·ªëi ∆∞u

#### **B∆∞·ªõc 1.1: T·∫°o Text Preprocessor**
```python
# T·∫°o file: src/retrieval/text_processor.py
# Implement c√°c functions:
# - normalize_vietnamese_text()
# - remove_stopwords()
# - expand_abbreviations()  
# - handle_special_characters()
```

#### **B∆∞·ªõc 1.2: T·∫°o Query Enhancer**
```python
# T·∫°o file: src/retrieval/query_enhancer.py
# Implement:
# - query_expansion() # m·ªü r·ªông t·ª´ ƒë·ªìng nghƒ©a
# - intent_detection() # ph√°t hi·ªán √Ω ƒë·ªãnh c√¢u h·ªèi
# - context_enrichment() # l√†m phong ph√∫ ng·ªØ c·∫£nh
```

#### **B∆∞·ªõc 1.3: Unit Tests**
```bash
# T·∫°o test cases
pytest tests/test_text_processor.py -v
pytest tests/test_query_enhancer.py -v

# Test v·ªõi d·ªØ li·ªáu th·∫≠t
python scripts/test_vietnamese_processing.py
```

#### **üîÑ Checkpoint 1: Text Processing**
- [ ] Vietnamese text ƒë∆∞·ª£c normalize ch√≠nh x√°c
- [ ] Stopwords ƒë∆∞·ª£c lo·∫°i b·ªè ƒë√∫ng 
- [ ] Query expansion ho·∫°t ƒë·ªông v·ªõi t·ª´ ƒë·ªìng nghƒ©a
- [ ] Performance: <100ms cho 1 query

#### **‚ùå ƒêi·ªÉm quay ƒë·∫ßu n·∫øu fail:**
- **V·∫•n ƒë·ªÅ encoding**: Ki·ªÉm tra UTF-8, Unicode normalization
- **Vietnamese processing l·ªói**: S·ª≠ d·ª•ng th∆∞ vi·ªán `underthesea` ho·∫∑c `pyvi`
- **Performance ch·∫≠m**: Cache k·∫øt qu·∫£, optimize regex patterns

---

### **PHASE 2: üî¢ EMBEDDING ENGINE**

#### **M·ª•c ti√™u:** T·∫°o vector representations cho text
#### **Th·ªùi gian:** 3-4 ng√†y
#### **Output:** High-quality embeddings cho search

#### **B∆∞·ªõc 2.1: Model Selection v√† Setup**
```python
# T·∫°o file: src/retrieval/embedding_models.py
# Implement support cho multiple models:
# - sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
# - sentence-transformers/distiluse-base-multilingual-cased  
# - OpenAI text-embedding-ada-002 (via API)
# - Local Vietnamese models n·∫øu c√≥
```

#### **B∆∞·ªõc 2.2: Embedding Service**
```python
# T·∫°o file: src/retrieval/embedding_service.py
# Implement:
# - batch_encode() # x·ª≠ l√Ω nhi·ªÅu text c√πng l√∫c
# - cache_embeddings() # cache ƒë·ªÉ tr√°nh t√≠nh to√°n l·∫°i
# - model_comparison() # so s√°nh performance c√°c models
```

#### **B∆∞·ªõc 2.3: Vector Storage Interface**
```python
# T·∫°o file: src/retrieval/vector_store.py
# Implement adapters cho:
# - ChromaDB (recommend for development)
# - FAISS (for production performance)
# - Weaviate (if need advanced features)
```

#### **üîÑ Checkpoint 2: Embedding Engine**
- [ ] Multiple embedding models ho·∫°t ƒë·ªông
- [ ] Batch processing v·ªõi performance t·ªëi ∆∞u
- [ ] Vector storage v√† retrieval ch√≠nh x√°c
- [ ] Cache ho·∫°t ƒë·ªông ƒë√∫ng (hit rate >70%)

#### **‚ùå ƒêi·ªÉm quay ƒë·∫ßu n·∫øu fail:**
- **Model load l·ªói**: Check GPU/CPU compatibility, memory available
- **Embedding quality th·∫•p**: Test v·ªõi Vietnamese benchmark datasets
- **Performance issues**: Implement batch processing, use GPU if available
- **Storage connection fail**: Verify database connectivity, check ports

---

### **PHASE 3: üîç VECTOR SEARCH ENGINE**

#### **M·ª•c ti√™u:** Semantic search v·ªõi cosine similarity
#### **Th·ªùi gian:** 3-4 ng√†y  
#### **Output:** Top-K similar documents v·ªõi scores

#### **B∆∞·ªõc 3.1: Similarity Search**
```python
# T·∫°o file: src/retrieval/semantic_search.py
# Implement:
# - cosine_similarity_search()
# - approximate_search() # using FAISS/Annoy for speed
# - result_scoring() # normalize scores 0-1
```

#### **B∆∞·ªõc 3.2: Search Optimization**  
```python
# Implement trong semantic_search.py:
# - index_optimization() # tune FAISS parameters
# - query_preprocessing() # optimize query embeddings
# - result_postprocessing() # filter and rank results
```

#### **B∆∞·ªõc 3.3: Performance Benchmarking**
```python
# T·∫°o file: scripts/benchmark_semantic_search.py
# Test v·ªõi:
# - Different similarity thresholds (0.5, 0.6, 0.7, 0.8)
# - Various K values (5, 10, 20, 50)  
# - Different index configurations
```

#### **üîÑ Checkpoint 3: Vector Search**
- [ ] Semantic search accuracy ‚â•80% tr√™n test dataset
- [ ] Search response time <500ms cho 10K documents
- [ ] Similarity scores ƒë∆∞·ª£c normalize ch√≠nh x√°c
- [ ] Index ƒë∆∞·ª£c optimize cho performance

#### **‚ùå ƒêi·ªÉm quay ƒë·∫ßu n·∫øu fail:**
- **Low accuracy**: Ki·ªÉm tra embedding model, tune similarity threshold
- **Slow performance**: Optimize vector index, check hardware resources
- **Memory issues**: Implement streaming search, reduce batch sizes
- **Inconsistent results**: Debug vector normalization, check data quality

---

### **PHASE 4: üîé KEYWORD SEARCH ENGINE**

#### **M·ª•c ti√™u:** Full-text search v·ªõi BM25 scoring
#### **Th·ªùi gian:** 2-3 ng√†y
#### **Output:** Keyword-based document ranking

#### **B∆∞·ªõc 4.1: Full-text Indexing**
```python
# T·∫°o file: src/retrieval/keyword_search.py
# Implement:
# - build_inverted_index() # t·∫°o inverted index
# - bm25_scoring() # implement BM25 algorithm
# - vietnamese_tokenization() # tokenize ti·∫øng Vi·ªát
```

#### **B∆∞·ªõc 4.2: Search Engine Implementation**
```python
# Implement trong keyword_search.py:
# - exact_match_search() # t√¨m ki·∫øm ch√≠nh x√°c
# - fuzzy_search() # t√¨m ki·∫øm m·ªù v·ªõi edit distance
# - phrase_search() # t√¨m c·ª•m t·ª´
# - boolean_search() # AND, OR, NOT operators
```

#### **B∆∞·ªõc 4.3: Integration v·ªõi Elasticsearch (Optional)**
```python
# T·∫°o file: src/retrieval/elasticsearch_adapter.py
# Implement n·∫øu mu·ªën d√πng Elasticsearch:
# - index_documents()
# - search_documents() 
# - custom_analyzers() # cho ti·∫øng Vi·ªát
```

#### **üîÑ Checkpoint 4: Keyword Search**
- [ ] BM25 scoring ho·∫°t ƒë·ªông ch√≠nh x√°c
- [ ] Vietnamese tokenization t·ªët
- [ ] Support exact match v√† fuzzy search
- [ ] Performance <300ms cho keyword queries

#### **‚ùå ƒêi·ªÉm quay ƒë·∫ßu n·∫øu fail:**
- **Tokenization issues**: S·ª≠ d·ª•ng `underthesea` cho Vietnamese NLP
- **Poor BM25 scores**: Tune k1, b parameters, verify document frequencies
- **Memory consumption**: Optimize inverted index structure
- **Elasticsearch connection**: Check cluster health, verify indices

---

### **PHASE 5: ‚öñÔ∏è HYBRID RANKING ENGINE**

#### **M·ª•c ti√™u:** K·∫øt h·ª£p semantic + keyword search
#### **Th·ªùi gian:** 4-5 ng√†y
#### **Output:** Unified ranking system

#### **B∆∞·ªõc 5.1: Score Normalization**
```python
# T·∫°o file: src/retrieval/score_normalizer.py
# Implement:
# - normalize_semantic_scores() # 0-1 range
# - normalize_keyword_scores() # 0-1 range  
# - calibrate_score_ranges() # ƒë·∫£m b·∫£o fair comparison
```

#### **B∆∞·ªõc 5.2: Hybrid Ranking Algorithm**
```python
# T·∫°o file: src/retrieval/hybrid_ranker.py
# Implement multiple strategies:
# - weighted_combination() # Œ±*semantic + Œ≤*keyword
# - rank_fusion() # RRF (Reciprocal Rank Fusion)
# - machine_learning_ranker() # ML-based combination
```

#### **B∆∞·ªõc 5.3: Parameter Tuning**
```python
# T·∫°o file: scripts/tune_hybrid_parameters.py
# Implement:
# - grid_search_weights() # t√¨m Œ±, Œ≤ optimal
# - cross_validation() # validate tr√™n multiple datasets
# - a_b_testing() # test different strategies
```

#### **üîÑ Checkpoint 5: Hybrid Ranking**
- [ ] Hybrid search outperform individual methods
- [ ] Optimal weights ƒë∆∞·ª£c t√¨m th·∫•y qua tuning
- [ ] Consistent ranking across different query types
- [ ] Performance <800ms cho hybrid search

#### **‚ùå ƒêi·ªÉm quay ƒë·∫ßu n·∫øu fail:**
- **Poor hybrid performance**: Re-examine score normalization methods
- **Inconsistent results**: Debug score calibration, check data distributions
- **Parameter tuning slow**: Implement parallel grid search
- **Overfitting**: Use more diverse validation datasets

---

### **PHASE 6: üõ°Ô∏è ACCESS CONTROL FILTER**

#### **M·ª•c ti√™u:** Filter results theo user permissions
#### **Th·ªùi gian:** 3-4 ng√†y
#### **Output:** Permission-aware search results

#### **B∆∞·ªõc 6.1: Permission Service**
```python
# T·∫°o file: src/retrieval/permission_service.py
# Implement:
# - get_user_permissions() # l·∫•y quy·ªÅn t·ª´ database
# - check_document_access() # ki·ªÉm tra quy·ªÅn truy c·∫≠p doc
# - cache_permissions() # cache ƒë·ªÉ performance
```

#### **B∆∞·ªõc 6.2: Access Control Filter**
```python
# T·∫°o file: src/retrieval/access_filter.py
# Implement:
# - filter_by_access_level() # l·ªçc theo c·∫•p ƒë·ªô truy c·∫≠p
# - filter_by_department() # l·ªçc theo ph√≤ng ban
# - filter_by_tags() # l·ªçc theo tags/categories
# - audit_access_attempts() # log access attempts
```

#### **B∆∞·ªõc 6.3: Security Testing**
```python
# T·∫°o file: tests/test_security.py
# Test cases:
# - User kh√¥ng th·ªÉ truy c·∫≠p docs c·∫•p cao h∆°n
# - Department isolation ho·∫°t ƒë·ªông
# - Permission changes ƒë∆∞·ª£c reflect immediately
# - No information leakage qua search results
```

#### **üîÑ Checkpoint 6: Access Control**
- [ ] 100% accuracy trong permission filtering
- [ ] No data leakage ra unauthorized users
- [ ] Performance impact <100ms overhead
- [ ] Comprehensive audit logging

#### **‚ùå ƒêi·ªÉm quay ƒë·∫ßu n·∫øu fail:**
- **Permission leaks**: Review filter logic, add more test cases
- **Performance degradation**: Optimize permission caching
- **Database connection issues**: Check connection pooling
- **Audit logging fails**: Verify logging infrastructure

---

### **PHASE 7: üéØ API INTEGRATION**

#### **M·ª•c ti√™u:** Expose retrieval qua REST API
#### **Th·ªùi gian:** 3-4 ng√†y
#### **Output:** Production-ready API endpoints

#### **B∆∞·ªõc 7.1: FastAPI Application**
```python
# T·∫°o file: src/api/retrieval_api.py
# Implement endpoints:
# - POST /api/search/semantic
# - POST /api/search/keyword  
# - POST /api/search/hybrid
# - GET /api/search/config
```

#### **B∆∞·ªõc 7.2: Request/Response Models**
```python
# T·∫°o file: src/api/models.py
# Define Pydantic models:
# - SearchRequest
# - SearchResponse
# - DocumentResult
# - ErrorResponse
```

#### **B∆∞·ªõc 7.3: Error Handling & Validation**
```python
# Implement trong retrieval_api.py:
# - input_validation() # validate search parameters
# - exception_handling() # handle errors gracefully  
# - rate_limiting() # prevent API abuse
# - response_caching() # cache common queries
```

#### **üîÑ Checkpoint 7: API Integration**
- [ ] All endpoints ho·∫°t ƒë·ªông ƒë√∫ng
- [ ] Input validation comprehensive
- [ ] Error responses user-friendly
- [ ] API documentation complete (Swagger)

#### **‚ùå ƒêi·ªÉm quay ƒë·∫ßu n·∫øu fail:**
- **API errors**: Check request/response serialization
- **Validation issues**: Review Pydantic models
- **Performance problems**: Implement async processing
- **Documentation incomplete**: Use FastAPI auto-documentation

---

## 5. TESTING STRATEGY

### 5.1 Unit Testing Plan

```python
# Test Structure
tests/
‚îú‚îÄ‚îÄ unit/
‚îÇ   ‚îú‚îÄ‚îÄ test_text_processor.py
‚îÇ   ‚îú‚îÄ‚îÄ test_embedding_service.py  
‚îÇ   ‚îú‚îÄ‚îÄ test_semantic_search.py
‚îÇ   ‚îú‚îÄ‚îÄ test_keyword_search.py
‚îÇ   ‚îú‚îÄ‚îÄ test_hybrid_ranker.py
‚îÇ   ‚îú‚îÄ‚îÄ test_access_filter.py
‚îÇ   ‚îî‚îÄ‚îÄ test_api.py
‚îú‚îÄ‚îÄ integration/
‚îÇ   ‚îú‚îÄ‚îÄ test_end_to_end.py
‚îÇ   ‚îú‚îÄ‚îÄ test_database_integration.py
‚îÇ   ‚îî‚îÄ‚îÄ test_performance.py
‚îî‚îÄ‚îÄ fixtures/
    ‚îú‚îÄ‚îÄ sample_documents.json
    ‚îú‚îÄ‚îÄ test_queries.json
    ‚îî‚îÄ‚îÄ user_permissions.json
```

### 5.2 Test Cases ch√≠nh

#### **Functional Tests:**
```python
def test_semantic_search_accuracy():
    """Test semantic search achieves ‚â•80% accuracy"""
    # Load test dataset v·ªõi ground truth
    # Run semantic search
    # Calculate precision@K, recall@K
    assert accuracy >= 0.8

def test_hybrid_search_improvement():  
    """Test hybrid search better than individual methods"""
    # Compare hybrid vs semantic vs keyword
    assert hybrid_score > max(semantic_score, keyword_score)

def test_access_control_security():
    """Test no unauthorized access to documents"""
    # Test v·ªõi different user roles
    # Verify no leakage occurs
    assert no_unauthorized_docs_returned
```

#### **Performance Tests:**
```python
def test_search_response_time():
    """Test search responds within time limits"""
    # Test v·ªõi different query types v√† document counts
    assert response_time < 1.0  # seconds

def test_concurrent_users():
    """Test system handles multiple concurrent searches"""
    # Simulate 100 concurrent users
    assert all_requests_successful
```

### 5.3 Acceptance Criteria Checklist

- [ ] **Semantic search accuracy ‚â•80%** tr√™n test dataset
- [ ] **Hybrid search outperforms** individual methods
- [ ] **Access control 100% accurate** - no permission leaks
- [ ] **Top-K configurable** v·ªõi default K=5
- [ ] **Response time <1 second** cho 95% requests
- [ ] **API documentation complete** v·ªõi examples
- [ ] **Error handling graceful** v·ªõi meaningful messages
- [ ] **Security tested** v·ªõi penetration testing

---

## 6. DEPLOYMENT CONFIGURATION

### 6.1 Production Environment Setup

#### **Docker Configuration**
```dockerfile
# Dockerfile cho retrieval service
FROM python:3.11-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY src/ ./src/
COPY config/ ./config/

EXPOSE 8000
CMD ["uvicorn", "src.api.retrieval_api:app", "--host", "0.0.0.0", "--port", "8000"]
```

#### **Environment Variables**
```bash
# .env file
VECTOR_DB_URL=http://chromadb:8000
POSTGRES_URL=postgresql://user:pass@postgres:5432/knowledge_db
REDIS_URL=redis://redis:6379/0
ELASTICSEARCH_URL=http://elasticsearch:9200

# Model configurations
EMBEDDING_MODEL=sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
EMBEDDING_CACHE_SIZE=10000
SEARCH_DEFAULT_K=5
HYBRID_SEMANTIC_WEIGHT=0.7
HYBRID_KEYWORD_WEIGHT=0.3

# Performance settings
MAX_CONCURRENT_SEARCHES=50
CACHE_TTL_SECONDS=3600
```

### 6.2 Monitoring & Logging

#### **Metrics to Track:**
```python
# Key Performance Indicators
- Search accuracy (precision@5, recall@5)  
- Response time (p95, p99)
- Query volume per minute
- Cache hit rate
- Error rate
- User satisfaction scores
```

#### **Logging Configuration:**
```python
# Structured logging v·ªõi JSON format
{
    "timestamp": "2025-09-01T10:00:00Z",
    "level": "INFO", 
    "service": "retrieval-engine",
    "user_id": "emp001",
    "query": "quy tr√¨nh mua h√†ng",
    "search_type": "hybrid",
    "results_count": 5,
    "response_time_ms": 423,
    "accuracy_score": 0.87
}
```

---

## 7. TROUBLESHOOTING GUIDE

### 7.1 Common Issues & Solutions

| V·∫•n ƒë·ªÅ | Tri·ªáu ch·ª©ng | Gi·∫£i ph√°p |
|--------|-------------|-----------|
| **Low Search Accuracy** | Results kh√¥ng relevant | ‚Ä¢ Retrain embedding model<br/>‚Ä¢ Tune similarity thresholds<br/>‚Ä¢ Improve data quality |
| **Slow Response Time** | Timeout errors, user complaints | ‚Ä¢ Optimize vector indices<br/>‚Ä¢ Implement caching<br/>‚Ä¢ Scale horizontally |
| **Memory Issues** | OOM errors, crashes | ‚Ä¢ Reduce batch sizes<br/>‚Ä¢ Use streaming processing<br/>‚Ä¢ Add more RAM |
| **Permission Errors** | Users see restricted docs | ‚Ä¢ Review access control logic<br/>‚Ä¢ Clear permission cache<br/>‚Ä¢ Check database sync |

### 7.2 Debug Commands

```bash
# Check service health
curl http://localhost:8000/health

# Test search endpoint
curl -X POST http://localhost:8000/api/search/hybrid \
  -H "Content-Type: application/json" \
  -d '{"query": "quy tr√¨nh mua h√†ng", "user_id": "emp001", "k": 5}'

# Check vector database
curl http://chromadb:8000/api/v1/collections

# Monitor logs
docker logs -f retrieval-service

# Performance monitoring
python scripts/benchmark_search.py --queries=100 --concurrent=10
```

---

## 8. PERFORMANCE BENCHMARKS

### 8.1 Target Metrics

| Metric | Target | Measurement Method |
|--------|--------|--------------------|
| **Search Accuracy** | ‚â•80% | Precision@5 on test dataset |
| **Response Time** | <1 second | p95 latency |
| **Throughput** | 100 QPS | Concurrent users simulation |
| **Cache Hit Rate** | >70% | Redis metrics |
| **Memory Usage** | <8GB | Container monitoring |
| **CPU Utilization** | <80% | System monitoring |

### 8.2 Benchmark Scripts

```python
# scripts/benchmark_retrieval.py
import asyncio
import time
from concurrent.futures import ThreadPoolExecutor

async def benchmark_search_performance():
    """Benchmark search performance with different parameters"""
    # Test v·ªõi different query types
    # Measure response times
    # Generate performance report

def benchmark_accuracy():
    """Measure search accuracy against ground truth"""
    # Load test queries v·ªõi expected results
    # Run search engine
    # Calculate precision, recall, F1-score

def stress_test():
    """Test system under high load"""
    # Simulate high concurrent load
    # Monitor system resources
    # Identify breaking points
```

---

## 9. PH·ª§C L·ª§C

### 9.1 Docker Compose Configuration

```yaml
# docker-compose.yml
version: '3.8'

services:
  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: knowledge_db
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: secure_password
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data

  chromadb:
    image: chromadb/chroma:latest
    ports:
      - "8000:8000"
    volumes:
      - chromadb_data:/chroma/chroma
    environment:
      - CHROMA_SERVER_HOST=0.0.0.0
      - CHROMA_SERVER_PORT=8000

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
    ports:
      - "9200:9200"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data

volumes:
  postgres_data:
  redis_data:
  chromadb_data:
  elasticsearch_data:
```

### 9.2 Sample Test Data Structure

```json
{
  "test_documents": [
    {
      "doc_id": "proc_001",
      "title": "Quy tr√¨nh Mua h√†ng",
      "content": "Quy tr√¨nh mua h√†ng g·ªìm 5 b∆∞·ªõc ch√≠nh...",
      "access_level": "employee_only",
      "department": "procurement",
      "tags": ["procurement", "process", "approval"]
    }
  ],
  "test_queries": [
    {
      "query": "l√†m th·∫ø n√†o ƒë·ªÉ mua h√†ng",
      "expected_docs": ["proc_001", "proc_002"],
      "user_role": "employee"
    }
  ]
}
```

### 9.3 API Documentation Sample

```yaml
# OpenAPI specification
openapi: 3.0.0
info:
  title: Document Retrieval API
  version: 1.0.0

paths:
  /api/search/hybrid:
    post:
      summary: Hybrid search combining semantic and keyword search
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                query:
                  type: string
                  description: Search query
                user_id:
                  type: string
                  description: User identifier for access control
                k:
                  type: integer
                  default: 5
                  description: Number of results to return
      responses:
        200:
          description: Search results
          content:
            application/json:
              schema:
                type: object
                properties:
                  results:
                    type: array
                    items:
                      type: object
                  total_count:
                    type: integer
### 9.3 API Documentation Sample (ti·∫øp)
# OpenAPI specification (ti·∫øp)
                  response_time_ms:
                    type: number
                  search_metadata:
                    type: object
                    properties:
                      semantic_score:
                        type: number
                      keyword_score:
                        type: number
                      hybrid_score:
                        type: number
              example:
                results:
                  - doc_id: "proc_001"
                    title: "Quy tr√¨nh Mua h√†ng"
                    content_snippet: "Quy tr√¨nh mua h√†ng g·ªìm 5 b∆∞·ªõc ch√≠nh..."
                    relevance_score: 0.92
                    access_level: "employee_only"
                    source: "HR/Procedures/Procurement.pdf"
                total_count: 3
                response_time_ms: 245
                search_metadata:
                  semantic_score: 0.89
                  keyword_score: 0.76
                  hybrid_score: 0.92
        400:
          description: Invalid request parameters
        403:
          description: Access denied
        500:
          description: Internal server error
```

---

## 10. ADVANCED FEATURES & OPTIMIZATION

### 10.1 Advanced Search Features

#### **Query Understanding & Enhancement**
```python
# src/retrieval/advanced_query_processor.py

class AdvancedQueryProcessor:
    def __init__(self):
        self.intent_classifier = self._load_intent_model()
        self.entity_extractor = self._load_ner_model()
    
    async def enhance_query(self, query: str, user_context: dict) -> dict:
        """
        Enhance query v·ªõi advanced NLP techniques
        """
        # 1. Intent Detection
        intent = self.detect_intent(query)
        
        # 2. Named Entity Recognition
        entities = self.extract_entities(query)
        
        # 3. Query Expansion v·ªõi domain knowledge
        expanded_terms = self.expand_domain_terms(query, entities)
        
        # 4. Contextual Enhancement
        context_terms = self.add_contextual_terms(user_context)
        
        return {
            "original_query": query,
            "enhanced_query": self._build_enhanced_query(
                query, expanded_terms, context_terms
            ),
            "intent": intent,
            "entities": entities,
            "confidence": self._calculate_confidence()
        }
```

#### **Multi-Modal Search Support**
```python
# src/retrieval/multimodal_search.py

class MultiModalSearchEngine:
    """
    Support t√¨m ki·∫øm v·ªõi images, tables, charts trong documents
    """
    def __init__(self):
        self.image_encoder = self._load_image_encoder()
        self.table_parser = self._load_table_parser()
    
    async def search_with_image(self, image_query: bytes, text_query: str):
        """T√¨m ki·∫øm documents c√≥ images t∆∞∆°ng t·ª±"""
        # Extract features t·ª´ image
        # Combine v·ªõi text search
        # Return multi-modal results
        pass
    
    async def search_tables(self, table_query: str):
        """T√¨m ki·∫øm trong table content"""
        # Parse table structures
        # Search table cells
        # Return structured results
        pass
```

### 10.2 Performance Optimization Techniques

#### **Intelligent Caching Strategy**
```python
# src/retrieval/intelligent_cache.py

class IntelligentCache:
    """
    Multi-level caching v·ªõi predictive pre-loading
    """
    def __init__(self):
        self.l1_cache = {}  # In-memory hot cache
        self.l2_cache = RedisCache()  # Redis distributed cache
        self.l3_cache = DatabaseCache()  # Database cache
        
    async def get_with_prediction(self, query: str, user_id: str):
        """
        Get cached results v√† predict next queries
        """
        # Check L1 -> L2 -> L3 cache
        result = await self._check_cache_levels(query)
        
        if result:
            # Predict v√† pre-load related queries
            await self._predictive_preload(query, user_id)
            return result
            
        # Cache miss - execute search v√† cache result
        result = await self._execute_search_and_cache(query)
        return result
```

#### **Dynamic Index Optimization**
```python
# src/retrieval/dynamic_optimizer.py

class DynamicIndexOptimizer:
    """
    T·ª± ƒë·ªông optimize indices based on query patterns
    """
    def __init__(self):
        self.query_analyzer = QueryPatternAnalyzer()
        self.index_manager = IndexManager()
    
    async def optimize_based_on_usage(self):
        """
        Analyze query patterns v√† optimize indices accordingly
        """
        # Analyze recent query patterns
        patterns = await self.query_analyzer.analyze_patterns()
        
        # Identify optimization opportunities
        optimizations = self._identify_optimizations(patterns)
        
        # Apply optimizations
        for opt in optimizations:
            await self.index_manager.apply_optimization(opt)
```

### 10.3 Quality Assurance & Monitoring

#### **Real-time Quality Monitoring**
```python
# src/monitoring/quality_monitor.py

class SearchQualityMonitor:
    """
    Monitor search quality in real-time
    """
    def __init__(self):
        self.metrics_collector = MetricsCollector()
        self.alert_manager = AlertManager()
    
    async def monitor_search_quality(self, query: str, results: list, user_feedback: dict):
        """
        Monitor v√† alert n·∫øu quality drops
        """
        # Calculate quality metrics
        relevance_score = self._calculate_relevance(query, results)
        user_satisfaction = self._parse_feedback(user_feedback)
        
        # Track metrics over time
        await self.metrics_collector.record_metrics({
            "relevance_score": relevance_score,
            "user_satisfaction": user_satisfaction,
            "timestamp": datetime.now()
        })
        
        # Alert n·∫øu quality drops below threshold
        if relevance_score < 0.7:
            await self.alert_manager.send_alert(
                "Search quality degradation detected",
                {"query": query, "score": relevance_score}
            )
```

#### **A/B Testing Framework**
```python
# src/experimentation/ab_testing.py

class SearchABTesting:
    """
    A/B test different search algorithms
    """
    def __init__(self):
        self.experiment_manager = ExperimentManager()
        self.results_analyzer = ResultsAnalyzer()
    
    async def run_search_experiment(self, user_id: str, query: str):
        """
        Route user to different search variants
        """
        # Determine which variant to show user
        variant = await self.experiment_manager.get_variant(user_id)
        
        # Execute search v·ªõi selected variant
        if variant == "semantic_heavy":
            results = await self._semantic_heavy_search(query)
        elif variant == "keyword_heavy":
            results = await self._keyword_heavy_search(query)
        else:  # control
            results = await self._standard_hybrid_search(query)
        
        # Log experiment results
        await self.results_analyzer.log_experiment_result(
            user_id, query, variant, results
        )
        
        return results
```

---

## 11. SECURITY CONSIDERATIONS

### 11.1 Advanced Security Features

#### **Query Sanitization & Injection Prevention**
```python
# src/security/query_sanitizer.py

class QuerySanitizer:
    """
    Prevent injection attacks v√† malicious queries
    """
    def __init__(self):
        self.malicious_patterns = self._load_malicious_patterns()
        self.rate_limiter = RateLimiter()
    
    async def sanitize_query(self, query: str, user_id: str) -> str:
        """
        Sanitize user query before processing
        """
        # Check rate limiting
        if not await self.rate_limiter.check_limit(user_id):
            raise RateLimitExceeded("Too many queries per minute")
        
        # Remove potentially malicious patterns
        sanitized = self._remove_malicious_patterns(query)
        
        # Length validation
        if len(sanitized) > 1000:
            raise QueryTooLong("Query exceeds maximum length")
        
        # Content validation
        if self._contains_sensitive_terms(sanitized):
            await self._log_suspicious_query(user_id, query)
            
        return sanitized
```

#### **Access Control v·ªõi Attribute-Based Control**
```python
# src/security/abac_controller.py

class ABACController:
    """
    Attribute-Based Access Control for fine-grained permissions
    """
    def __init__(self):
        self.policy_engine = PolicyEngine()
        self.attribute_manager = AttributeManager()
    
    async def check_document_access(self, user_attributes: dict, 
                                  document_attributes: dict) -> bool:
        """
        Check access based on multiple attributes
        """
        # Get applicable policies
        policies = await self.policy_engine.get_policies(
            user_attributes, document_attributes
        )
        
        # Evaluate each policy
        for policy in policies:
            result = await self._evaluate_policy(
                policy, user_attributes, document_attributes
            )
            if not result.allowed:
                await self._log_access_denied(user_attributes, document_attributes)
                return False
                
        return True
    
    async def _evaluate_policy(self, policy, user_attrs, doc_attrs):
        """
        Evaluate policy rules
        Example: user.department == document.owner_department AND 
                user.clearance_level >= document.classification_level
        """
        return await self.policy_engine.evaluate(policy, user_attrs, doc_attrs)
```

### 11.2 Audit & Compliance

#### **Comprehensive Audit Logging**
```python
# src/audit/audit_logger.py

class AuditLogger:
    """
    Log all search activities for compliance
    """
    def __init__(self):
        self.secure_logger = SecureLogger()
        self.compliance_checker = ComplianceChecker()
    
    async def log_search_activity(self, activity: SearchActivity):
        """
        Log search activity v·ªõi tamper-proof logging
        """
        audit_entry = {
            "timestamp": datetime.utcnow().isoformat(),
            "user_id": activity.user_id,
            "user_ip": activity.ip_address,
            "query": self._hash_sensitive_content(activity.query),
            "results_count": len(activity.results),
            "accessed_documents": [doc.doc_id for doc in activity.results],
            "access_granted": activity.access_granted,
            "session_id": activity.session_id,
            "user_agent": activity.user_agent
        }
        
        # Add digital signature
        audit_entry["signature"] = self._sign_entry(audit_entry)
        
        # Store in secure audit database
        await self.secure_logger.log_entry(audit_entry)
        
        # Check compliance requirements
        await self.compliance_checker.validate_activity(audit_entry)
```

---

## 12. SCALING & PRODUCTION READINESS

### 12.1 Horizontal Scaling Strategy

#### **Microservices Architecture**
```python
# src/services/search_coordinator.py

class SearchCoordinator:
    """
    Coordinate multiple search service instances
    """
    def __init__(self):
        self.service_registry = ServiceRegistry()
        self.load_balancer = SearchLoadBalancer()
        self.circuit_breaker = CircuitBreaker()
    
    async def distribute_search(self, search_request: SearchRequest):
        """
        Distribute search across multiple instances
        """
        # Select optimal service instance
        service_instance = await self.load_balancer.select_instance(
            search_request.complexity
        )
        
        try:
            # Execute search v·ªõi circuit breaker protection
            result = await self.circuit_breaker.execute(
                service_instance.search, search_request
            )
            return result
            
        except ServiceUnavailable:
            # Fallback to alternative instance
            fallback_instance = await self.service_registry.get_fallback()
            return await fallback_instance.search(search_request)
```

#### **Database Sharding Strategy**
```python
# src/storage/sharding_manager.py

class ShardingManager:
    """
    Manage document sharding across multiple databases
    """
    def __init__(self):
        self.shard_router = ShardRouter()
        self.consistency_manager = ConsistencyManager()
    
    async def route_search_query(self, query: str, user_context: dict):
        """
        Route query to appropriate shards
        """
        # Determine which shards to search
        relevant_shards = await self.shard_router.get_relevant_shards(
            query, user_context
        )
        
        # Execute parallel searches
        search_tasks = [
            self._search_shard(shard, query, user_context)
            for shard in relevant_shards
        ]
        
        shard_results = await asyncio.gather(*search_tasks)
        
        # Merge v√† rank results
        merged_results = await self._merge_shard_results(shard_results)
        
        return merged_results
```

### 12.2 Performance Optimization for Production

#### **Advanced Caching Strategies**
```python
# src/caching/multilevel_cache.py

class MultiLevelCache:
    """
    Sophisticated multi-level caching system
    """
    def __init__(self):
        self.memory_cache = LRUCache(maxsize=10000)  # L1: In-memory
        self.redis_cache = RedisCluster()            # L2: Distributed
        self.db_cache = DatabaseCache()              # L3: Persistent
        self.cdn_cache = CDNCache()                  # L4: Edge caching
    
    async def get_cached_result(self, cache_key: str):
        """
        Check caches in order of speed
        """
        # L1: Memory cache (fastest)
        if cache_key in self.memory_cache:
            return self.memory_cache[cache_key]
        
        # L2: Redis cache
        result = await self.redis_cache.get(cache_key)
        if result:
            # Promote to L1
            self.memory_cache[cache_key] = result
            return result
        
        # L3: Database cache
        result = await self.db_cache.get(cache_key)
        if result:
            # Promote to L2 and L1
            await self.redis_cache.set(cache_key, result)
            self.memory_cache[cache_key] = result
            return result
        
        # L4: CDN cache (for static content)
        return await self.cdn_cache.get(cache_key)
```

#### **Resource Management & Auto-scaling**
```python
# src/scaling/auto_scaler.py

class AutoScaler:
    """
    Automatically scale search services based on load
    """
    def __init__(self):
        self.metrics_monitor = MetricsMonitor()
        self.kubernetes_client = KubernetesClient()
        self.scaling_policy = ScalingPolicy()
    
    async def monitor_and_scale(self):
        """
        Monitor metrics v√† auto-scale services
        """
        while True:
            # Collect current metrics
            metrics = await self.metrics_monitor.get_current_metrics()
            
            # Make scaling decision
            scaling_decision = await self.scaling_policy.evaluate(metrics)
            
            if scaling_decision.scale_up:
                await self.kubernetes_client.scale_up(
                    service="retrieval-service",
                    replicas=scaling_decision.target_replicas
                )
                
            elif scaling_decision.scale_down:
                await self.kubernetes_client.scale_down(
                    service="retrieval-service", 
                    replicas=scaling_decision.target_replicas
                )
            
            # Wait before next evaluation
            await asyncio.sleep(60)  # Check every minute
```

---

## 13. MAINTENANCE & OPERATIONS

### 13.1 Health Monitoring & Alerting

#### **Comprehensive Health Checks**
```python
# src/health/health_checker.py

class HealthChecker:
    """
    Comprehensive system health monitoring
    """
    def __init__(self):
        self.component_checkers = {
            "vector_db": VectorDBHealthChecker(),
            "postgres": PostgreSQLHealthChecker(),
            "redis": RedisHealthChecker(),
            "elasticsearch": ElasticsearchHealthChecker(),
            "embedding_service": EmbeddingServiceHealthChecker()
        }
        
    async def check_system_health(self) -> HealthReport:
        """
        Check health c·ªßa t·∫•t c·∫£ components
        """
        health_results = {}
        
        for component_name, checker in self.component_checkers.items():
            try:
                health_status = await checker.check_health()
                health_results[component_name] = health_status
                
            except Exception as e:
                health_results[component_name] = HealthStatus(
                    status="unhealthy",
                    error=str(e),
                    timestamp=datetime.utcnow()
                )
        
        # Generate overall health report
        return HealthReport(
            overall_status=self._calculate_overall_status(health_results),
            component_status=health_results,
            recommendations=self._generate_recommendations(health_results)
        )
```

#### **Alerting & Incident Response**
```python
# src/monitoring/alert_manager.py

class AlertManager:
    """
    Manage alerts v√† incident response
    """
    def __init__(self):
        self.notification_channels = {
            "email": EmailNotifier(),
            "slack": SlackNotifier(), 
            "pagerduty": PagerDutyNotifier()
        }
        self.escalation_rules = EscalationRules()
    
    async def handle_alert(self, alert: Alert):
        """
        Handle alert v·ªõi appropriate escalation
        """
        # Determine severity
        severity = self._assess_severity(alert)
        
        # Get escalation rules for this severity
        escalation = await self.escalation_rules.get_escalation(severity)
        
        # Send notifications
        for level in escalation.levels:
            await self._send_notifications(alert, level)
            
            # Wait for acknowledgment
            ack_received = await self._wait_for_acknowledgment(
                alert, level.timeout
            )
            
            if ack_received:
                break  # Stop escalation
```

### 13.2 Backup & Disaster Recovery

#### **Automated Backup Strategy**
```python
# src/backup/backup_manager.py

class BackupManager:
    """
    Manage automated backups c·ªßa all critical data
    """
    def __init__(self):
        self.backup_strategies = {
            "vector_db": VectorDBBackup(),
            "postgres": PostgreSQLBackup(),
            "redis": RedisBackup(),
            "file_storage": FileStorageBackup()
        }
        
    async def execute_backup_cycle(self):
        """
        Execute full backup cycle
        """
        backup_session = BackupSession(
            session_id=generate_uuid(),
            timestamp=datetime.utcnow()
        )
        
        backup_results = {}
        
        for component, strategy in self.backup_strategies.items():
            try:
                # Execute backup
                backup_result = await strategy.backup(backup_session)
                backup_results[component] = backup_result
                
                # Verify backup integrity  
                verification = await strategy.verify_backup(backup_result)
                backup_result.verified = verification.success
                
            except Exception as e:
                backup_results[component] = BackupResult(
                    status="failed",
                    error=str(e)
                )
        
        # Generate backup report
        await self._generate_backup_report(backup_session, backup_results)
```

---

## 14. FINAL CHECKLIST & DEPLOYMENT

### 14.1 Pre-Deployment Checklist

#### **‚úÖ Development Completed**
- [ ] All 7 phases implemented v√† tested
- [ ] Unit tests pass v·ªõi >80% coverage
- [ ] Integration tests pass
- [ ] Performance benchmarks meet targets
- [ ] Security tests pass
- [ ] API documentation complete

#### **‚úÖ Quality Assurance**
- [ ] Search accuracy ‚â•80% verified
- [ ] Response time <1s for 95% of queries
- [ ] Concurrent user testing completed (100 users)
- [ ] Memory v√† CPU usage within limits
- [ ] No data leakage in access control tests
- [ ] Error handling covers all edge cases

#### **‚úÖ Security & Compliance**
- [ ] Penetration testing completed
- [ ] Audit logging implemented
- [ ] Access control thoroughly tested
- [ ] Data encryption verified
- [ ] Compliance requirements met

#### **‚úÖ Operations Readiness**
- [ ] Monitoring dashboards configured
- [ ] Alerting rules defined
- [ ] Backup procedures tested
- [ ] Disaster recovery plan validated
- [ ] Runbooks created for common issues
- [ ] On-call procedures established

### 14.2 Deployment Script

```bash
#!/bin/bash
# deploy_retrieval_service.sh

set -e  # Exit on any error

echo "üöÄ Starting deployment of Retrieval Service..."

# Step 1: Pre-deployment checks
echo "üîç Running pre-deployment checks..."
python scripts/pre_deployment_check.py

# Step 2: Database migrations
echo "üìä Running database migrations..."
python scripts/migrate_database.py

# Step 3: Build v√† push Docker image
echo "üê≥ Building Docker image..."
docker build -t retrieval-service:$(git rev-parse --short HEAD) .
docker tag retrieval-service:$(git rev-parse --short HEAD) retrieval-service:latest

# Step 4: Deploy to Kubernetes
echo "‚ò∏Ô∏è Deploying to Kubernetes..."
kubectl apply -f k8s/namespace.yaml
kubectl apply -f k8s/configmap.yaml
kubectl apply -f k8s/secret.yaml
kubectl apply -f k8s/deployment.yaml
kubectl apply -f k8s/service.yaml
kubectl apply -f k8s/ingress.yaml

# Step 5: Wait for deployment
echo "‚è≥ Waiting for deployment to be ready..."
kubectl wait --for=condition=available --timeout=300s deployment/retrieval-service

# Step 6: Health check
echo "‚ù§Ô∏è Running health checks..."
python scripts/post_deployment_health_check.py

# Step 7: Smoke tests
echo "üß™ Running smoke tests..."
python scripts/smoke_tests.py

echo "‚úÖ Deployment completed successfully!"
echo "üìä Monitor at: http://monitoring.company.com/retrieval-service"
```

### 14.3 Go-Live Plan

#### **Phase 1: Soft Launch (1 tu·∫ßn)**
- Deploy to production environment
- Enable access cho 10% users (beta testers)
- Monitor performance metrics closely
- Collect user feedback
- Fix any critical issues

#### **Phase 2: Gradual Rollout (2 tu·∫ßn)**
- Increase user access to 50%
- Monitor system stability
- Optimize performance based on real usage
- Train support team
- Prepare documentation

#### **Phase 3: Full Production (1 tu·∫ßn)**
- Enable access cho all users (100)
- Full monitoring v√† alerting active
- Support team ready
- Performance optimization ongoing
- User training sessions

### 14.4 Success Metrics

#### **Technical KPIs:**
```python
success_metrics = {
    "search_accuracy": {"target": "‚â•80%", "measurement": "precision@5"},
    "response_time": {"target": "<1s", "measurement": "p95 latency"},
    "availability": {"target": "99.5%", "measurement": "uptime percentage"},
    "error_rate": {"target": "<1%", "measurement": "error/total requests"},
    "concurrent_users": {"target": "100", "measurement": "simultaneous users"},
    "memory_usage": {"target": "<8GB", "measurement": "peak memory"},
    "cpu_utilization": {"target": "<80%", "measurement": "average CPU"}
}
```

#### **Business KPIs:**
```python
business_metrics = {
    "user_adoption": {"target": "80%", "measurement": "active users/total users"},
    "query_volume": {"target": "500/day", "measurement": "queries per day"},
    "user_satisfaction": {"target": "4.0/5", "measurement": "survey score"},
    "time_saved": {"target": "30min/user/day", "measurement": "before vs after"},
    "support_tickets": {"target": "-50%", "measurement": "reduction in tickets"}
}
```

---

## 15. POST-DEPLOYMENT SUPPORT

### 15.1 Monitoring Dashboard

```python
# Dashboard metrics to track
dashboard_widgets = {
    "search_performance": {
        "metrics": ["avg_response_time", "p95_response_time", "throughput"],
        "alerts": ["response_time > 1s", "throughput < 50 QPS"]
    },
    "search_quality": {
        "metrics": ["accuracy_score", "user_satisfaction", "result_relevance"], 
        "alerts": ["accuracy < 80%", "satisfaction < 4.0"]
    },
    "system_health": {
        "metrics": ["cpu_usage", "memory_usage", "disk_usage", "error_rate"],
        "alerts": ["cpu > 80%", "memory > 8GB", "error_rate > 1%"]
    },
    "user_activity": {
        "metrics": ["active_users", "query_volume", "peak_concurrent_users"],
        "alerts": ["no_activity_1h", "concurrent_users > 100"]
    }
}
```

### 15.2 Support Runbooks

#### **Common Issues & Solutions:**

**üî¥ HIGH SEVERITY:**
```markdown
## Search Service Down
**Symptoms:** All search requests return 500 errors
**Impact:** Complete service outage
**Resolution:**
1. Check service pods: `kubectl get pods -n retrieval`
2. Check logs: `kubectl logs deployment/retrieval-service`
3. Restart service: `kubectl rollout restart deployment/retrieval-service`
4. If database issue: Check vector DB and PostgreSQL connectivity
**Escalation:** Page on-call engineer if not resolved in 10 minutes
```

**üü° MEDIUM SEVERITY:**
```markdown
## Slow Search Performance
**Symptoms:** Response time > 2 seconds
**Impact:** Poor user experience
**Resolution:**
1. Check system resources: CPU, Memory, Disk I/O
2. Check database performance metrics
3. Clear caches if corrupted: `redis-cli FLUSHALL`
4. Scale up pods if needed: `kubectl scale deployment retrieval-service --replicas=5`
**Escalation:** Notify team lead if performance doesn't improve in 30 minutes
```

### 15.3 Continuous Improvement Plan

#### **Monthly Reviews:**
- Performance metrics analysis
- User feedback review
- Search accuracy assessment
- System optimization opportunities
- Technology updates evaluation

#### **Quarterly Upgrades:**
- Embedding model updates
- Search algorithm improvements
- Infrastructure scaling adjustments
- Security updates v√† patches
- Feature enhancements based on user requests

---

**üéâ CONGRATULATIONS!**

B·∫°n ƒë√£ c√≥ m·ªôt **roadmap ho√†n ch·ªânh** ƒë·ªÉ implement FR-04.1 Retrieval module. Document n√†y cung c·∫•p:

‚úÖ **Step-by-step implementation guide**  
‚úÖ **Detailed technical specifications**  
‚úÖ **Comprehensive testing strategy**  
‚úÖ **Production deployment procedures**  
‚úÖ **Monitoring v√† maintenance plans**  
‚úÖ **Troubleshooting guides**  

Team k·ªπ thu·∫≠t c√≥ th·ªÉ follow guide n√†y ƒë·ªÉ x√¢y d·ª±ng m·ªôt **search engine ch·∫•t l∆∞·ª£ng cao** cho h·ªá th·ªëng chatbot n·ªôi b·ªô!
