Nếu đã đủ thông tin hãy giúp tôi viết tài liệu thiết kế module dưới đây.
Tài liệu này cung cấp roadmap đầy đủ để implement FR-04.1 mà không cần code cụ thể nhưng 
có các bước làm cụ thể như chuẩn bị môi trường PC thế nào, phần mềm gì?
 để Team kỹ thuật có thể follow step-by-step guide này để thiết kế tối ưu cho hệ thống chatbot
 và có mermaidchart luồng công việc, điểm quay đầu khi bị fail.

### 3.4 Module RAG Core Engine (FR-04)

**FR-04.1 - Retrieval (Truy xuất)**
- Semantic search với độ chính xác tối thiểu 80%
- Hybrid search (kết hợp semantic và keyword)
- Filtering theo access level của user
- Trả về top-K documents có liên quan (K configurable, default=5)
---
# TÀI LIỆU THIẾT KẾ MODULE RAG CORE ENGINE
## FR-04.1 - RETRIEVAL (TRUY XUẤT) IMPLEMENTATION GUIDE

---

**Phiên bản:** 1.0  
**Ngày:** 01/09/2025  
**Module:** FR-04.1 - Retrieval (Truy xuất)  
**Đối tượng:** Team Kỹ thuật  

---

## 1. TỔNG QUAN MODULE

### 1.1 Mục tiêu
Xây dựng hệ thống truy xuất tài liệu thông minh với khả năng:
- **Semantic Search**: Độ chính xác ≥80%
- **Hybrid Search**: Kết hợp semantic + keyword search
- **Access Control**: Lọc theo quyền truy cập người dùng
- **Configurable Results**: Top-K documents (default K=5)

### 1.2 Kiến trúc tổng quan
```mermaid
graph LR
    Query[User Query] --> QueryProcessor[Query Processor]
    QueryProcessor --> SemanticEngine[Semantic Search Engine]
    QueryProcessor --> KeywordEngine[Keyword Search Engine]
    SemanticEngine --> HybridRanker[Hybrid Ranker]
    KeywordEngine --> HybridRanker
    HybridRanker --> AccessFilter[Access Control Filter]
    AccessFilter --> TopKSelector[Top-K Selector]
    TopKSelector --> Results[Final Results]
```

---

## 2. CHUẨN BỊ MÔI TRƯỜNG PHÁT TRIỂN

### 2.1 Yêu cầu hệ thống
```bash
# Hardware Requirements
CPU: ≥8 cores (Intel i7/AMD Ryzen 7 hoặc tương đương)
RAM: ≥32GB (khuyến nghị 64GB cho embedding operations)
Storage: ≥500GB SSD NVMe
GPU: Optional - NVIDIA GTX 1080 hoặc cao hơn (để chạy local embedding models)

# Operating System
Ubuntu 22.04 LTS / CentOS 8 / Windows 11 Pro
Docker Desktop 4.0+ với WSL2 (nếu dùng Windows)
```

### 2.2 Cài đặt môi trường cơ bản

#### **Bước 1: Cài đặt Python và dependencies**
```bash
# Cài đặt Python 3.11
sudo apt update
sudo apt install python3.11 python3.11-venv python3.11-dev
python3.11 -m pip install --upgrade pip

# Tạo virtual environment
python3.11 -m venv rag_env
source rag_env/bin/activate

# Cài đặt core libraries
pip install -r requirements.txt
```

#### **requirements.txt**
```txt
# Core Libraries
fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.5.0
python-multipart==0.0.6

# Vector Search & Embeddings
chromadb==0.4.18
sentence-transformers==2.2.2
transformers==4.35.2
torch==2.1.1
faiss-cpu==1.7.4  # hoặc faiss-gpu nếu có GPU

# Full-text Search
elasticsearch==8.11.0
whoosh==2.7.4  # lightweight alternative

# Database & Caching
asyncpg==0.29.0
redis==5.0.1
sqlalchemy[asyncio]==2.0.23

# Utilities
numpy==1.24.3
pandas==2.0.3
scikit-learn==1.3.2
nltk==3.8.1
spacy==3.7.2

# Testing & Development
pytest==7.4.3
pytest-asyncio==0.21.1
black==23.11.0
flake8==6.1.0
```

#### **Bước 2: Cài đặt Docker và Database services**
```bash
# Cài đặt Docker
sudo apt install docker.io docker-compose
sudo usermod -aG docker $USER

# Khởi tạo services bằng Docker Compose
# Tạo file docker-compose.yml (xem phần phụ lục)
docker-compose up -d postgres redis elasticsearch
```

#### **Bước 3: Cài đặt các công cụ phát triển**
```bash
# VSCode với Python extensions
# Hoặc PyCharm Professional

# Git và version control
git config --global user.name "Your Name"
git config --global user.email "your.email@company.com"

# Postman hoặc curl để test APIs
# DBeaver để quản lý database
```

---

## 3. IMPLEMENTATION ROADMAP

### 3.1 Workflow tổng thể

```mermaid
graph TD
    Start([🚀 Start Implementation]) --> Setup[📦 Setup Environment]
    
    Setup --> Phase1[🔤 Phase 1: Text Preprocessing]
    Phase1 --> TestPhase1{✅ Test Phase 1}
    TestPhase1 -->|❌ Fail| Debug1[🐛 Debug Text Processing]
    Debug1 --> Phase1
    TestPhase1 -->|✅ Pass| Phase2[🔢 Phase 2: Embedding Engine]
    
    Phase2 --> TestPhase2{✅ Test Phase 2}
    TestPhase2 -->|❌ Fail| Debug2[🐛 Debug Embedding]
    Debug2 --> Phase2
    TestPhase2 -->|✅ Pass| Phase3[🔍 Phase 3: Vector Search]
    
    Phase3 --> TestPhase3{✅ Test Phase 3}
    TestPhase3 -->|❌ Fail| Debug3[🐛 Debug Vector Search]
    Debug3 --> Phase3
    TestPhase3 -->|✅ Pass| Phase4[🔎 Phase 4: Keyword Search]
    
    Phase4 --> TestPhase4{✅ Test Phase 4}
    TestPhase4 -->|❌ Fail| Debug4[🐛 Debug Keyword Search]
    Debug4 --> Phase4
    TestPhase4 -->|✅ Pass| Phase5[⚖️ Phase 5: Hybrid Ranking]
    
    Phase5 --> TestPhase5{✅ Test Phase 5}
    TestPhase5 -->|❌ Fail| Debug5[🐛 Debug Hybrid Ranking]
    Debug5 --> Phase5
    TestPhase5 -->|✅ Pass| Phase6[🛡️ Phase 6: Access Control]
    
    Phase6 --> TestPhase6{✅ Test Phase 6}
    TestPhase6 -->|❌ Fail| Debug6[🐛 Debug Access Control]
    Debug6 --> Phase6
    TestPhase6 -->|✅ Pass| Phase7[🎯 Phase 7: API Integration]
    
    Phase7 --> TestPhase7{✅ Test Phase 7}
    TestPhase7 -->|❌ Fail| Debug7[🐛 Debug API]
    Debug7 --> Phase7
    TestPhase7 -->|✅ Pass| FinalTest[🏁 End-to-End Testing]
    
    FinalTest --> Deploy([🚀 Ready for Deployment])
    
    %% Styling
    classDef startEnd fill:#4caf50,stroke:#2e7d32,stroke-width:2px,color:#fff
    classDef phase fill:#2196f3,stroke:#1565c0,stroke-width:2px,color:#fff
    classDef test fill:#ff9800,stroke:#ef6c00,stroke-width:2px,color:#fff
    classDef debug fill:#f44336,stroke:#c62828,stroke-width:2px,color:#fff
    
    class Start,Deploy startEnd
    class Phase1,Phase2,Phase3,Phase4,Phase5,Phase6,Phase7 phase
    class TestPhase1,TestPhase2,TestPhase3,TestPhase4,TestPhase5,TestPhase6,TestPhase7,FinalTest test
    class Debug1,Debug2,Debug3,Debug4,Debug5,Debug6,Debug7 debug
```

---

## 4. CHI TIẾT TRIỂN KHAI TỪNG PHASE

### **PHASE 1: 🔤 TEXT PREPROCESSING ENGINE**

#### **Mục tiêu:** Chuẩn hóa và xử lý query đầu vào
#### **Thời gian:** 2-3 ngày
#### **Output:** Query được làm sạch và tối ưu

#### **Bước 1.1: Tạo Text Preprocessor**
```python
# Tạo file: src/retrieval/text_processor.py
# Implement các functions:
# - normalize_vietnamese_text()
# - remove_stopwords()
# - expand_abbreviations()  
# - handle_special_characters()
```

#### **Bước 1.2: Tạo Query Enhancer**
```python
# Tạo file: src/retrieval/query_enhancer.py
# Implement:
# - query_expansion() # mở rộng từ đồng nghĩa
# - intent_detection() # phát hiện ý định câu hỏi
# - context_enrichment() # làm phong phú ngữ cảnh
```

#### **Bước 1.3: Unit Tests**
```bash
# Tạo test cases
pytest tests/test_text_processor.py -v
pytest tests/test_query_enhancer.py -v

# Test với dữ liệu thật
python scripts/test_vietnamese_processing.py
```

#### **🔄 Checkpoint 1: Text Processing**
- [ ] Vietnamese text được normalize chính xác
- [ ] Stopwords được loại bỏ đúng 
- [ ] Query expansion hoạt động với từ đồng nghĩa
- [ ] Performance: <100ms cho 1 query

#### **❌ Điểm quay đầu nếu fail:**
- **Vấn đề encoding**: Kiểm tra UTF-8, Unicode normalization
- **Vietnamese processing lỗi**: Sử dụng thư viện `underthesea` hoặc `pyvi`
- **Performance chậm**: Cache kết quả, optimize regex patterns

---

### **PHASE 2: 🔢 EMBEDDING ENGINE**

#### **Mục tiêu:** Tạo vector representations cho text
#### **Thời gian:** 3-4 ngày
#### **Output:** High-quality embeddings cho search

#### **Bước 2.1: Model Selection và Setup**
```python
# Tạo file: src/retrieval/embedding_models.py
# Implement support cho multiple models:
# - sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
# - sentence-transformers/distiluse-base-multilingual-cased  
# - OpenAI text-embedding-ada-002 (via API)
# - Local Vietnamese models nếu có
```

#### **Bước 2.2: Embedding Service**
```python
# Tạo file: src/retrieval/embedding_service.py
# Implement:
# - batch_encode() # xử lý nhiều text cùng lúc
# - cache_embeddings() # cache để tránh tính toán lại
# - model_comparison() # so sánh performance các models
```

#### **Bước 2.3: Vector Storage Interface**
```python
# Tạo file: src/retrieval/vector_store.py
# Implement adapters cho:
# - ChromaDB (recommend for development)
# - FAISS (for production performance)
# - Weaviate (if need advanced features)
```

#### **🔄 Checkpoint 2: Embedding Engine**
- [ ] Multiple embedding models hoạt động
- [ ] Batch processing với performance tối ưu
- [ ] Vector storage và retrieval chính xác
- [ ] Cache hoạt động đúng (hit rate >70%)

#### **❌ Điểm quay đầu nếu fail:**
- **Model load lỗi**: Check GPU/CPU compatibility, memory available
- **Embedding quality thấp**: Test với Vietnamese benchmark datasets
- **Performance issues**: Implement batch processing, use GPU if available
- **Storage connection fail**: Verify database connectivity, check ports

---

### **PHASE 3: 🔍 VECTOR SEARCH ENGINE**

#### **Mục tiêu:** Semantic search với cosine similarity
#### **Thời gian:** 3-4 ngày  
#### **Output:** Top-K similar documents với scores

#### **Bước 3.1: Similarity Search**
```python
# Tạo file: src/retrieval/semantic_search.py
# Implement:
# - cosine_similarity_search()
# - approximate_search() # using FAISS/Annoy for speed
# - result_scoring() # normalize scores 0-1
```

#### **Bước 3.2: Search Optimization**  
```python
# Implement trong semantic_search.py:
# - index_optimization() # tune FAISS parameters
# - query_preprocessing() # optimize query embeddings
# - result_postprocessing() # filter and rank results
```

#### **Bước 3.3: Performance Benchmarking**
```python
# Tạo file: scripts/benchmark_semantic_search.py
# Test với:
# - Different similarity thresholds (0.5, 0.6, 0.7, 0.8)
# - Various K values (5, 10, 20, 50)  
# - Different index configurations
```

#### **🔄 Checkpoint 3: Vector Search**
- [ ] Semantic search accuracy ≥80% trên test dataset
- [ ] Search response time <500ms cho 10K documents
- [ ] Similarity scores được normalize chính xác
- [ ] Index được optimize cho performance

#### **❌ Điểm quay đầu nếu fail:**
- **Low accuracy**: Kiểm tra embedding model, tune similarity threshold
- **Slow performance**: Optimize vector index, check hardware resources
- **Memory issues**: Implement streaming search, reduce batch sizes
- **Inconsistent results**: Debug vector normalization, check data quality

---

### **PHASE 4: 🔎 KEYWORD SEARCH ENGINE**

#### **Mục tiêu:** Full-text search với BM25 scoring
#### **Thời gian:** 2-3 ngày
#### **Output:** Keyword-based document ranking

#### **Bước 4.1: Full-text Indexing**
```python
# Tạo file: src/retrieval/keyword_search.py
# Implement:
# - build_inverted_index() # tạo inverted index
# - bm25_scoring() # implement BM25 algorithm
# - vietnamese_tokenization() # tokenize tiếng Việt
```

#### **Bước 4.2: Search Engine Implementation**
```python
# Implement trong keyword_search.py:
# - exact_match_search() # tìm kiếm chính xác
# - fuzzy_search() # tìm kiếm mờ với edit distance
# - phrase_search() # tìm cụm từ
# - boolean_search() # AND, OR, NOT operators
```

#### **Bước 4.3: Integration với Elasticsearch (Optional)**
```python
# Tạo file: src/retrieval/elasticsearch_adapter.py
# Implement nếu muốn dùng Elasticsearch:
# - index_documents()
# - search_documents() 
# - custom_analyzers() # cho tiếng Việt
```

#### **🔄 Checkpoint 4: Keyword Search**
- [ ] BM25 scoring hoạt động chính xác
- [ ] Vietnamese tokenization tốt
- [ ] Support exact match và fuzzy search
- [ ] Performance <300ms cho keyword queries

#### **❌ Điểm quay đầu nếu fail:**
- **Tokenization issues**: Sử dụng `underthesea` cho Vietnamese NLP
- **Poor BM25 scores**: Tune k1, b parameters, verify document frequencies
- **Memory consumption**: Optimize inverted index structure
- **Elasticsearch connection**: Check cluster health, verify indices

---

### **PHASE 5: ⚖️ HYBRID RANKING ENGINE**

#### **Mục tiêu:** Kết hợp semantic + keyword search
#### **Thời gian:** 4-5 ngày
#### **Output:** Unified ranking system

#### **Bước 5.1: Score Normalization**
```python
# Tạo file: src/retrieval/score_normalizer.py
# Implement:
# - normalize_semantic_scores() # 0-1 range
# - normalize_keyword_scores() # 0-1 range  
# - calibrate_score_ranges() # đảm bảo fair comparison
```

#### **Bước 5.2: Hybrid Ranking Algorithm**
```python
# Tạo file: src/retrieval/hybrid_ranker.py
# Implement multiple strategies:
# - weighted_combination() # α*semantic + β*keyword
# - rank_fusion() # RRF (Reciprocal Rank Fusion)
# - machine_learning_ranker() # ML-based combination
```

#### **Bước 5.3: Parameter Tuning**
```python
# Tạo file: scripts/tune_hybrid_parameters.py
# Implement:
# - grid_search_weights() # tìm α, β optimal
# - cross_validation() # validate trên multiple datasets
# - a_b_testing() # test different strategies
```

#### **🔄 Checkpoint 5: Hybrid Ranking**
- [ ] Hybrid search outperform individual methods
- [ ] Optimal weights được tìm thấy qua tuning
- [ ] Consistent ranking across different query types
- [ ] Performance <800ms cho hybrid search

#### **❌ Điểm quay đầu nếu fail:**
- **Poor hybrid performance**: Re-examine score normalization methods
- **Inconsistent results**: Debug score calibration, check data distributions
- **Parameter tuning slow**: Implement parallel grid search
- **Overfitting**: Use more diverse validation datasets

---

### **PHASE 6: 🛡️ ACCESS CONTROL FILTER**

#### **Mục tiêu:** Filter results theo user permissions
#### **Thời gian:** 3-4 ngày
#### **Output:** Permission-aware search results

#### **Bước 6.1: Permission Service**
```python
# Tạo file: src/retrieval/permission_service.py
# Implement:
# - get_user_permissions() # lấy quyền từ database
# - check_document_access() # kiểm tra quyền truy cập doc
# - cache_permissions() # cache để performance
```

#### **Bước 6.2: Access Control Filter**
```python
# Tạo file: src/retrieval/access_filter.py
# Implement:
# - filter_by_access_level() # lọc theo cấp độ truy cập
# - filter_by_department() # lọc theo phòng ban
# - filter_by_tags() # lọc theo tags/categories
# - audit_access_attempts() # log access attempts
```

#### **Bước 6.3: Security Testing**
```python
# Tạo file: tests/test_security.py
# Test cases:
# - User không thể truy cập docs cấp cao hơn
# - Department isolation hoạt động
# - Permission changes được reflect immediately
# - No information leakage qua search results
```

#### **🔄 Checkpoint 6: Access Control**
- [ ] 100% accuracy trong permission filtering
- [ ] No data leakage ra unauthorized users
- [ ] Performance impact <100ms overhead
- [ ] Comprehensive audit logging

#### **❌ Điểm quay đầu nếu fail:**
- **Permission leaks**: Review filter logic, add more test cases
- **Performance degradation**: Optimize permission caching
- **Database connection issues**: Check connection pooling
- **Audit logging fails**: Verify logging infrastructure

---

### **PHASE 7: 🎯 API INTEGRATION**

#### **Mục tiêu:** Expose retrieval qua REST API
#### **Thời gian:** 3-4 ngày
#### **Output:** Production-ready API endpoints

#### **Bước 7.1: FastAPI Application**
```python
# Tạo file: src/api/retrieval_api.py
# Implement endpoints:
# - POST /api/search/semantic
# - POST /api/search/keyword  
# - POST /api/search/hybrid
# - GET /api/search/config
```

#### **Bước 7.2: Request/Response Models**
```python
# Tạo file: src/api/models.py
# Define Pydantic models:
# - SearchRequest
# - SearchResponse
# - DocumentResult
# - ErrorResponse
```

#### **Bước 7.3: Error Handling & Validation**
```python
# Implement trong retrieval_api.py:
# - input_validation() # validate search parameters
# - exception_handling() # handle errors gracefully  
# - rate_limiting() # prevent API abuse
# - response_caching() # cache common queries
```

#### **🔄 Checkpoint 7: API Integration**
- [ ] All endpoints hoạt động đúng
- [ ] Input validation comprehensive
- [ ] Error responses user-friendly
- [ ] API documentation complete (Swagger)

#### **❌ Điểm quay đầu nếu fail:**
- **API errors**: Check request/response serialization
- **Validation issues**: Review Pydantic models
- **Performance problems**: Implement async processing
- **Documentation incomplete**: Use FastAPI auto-documentation

---

## 5. TESTING STRATEGY

### 5.1 Unit Testing Plan

```python
# Test Structure
tests/
├── unit/
│   ├── test_text_processor.py
│   ├── test_embedding_service.py  
│   ├── test_semantic_search.py
│   ├── test_keyword_search.py
│   ├── test_hybrid_ranker.py
│   ├── test_access_filter.py
│   └── test_api.py
├── integration/
│   ├── test_end_to_end.py
│   ├── test_database_integration.py
│   └── test_performance.py
└── fixtures/
    ├── sample_documents.json
    ├── test_queries.json
    └── user_permissions.json
```

### 5.2 Test Cases chính

#### **Functional Tests:**
```python
def test_semantic_search_accuracy():
    """Test semantic search achieves ≥80% accuracy"""
    # Load test dataset với ground truth
    # Run semantic search
    # Calculate precision@K, recall@K
    assert accuracy >= 0.8

def test_hybrid_search_improvement():  
    """Test hybrid search better than individual methods"""
    # Compare hybrid vs semantic vs keyword
    assert hybrid_score > max(semantic_score, keyword_score)

def test_access_control_security():
    """Test no unauthorized access to documents"""
    # Test với different user roles
    # Verify no leakage occurs
    assert no_unauthorized_docs_returned
```

#### **Performance Tests:**
```python
def test_search_response_time():
    """Test search responds within time limits"""
    # Test với different query types và document counts
    assert response_time < 1.0  # seconds

def test_concurrent_users():
    """Test system handles multiple concurrent searches"""
    # Simulate 100 concurrent users
    assert all_requests_successful
```

### 5.3 Acceptance Criteria Checklist

- [ ] **Semantic search accuracy ≥80%** trên test dataset
- [ ] **Hybrid search outperforms** individual methods
- [ ] **Access control 100% accurate** - no permission leaks
- [ ] **Top-K configurable** với default K=5
- [ ] **Response time <1 second** cho 95% requests
- [ ] **API documentation complete** với examples
- [ ] **Error handling graceful** với meaningful messages
- [ ] **Security tested** với penetration testing

---

## 6. DEPLOYMENT CONFIGURATION

### 6.1 Production Environment Setup

#### **Docker Configuration**
```dockerfile
# Dockerfile cho retrieval service
FROM python:3.11-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY src/ ./src/
COPY config/ ./config/

EXPOSE 8000
CMD ["uvicorn", "src.api.retrieval_api:app", "--host", "0.0.0.0", "--port", "8000"]
```

#### **Environment Variables**
```bash
# .env file
VECTOR_DB_URL=http://chromadb:8000
POSTGRES_URL=postgresql://user:pass@postgres:5432/knowledge_db
REDIS_URL=redis://redis:6379/0
ELASTICSEARCH_URL=http://elasticsearch:9200

# Model configurations
EMBEDDING_MODEL=sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
EMBEDDING_CACHE_SIZE=10000
SEARCH_DEFAULT_K=5
HYBRID_SEMANTIC_WEIGHT=0.7
HYBRID_KEYWORD_WEIGHT=0.3

# Performance settings
MAX_CONCURRENT_SEARCHES=50
CACHE_TTL_SECONDS=3600
```

### 6.2 Monitoring & Logging

#### **Metrics to Track:**
```python
# Key Performance Indicators
- Search accuracy (precision@5, recall@5)  
- Response time (p95, p99)
- Query volume per minute
- Cache hit rate
- Error rate
- User satisfaction scores
```

#### **Logging Configuration:**
```python
# Structured logging với JSON format
{
    "timestamp": "2025-09-01T10:00:00Z",
    "level": "INFO", 
    "service": "retrieval-engine",
    "user_id": "emp001",
    "query": "quy trình mua hàng",
    "search_type": "hybrid",
    "results_count": 5,
    "response_time_ms": 423,
    "accuracy_score": 0.87
}
```

---

## 7. TROUBLESHOOTING GUIDE

### 7.1 Common Issues & Solutions

| Vấn đề | Triệu chứng | Giải pháp |
|--------|-------------|-----------|
| **Low Search Accuracy** | Results không relevant | • Retrain embedding model<br/>• Tune similarity thresholds<br/>• Improve data quality |
| **Slow Response Time** | Timeout errors, user complaints | • Optimize vector indices<br/>• Implement caching<br/>• Scale horizontally |
| **Memory Issues** | OOM errors, crashes | • Reduce batch sizes<br/>• Use streaming processing<br/>• Add more RAM |
| **Permission Errors** | Users see restricted docs | • Review access control logic<br/>• Clear permission cache<br/>• Check database sync |

### 7.2 Debug Commands

```bash
# Check service health
curl http://localhost:8000/health

# Test search endpoint
curl -X POST http://localhost:8000/api/search/hybrid \
  -H "Content-Type: application/json" \
  -d '{"query": "quy trình mua hàng", "user_id": "emp001", "k": 5}'

# Check vector database
curl http://chromadb:8000/api/v1/collections

# Monitor logs
docker logs -f retrieval-service

# Performance monitoring
python scripts/benchmark_search.py --queries=100 --concurrent=10
```

---

## 8. PERFORMANCE BENCHMARKS

### 8.1 Target Metrics

| Metric | Target | Measurement Method |
|--------|--------|--------------------|
| **Search Accuracy** | ≥80% | Precision@5 on test dataset |
| **Response Time** | <1 second | p95 latency |
| **Throughput** | 100 QPS | Concurrent users simulation |
| **Cache Hit Rate** | >70% | Redis metrics |
| **Memory Usage** | <8GB | Container monitoring |
| **CPU Utilization** | <80% | System monitoring |

### 8.2 Benchmark Scripts

```python
# scripts/benchmark_retrieval.py
import asyncio
import time
from concurrent.futures import ThreadPoolExecutor

async def benchmark_search_performance():
    """Benchmark search performance with different parameters"""
    # Test với different query types
    # Measure response times
    # Generate performance report

def benchmark_accuracy():
    """Measure search accuracy against ground truth"""
    # Load test queries với expected results
    # Run search engine
    # Calculate precision, recall, F1-score

def stress_test():
    """Test system under high load"""
    # Simulate high concurrent load
    # Monitor system resources
    # Identify breaking points
```

---

## 9. PHỤC LỤC

### 9.1 Docker Compose Configuration

```yaml
# docker-compose.yml
version: '3.8'

services:
  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: knowledge_db
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: secure_password
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data

  chromadb:
    image: chromadb/chroma:latest
    ports:
      - "8000:8000"
    volumes:
      - chromadb_data:/chroma/chroma
    environment:
      - CHROMA_SERVER_HOST=0.0.0.0
      - CHROMA_SERVER_PORT=8000

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
    ports:
      - "9200:9200"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data

volumes:
  postgres_data:
  redis_data:
  chromadb_data:
  elasticsearch_data:
```

### 9.2 Sample Test Data Structure

```json
{
  "test_documents": [
    {
      "doc_id": "proc_001",
      "title": "Quy trình Mua hàng",
      "content": "Quy trình mua hàng gồm 5 bước chính...",
      "access_level": "employee_only",
      "department": "procurement",
      "tags": ["procurement", "process", "approval"]
    }
  ],
  "test_queries": [
    {
      "query": "làm thế nào để mua hàng",
      "expected_docs": ["proc_001", "proc_002"],
      "user_role": "employee"
    }
  ]
}
```

### 9.3 API Documentation Sample

```yaml
# OpenAPI specification
openapi: 3.0.0
info:
  title: Document Retrieval API
  version: 1.0.0

paths:
  /api/search/hybrid:
    post:
      summary: Hybrid search combining semantic and keyword search
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                query:
                  type: string
                  description: Search query
                user_id:
                  type: string
                  description: User identifier for access control
                k:
                  type: integer
                  default: 5
                  description: Number of results to return
      responses:
        200:
          description: Search results
          content:
            application/json:
              schema:
                type: object
                properties:
                  results:
                    type: array
                    items:
                      type: object
                  total_count:
                    type: integer
### 9.3 API Documentation Sample (tiếp)
# OpenAPI specification (tiếp)
                  response_time_ms:
                    type: number
                  search_metadata:
                    type: object
                    properties:
                      semantic_score:
                        type: number
                      keyword_score:
                        type: number
                      hybrid_score:
                        type: number
              example:
                results:
                  - doc_id: "proc_001"
                    title: "Quy trình Mua hàng"
                    content_snippet: "Quy trình mua hàng gồm 5 bước chính..."
                    relevance_score: 0.92
                    access_level: "employee_only"
                    source: "HR/Procedures/Procurement.pdf"
                total_count: 3
                response_time_ms: 245
                search_metadata:
                  semantic_score: 0.89
                  keyword_score: 0.76
                  hybrid_score: 0.92
        400:
          description: Invalid request parameters
        403:
          description: Access denied
        500:
          description: Internal server error
```

---

## 10. ADVANCED FEATURES & OPTIMIZATION

### 10.1 Advanced Search Features

#### **Query Understanding & Enhancement**
```python
# src/retrieval/advanced_query_processor.py

class AdvancedQueryProcessor:
    def __init__(self):
        self.intent_classifier = self._load_intent_model()
        self.entity_extractor = self._load_ner_model()
    
    async def enhance_query(self, query: str, user_context: dict) -> dict:
        """
        Enhance query với advanced NLP techniques
        """
        # 1. Intent Detection
        intent = self.detect_intent(query)
        
        # 2. Named Entity Recognition
        entities = self.extract_entities(query)
        
        # 3. Query Expansion với domain knowledge
        expanded_terms = self.expand_domain_terms(query, entities)
        
        # 4. Contextual Enhancement
        context_terms = self.add_contextual_terms(user_context)
        
        return {
            "original_query": query,
            "enhanced_query": self._build_enhanced_query(
                query, expanded_terms, context_terms
            ),
            "intent": intent,
            "entities": entities,
            "confidence": self._calculate_confidence()
        }
```

#### **Multi-Modal Search Support**
```python
# src/retrieval/multimodal_search.py

class MultiModalSearchEngine:
    """
    Support tìm kiếm với images, tables, charts trong documents
    """
    def __init__(self):
        self.image_encoder = self._load_image_encoder()
        self.table_parser = self._load_table_parser()
    
    async def search_with_image(self, image_query: bytes, text_query: str):
        """Tìm kiếm documents có images tương tự"""
        # Extract features từ image
        # Combine với text search
        # Return multi-modal results
        pass
    
    async def search_tables(self, table_query: str):
        """Tìm kiếm trong table content"""
        # Parse table structures
        # Search table cells
        # Return structured results
        pass
```

### 10.2 Performance Optimization Techniques

#### **Intelligent Caching Strategy**
```python
# src/retrieval/intelligent_cache.py

class IntelligentCache:
    """
    Multi-level caching với predictive pre-loading
    """
    def __init__(self):
        self.l1_cache = {}  # In-memory hot cache
        self.l2_cache = RedisCache()  # Redis distributed cache
        self.l3_cache = DatabaseCache()  # Database cache
        
    async def get_with_prediction(self, query: str, user_id: str):
        """
        Get cached results và predict next queries
        """
        # Check L1 -> L2 -> L3 cache
        result = await self._check_cache_levels(query)
        
        if result:
            # Predict và pre-load related queries
            await self._predictive_preload(query, user_id)
            return result
            
        # Cache miss - execute search và cache result
        result = await self._execute_search_and_cache(query)
        return result
```

#### **Dynamic Index Optimization**
```python
# src/retrieval/dynamic_optimizer.py

class DynamicIndexOptimizer:
    """
    Tự động optimize indices based on query patterns
    """
    def __init__(self):
        self.query_analyzer = QueryPatternAnalyzer()
        self.index_manager = IndexManager()
    
    async def optimize_based_on_usage(self):
        """
        Analyze query patterns và optimize indices accordingly
        """
        # Analyze recent query patterns
        patterns = await self.query_analyzer.analyze_patterns()
        
        # Identify optimization opportunities
        optimizations = self._identify_optimizations(patterns)
        
        # Apply optimizations
        for opt in optimizations:
            await self.index_manager.apply_optimization(opt)
```

### 10.3 Quality Assurance & Monitoring

#### **Real-time Quality Monitoring**
```python
# src/monitoring/quality_monitor.py

class SearchQualityMonitor:
    """
    Monitor search quality in real-time
    """
    def __init__(self):
        self.metrics_collector = MetricsCollector()
        self.alert_manager = AlertManager()
    
    async def monitor_search_quality(self, query: str, results: list, user_feedback: dict):
        """
        Monitor và alert nếu quality drops
        """
        # Calculate quality metrics
        relevance_score = self._calculate_relevance(query, results)
        user_satisfaction = self._parse_feedback(user_feedback)
        
        # Track metrics over time
        await self.metrics_collector.record_metrics({
            "relevance_score": relevance_score,
            "user_satisfaction": user_satisfaction,
            "timestamp": datetime.now()
        })
        
        # Alert nếu quality drops below threshold
        if relevance_score < 0.7:
            await self.alert_manager.send_alert(
                "Search quality degradation detected",
                {"query": query, "score": relevance_score}
            )
```

#### **A/B Testing Framework**
```python
# src/experimentation/ab_testing.py

class SearchABTesting:
    """
    A/B test different search algorithms
    """
    def __init__(self):
        self.experiment_manager = ExperimentManager()
        self.results_analyzer = ResultsAnalyzer()
    
    async def run_search_experiment(self, user_id: str, query: str):
        """
        Route user to different search variants
        """
        # Determine which variant to show user
        variant = await self.experiment_manager.get_variant(user_id)
        
        # Execute search với selected variant
        if variant == "semantic_heavy":
            results = await self._semantic_heavy_search(query)
        elif variant == "keyword_heavy":
            results = await self._keyword_heavy_search(query)
        else:  # control
            results = await self._standard_hybrid_search(query)
        
        # Log experiment results
        await self.results_analyzer.log_experiment_result(
            user_id, query, variant, results
        )
        
        return results
```

---

## 11. SECURITY CONSIDERATIONS

### 11.1 Advanced Security Features

#### **Query Sanitization & Injection Prevention**
```python
# src/security/query_sanitizer.py

class QuerySanitizer:
    """
    Prevent injection attacks và malicious queries
    """
    def __init__(self):
        self.malicious_patterns = self._load_malicious_patterns()
        self.rate_limiter = RateLimiter()
    
    async def sanitize_query(self, query: str, user_id: str) -> str:
        """
        Sanitize user query before processing
        """
        # Check rate limiting
        if not await self.rate_limiter.check_limit(user_id):
            raise RateLimitExceeded("Too many queries per minute")
        
        # Remove potentially malicious patterns
        sanitized = self._remove_malicious_patterns(query)
        
        # Length validation
        if len(sanitized) > 1000:
            raise QueryTooLong("Query exceeds maximum length")
        
        # Content validation
        if self._contains_sensitive_terms(sanitized):
            await self._log_suspicious_query(user_id, query)
            
        return sanitized
```

#### **Access Control với Attribute-Based Control**
```python
# src/security/abac_controller.py

class ABACController:
    """
    Attribute-Based Access Control for fine-grained permissions
    """
    def __init__(self):
        self.policy_engine = PolicyEngine()
        self.attribute_manager = AttributeManager()
    
    async def check_document_access(self, user_attributes: dict, 
                                  document_attributes: dict) -> bool:
        """
        Check access based on multiple attributes
        """
        # Get applicable policies
        policies = await self.policy_engine.get_policies(
            user_attributes, document_attributes
        )
        
        # Evaluate each policy
        for policy in policies:
            result = await self._evaluate_policy(
                policy, user_attributes, document_attributes
            )
            if not result.allowed:
                await self._log_access_denied(user_attributes, document_attributes)
                return False
                
        return True
    
    async def _evaluate_policy(self, policy, user_attrs, doc_attrs):
        """
        Evaluate policy rules
        Example: user.department == document.owner_department AND 
                user.clearance_level >= document.classification_level
        """
        return await self.policy_engine.evaluate(policy, user_attrs, doc_attrs)
```

### 11.2 Audit & Compliance

#### **Comprehensive Audit Logging**
```python
# src/audit/audit_logger.py

class AuditLogger:
    """
    Log all search activities for compliance
    """
    def __init__(self):
        self.secure_logger = SecureLogger()
        self.compliance_checker = ComplianceChecker()
    
    async def log_search_activity(self, activity: SearchActivity):
        """
        Log search activity với tamper-proof logging
        """
        audit_entry = {
            "timestamp": datetime.utcnow().isoformat(),
            "user_id": activity.user_id,
            "user_ip": activity.ip_address,
            "query": self._hash_sensitive_content(activity.query),
            "results_count": len(activity.results),
            "accessed_documents": [doc.doc_id for doc in activity.results],
            "access_granted": activity.access_granted,
            "session_id": activity.session_id,
            "user_agent": activity.user_agent
        }
        
        # Add digital signature
        audit_entry["signature"] = self._sign_entry(audit_entry)
        
        # Store in secure audit database
        await self.secure_logger.log_entry(audit_entry)
        
        # Check compliance requirements
        await self.compliance_checker.validate_activity(audit_entry)
```

---

## 12. SCALING & PRODUCTION READINESS

### 12.1 Horizontal Scaling Strategy

#### **Microservices Architecture**
```python
# src/services/search_coordinator.py

class SearchCoordinator:
    """
    Coordinate multiple search service instances
    """
    def __init__(self):
        self.service_registry = ServiceRegistry()
        self.load_balancer = SearchLoadBalancer()
        self.circuit_breaker = CircuitBreaker()
    
    async def distribute_search(self, search_request: SearchRequest):
        """
        Distribute search across multiple instances
        """
        # Select optimal service instance
        service_instance = await self.load_balancer.select_instance(
            search_request.complexity
        )
        
        try:
            # Execute search với circuit breaker protection
            result = await self.circuit_breaker.execute(
                service_instance.search, search_request
            )
            return result
            
        except ServiceUnavailable:
            # Fallback to alternative instance
            fallback_instance = await self.service_registry.get_fallback()
            return await fallback_instance.search(search_request)
```

#### **Database Sharding Strategy**
```python
# src/storage/sharding_manager.py

class ShardingManager:
    """
    Manage document sharding across multiple databases
    """
    def __init__(self):
        self.shard_router = ShardRouter()
        self.consistency_manager = ConsistencyManager()
    
    async def route_search_query(self, query: str, user_context: dict):
        """
        Route query to appropriate shards
        """
        # Determine which shards to search
        relevant_shards = await self.shard_router.get_relevant_shards(
            query, user_context
        )
        
        # Execute parallel searches
        search_tasks = [
            self._search_shard(shard, query, user_context)
            for shard in relevant_shards
        ]
        
        shard_results = await asyncio.gather(*search_tasks)
        
        # Merge và rank results
        merged_results = await self._merge_shard_results(shard_results)
        
        return merged_results
```

### 12.2 Performance Optimization for Production

#### **Advanced Caching Strategies**
```python
# src/caching/multilevel_cache.py

class MultiLevelCache:
    """
    Sophisticated multi-level caching system
    """
    def __init__(self):
        self.memory_cache = LRUCache(maxsize=10000)  # L1: In-memory
        self.redis_cache = RedisCluster()            # L2: Distributed
        self.db_cache = DatabaseCache()              # L3: Persistent
        self.cdn_cache = CDNCache()                  # L4: Edge caching
    
    async def get_cached_result(self, cache_key: str):
        """
        Check caches in order of speed
        """
        # L1: Memory cache (fastest)
        if cache_key in self.memory_cache:
            return self.memory_cache[cache_key]
        
        # L2: Redis cache
        result = await self.redis_cache.get(cache_key)
        if result:
            # Promote to L1
            self.memory_cache[cache_key] = result
            return result
        
        # L3: Database cache
        result = await self.db_cache.get(cache_key)
        if result:
            # Promote to L2 and L1
            await self.redis_cache.set(cache_key, result)
            self.memory_cache[cache_key] = result
            return result
        
        # L4: CDN cache (for static content)
        return await self.cdn_cache.get(cache_key)
```

#### **Resource Management & Auto-scaling**
```python
# src/scaling/auto_scaler.py

class AutoScaler:
    """
    Automatically scale search services based on load
    """
    def __init__(self):
        self.metrics_monitor = MetricsMonitor()
        self.kubernetes_client = KubernetesClient()
        self.scaling_policy = ScalingPolicy()
    
    async def monitor_and_scale(self):
        """
        Monitor metrics và auto-scale services
        """
        while True:
            # Collect current metrics
            metrics = await self.metrics_monitor.get_current_metrics()
            
            # Make scaling decision
            scaling_decision = await self.scaling_policy.evaluate(metrics)
            
            if scaling_decision.scale_up:
                await self.kubernetes_client.scale_up(
                    service="retrieval-service",
                    replicas=scaling_decision.target_replicas
                )
                
            elif scaling_decision.scale_down:
                await self.kubernetes_client.scale_down(
                    service="retrieval-service", 
                    replicas=scaling_decision.target_replicas
                )
            
            # Wait before next evaluation
            await asyncio.sleep(60)  # Check every minute
```

---

## 13. MAINTENANCE & OPERATIONS

### 13.1 Health Monitoring & Alerting

#### **Comprehensive Health Checks**
```python
# src/health/health_checker.py

class HealthChecker:
    """
    Comprehensive system health monitoring
    """
    def __init__(self):
        self.component_checkers = {
            "vector_db": VectorDBHealthChecker(),
            "postgres": PostgreSQLHealthChecker(),
            "redis": RedisHealthChecker(),
            "elasticsearch": ElasticsearchHealthChecker(),
            "embedding_service": EmbeddingServiceHealthChecker()
        }
        
    async def check_system_health(self) -> HealthReport:
        """
        Check health của tất cả components
        """
        health_results = {}
        
        for component_name, checker in self.component_checkers.items():
            try:
                health_status = await checker.check_health()
                health_results[component_name] = health_status
                
            except Exception as e:
                health_results[component_name] = HealthStatus(
                    status="unhealthy",
                    error=str(e),
                    timestamp=datetime.utcnow()
                )
        
        # Generate overall health report
        return HealthReport(
            overall_status=self._calculate_overall_status(health_results),
            component_status=health_results,
            recommendations=self._generate_recommendations(health_results)
        )
```

#### **Alerting & Incident Response**
```python
# src/monitoring/alert_manager.py

class AlertManager:
    """
    Manage alerts và incident response
    """
    def __init__(self):
        self.notification_channels = {
            "email": EmailNotifier(),
            "slack": SlackNotifier(), 
            "pagerduty": PagerDutyNotifier()
        }
        self.escalation_rules = EscalationRules()
    
    async def handle_alert(self, alert: Alert):
        """
        Handle alert với appropriate escalation
        """
        # Determine severity
        severity = self._assess_severity(alert)
        
        # Get escalation rules for this severity
        escalation = await self.escalation_rules.get_escalation(severity)
        
        # Send notifications
        for level in escalation.levels:
            await self._send_notifications(alert, level)
            
            # Wait for acknowledgment
            ack_received = await self._wait_for_acknowledgment(
                alert, level.timeout
            )
            
            if ack_received:
                break  # Stop escalation
```

### 13.2 Backup & Disaster Recovery

#### **Automated Backup Strategy**
```python
# src/backup/backup_manager.py

class BackupManager:
    """
    Manage automated backups của all critical data
    """
    def __init__(self):
        self.backup_strategies = {
            "vector_db": VectorDBBackup(),
            "postgres": PostgreSQLBackup(),
            "redis": RedisBackup(),
            "file_storage": FileStorageBackup()
        }
        
    async def execute_backup_cycle(self):
        """
        Execute full backup cycle
        """
        backup_session = BackupSession(
            session_id=generate_uuid(),
            timestamp=datetime.utcnow()
        )
        
        backup_results = {}
        
        for component, strategy in self.backup_strategies.items():
            try:
                # Execute backup
                backup_result = await strategy.backup(backup_session)
                backup_results[component] = backup_result
                
                # Verify backup integrity  
                verification = await strategy.verify_backup(backup_result)
                backup_result.verified = verification.success
                
            except Exception as e:
                backup_results[component] = BackupResult(
                    status="failed",
                    error=str(e)
                )
        
        # Generate backup report
        await self._generate_backup_report(backup_session, backup_results)
```

---

## 14. FINAL CHECKLIST & DEPLOYMENT

### 14.1 Pre-Deployment Checklist

#### **✅ Development Completed**
- [ ] All 7 phases implemented và tested
- [ ] Unit tests pass với >80% coverage
- [ ] Integration tests pass
- [ ] Performance benchmarks meet targets
- [ ] Security tests pass
- [ ] API documentation complete

#### **✅ Quality Assurance**
- [ ] Search accuracy ≥80% verified
- [ ] Response time <1s for 95% of queries
- [ ] Concurrent user testing completed (100 users)
- [ ] Memory và CPU usage within limits
- [ ] No data leakage in access control tests
- [ ] Error handling covers all edge cases

#### **✅ Security & Compliance**
- [ ] Penetration testing completed
- [ ] Audit logging implemented
- [ ] Access control thoroughly tested
- [ ] Data encryption verified
- [ ] Compliance requirements met

#### **✅ Operations Readiness**
- [ ] Monitoring dashboards configured
- [ ] Alerting rules defined
- [ ] Backup procedures tested
- [ ] Disaster recovery plan validated
- [ ] Runbooks created for common issues
- [ ] On-call procedures established

### 14.2 Deployment Script

```bash
#!/bin/bash
# deploy_retrieval_service.sh

set -e  # Exit on any error

echo "🚀 Starting deployment of Retrieval Service..."

# Step 1: Pre-deployment checks
echo "🔍 Running pre-deployment checks..."
python scripts/pre_deployment_check.py

# Step 2: Database migrations
echo "📊 Running database migrations..."
python scripts/migrate_database.py

# Step 3: Build và push Docker image
echo "🐳 Building Docker image..."
docker build -t retrieval-service:$(git rev-parse --short HEAD) .
docker tag retrieval-service:$(git rev-parse --short HEAD) retrieval-service:latest

# Step 4: Deploy to Kubernetes
echo "☸️ Deploying to Kubernetes..."
kubectl apply -f k8s/namespace.yaml
kubectl apply -f k8s/configmap.yaml
kubectl apply -f k8s/secret.yaml
kubectl apply -f k8s/deployment.yaml
kubectl apply -f k8s/service.yaml
kubectl apply -f k8s/ingress.yaml

# Step 5: Wait for deployment
echo "⏳ Waiting for deployment to be ready..."
kubectl wait --for=condition=available --timeout=300s deployment/retrieval-service

# Step 6: Health check
echo "❤️ Running health checks..."
python scripts/post_deployment_health_check.py

# Step 7: Smoke tests
echo "🧪 Running smoke tests..."
python scripts/smoke_tests.py

echo "✅ Deployment completed successfully!"
echo "📊 Monitor at: http://monitoring.company.com/retrieval-service"
```

### 14.3 Go-Live Plan

#### **Phase 1: Soft Launch (1 tuần)**
- Deploy to production environment
- Enable access cho 10% users (beta testers)
- Monitor performance metrics closely
- Collect user feedback
- Fix any critical issues

#### **Phase 2: Gradual Rollout (2 tuần)**
- Increase user access to 50%
- Monitor system stability
- Optimize performance based on real usage
- Train support team
- Prepare documentation

#### **Phase 3: Full Production (1 tuần)**
- Enable access cho all users (100)
- Full monitoring và alerting active
- Support team ready
- Performance optimization ongoing
- User training sessions

### 14.4 Success Metrics

#### **Technical KPIs:**
```python
success_metrics = {
    "search_accuracy": {"target": "≥80%", "measurement": "precision@5"},
    "response_time": {"target": "<1s", "measurement": "p95 latency"},
    "availability": {"target": "99.5%", "measurement": "uptime percentage"},
    "error_rate": {"target": "<1%", "measurement": "error/total requests"},
    "concurrent_users": {"target": "100", "measurement": "simultaneous users"},
    "memory_usage": {"target": "<8GB", "measurement": "peak memory"},
    "cpu_utilization": {"target": "<80%", "measurement": "average CPU"}
}
```

#### **Business KPIs:**
```python
business_metrics = {
    "user_adoption": {"target": "80%", "measurement": "active users/total users"},
    "query_volume": {"target": "500/day", "measurement": "queries per day"},
    "user_satisfaction": {"target": "4.0/5", "measurement": "survey score"},
    "time_saved": {"target": "30min/user/day", "measurement": "before vs after"},
    "support_tickets": {"target": "-50%", "measurement": "reduction in tickets"}
}
```

---

## 15. POST-DEPLOYMENT SUPPORT

### 15.1 Monitoring Dashboard

```python
# Dashboard metrics to track
dashboard_widgets = {
    "search_performance": {
        "metrics": ["avg_response_time", "p95_response_time", "throughput"],
        "alerts": ["response_time > 1s", "throughput < 50 QPS"]
    },
    "search_quality": {
        "metrics": ["accuracy_score", "user_satisfaction", "result_relevance"], 
        "alerts": ["accuracy < 80%", "satisfaction < 4.0"]
    },
    "system_health": {
        "metrics": ["cpu_usage", "memory_usage", "disk_usage", "error_rate"],
        "alerts": ["cpu > 80%", "memory > 8GB", "error_rate > 1%"]
    },
    "user_activity": {
        "metrics": ["active_users", "query_volume", "peak_concurrent_users"],
        "alerts": ["no_activity_1h", "concurrent_users > 100"]
    }
}
```

### 15.2 Support Runbooks

#### **Common Issues & Solutions:**

**🔴 HIGH SEVERITY:**
```markdown
## Search Service Down
**Symptoms:** All search requests return 500 errors
**Impact:** Complete service outage
**Resolution:**
1. Check service pods: `kubectl get pods -n retrieval`
2. Check logs: `kubectl logs deployment/retrieval-service`
3. Restart service: `kubectl rollout restart deployment/retrieval-service`
4. If database issue: Check vector DB and PostgreSQL connectivity
**Escalation:** Page on-call engineer if not resolved in 10 minutes
```

**🟡 MEDIUM SEVERITY:**
```markdown
## Slow Search Performance
**Symptoms:** Response time > 2 seconds
**Impact:** Poor user experience
**Resolution:**
1. Check system resources: CPU, Memory, Disk I/O
2. Check database performance metrics
3. Clear caches if corrupted: `redis-cli FLUSHALL`
4. Scale up pods if needed: `kubectl scale deployment retrieval-service --replicas=5`
**Escalation:** Notify team lead if performance doesn't improve in 30 minutes
```

### 15.3 Continuous Improvement Plan

#### **Monthly Reviews:**
- Performance metrics analysis
- User feedback review
- Search accuracy assessment
- System optimization opportunities
- Technology updates evaluation

#### **Quarterly Upgrades:**
- Embedding model updates
- Search algorithm improvements
- Infrastructure scaling adjustments
- Security updates và patches
- Feature enhancements based on user requests

---

**🎉 CONGRATULATIONS!**

Bạn đã có một **roadmap hoàn chỉnh** để implement FR-04.1 Retrieval module. Document này cung cấp:

✅ **Step-by-step implementation guide**  
✅ **Detailed technical specifications**  
✅ **Comprehensive testing strategy**  
✅ **Production deployment procedures**  
✅ **Monitoring và maintenance plans**  
✅ **Troubleshooting guides**  

Team kỹ thuật có thể follow guide này để xây dựng một **search engine chất lượng cao** cho hệ thống chatbot nội bộ!
