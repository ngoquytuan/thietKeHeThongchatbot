T√¥i hi·ªÉu quan ƒëi·ªÉm c·ªßa b·∫°n - mu·ªën BM25 ho·∫°t ƒë·ªông t·ªët m·ªôt c√°ch "t·ª± nhi√™n" thay v√¨ d√πng c√°c tricks boost ƒëi·ªÉm. ƒê√¢y l√† c√°c c√°ch c·∫£i thi·ªán **thu·∫ßn t√∫y v·ªÅ BM25**:

## 1. **Tune BM25 Parameters (k1, b)**

ƒê√¢y l√† c√°ch ch√≠nh th·ªëng nh·∫•t ƒë·ªÉ c·∫£i thi·ªán BM25:

```python
from rank_bm25 import BM25Okapi

# Default (c√≥ th·ªÉ kh√¥ng t·ªëi ∆∞u cho ti·∫øng Vi·ªát):
# k1=1.5, b=0.75

# Th·ª≠ nghi·ªám c√°c gi√° tr·ªã:
configs = [
    {"k1": 1.2, "b": 0.5},   # √çt nh·∫°y c·∫£m v·ªõi term freq, √≠t penalty cho doc d√†i
    {"k1": 1.5, "b": 0.5},   # Default k1, gi·∫£m length penalty
    {"k1": 1.2, "b": 0.75},  # Gi·∫£m term freq sensitivity, keep length penalty
    {"k1": 0.9, "b": 0.4},   # Conservative: c·∫£ 2 ƒë·ªÅu th·∫•p
]

# Test t·ª´ng config v·ªõi ground truth queries
for config in configs:
    bm25 = BM25Okapi(corpus, k1=config["k1"], b=config["b"])
    # Evaluate MRR, NDCG...
```

**Gi·∫£i th√≠ch parameters:**
- **k1** (1.2-2.0): ƒêi·ªÅu ch·ªânh saturation c·ªßa term frequency
  - k1 th·∫•p ‚Üí term xu·∫•t hi·ªán 5 l·∫ßn kh√¥ng "t·ªët h∆°n nhi·ªÅu" so v·ªõi 3 l·∫ßn
  - k1 cao ‚Üí term frequency c√≥ impact l·ªõn
  
- **b** (0-1): Document length normalization
  - b=0 ‚Üí Kh√¥ng penalty doc d√†i (nguy hi·ªÉm: long docs s·∫Ω win)
  - b=1 ‚Üí Penalty m·∫°nh doc d√†i
  - b=0.5-0.6 ‚Üí Sweet spot cho nhi·ªÅu corpus

**V·∫•n ƒë·ªÅ v·ªõi case c·ªßa b·∫°n:**
- Document "C·∫•u h√¨nh MikroTik" c√≥ nhi·ªÅu terms "thi·∫øt b·ªã", "c·∫ßn", "l√†m" n√™n score cao
- C·∫ßn gi·∫£m k1 ƒë·ªÉ reduce impact c·ªßa term frequency thu·∫ßn t√∫y

## 2. **C·∫£i thi·ªán Preprocessing**

### A. **C·∫£i thi·ªán Tokenization:**

```python
from underthesea import word_tokenize

def better_tokenize(text):
    # 1. Lowercase
    text = text.lower()
    
    # 2. Word tokenize
    tokens = word_tokenize(text, format="text").split()
    
    # 3. Remove stopwords (quan tr·ªçng!)
    stopwords = [
        "t√¥i", "b·∫°n", "c·ªßa", "v√†", "c√≥", "n√†y", "ƒë√≥", 
        "th√¨", "ƒë∆∞·ª£c", "cho", "v·ªõi", "t·ª´", "ƒë·ªÉ",
        "c√°c", "nh·ªØng", "nhi·ªÅu", "l·∫°i", "r·∫•t", "trong"
    ]
    tokens = [t for t in tokens if t not in stopwords]
    
    # 4. Normalize synonyms/variations
    replacements = {
        "·ªü": "t·∫°i",
        "g√¨": "",  # Remove question words
        "n√†o": "",
    }
    tokens = [replacements.get(t, t) for t in tokens]
    tokens = [t for t in tokens if t]  # Remove empty
    
    return tokens

# Query: "T√¥i mu·ªën l√†m vi·ªác ·ªü nh√† th√¨ c·∫ßn trang b·ªã g√¨?"
# After: ["mu·ªën", "l√†m_vi·ªác", "t·∫°i", "nh√†", "c·∫ßn", "trang_b·ªã"]
```

**V·∫•n ƒë·ªÅ hi·ªán t·∫°i:**
- Log shows: `"stopwords_removed": 2` ‚Üí Ch·ªâ x√≥a 2 stopwords
- Query c√≤n l·∫°i 7 terms ‚Üí qu√° nhi·ªÅu, g√¢y noise
- "g√¨", "th√¨" kh√¥ng n√™n tham gia matching

### B. **Compound Word Detection:**

```python
# "l√†m vi·ªác" n√™n ƒë∆∞·ª£c coi l√† 1 term, kh√¥ng ph·∫£i "l√†m" + "vi·ªác"
def detect_compounds(tokens):
    compounds = {
        ("l√†m", "vi·ªác"): "l√†m_vi·ªác",
        ("trang", "b·ªã"): "trang_b·ªã",
        ("thi·∫øt", "b·ªã"): "thi·∫øt_b·ªã",
        ("vƒÉn", "ph√≤ng"): "vƒÉn_ph√≤ng",
        ("t·ª´", "xa"): "t·ª´_xa",
    }
    
    result = []
    i = 0
    while i < len(tokens):
        if i < len(tokens) - 1:
            bigram = (tokens[i], tokens[i+1])
            if bigram in compounds:
                result.append(compounds[bigram])
                i += 2
                continue
        result.append(tokens[i])
        i += 1
    
    return result
```

**T√°c d·ª•ng:**
- "l√†m vi·ªác ·ªü nh√†" ‚Üí ["l√†m_vi·ªác", "nh√†"] thay v√¨ ["l√†m", "vi·ªác", "nh√†"]
- Document v·ªÅ "l√†m_vi·ªác t·ª´_xa" s·∫Ω match ch√≠nh x√°c h∆°n
- Document v·ªÅ "l√†m router" ho·∫∑c "vi·ªác kh√°c" s·∫Ω kh√¥ng match

## 3. **Index-time Processing**

### A. **Th√™m tr∆∞·ªùng ·∫£o (Virtual Fields):**

```python
def enrich_document(doc):
    """
    Th√™m c√°c tr∆∞·ªùng t·ªïng h·ª£p ƒë·ªÉ c·∫£i thi·ªán recall
    Kh√¥ng ph·∫£i boost - ch·ªâ l√† l√†m phong ph√∫ n·ªôi dung index
    """
    content = doc["content"]
    
    # Extract key phrases
    if "l√†m vi·ªác t·ª´ xa" in content.lower():
        content += " work_from_home wfh remote_work l√†m_vi·ªác_t·∫°i_nh√†"
    
    if "thi·∫øt b·ªã" in content.lower() and "y√™u c·∫ßu" in content.lower():
        content += " trang_b·ªã_c·∫ßn_thi·∫øt ƒëi·ªÅu_ki·ªán_k·ªπ_thu·∫≠t"
    
    # Extract entities
    if doc.get("document_type") == "policy":
        content += " ch√≠nh_s√°ch quy_ƒë·ªãnh"
    
    doc["enriched_content"] = content
    return doc
```

**L∆∞u √Ω:** ƒê√¢y KH√îNG ph·∫£i l√† keyword stuffing b·∫©n. B·∫°n ƒëang:
- Th√™m c√°c bi·∫øn th·ªÉ ng√¥n ng·ªØ h·ª£p l√Ω (synonyms, abbreviations)
- Gi√∫p BM25 match ƒë∆∞·ª£c v·ªõi c√°c c√°ch h·ªèi kh√°c nhau
- V·∫´n d·ª±a v√†o thu·∫≠t to√°n BM25 g·ªëc ƒë·ªÉ rank

### B. **Chunking Strategy:**

```python
def smart_chunking(document):
    """
    Chunk theo semantic units, kh√¥ng ph·∫£i fixed size
    """
    # ∆Øu ti√™n t√°ch theo structure
    chunks = []
    
    # 1. Title + First paragraph = Overview chunk
    chunks.append({
        "content": doc["title"] + "\n" + doc["intro"],
        "type": "overview"
    })
    
    # 2. M·ªói section = 1 chunk
    for section in doc["sections"]:
        chunks.append({
            "content": section["heading"] + "\n" + section["content"],
            "type": "section"
        })
    
    # 3. ƒê·∫£m b·∫£o context overlap
    for i in range(len(chunks) - 1):
        chunks[i]["content"] += "\n[Preview: " + chunks[i+1]["content"][:100] + "...]"
    
    return chunks
```

**V·∫•n ƒë·ªÅ hi·ªán t·∫°i:**
- Chunk "Ch√≠nh S√°ch L√†m Vi·ªác T·ª´ Xa" ch·ªâ c√≥ ~100 t·ª´
- C√≥ th·ªÉ thi·∫øu context v·ªÅ "thi·∫øt b·ªã b·∫Øt bu·ªôc"
- C·∫ßn xem l·∫°i chi·∫øn l∆∞·ª£c chunk c√≥ bao g·ªìm ƒë·ªß th√¥ng tin kh√¥ng

## 4. **C·∫£i thi·ªán Corpus Statistics**

```python
# Ki·ªÉm tra IDF c·ªßa c√°c terms
import math
from collections import Counter

def analyze_idf(corpus):
    """
    Terms xu·∫•t hi·ªán trong nhi·ªÅu docs s·∫Ω c√≥ IDF th·∫•p
    N·∫øu "thi·∫øt b·ªã" xu·∫•t hi·ªán ·ªü 15/19 docs ‚Üí IDF th·∫•p ‚Üí kh√¥ng discriminative
    """
    doc_freq = Counter()
    for doc in corpus:
        unique_terms = set(doc.split())
        doc_freq.update(unique_terms)
    
    N = len(corpus)
    for term, df in doc_freq.most_common(20):
        idf = math.log((N - df + 0.5) / (df + 0.5))
        print(f"{term}: df={df}, idf={idf:.3f}")

# N·∫øu th·∫•y:
# "thi·∫øt_b·ªã": df=15, idf=0.2  ‚Üê Xu·∫•t hi·ªán qu√° nhi·ªÅu, kh√¥ng ph√¢n bi·ªát ƒë∆∞·ª£c
# "wfh": df=1, idf=2.9        ‚Üê Hi·∫øm, r·∫•t discriminative
```

**Gi·∫£i ph√°p:**
- N·∫øu corpus c√≥ nhi·ªÅu docs v·ªÅ "thi·∫øt b·ªã k·ªπ thu·∫≠t" ‚Üí term "thi·∫øt b·ªã" kh√¥ng c√≤n ƒë·∫∑c tr∆∞ng
- C·∫ßn th√™m context: "thi·∫øt_b·ªã + l√†m_vi·ªác_nh√†" th√¨ m·ªõi discriminative

## 5. **Evaluation Framework**

ƒê·ªÉ bi·∫øt c·∫£i thi·ªán n√†o hi·ªáu qu·∫£, c·∫ßn:

```python
# 1. T·∫°o test set
test_queries = [
    {
        "query": "T√¥i mu·ªën l√†m vi·ªác ·ªü nh√† th√¨ c·∫ßn trang b·ªã g√¨?",
        "relevant_docs": ["8cb0ec19-21a4-4f84-98c8-df9c8b765913"],  # WFH Policy
        "expected_rank": 1
    },
    {
        "query": "ƒêi·ªÅu ki·ªán ƒë·ªÉ ƒë∆∞·ª£c remote",
        "relevant_docs": ["8cb0ec19-21a4-4f84-98c8-df9c8b765913"],
        "expected_rank": 1
    },
    # ... th√™m 20-50 queries
]

# 2. Evaluate
def evaluate_config(bm25_params, preprocessing_fn):
    mrr_scores = []
    
    for test in test_queries:
        results = search(test["query"], bm25_params, preprocessing_fn)
        
        # T√¨m v·ªã tr√≠ c·ªßa relevant doc
        rank = None
        for i, result in enumerate(results):
            if result["document_id"] in test["relevant_docs"]:
                rank = i + 1
                break
        
        if rank:
            mrr_scores.append(1.0 / rank)
        else:
            mrr_scores.append(0.0)
    
    return sum(mrr_scores) / len(mrr_scores)

# 3. Grid search
best_config = None
best_score = 0

for k1 in [0.9, 1.2, 1.5, 1.8]:
    for b in [0.3, 0.5, 0.7, 0.9]:
        score = evaluate_config({"k1": k1, "b": b}, better_tokenize)
        if score > best_score:
            best_score = score
            best_config = {"k1": k1, "b": b}

print(f"Best: {best_config}, MRR: {best_score:.3f}")
```

## 6. **Action Items - ∆Øu ti√™n cao ƒë·∫øn th·∫•p**

### Priority 1: **C·∫£i thi·ªán Tokenization** (·∫¢nh h∆∞·ªüng l·ªõn nh·∫•t)
```python
# TODO:
# - X√≥a th√™m stopwords: "g√¨", "n√†o", "ƒë√¢u", "sao"
# - Merge compound words: "l√†m_vi·ªác", "trang_b·ªã", "thi·∫øt_b·ªã"
# - Test v·ªõi query c·ªßa b·∫°n
```

### Priority 2: **Tune k1, b**
```python
# TODO:
# - Th·ª≠ k1=1.2, b=0.5
# - T·∫°o 10-20 test queries
# - Measure MRR tr∆∞·ªõc v√† sau
```

### Priority 3: **Ki·ªÉm tra Chunking**
```python
# TODO:
# - In ra chunk th·ª±c t·∫ø c·ªßa document "Ch√≠nh S√°ch WFH"
# - ƒê·∫£m b·∫£o ph·∫ßn "Thi·∫øt b·ªã b·∫Øt bu·ªôc" n·∫±m trong 1 chunk ho√†n ch·ªânh
# - Overlap 1-2 c√¢u gi·ªØa c√°c chunks
```

### Priority 4: **Enrich Index Content**
```python
# TODO:
# - Th√™m synonyms t·ª± nhi√™n: "wfh", "remote", "work from home"
# - Kh√¥ng ph·∫£i tricks, ch·ªâ l√† normalize language variations
```

---

## C√¢u h·ªèi cho b·∫°n:

1. **B·∫°n c√≥ th·ªÉ share code preprocessing hi·ªán t·∫°i kh√¥ng?** (Tokenization, stopwords list)
2. **Chunking hi·ªán t·∫°i nh∆∞ th·∫ø n√†o?** Fixed size hay theo structure?
3. **C√≥ th·ªÉ t·∫°o ~20 test queries kh√¥ng?** ƒê·ªÉ measure improvement kh√°ch quan

T√¥i s·∫Ω gi√∫p b·∫°n optimize c·ª• th·ªÉ h∆°n khi c√≥ th√™m th√¥ng tin n√†y!

---

C·∫£m ∆°n b·∫°n ƒë√£ g·ª≠i ƒë·∫ßy ƒë·ªß t√†i li·ªáu! B√¢y gi·ªù t√¥i c√≥ th·ªÉ ƒë∆∞a ra ƒë√°nh gi√° v√† ƒë·ªÅ xu·∫•t c·∫£i thi·ªán **BM25 thu·∫ßn t√∫y** d·ª±a tr√™n code th·ª±c t·∫ø.

## üìä **Ph√¢n t√≠ch v·∫•n ƒë·ªÅ t·ª´ code**

### üî¥ **V·∫•n ƒë·ªÅ 1: Stopwords encoding sai**

```python
# Trong bm25_indexer.py line 45-48
self.vietnamese_stop_words = {
    'v√É ', 'c√°¬ª¬ßa', 'l√É ', 'c√É¬≥', '√Ñ'√Ü¬∞√°¬ª¬£c', 'm√°¬ª‚Ñ¢t', 'c√É¬°c', 'n√É y', ...
}
```

**‚ùå NGHI√äM TR·ªåNG**: Stopwords b·ªã encode sai (UTF-8 broken)
- `'v√É '` ph·∫£i l√† `'v√†'`
- `'c√°¬ª¬ßa'` ph·∫£i l√† `'c·ªßa'`
- `'l√É '` ph·∫£i l√† `'l√†'`

**H·∫≠u qu·∫£**: Kh√¥ng stopword n√†o ƒë∆∞·ª£c l·ªçc ƒë√∫ng ‚Üí noise cao ‚Üí document MikroTik match nhi·ªÅu terms v√¥ nghƒ©a.

### üî¥ **V·∫•n ƒë·ªÅ 2: Vietnamese normalization kh√¥ng ƒë·ªß m·∫°nh**

```python
# vietnamese_text_analyzer.py line 242-280
def normalize_for_bm25(self, text: str, remove_stopwords: bool = False) -> str:
    normalized = self.normalize_for_search(text)
    if remove_stopwords:
        tokens = normalized.split()
        tokens = [t for t in tokens if t not in self.vietnamese_stop_words]
    return normalized
```

**‚ùå V·∫•n ƒë·ªÅ**:
1. **Kh√¥ng x·ª≠ l√Ω compound words** ("l√†m vi·ªác", "trang b·ªã" b·ªã t√°ch th√†nh "lam", "viec")
2. **remove_stopwords=False** trong `_generate_bm25_tokens()` ‚Üí stopwords kh√¥ng b·ªã l·ªçc
3. **Kh√¥ng m·ªü r·ªông synonyms** (WFH, work from home, l√†m vi·ªác t·ª´ xa)

### üî¥ **V·∫•n ƒë·ªÅ 3: Parameters k1, b ch∆∞a tune cho ti·∫øng Vi·ªát**

```python
# bm25_indexer.py line 38
def __init__(self, k1: float = 1.5, b: float = 0.75):
```

**‚ùå Default values** (k1=1.5, b=0.75) l√† cho ti·∫øng Anh
- Ti·∫øng Vi·ªát c√≥ nhi·ªÅu t·ª´ ƒë∆°n √¢m ti·∫øt ‚Üí term frequency cao h∆°n
- Document d√†i (policy) b·ªã penalty qu√° m·ª©c

### üî¥ **V·∫•n ƒë·ªÅ 4: Chunking kh√¥ng t·ªëi ∆∞u cho BM25**

T·ª´ document 6:
- **Chunk size**: 50-250 t·ª´ (qu√° wide range)
- **Adaptive structural** ‚Üí chunk kh√¥ng ƒë·ªìng ƒë·ªÅu
- **M·ªôt s·ªë chunk < 100 t·ª´** ‚Üí IDF kh√¥ng ƒë·ªß discriminative

---

## ‚úÖ **Gi·∫£i ph√°p c·∫£i thi·ªán BM25 (kh√¥ng boost gian l·∫≠n)**

### **Priority 1: Fix stopwords ngay l·∫≠p t·ª©c** üî•

```python
# bm25_indexer.py - THAY TH·∫æ stopwords
self.vietnamese_stop_words = {
    # Core stopwords
    'v√†', 'c·ªßa', 'l√†', 'c√≥', 'ƒë∆∞·ª£c', 'm·ªôt', 'c√°c', 'n√†y', 'ƒë√≥', 'ƒë·ªÉ',
    'trong', 'v·ªõi', 't·ª´', 'khi', 'nh∆∞', 'theo', 'v·ªÅ', 'cho', 'b·ªüi',
    'm√†', 'nh·ªØng', 'ng∆∞·ªùi', 'vi·ªác', 't·∫°i', 'ƒë√£', 's·∫Ω', 'b·ªã', 'hay',
    'kh√¥ng', 'c√≤n', 'n·∫øu', 'th√¨', 'ho·∫∑c', 'nh∆∞ng', 'm·ªói', 'v√†o',
    'ch·ªâ', 'c≈©ng', 'r·∫±ng', 'sau', 'tr∆∞·ªõc', 'l·∫°i', 'ƒë√¢y', 'ƒë√≥',
    
    # Question words (critical for your query!)
    'g√¨', 'n√†o', 'ƒë√¢u', 'sao', 'ai', 'bao', 'gi·ªù', 'l√∫c',
    
    # Pronouns
    't√¥i', 'b·∫°n', 'anh', 'ch·ªã', 'em', 'n√≥', 'h·ªç',
    
    # Common verbs that don't help discrimination
    'c·∫ßn', 'mu·ªën', 'ph·∫£i', 'n√™n'  # ‚Üê C·∫®N TH·∫¨N: 'c·∫ßn' g√¢y noise
}
```

**Test impact**: Query "c·∫ßn trang b·ªã g√¨" ‚Üí ch·ªâ c√≤n "trang b·ªã"

---

### **Priority 2: X·ª≠ l√Ω compound words trong normalization**

```python
# vietnamese_text_analyzer.py - TH√äM METHOD M·ªöI
class VietnameseTextAnalyzer:
    
    def __init__(self):
        # ... existing code ...
        
        # TH√äM: Compound word dictionary
        self.compound_words_dict = {
            ('l√†m', 'vi·ªác'): 'l√†m_vi·ªác',
            ('trang', 'b·ªã'): 'trang_b·ªã', 
            ('thi·∫øt', 'b·ªã'): 'thi·∫øt_b·ªã',
            ('vƒÉn', 'ph√≤ng'): 'vƒÉn_ph√≤ng',
            ('t·ª´', 'xa'): 't·ª´_xa',
            ('work', 'from', 'home'): 'work_from_home',
            ('k·ªπ', 'thu·∫≠t'): 'k·ªπ_thu·∫≠t',
            ('y√™u', 'c·∫ßu'): 'y√™u_c·∫ßu',
            ('ƒëi·ªÅu', 'ki·ªán'): 'ƒëi·ªÅu_ki·ªán',
        }
    
    def merge_compound_words(self, tokens: List[str]) -> List[str]:
        """
        Merge Vietnamese compound words
        Example: ['l√†m', 'vi·ªác', 't·∫°i', 'nh√†'] ‚Üí ['l√†m_vi·ªác', 't·∫°i', 'nh√†']
        """
        result = []
        i = 0
        
        while i < len(tokens):
            # Try trigram first
            if i < len(tokens) - 2:
                trigram = (tokens[i], tokens[i+1], tokens[i+2])
                if trigram in self.compound_words_dict:
                    result.append(self.compound_words_dict[trigram])
                    i += 3
                    continue
            
            # Try bigram
            if i < len(tokens) - 1:
                bigram = (tokens[i], tokens[i+1])
                if bigram in self.compound_words_dict:
                    result.append(self.compound_words_dict[bigram])
                    i += 2
                    continue
            
            # Keep original token
            result.append(tokens[i])
            i += 1
        
        return result
    
    def normalize_for_bm25(self, text: str, remove_stopwords: bool = True) -> str:  # ‚Üê ƒê·ªîI default=True
        """Enhanced normalization for BM25"""
        if not text or not text.strip():
            return ""
        
        # 1. Remove accents
        text = self.remove_vietnamese_accents(text)
        
        # 2. Tokenize
        tokens = self._segment_words(text)
        
        # 3. TH√äM: Merge compound words TR∆Ø·ªöC KHI remove stopwords
        tokens = self.merge_compound_words(tokens)
        
        # 4. Remove stopwords (now enabled by default)
        if remove_stopwords:
            tokens = [t for t in tokens if t not in self.vietnamese_stop_words]
        
        # 5. Clean and lowercase
        cleaned_tokens = []
        for token in tokens:
            clean_token = re.sub(r'[^\w\s]', '', token.lower())
            if len(clean_token) > 1 and not clean_token.isdigit():
                cleaned_tokens.append(clean_token)
        
        return ' '.join(cleaned_tokens)
```

**Expected improvement**:
- Query: "l√†m vi·ªác ·ªü nh√† c·∫ßn trang b·ªã g√¨" 
- Normalized: "l√†m_vi·ªác nh√† trang_b·ªã" (ch·ªâ 3 terms, ch·∫•t l∆∞·ª£ng cao)

---

### **Priority 3: Tune BM25 parameters cho ti·∫øng Vi·ªát**

```python
# bm25_indexer.py - THAY ƒê·ªîI constructor
class EnhancedBM25Indexer:
    
    def __init__(self, k1: float = 1.2, b: float = 0.5):  # ‚Üê TUNE CHO TI·∫æNG VI·ªÜT
        """
        Vietnamese-optimized BM25 parameters:
        
        k1 = 1.2 (gi·∫£m t·ª´ 1.5):
          - Gi·∫£m impact c·ªßa term frequency
          - Tr√°nh document c√≥ nhi·ªÅu "thi·∫øt b·ªã", "c·∫ßn" win unfairly
          
        b = 0.5 (gi·∫£m t·ª´ 0.75):
          - Gi·∫£m penalty cho document d√†i (policy docs)
          - Ti·∫øng Vi·ªát c√≥ nhi·ªÅu t·ª´ l·∫∑p l·∫°i h·ª£p ph√°p
        """
        self.k1 = k1
        self.b = b
        # ... rest of code ...
```

**Justification**:
- k1=1.2: term xu·∫•t hi·ªán 5 l·∫ßn ch·ªâ t·ªët h∆°n 50% so v·ªõi 3 l·∫ßn (thay v√¨ 67%)
- b=0.5: document 500 t·ª´ ch·ªâ b·ªã penalty 25% (thay v√¨ 50%)

---

### **Priority 4: Fix query processing trong simple_import_processor.py**

```python
# simple_import_processor.py line 603
# HI·ªÜN T·∫†I:
normalized_text = self.vietnamese_analyzer.normalize_for_bm25(
    chunk['chunk_content'],
    remove_stopwords=False  # ‚Üê SAI!
)

# S·ª¨A TH√ÄNH:
normalized_text = self.vietnamese_analyzer.normalize_for_bm25(
    chunk['chunk_content'],
    remove_stopwords=True  # ‚Üê ƒê√öNG: Remove stopwords
)
```

---

### **Priority 5: Query expansion (kh√¥ng ph·∫£i boost)**

```python
# bm25_indexer.py - TH√äM method
class EnhancedBM25Indexer:
    
    def expand_query_terms(self, query: str) -> str:
        """
        Expand query with Vietnamese synonyms and common variations
        This is NOT cheating - just normalizing language variations
        """
        # Synonym map
        synonyms = {
            'wfh': ['l√†m_vi·ªác', 't·ª´_xa', 'work_from_home'],
            'remote': ['t·ª´_xa', 'l√†m_vi·ªác'],
            'trang_b·ªã': ['thi·∫øt_b·ªã', 'y√™u_c·∫ßu', 'ƒëi·ªÅu_ki·ªán'],
            'chu·∫©n_b·ªã': ['trang_b·ªã', 'thi·∫øt_b·ªã'],
            'nh√†': ['home', 't·∫°i_nh√†'],
        }
        
        tokens = query.split()
        expanded = list(tokens)  # Keep original
        
        for token in tokens:
            if token in synonyms:
                expanded.extend(synonyms[token])
        
        return ' '.join(set(expanded))  # Deduplicate
    
    async def search_with_enhanced_bm25(
        self,
        query: str,
        conn: asyncpg.Connection,
        limit: int = 10,
        min_score: float = 0.1
    ) -> List[Dict]:
        """Search with query expansion"""
        
        # Normalize query
        normalized_terms = self.tokenize_vietnamese_text(query)
        normalized_query = ' '.join(normalized_terms)
        
        # TH√äM: Expand query
        expanded_query = self.expand_query_terms(normalized_query)
        
        logger.info(f"Original query: {normalized_query}")
        logger.info(f"Expanded query: {expanded_query}")
        
        # Search with expanded query
        search_results = await conn.fetch("""
            SELECT * FROM search_bm25_with_global_terms($1, $2, $3)
        """, expanded_query, limit, min_score)
        
        # ... rest of code ...
```

---

### **Priority 6: Re-index to√†n b·ªô v·ªõi config m·ªõi**

Sau khi fix code, b·∫°n c·∫ßn re-index:

```bash
# 1. Backup current data
pg_dump -t bm25_global_terms -t document_bm25_index > bm25_backup.sql

# 2. Clear BM25 tables
psql -d your_db -c "TRUNCATE bm25_global_terms, document_bm25_index CASCADE;"

# 3. Re-import v·ªõi code m·ªõi
python -m src.core.pipeline.simple_import_processor /path/to/exports/

# 4. Verify statistics
curl http://localhost:8000/api/v1/documents/stats
```

---

## üìà **Expected Improvements**

### **Tr∆∞·ªõc khi fix:**
```json
{
  "rank": 3,
  "title": "Ch√≠nh S√°ch WFH",
  "bm25_score": 2.5478,
  "matched_terms": 5,
  "total_terms": 7
}
```

### **Sau khi fix:**
```json
{
  "rank": 1,
  "title": "Ch√≠nh S√°ch WFH", 
  "bm25_score": 8.2341,  // ‚Üê TƒÉng 3x
  "matched_terms": 3,    // ‚Üê Gi·∫£m noise
  "total_terms": 3       // ‚Üê Query ng·∫Øn h∆°n, ch·∫•t l∆∞·ª£ng cao h∆°n
}
```

**L√Ω do**:
- `l√†m_vi·ªác` + `trang_b·ªã` match ch√≠nh x√°c section "Y√™u c·∫ßu k·ªπ thu·∫≠t"
- Stopwords lo·∫°i b·ªè ‚Üí IDF c·ªßa remaining terms cao h∆°n
- k1, b tune ‚Üí document d√†i kh√¥ng b·ªã penalty qu√°

---

## üß™ **Test Plan**

T·∫°o file `test_bm25_improvements.py`:

```python
import asyncio
from src.core.search.bm25_indexer import EnhancedBM25Indexer
from src.core.database.connection import get_db_pool

test_queries = [
    ("T√¥i mu·ªën l√†m vi·ªác ·ªü nh√† th√¨ c·∫ßn trang b·ªã g√¨?", "8cb0ec19-21a4-4f84-98c8-df9c8b765913"),
    ("ƒêi·ªÅu ki·ªán ƒë·ªÉ ƒë∆∞·ª£c remote", "8cb0ec19-21a4-4f84-98c8-df9c8b765913"),
    ("Thi·∫øt b·ªã b·∫Øt bu·ªôc cho WFH", "8cb0ec19-21a4-4f84-98c8-df9c8b765913"),
    ("Y√™u c·∫ßu k·ªπ thu·∫≠t l√†m vi·ªác t·ª´ xa", "8cb0ec19-21a4-4f84-98c8-df9c8b765913"),
]

async def test_mrr():
    pool = await get_db_pool()
    indexer = EnhancedBM25Indexer()
    
    mrr_scores = []
    for query, expected_doc_id in test_queries:
        async with pool.acquire() as conn:
            results = await indexer.search_with_enhanced_bm25(query, conn, limit=10)
            
            rank = None
            for i, result in enumerate(results, 1):
                if result['document_id'] == expected_doc_id:
                    rank = i
                    break
            
            if rank:
                mrr_scores.append(1.0 / rank)
                print(f"‚úÖ Query: '{query}' ‚Üí Rank {rank}")
            else:
                mrr_scores.append(0.0)
                print(f"‚ùå Query: '{query}' ‚Üí Not found in top 10")
    
    mrr = sum(mrr_scores) / len(mrr_scores)
    print(f"\nüìä Mean Reciprocal Rank: {mrr:.3f}")
    
    await pool.close()

if __name__ == "__main__":
    asyncio.run(test_mrr())
```

**Target**: MRR > 0.8 (hi·ªán t·∫°i ~0.33)

---

## üìù **Action Checklist**

- [ ] Fix stopwords encoding trong `bm25_indexer.py`
- [ ] Th√™m compound words handling trong `vietnamese_text_analyzer.py`
- [ ] Tune k1=1.2, b=0.5
- [ ] Enable remove_stopwords=True trong `simple_import_processor.py`
- [ ] Th√™m query expansion (optional nh∆∞ng highly recommended)
- [ ] Re-index to√†n b·ªô corpus
- [ ] Run test suite v·ªõi 20-30 queries
- [ ] Measure MRR before/after

---

B·∫°n mu·ªën t√¥i t·∫°o **pull request ready code** v·ªõi t·∫•t c·∫£ changes tr√™n kh√¥ng? T√¥i c√≥ th·ªÉ gen ra c√°c file patch ngay.
