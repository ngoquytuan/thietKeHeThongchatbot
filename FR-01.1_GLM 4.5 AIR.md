I'll implement the Vietnamese embedding model evaluation system according to the tutorial document you provided. Let me start by creating the directory structure and implementing the core files.

First, I'll create the directory structure and the main configuration files:

## Project Structure and Configuration Files
```bash
# Create directory structure
mkdir -p vietnamese_embedding_evaluator/{configs/{data,models,evaluation},data/{raw/{company_documents,external_datasets},processed,ground_truth},src,notebooks,reports/{performance_charts},scripts,tests}
```

Now, let's implement the core files:

### requirements.txt
```txt
# Embedding & ML Libraries
sentence-transformers>=2.2.2
transformers>=4.21.0
torch>=2.0.0
numpy>=1.21.0
scikit-learn>=1.1.0
accelerate>=0.20.0
cuda-python>=12.0.0

# Vietnamese Text Processing
pyvi>=0.1.1
regex>=2022.7.9
unicodedata2>=15.0.0

# Data Processing & Analysis
pandas>=1.5.0
scipy>=1.9.0

# Visualization & Reporting
matplotlib>=3.5.0
seaborn>=0.11.0
plotly>=5.10.0
jinja2>=3.1.0

# Utilities
tqdm>=4.64.0
python-dotenv>=0.19.0
pydantic>=1.10.0
typer>=0.6.0
```

### setup.py
```python
from setuptools import setup, find_packages

setup(
    name="vietnamese-embedding-evaluator",
    version="1.0.0",
    description="A comprehensive framework for evaluating Vietnamese embedding models",
    author="AI Team",
    author_email="ai.team@example.com",
    packages=find_packages(),
    python_requires=">=3.8",
    install_requires=[
        "sentence-transformers>=2.2.2",
        "transformers>=4.21.0",
        "torch>=2.0.0",
        "numpy>=1.21.0",
        "scikit-learn>=1.1.0",
        "accelerate>=0.20.0",
        "cuda-python>=12.0.0",
        "pyvi>=0.1.1",
        "regex>=2022.7.9",
        "unicodedata2>=15.0.0",
        "pandas>=1.5.0",
        "scipy>=1.9.0",
        "matplotlib>=3.5.0",
        "seaborn>=0.11.0",
        "plotly>=5.10.0",
        "jinja2>=3.1.0",
        "tqdm>=4.64.0",
        "python-dotenv>=0.19.0",
        "pydantic>=1.10.0",
        "typer>=0.6.0",
    ],
    entry_points={
        "console_scripts": [
            "run-evaluation=scripts.run_evaluation:main",
            "prepare-data=scripts.prepare_data:main",
            "export-results=scripts.export_results:main",
        ],
    },
    classifiers=[
        "Development Status :: 4 - Beta",
        "Intended Audience :: Developers",
        "License :: OSI Approved :: MIT License",
        "Programming Language :: Python :: 3",
        "Programming Language :: Python :: 3.8",
        "Programming Language :: Python :: 3.9",
        "Programming Language :: Python :: 3.10",
        "Programming Language :: Python :: 3.11",
    ],
)
```

### configs/models.json
```json
{
  "models": [
    {
      "name": "vietnamese_embedding_v1",
      "model_id": "AITeamVN/Vietnamese_Embedding",
      "provider": "huggingface",
      "max_seq_length": 512,
      "batch_size": 32,
      "normalize_embeddings": true,
      "priority": 1
    },
    {
      "name": "qwen3_embedding",
      "model_id": "Qwen/Qwen2.5-72B-Instruct",
      "provider": "huggingface",
      "max_seq_length": 512,
      "batch_size": 16,
      "normalize_embeddings": true,
      "priority": 2
    },
    {
      "name": "multilingual_minilm",
      "model_id": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
      "provider": "huggingface",
      "max_seq_length": 512,
      "batch_size": 32,
      "normalize_embeddings": true,
      "priority": 3
    },
    {
      "name": "multilingual_e5",
      "model_id": "intfloat/multilingual-e5-base",
      "provider": "huggingface",
      "max_seq_length": 512,
      "batch_size": 32,
      "normalize_embeddings": true,
      "priority": 4
    },
    {
      "name": "bge_m3",
      "model_id": "BAAI/bge-m3",
      "provider": "huggingface",
      "max_seq_length": 512,
      "batch_size": 32,
      "normalize_embeddings": true,
      "priority": 5
    },
    {
      "name": "vietnamese_sbert",
      "model_id": "keepitreal/vietnamese-sbert",
      "provider": "huggingface",
      "max_seq_length": 512,
      "batch_size": 32,
      "normalize_embeddings": true,
      "priority": 6
    }
  ],
  "evaluation_settings": {
    "top_k": [1, 3, 5, 10],
    "similarity_threshold": 0.7,
    "batch_processing": true,
    "num_runs": 3,
    "random_seed": 42
  }
}
```

### configs/evaluation_settings.json
```json
{
  "test_cases_file": "data/ground_truth/query_document_pairs.json",
  "corpus_file": "data/processed/cleaned_corpus.json",
  "output_dir": "reports",
  "metrics": {
    "hit_rate": true,
    "mrr": true,
    "ndcg": true,
    "precision_recall": true
  },
  "performance_metrics": {
    "measure_speed": true,
    "measure_memory": true,
    "measure_gpu_utilization": true
  },
  "categories": [
    "general_knowledge",
    "technical_documents",
    "process_policy",
    "product_info",
    "cross_department"
  ]
}
```

### configs/gpu_settings.json
```json
{
  "device_detection": {
    "cuda_available": true,
    "cuda_version": "11.8",
    "gpu_count": 1,
    "gpu_memory": 16384,
    "gpu_name": "NVIDIA GeForce RTX 3090"
  },
  "optimization_settings": {
    "mixed_precision": true,
    "fp16": true,
    "batch_size_scaling": 1.5,
    "gradient_accumulation": 1
  },
  "memory_optimization": {
    "max_gpu_memory_percent": 85,
    "offload_to_cpu": true,
    "gradient_checkpointing": false
  }
}
```

### src/__init__.py
```python
"""
Vietnamese Embedding Evaluator
==============================

A comprehensive framework for evaluating Vietnamese embedding models.
"""

__version__ = "1.0.0"
__author__ = "AI Team"
__email__ = "ai.team@example.com"

from .data_processor import VietnameseTextProcessor
from .embedding_manager import EmbeddingManager
from .evaluator import ModelEvaluator
from .metrics import calculate_hit_rate, calculate_mrr, calculate_ndcg
from .gpu_optimizer import GPUOptimizer
from .visualizer import ReportGenerator

__all__ = [
    "VietnameseTextProcessor",
    "EmbeddingManager", 
    "ModelEvaluator",
    "calculate_hit_rate",
    "calculate_mrr",
    "calculate_ndcg",
    "GPUOptimizer",
    "ReportGenerator"
]
```

### src/data_processor.py
```python
"""
Data Processing Module for Vietnamese Text
=========================================

This module handles all data preprocessing tasks for Vietnamese text including:
- Text cleaning and normalization
- Unicode normalization
- Intelligent chunking of documents
- Creation of test queries
"""

import unicodedata2 as unicodedata
import re
import json
from typing import List, Dict, Tuple
from pathlib import Path
import pandas as pd
import numpy as np


class VietnameseTextProcessor:
    """
    Vietnamese text processing class with specialized handling for Vietnamese language.
    """
    
    def __init__(self):
        """Initialize the Vietnamese text processor with necessary components."""
        self.default_chunk_size = 512
        self.max_chunk_size = 8192
        
    def normalize_unicode(self, text: str) -> str:
        """
        Normalize Vietnamese Unicode characters.
        
        Args:
            text: Input text to normalize
            
        Returns:
            Normalized text
        """
        # Normalize to composed form (NFC) then to decomposed form (NFD) then back to composed
        text = unicodedata.normalize('NFC', text)
        text = unicodedata.normalize('NFD', text)
        return unicodedata.normalize('NFC', text)
    
    def remove_special_chars(self, text: str) -> str:
        """
        Remove special characters while keeping Vietnamese diacritics.
        
        Args:
            text: Input text to clean
            
        Returns:
            Text without special characters
        """
        # Keep Vietnamese characters, digits, punctuation and whitespace
        pattern = r'[^\u0020-\u007E\u0080-\u00FF\u0100-\u017F\u1E00-\u1EFF0-9,;.\-\?!]'
        return re.sub(pattern, '', text)
    
    def normalize_whitespace(self, text: str) -> str:
        """
        Normalize whitespace sequences.
        
        Args:
            text: Input text
            
        Returns:
            Text with normalized whitespace
        """
        # Replace multiple spaces with single space
        return ' '.join(text.split())
    
    def clean_text(self, text: str) -> str:
        """
        comprehensive text cleaning pipeline for Vietnamese.
        
        Args:
            text: Input text to clean
            
        Returns:
            Cleaned text
        """
        # Apply all cleaning steps in sequence
        text = self.normalize_unicode(text)
        text = self.remove_special_chars(text)
        text = self.normalize_whitespace(text)
        
        # Handle Vietnamese-specific patterns
        text = self._handle_vietnamese_patterns(text)
        
        return text.strip()
    
    def _handle_vietnamese_patterns(self, text: str) -> str:
        """
        Handle Vietnamese-specific text patterns.
        
        Args:
            text: Input text
            
        Returns:
            Processed text with Vietnamese patterns handled
        """
        # Normalize Vietnamese quotation marks
        replacements = {
            "''": '"',
            '``': '"',
            '«': '"',
            '»': '"',
            '„': '"',
            '‟': '"'
        }
        
        for old, new in replacements.items():
            text = text.replace(old, new)
            
        return text
    
    def tokenize_vietnamese(self, text: str) -> List[str]:
        """
        Tokenize Vietnamese text.
        
        Args:
            text: Input text to tokenize
            
        Returns:
            List of tokens
        """
        # Simple whitespace tokenization - can be enhanced with Vietnamese tokenizers
        return text.split()
    
    def split_into_sentences(self, text: str) -> List[str]:
        """
        Split text into sentences respecting Vietnamese punctuation.
        
        Args:
            text: Input text
            
        Returns:
            List of sentences
        """
        # Split on common sentence delimiters
        sentence_endings = r'[.!?;]+'
        sentences = re.split(sentence_endings, text)
        
        # Clean up sentences
        sentences = [s.strip() for s in sentences if s.strip()]
        return sentences
    
    def create_chunks(self, text: str, chunk_size: int = None) -> List[str]:
        """
        Create chunks from text while respecting sentence boundaries.
        
        Args:
            text: Input text to chunk
            chunk_size: Maximum size of each chunk
            
        Returns:
            List of text chunks
        """
        if chunk_size is None:
            chunk_size = self.default_chunk_size
            
        # Edge case: text is shorter than chunk_size
        if len(text) <= chunk_size:
            return [text]
        
        # Split into sentences first
        sentences = self.split_into_sentences(text)
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            # Try to add sentence to current chunk
            test_chunk = current_chunk + " " + sentence if current_chunk else sentence
            
            if len(test_chunk) <= chunk_size:
                current_chunk = test_chunk
            else:
                # If current chunk is not empty, save it
                if current_chunk:
                    chunks.append(current_chunk)
                    current_chunk = sentence
                
                # If a single sentence is too long, split it by words
                if len(sentence) > chunk_size:
                    words = sentence.split()
                    temp_chunk = ""
                    
                    for word in words:
                        test_word_chunk = temp_chunk + " " + word if temp_chunk else word
                        
                        if len(test_word_chunk) <= chunk_size:
                            temp_chunk = test_word_chunk
                        else:
                            if temp_chunk:
                                chunks.append(temp_chunk)
                                temp_chunk = word
                            else:
                                # If a single word is too long, split it arbitrarily
                                chunks.append(word[:chunk_size])
                                remaining = word[chunk_size:]
                                if remaining:
                                    chunks.append(remaining)
                    
                    if temp_chunk:
                        current_chunk = temp_chunk
        
        # Add the last chunk if it's not empty
        if current_chunk:
            chunks.append(current_chunk)
            
        return chunks
    
    def create_documents_from_files(self, file_paths: List[Path], chunk_size: int = None) -> List[Dict]:
        """
        Create documents from a list of files.
        
        Args:
            file_paths: List of file paths to process
            chunk_size: Maximum size of each chunk
            
        Returns:
            List of document dictionaries
        """
        documents = []
        
        for file_path in file_paths:
            try:
                # Read file content based on extension
                if file_path.suffix == '.txt':
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                elif file_path.suffix in ['.pdf', '.docx']:
                    # Handle different file formats as needed
                    continue
                
                # Clean and chunk content
                cleaned_content = self.clean_text(content)
                chunks = self.create_chunks(cleaned_content, chunk_size)
                
                # Create document entries
                for i, chunk in enumerate(chunks):
                    doc = {
                        'document_id': f"{file_path.stem}_{i}",
                        'source': str(file_path),
                        'content': chunk,
                        'department': self._infer_department(content),
                        'page_number': i,
                        'word_count': len(self.tokenize_vietnamese(chunk))
                    }
                    documents.append(doc)
                    
            except Exception as e:
                print(f"Error processing file {file_path}: {str(e)}")
                
        return documents
    
    def _infer_department(self, text: str) -> str:
        """
        Infer department from text content.
        
        Args:
            text: Input text to analyze
            
        Returns:
            Department category
        """
        # Simple keyword-based department classification
        categories = {
            'technical': ['kỹ thuật', 'công nghệ', 'technical', 'technology', 'hướng dẫn'],
            'procurement': ['mua hàng', 'procurement', 'mua sắm', 'purchase'],
            'hr': ['nhân sự', 'human resources', 'hr', 'tuyển dụng'],
            'finance': ['tài chính', 'finance', 'kế toán', 'accounting'],
            'product': ['sản phẩm', 'product', 'đặc điểm', 'feature'],
            'general': ['chính sách', 'policy', 'quy định', 'regulation']
        }
        
        text_lower = text.lower()
        for category, keywords in categories.items():
            if any(keyword in text_lower for keyword in keywords):
                return category
                
        return 'general'
    
    def create_test_queries(self, documents: List[Dict], num_queries: int = 100) -> List[Dict]:
        """
        Create test queries from documents for evaluation.
        
        Args:
            documents: List of document dictionaries
            num_queries: Number of queries to generate
            
        Returns:
            List of query dictionaries
        """
        queries = []
        
        # Sample documents to create queries
        sample_docs = np.random.choice(len(documents), min(num_queries, len(documents)), replace=False)
        
        for i, doc_idx in enumerate(sample_docs):
            doc = documents[doc_idx]
            content = doc['content']
            
            # Extract key phrases from document as potential queries
            sentences = self.split_into_sentences(content)
            if len(sentences) > 0:
                # Use the first sentence as a baseline query
                baseline_query = sentences[0]
                
                # Variations of the query
                variations = [
                    baseline_query,
                    f"Câu hỏi về {baseline_query}",
                    f"Tìm thông tin về {baseline_query}",
                    f"Giải thích {baseline_query}"
                ]
                
                # Select a random variation
                query = np.random.choice(variations)
                
                query_entry = {
                    'query_id': f"Q{i+1:03d}",
                    'query': query,
                    'department': doc['department'],
                    'difficulty': np.random.choice(['easy', 'medium', 'hard'], p=[0.6, 0.3, 0.1]),
                    'relevant_documents': [doc['document_id']],
                    'explanation': f"Trích xuất từ tài liệu: {doc['source']}"
                }
                
                queries.append(query_entry)
                
        return queries
    
    def save_processed_data(self, documents: List[Dict], output_path: Path) -> None:
        """
        Save processed documents to file.
        
        Args:
            documents: List of document dictionaries
            output_path: Path to save the file
        """
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(documents, f, ensure_ascii=False, indent=2)
            
    def save_test_queries(self, queries: List[Dict], output_path: Path) -> None:
        """
        Save test queries to file.
        
        Args:
            queries: List of query dictionaries
            output_path: Path to save the file
        """
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump({'test_cases': queries}, f, ensure_ascii=False, indent=2)
```

### src/embedding_manager.py
```python
"""
Embedding Manager Module
========================

This module manages different embedding models and their usage.
"""

import time
import torch
import numpy as np
from typing import List, Dict, Union, Optional
from pathlib import Path
import logging
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
from dataclasses import dataclass

from .gpu_optimizer import GPUOptimizer
from .data_processor import VietnameseTextProcessor


logger = logging.getLogger(__name__)


@dataclass
class EmbeddingModelConfig:
    """Configuration for an embedding model."""
    name: str
    model_id: str
    provider: str
    max_seq_length: int
    batch_size: int
    normalize_embeddings: bool
    priority: int
    device: str = None
    model: Optional[SentenceTransformer] = None


class EmbeddingManager:
    """
    Manages loading, caching, and using multiple embedding models.
    """
    
    def __init__(self, gpu_optimizer: GPUOptimizer):
        """
        Initialize the Embedding Manager.
        
        Args:
            gpu_optimizer: GPU optimization instance
        """
        self.gpu_optimizer = gpu_optimizer
        self.models = {}
        self.model_configs = {}
        self.device = gpu_optimizer.device
        self.text_processor = VietnameseTextProcessor()
        
    def init_from_config(self, config_path: Path) -> None:
        """
        Initialize embedding models from a configuration file.
        
        Args:
            config_path: Path to the JSON configuration file
        """
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
            
        for model_config in config['models']:
            self.add_model(
                name=model_config['name'],
                model_id=model_config['model_id'],
                provider=model_config['provider'],
                max_seq_length=model_config.get('max_seq_length', 512),
                batch_size=model_config.get('batch_size', 32),
                normalize_embeddings=model_config.get('normalize_embeddings', True),
                priority=model_config.get('priority', 5)
            )
    
    def add_model(self, name: str, model_id: str, provider: str, 
                  max_seq_length: int = 512, batch_size: int = 32, 
                  normalize_embeddings: bool = True, priority: int = 5) -> None:
        """
        Add a new embedding model to the manager.
        
        Args:
            name: Name identifier for the model
            model_id: Model identifier (HF model ID)
            provider: Model provider
            max_seq_length: Maximum sequence length
            batch_size: Batch size for processing
            normalize_embeddings: Whether to normalize embeddings
            priority: Model priority for selection
        """
        # Use GPU optimizer to load the model on the best available device
        try:
            model = SentenceTransformer(model_id, device=self.device)
            self.gpu_optimizer.optimize_model_loading(name)
        except Exception as e:
            logger.error(f"Failed to load model {model_id}: {str(e)}")
            return
            
        config = EmbeddingModelConfig(
            name=name,
            model_id=model_id,
            provider=provider,
            max_seq_length=max_seq_length,
            batch_size=batch_size,
            normalize_embeddings=normalize_embeddings,
            priority=priority,
            device=self.device,
            model=model
        )
        
        self.models[name] = model
        self.model_configs[name] = config
        logger.info(f"Added embedding model: {name} ({model_id})")
    
    def get_model(self, name: str = None) -> SentenceTransformer:
        """
        Get an embedding model by name.
        
        Args:
            name: Model name. If None, returns the highest priority model.
            
        Returns:
            The SentenceTransformer model
        """
        if name is not None:
            return self.models.get(name)
            
        # Return the model with the highest priority (lowest priority number)
        if not self.model_configs:
            raise ValueError("No models loaded")
            
        best_model = min(self.model_configs.values(), key=lambda x: x.priority)
        return self.models[best_model.name]
    
    def set_priority(self, name: str, priority: int) -> None:
        """
        Set the priority of a model.
        
        Args:
            name: Model name
            priority: New priority value
        """
        if name in self.model_configs:
            self.model_configs[name].priority = priority
            logger.info(f"Updated priority for model {name} to {priority}")
    
    def generate_embeddings(self, texts: List[str], model_name: str = None) -> np.ndarray:
        """
        Generate embeddings for a list of texts using a specific model.
        
        Args:
            texts: List of texts to embed
            model_name: Name of the model to use
            
        Returns:
            Numpy array of embeddings
        """
        model = self.get_model(model_name)
        start_time = time.time()
        
        # If on GPU, use faster batch processing
        if torch.cuda.is_available():
            with torch.no_grad():
                embeddings = model.encode(
                    texts,
                    batch_size=self.model_configs[model_name].batch_size,
                    show_progress_bar=False,
                    normalize_embeddings=self.model_configs[model_name].normalize_embeddings
                )
        else:
            embeddings = model.encode(
                texts,
                batch_size=self.model_configs[model_name].batch_size,
                show_progress_bar=False,
                normalize_embeddings=self.model_configs[model_name].normalize_embeddings
            )
        
        processing_time = time.time() - start_time
        texts_per_second = len(texts) / processing_time
        logger.info(f"Generated {len(texts)} embeddings in {processing_time:.2f}s ({texts_per_second:.2f} texts/s)")
        
        return embeddings
    
    def generate_embeddings_batch(self, texts: List[str], model_name: str = None, 
                                 batch_size: int = None) -> np.ndarray:
        """
        Generate embeddings in batches with progress tracking.
        
        Args:
            texts: List of texts to embed
            model_name: Name of the model to use
            batch_size: Custom batch size for processing
            
        Returns:
            Numpy array of embeddings
        """
        model = self.get_model(model_name)
        actual_batch_size = batch_size or self.model_configs[model_name].batch_size
        
        all_embeddings = []
        start_time = time.time()
        
        # Process in batches
        for i in tqdm(range(0, len(texts), actual_batch_size), desc=f"Processing with {model_name}"):
            batch_texts = texts[i:i+actual_batch_size]
            embeddings = self.generate_embeddings(batch_texts, model_name)
            all_embeddings.append(embeddings)
            
        # Concatenate all batches
        final_embeddings = np.vstack(all_embeddings)
        total_time = time.time() - start_time
        texts_per_second = len(texts) / total_time
        
        logger.info(f"Batch processed {len(texts)} embeddings in {total_time:.2f}s ({texts_per_second:.2f} texts/s)")
        return final_embeddings
    
    def compare_models_parallel(self, texts: List[str]) -> Dict:
        """
        Generate embeddings using all loaded models in parallel.
        
        Args:
            texts: List of texts to embed
            
        Returns:
            Dictionary with model names as keys and embeddings as values
        """
        results = {}
        start_time = time.time()
        
        for model_name in self.model_configs.keys():
            logger.info(f"Processing with model: {model_name}")
            results[model_name] = self.generate_embeddings(texts, model_name)
            
        total_time = time.time() - start_time
        logger.info(f"Model comparison completed in {total_time:.2f}s for {len(texts)} texts")
        
        return results
    
    def find_similar_documents(self, query: str, document_embeddings: np.ndarray, 
                             model_name: str = None, top_k: int = 5) -> List[Dict]:
        """
        Find most similar documents to a query.
        
        Args:
            query: Query text
            document_embeddings: Already computed embeddings of documents
            model_name: Name of the model to use for query embedding
            top_k: Number of top results to return
            
        Returns:
            List of dictionaries with similarity scores and document indices
        """
        # Generate query embedding
        query_embedding = self.generate_embeddings([query], model_name)
        
        # Calculate cosine similarity
        from sklearn.metrics.pairwise import cosine_similarity
        similarities = cosine_similarity(query_embedding, document_embeddings)[0]
        
        # Get top-k indices
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        results = [
            {'document_index': idx, 'similarity_score': similarities[idx]}
            for idx in top_indices
        ]
        
        return results
    
    def benchmark_model_speed(self, model_name: str, test_texts: List[str], 
                            max_batch_size: int = 64) -> Dict:
        """
        Benchmark embedding generation speed for a model.
        
        Args:
            model_name: Name of the model to benchmark
            test_texts: Sample texts for benchmarking
            max_batch_size: Maximum batch size to test
            
        Returns:
            Dictionary with benchmark results
        """
        results = {
            'model_name': model_name,
            'num_texts': len(test_texts),
            'num_parameters': self._count_model_parameters(model_name),
            'runs': []
        }
        
        # Test different batch sizes
        batch_sizes = [1, 4, 8, 16, 32, 64]
        batch_sizes = [bs for bs in batch_sizes if bs <= max_batch_size]
        
        for batch_size in batch_sizes:
            if batch_size > len(test_texts):
                continue
                
            # Warm-up
            _ = self.generate_embeddings(test_texts[:10], model_name)
            
            # Benchmark
            times = []
            for _ in range(5):  # 5 runs
                start_time = time.time()
                _ = self.generate_embeddings(test_texts[:batch_size], model_name)
                times.append(time.time() - start_time)
            
            avg_time = np.mean(times)
            std_time = np.std(times)
            texts_per_sec = batch_size / avg_time
            
            results['runs'].append({
                'batch_size': batch_size,
                'avg_time_sec': avg_time,
                'std_time_sec': std_time,
                'texts_per_sec': texts_per_sec
            })
            
        return results
    
    def _count_model_parameters(self, model_name: str) -> int:
        """Count the number of parameters in a model."""
        model = self.get_model(model_name)
        return sum(p.numel() for p in model.parameters())
    
    def benchmark_memory_usage(self, model_name: str, test_texts: List[str]) -> Dict:
        """
        Benchmark memory usage for a model.
        
        Args:
            model_name: Name of the model to benchmark
            test_texts: Sample texts for benchmarking
            
        Returns:
            Dictionary with memory benchmark results
        """
        if not torch.cuda.is_available():
            return {'model_name': model_name, 'memory_mb': 'N/A', 'gpu_available': False}
        
        # Clear GPU cache
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats()
        
        # Record initial memory
        torch.cuda.synchronize()
        initial_memory = torch.cuda.memory_allocated() / (1024**2)  # MB
        
        # Generate embeddings
        _ = self.generate_embeddings(test_texts, model_name)
        torch.cuda.synchronize()
        
        # Record peak memory
        peak_memory = torch.cuda.max_memory_allocated() / (1024**2)  # MB
        
        return {
            'model_name': model_name,
            'memory_mb': peak_memory,
            'initial_memory_mb': initial_memory,
            'memory_increase_mb': peak_memory - initial_memory,
            'gpu_available': True
        }
    
    def unload_model(self, model_name: str) -> None:
        """
        Unload a model to free up memory.
        
        Args:
            model_name: Name of the model to unload
        """
        if model_name in self.models:
            del self.models[model_name]
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            logger.info(f"Unloaded model: {model_name}")
    
    def unload_all_models(self) -> None:
        """Unload all models to free up memory."""
        model_names = list(self.models.keys())
        for model_name in model_names:
            self.unload_model(model_name)
```

### src/metrics.py
```python
"""
Metrics Module
==============

This module implements evaluation metrics for embedding models, including:
- Hit Rate
- Mean Reciprocal Rank (MRR)
- NDCG
- Precision and Recall
"""

import numpy as np
from typing import List, Dict, Tuple


def calculate_hit_rate(query_results: List[List[str]], 
                      ground_truth: List[List[str]], 
                      k: int = 5) -> float:
    """
    Calculate Hit Rate@K.
    
    Args:
        query_results: List of lists containing top-K document IDs for each query
        ground_truth: List of lists containing relevant document IDs for each query
        k: Number of top results to consider
        
    Returns:
        Hit rate score (0.0 to 1.0)
    """
    if len(query_results) != len(ground_truth):
        raise ValueError("Length of query_results and ground_truth must match")
    
    hits = 0
    for result, truth in zip(query_results, ground_truth):
        # Get top-k results
        top_k_results = result[:k] if len(result) > k else result
        # Check if any relevant document is in top-k results
        if any(doc in truth for doc in top_k_results):
            hits += 1
    
    return hits / len(query_results)


def calculate_mrr(query_results: List[List[str]], 
                  ground_truth: List[List[str]]) -> float:
    """
    Calculate Mean Reciprocal Rank (MRR).
    
    MRR = (1/|Q|) * Σ(1/rank_i)
    Where rank_i is the position of the first relevant document
    
    Args:
        query_results: List of lists containing top-K document IDs for each query
        ground_truth: List of lists containing relevant document IDs for each query
        
    Returns:
        MRR score (0.0 to 1.0)
    """
    if len(query_results) != len(ground_truth):
        raise ValueError("Length of query_results and ground_truth must match")
    
    reciprocal_ranks = []
    
    for result, truth in zip(query_results, ground_truth):
        # Find the rank of the first relevant document
        for i, doc in enumerate(result):
            if doc in truth:
                reciprocal_ranks.append(1.0 / (i + 1))
                break
        else:
            reciprocal_ranks.append(0.0)
    
    return np.mean(reciprocal_ranks)


def calculate_ndcg(query_results: List[List[str]], 
                   ground_truth: List[List[str]], 
                   k: int = 10,
                   relevancy_scores: List[List[float]] = None) -> float:
    """
    Calculate Normalized Discounted Cumulative Gain (NDCG@K).
    
    Args:
        query_results: List of lists containing top-K document IDs for each query
        ground_truth: List of lists containing relevant document IDs for each query
        k: Number of top results to consider
        relevancy_scores: Optional list of relevancy scores for each document
        
    Returns:
        NDCG score (0.0 to 1.0)
    """
    if len(query_results) != len(ground_truth):
        raise ValueError("Length of query_results and ground_truth must match")
    
    # If no relevancy scores provided, assume binary relevant (1) or not (0)
    if relevancy_scores is None:
        relevancy_scores = [[1.0 if doc in truth else 0.0 for doc in result] 
                           for result, truth in zip(query_results, ground_truth)]
    
    ndcg_scores = []
    
    for result, truth, scores in zip(query_results, ground_truth, relevancy_scores):
        # Get top-k results
        top_k_results = result[:k] if len(result) > k else result
        top_k_scores = scores[:k] if len(scores) > k else scores
        
        # Calculate DCG
        dcg = sum(score / np.log2(i + 2) for i, score in enumerate(top_k_scores))
        
        # Calculate IDCG (perfect ranking)
        ideal_scores = sorted(truth, reverse=True, key=lambda x: relevancy_scores[result.index(x)][0])
        idcg = sum(1.0 / np.log2(i + 2) for i in range(min(k, len(ideal_scores))))
        
        # Calculate NDCG
        ndcg = dcg / idcg if idcg > 0 else 0.0
        ndcg_scores.append(ndcg)
    
    return np.mean(ndcg_scores)


def calculate_precision_recall(query_results: List[List[str]], 
                              ground_truth: List[List[str]], 
                              k_values: List[int] = None) -> Dict[str, float]:
    """
    Calculate Precision and Recall at different cut-offs.
    
    Args:
        query_results: List of lists containing top-K document IDs for each query
        ground_truth: List of lists containing relevant document IDs for each query
        k_values: List of k values to calculate for
        
    Returns:
        Dictionary with precision and recall scores for each k
    """
    if len(query_results) != len(ground_truth):
        raise ValueError("Length of query_results and ground_truth must match")
    
    if k_values is None:
        k_values = [1, 3, 5, 10]
    
    metrics = {}
    
    for k in k_values:
        precisions = []
        recalls = []
        
        for result, truth in zip(query_results, ground_truth):
            # Get top-k results
            top_k_results = result[:k] if len(result) > k else result
            
            # Calculate precision
            relevant_retrieved = len(set(top_k_results) & set(truth))
            precision = relevant_retrieved / len(top_k_results) if len(top_k_results) > 0 else 0.0
            precisions.append(precision)
            
            # Calculate recall
            recall = relevant_retrieved / len(truth) if len(truth) > 0 else 0.0
            recalls.append(recall)
        
        metrics[f'precision_at_{k}'] = np.mean(precisions)
        metrics[f'recall_at_{k}'] = np.mean(recalls)
    
    return metrics


def calculate_average_precision(query_results: List[str], 
                               ground_truth: List[str]) -> float:
    """
    Calculate Average Precision (AP) for a single query.
    AP is the average of precision values at each relevant document.
    
    Args:
        query_results: List of document IDs for a query
        ground_truth: List of relevant document IDs for the query
        
    Returns:
        Average Precision score (0.0 to 1.0)
    """
    if not ground_truth:
        return 0.0
    
    relevant_count = 0
    precision_sum = 0.0
    
    for i, doc in enumerate(query_results):
        if doc in ground_truth:
            relevant_count += 1
            precision = relevant_count / (i + 1)
            precision_sum += precision
    
    return precision_sum / len(ground_truth)


def calculate_map(query_results: List[List[str]], 
                 ground_truth: List[List[str]]) -> float:
    """
    Calculate Mean Average Precision (MAP) across all queries.
    Mean of Average Precision scores for each query.
    
    Args:
        query_results: List of lists containing document IDs for each query
        ground_truth: List of lists containing relevant document IDs for each query
        
    Returns:
        MAP score (0.0 to 1.0)
    """
    if len(query_results) != len(ground_truth):
        raise ValueError("Length of query_results and ground_truth must match")
    
    ap_scores = []
    for result, truth in zip(query_results, ground_truth):
        ap = calculate_average_precision(result, truth)
        ap_scores.append(ap)
    
    return np.mean(ap_scores)


def calculate_success_rate(query_results: List[List[str]], 
                          ground_truth: List[List[str]], 
                          thresholds: List[float] = None,
                          similarity_scores: List[List[float]] = None) -> Dict[str, float]:
    """
    Calculate Success Rate at different similarity score thresholds.
    
    Args:
        query_results: List of lists containing top document IDs for each query
        ground_truth: List of lists containing relevant document IDs for each query
        thresholds: List of similarity score thresholds
        similarity_scores: List of lists containing similarity scores for each query
        
    Returns:
        Dictionary with success rates for each threshold
    """
    if similarity_scores is None:
        # If no similarity scores provided, assume all retrieved documents are relevant
        return calculate_hit_rate(query_results, ground_truth, k=1)
    
    if len(query_results) != len(ground_truth) or len(query_results) != len(similarity_scores):
        raise ValueError("Length of query_results, ground_truth, and similarity_scores must match")
    
    if thresholds is None:
        thresholds = [0.5, 0.6, 0.7, 0.8, 0.9]
    
    metrics = {}
    
    for threshold in thresholds:
        success_count = 0
        
        for results, truth, scores in zip(query_results, ground_truth, similarity_scores):
            for i, (doc, score) in enumerate(zip(results, scores)):
                if doc in truth and score >= threshold:
                    success_count += 1
                    break
        
        metrics[f'success_rate_at_{threshold}'] = success_count / len(query_results)
    
    return metrics
```

### src/evaluator.py
```python
"""
Evaluator Module
================

This module coordinates the evaluation process for embedding models.
"""

import time
import json
import numpy as np
import logging
from typing import Dict, List, Any, Optional
from pathlib import Path
import pandas as pd
from tqdm import tqdm

from .embedding_manager import EmbeddingManager
from .metrics import (
    calculate_hit_rate, 
    calculate_mrr,
    calculate_ndcg,
    calculate_precision_recall,
    calculate_map,
    calculate_success_rate
)
from .gpu_optimizer import GPUOptimizer


logger = logging.getLogger(__name__)


class ModelEvaluator:
    """
    Coordinates the evaluation process for embedding models.
    """
    
    def __init__(self, embedding_manager: EmbeddingManager, gpu_optimizer: GPUOptimizer):
        """
        Initialize the Model Evaluator.
        
        Args:
            embedding_manager: Embedding manager instance
            gpu_optimizer: GPU optimization instance
        """
        self.embedding_manager = embedding_manager
        self.gpu_optimizer = gpu_optimizer
        self.results = {}
        
    def load_data(self, test_queries_path: Path, corpus_path: Path) -> None:
        """
        Load test queries and document corpus.
        
        Args:
            test_queries_path: Path to the test queries JSON file
            corpus_path: Path to the document corpus JSON file
        """
        # Load test queries
        with open(test_queries_path, 'r', encoding='utf-8') as f:
            queries_data = json.load(f)
            self.test_queries = queries_data.get('test_cases', [])
        
        # Load corpus
        with open(corpus_path, 'r', encoding='utf-8') as f:
            self.corpus = json.load(f)
        
        # Extract document texts and IDs
        self.document_texts = [doc['content'] for doc in self.corpus]
        self.document_ids = [doc['document_id'] for doc in self.corpus]
        logger.info(f"Loaded {len(self.test_queries)} test queries and {len(self.corpus)} documents")
        
    def run_full_evaluation(self, output_path: Path = None, num_runs: int = 3) -> Dict:
        """
        Run a complete evaluation of all loaded models.
        
        Args:
            output_path: Path to save evaluation results
            num_runs: Number of evaluation runs to perform

        Returns:
            Dictionary containing all evaluation results
        """
        start_time = time.time()
        logger.info("Starting full model evaluation")
        
        # Initialize results structure
        self.results = {
            'summary': {
                'num_models': len(self.embedding_manager.model_configs),
                'num_queries': len(self.test_queries),
                'num_documents': len(self.corpus),
                'num_runs': num_runs
            },
            'models': {},
            'run_times': {},
            'gpu_utilization': {}
        }
        
        # Process each model
        for model_name, model_config in self.embedding_manager.model_configs.items():
            logger.info(f"Evaluating model: {model_name}")
            model_results = self._evaluate_single_model(model_name, num_runs)
            self.results['models'][model_name] = model_results
            
            # Also run benchmarking for this model
            benchmark_results = self._benchmark_model(model_name)
            self.results['models'][model_name]['benchmark'] = benchmark_results
            
            # Track GPU usage if available
            if self.gpu_optimizer.device.type == 'cuda':
                gpu_usage = self.gpu_optimizer.monitor_gpu_usage()
                self.results['gpu_utilization'][model_name] = gpu_usage
        
        # Calculate overall rankings
        self.results['rankings'] = self._calculate_model_rankings()
        
        # Save results if output path provided
        if output_path:
            self.save_results(output_path)
            
        total_time = time.time() - start_time
        logger.info(f"Full evaluation completed in {total_time:.2f}s")
        
        return self.results
    
    def _evaluate_single_model(self, model_name: str, num_runs: int) -> Dict:
        """
        Evaluate a single model across multiple runs.
        
        Args:
            model_name: Name of the model to evaluate
            num_runs: Number of evaluation runs
            
        Returns:
            Dictionary containing evaluation metrics for the model
        """
        model_results = {
            'metrics': {
                'hit_rate': [],
                'mrr': [],
                'ndcg': [],
                'precision_recall': [],
                'map': [],
                'success_rate': []
            },
            'category_performance': {},
            'difficulty_performance': {}
        }
        
        for run in range(num_runs):
            results = self._run_model_evaluation(model_name)
            
            # Store metrics
            model_results['metrics']['hit_rate'].append(results['hit_rate'])
            model_results['metrics']['mrr'].append(results['mrr'])
            model_results['metrics']['ndcg'].append(results['ndcg'])
            model_results['metrics'].update(results['precision_recall'])
            model_results['metrics']['map'].append(results['map'])
            model_results['metrics'].update(results['success_rate'])
            
            # Store category performance
            for category, metrics in results['category_performance'].items():
                if category not in model_results['category_performance']:
                    model_results['category_performance'][category] = {
                        'hit_rate': [],
                        'mrr': [],
                        'ndcg': []
                    }
                
                model_results['category_performance'][category]['hit_rate'].append(metrics['hit_rate'])
                model_results['category_performance'][category]['mrr'].append(metrics['mrr'])
                model_results['category_performance'][category]['ndcg'].append(metrics['ndcg'])
            
            # Store difficulty performance
            for difficulty, metrics in results['difficulty_performance'].items():
                if difficulty not in model_results['difficulty_performance']:
                    model_results['difficulty_performance'][difficulty] = {
                        'hit_rate': [],
                        'mrr': [],
                        'ndcg': []
                    }
                
                model_results['difficulty_performance'][difficulty]['hit_rate'].append(metrics['hit_rate'])
                model_results['difficulty_performance'][difficulty]['mrr'].append(metrics['mrr'])
                model_results['difficulty_performance'][difficulty]['ndcg'].append(metrics['ndcg'])
        
        # Calculate mean and std across runs
        final_metrics = {}
        for metric, values in model_results['metrics'].items():
            if isinstance(values[0], dict):
                # Handle precision_recall and success_rate which are dicts
                metric_dict = {}
                for k in values[0].keys():
                    metric_values = [v[k] for v in values]
                    metric_dict[f'{metric}_{k}_mean'] = np.mean(metric_values)
                    metric_dict[f'{metric}_{k}_std'] = np.std(metric_values)
                final_metrics.update(metric_dict)
            else:
                final_metrics[f'{metric}_mean'] = np.mean(values)
                final_metrics[f'{metric}_std'] = np.std(values)
        
        model_results['metrics'] = final_metrics
        
        # Calculate mean performance for categories
        for category, metrics in model_results['category_performance'].items():
            category_metrics = {}
            for metric, values in metrics.items():
                category_metrics[f'{metric}_mean'] = np.mean(values)
                category_metrics[f'{metric}_std'] = np.std(values)
            model_results['category_performance'][category] = category_metrics
        
        # Calculate mean performance for difficulties
        for difficulty, metrics in model_results['difficulty_performance'].items():
            difficulty_metrics = {}
            for metric, values in metrics.items():
                difficulty_metrics[f'{metric}_mean'] = np.mean(values)
                difficulty_metrics[f'{metric}_std'] = np.std(values)
            model_results['difficulty_performance'][difficulty] = difficulty_metrics
        
        return model_results
    
    def _run_model_evaluation(self, model_name: str) -> Dict:
        """
        Run a single evaluation pass with a model.
        
        Args:
            model_name: Name of the model to evaluate
            
        Returns:
            Dictionary with evaluation metrics
        """
        # Generate embeddings for all documents
        doc_embeddings = self.embedding_manager.generate_embeddings(
            self.document_texts, model_name
        )
        
        # Prepare results storage
        query_results = []
        ground_truth = []
        similarities = []
        
        category_performance = {}
        difficulty_performance = {}
        
        # Process each query
        for query in tqdm(self.test_queries, desc=f"Evaluating {model_name}"):
            query_text = query['query']
            relevant_docs = query['relevant_documents']
            category = query['department']
            difficulty = query['difficulty']
            
            # Find similar documents
            sim_results = self.embedding_manager.find_similar_documents(
                query_text, doc_embeddings, model_name, top_k=10
            )
            
            # Extract top document IDs and similarity scores
            top_docs = [doc_id for doc_id, _ in self.document_ids if doc_id in [r['document_id'] for r in sim_results]]
            sim_scores = [r['similarity_score'] for r in sim_results]
            
            query_results.append(top_docs)
            ground_truth.append(relevant_docs)
            similarities.append(sim_scores)
            
            # Track category performance
            if category not in category_performance:
                category_indices = [i for i, q in enumerate(self.test_queries) if q['department'] == category]
                category_queries = [self.test_queries[i] for i in category_indices]
                category_results = [query_results[i] for i in category_indices]
                category_ground_truth = [ground_truth[i] for i in category_indices]
                
                category_performance[category] = {
                    'hit_rate': calculate_hit_rate(category_results, category_ground_truth),
                    'mrr': calculate_mrr(category_results, category_ground_truth),
                    'ndcg': calculate_ndcg(category_results, category_ground_truth)
                }
            
            # Track difficulty performance
            if difficulty not in difficulty_performance:
                difficulty_indices = [i for i, q in enumerate(self.test_queries) if q['difficulty'] == difficulty]
                difficulty_queries = [self.test_queries[i] for i in difficulty_indices]
                difficulty_results = [query_results[i] for i in difficulty_indices]
                difficulty_ground_truth = [ground_truth[i] for i in difficulty_indices]
                
                difficulty_performance[difficulty] = {
                    'hit_rate': calculate_hit_rate(difficulty_results, difficulty_ground_truth),
                    'mrr': calculate_mrr(difficulty_results, difficulty_ground_truth),
                    'ndcg': calculate_ndcg(difficulty_results, difficulty_ground_truth)
                }
        
        # Calculate overall metrics
        metrics = {
            'hit_rate': calculate_hit_rate(query_results, ground_truth),
            'mrr': calculate_mrr(query_results, ground_truth),
            'ndcg': calculate_ndcg(query_results, ground_truth),
            'precision_recall': calculate_precision_recall(query_results, ground_truth),
            'map': calculate_map(query_results, ground_truth),
            'success_rate': calculate_success_rate(query_results, ground_truth, similarity_scores=similarities)
        }
        
        return {
            'metrics': metrics,
            'query_results': query_results,
            'ground_truth': ground_truth,
            'category_performance': category_performance,
            'difficulty_performance': difficulty_performance
        }
    
    def _benchmark_model(self, model_name: str) -> Dict:
        """
        Benchmark performance metrics for a model.
        
        Args:
            model_name: Name of the model to benchmark
            
        Returns:
            Dictionary with benchmark results
        """
        # Create sample texts for benchmarking
        sample_texts = self.document_texts[:100]  # Sample first 100 documents
        
        # Benchmark speed
        speed_results = self.embedding_manager.benchmark_model_speed(
            model_name, sample_texts
        )
        
        # Benchmark memory
        memory_results = self.embedding_manager.benchmark_memory_usage(
            model_name, sample_texts
        )
        
        return {
            'speed': speed_results,
            'memory': memory_results
        }
    
    def _calculate_model_rankings(self) -> Dict:
        """
        Calculate rankings for models based on performance metrics.
        
        Returns:
            Dictionary with model rankings
        """
        # Define metric weights
        metric_weights = {
            'hit_rate_mean': 0.20,
            'mrr_mean': 0.20,
            'precision_recall_at_5_mean': 0.15,
            'map_mean': 0.15,
            'success_rate_at_0.7_mean': 0.10,
            'speed_texts_per_sec': 0.05,  # from benchmark results
            'memory_increase_mb': -0.05   # negative because lower is better
        }
        
        scores = {}
        
        for model_name, model_results in self.results['models'].items():
            total_score = 0.0
            
            # Calculate score from metrics
            for metric, weight in metric_weights.items():
                if metric in ['speed_texts_per_sec', 'memory_increase_mb']:
                    # Get from benchmark results
                    benchmark = model_results['benchmark']
                    
                    if metric == 'speed_texts_per_sec':
                        # Get highest speed across batch sizes
                        max_speed = max(run['texts_per_sec'] for run in benchmark['speed']['runs'])
                        normalized_speed = min(max_speed / 100, 1.0)  # normalize to 0-1
                        score = normalized_speed * weight
                    elif metric == 'memory_increase_mb':
                        # Lower memory is better, so invert
                        mem_increase = benchmark['memory']['memory_increase_mb']
                        normalized_memory = min(mem_increase / 1000, 1.0)  # normalize to 0-1
                        score = (1.0 - normalized_memory) * weight
                else:
                    # Get from metrics
                    metric_value = model_results['metrics'].get(metric, 0.0)
                    normalized_value = min(metric_value, 1.0)  # ensure it's 0-1
                    score = normalized_value * weight
                
                total_score += score
            
            scores[model_name] = {
                'total_score': total_score,
                'rank': None  # will be set later
            }
        
        # Assign ranks
        sorted_models = sorted(scores.keys(), key=lambda x: scores[x]['total_score'], reverse=True)
        for rank, model_name in enumerate(sorted_models):
            scores[model_name]['rank'] = rank + 1
        
        return {
            'weighted_scores': scores,
            'rankings_list': sorted_models
        }
    
    def save_results(self, output_path: Path) -> None:
        """
        Save evaluation results to a JSON file.
        
        Args:
            output_path: Path to save the results
        """
        # Create output directory if it doesn't exist
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # Convert numpy types to serializable formats
        def convert_numpy(obj):
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, dict):
                return {k: convert_numpy(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy(item) for item in obj]
            return obj
        
        # Convert results
        serializable_results = convert_numpy(self.results)
        
        # Save to file
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"Evaluation results saved to: {output_path}")
```

### src/gpu_optimizer.py
```python
"""
GPU Optimizer Module
===================

This module handles GPU optimization for embedding models.
"""

import torch
import logging
from typing import Dict, Any, Optional
import numpy as np
import psutil


logger = logging.getLogger(__name__)


class GPUOptimizer:
    """
    Handles GPU optimization for embedding models.
    """
    
    def __init__(self):
        """Initialize the GPU Optimizer."""
        self.device = self._detect_device()
        self.gpu_info = self._get_gpu_info() if self.device.type == 'cuda' else None
        self.logger = logging.getLogger(__name__)
        
        self.logger.info(f"Using device: {self.device}")
        if self.gpu_info:
            self.logger.info(f"GPU Info: {self.gpu_info}")
    
    def _detect_device(self) -> torch.device:
        """
        Detect the best available device.
        
        Returns:
            torch.device: Detected device
        """
        if torch.cuda.is_available():
            # Use the first GPU by default
            device = torch.device('cuda:0')
            torch.cuda.set_device(device)
            logger.info(f"CUDA detected, using {device}")
        else:
            device = torch.device('cpu')
            logger.info("CUDA not available, using CPU")
            
        return device
    
    def _get_gpu_info(self) -> Dict[str, Any]:
        """
        Get information about available GPUs.
        
        Returns:
            Dictionary with GPU information
        """
        if not torch.cuda.is_available():
            return {}
        
        # Get GPU properties
        gpu_count = torch.cuda.device_count()
        gpu_name = torch.cuda.get_device_name(0)
        gpu_capability = torch.cuda.get_device_capability(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)  # GB
        
        gpu_info = {
            'count': gpu_count,
            'name': gpu_name,
            'capability': gpu_capability,
            'memory_gb': gpu_memory,
            'driver_version': None
        }
        
        # Try to get driver version
        try:
            from cuda import cuda
            err, driver_version = cuda.cuDriverGetVersion()
            if err == cuda.CUDA_SUCCESS:
                gpu_info['driver_version'] = f"{driver_version // 1000}.{(driver_version % 1000) // 10}"
        except ImportError:
            pass
        
        return gpu_info
    
    def optimize_model_loading(self, model_name: str) -> None:
        """
        Optimize loading conditions for a model.
        
        Args:
            model_name: Name of the model being loaded
        """
        if self.device.type == 'cpu':
            return
        
        # Clear GPU cache before loading
        torch.cuda.empty_cache()
        
        # Set memory allocation strategy
        if self.gpu_info:
            gpu_memory_gb = self.gpu_info['memory_gb']
            
            # Use memory-efficient settings for limited GPU memory
            if gpu_memory_gb < 8:  # Less than 8GB
                torch.cuda.set_per_process_memory_fraction(0.5)
            elif gpu_memory_gb < 16:  # Less than 16GB
                torch.cuda.set_per_process_memory_fraction(0.8)
            else:  # 16GB or more
                torch.cuda.set_per_process_memory_fraction(0.9)
            
            self.logger.info(f"Set memory allocation for {model_name} to {torch.cuda.get_per_process_memory_fraction() * 100:.1f}%")
    
    def monitor_gpu_usage(self) -> Dict[str, Any]:
        """
        Monitor current GPU usage.
        
        Returns:
            Dictionary with GPU usage metrics
        """
        if self.device.type == 'cpu':
            return {'device': 'cpu', 'usage_percent': 0}
        
        # Get GPU memory usage
        allocated_memory = torch.cuda.memory_allocated() / (1024 ** 3)  # GB
        reserved_memory = torch.cuda.memory_reserved() / (1024 ** 3)   # GB
        max_memory = torch.cuda.max_memory_allocated() / (1024 ** 3)   # GB
        total_memory = self.gpu_info['memory_gb']
        
        # Calculate utilization percentage
        utilization_percent = (allocated_memory / total_memory) * 100
        
        # Get GPU utilization (if available)
        try:
            # This requires nvidia-ml-py package
            import pynvml
            pynvml.nvmlInit()
            handle = pynvml.nvmlDeviceGetHandleByIndex(0)
            gpu_utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)
            gpu_util = gpu_utilization.gpu
            memory_util = gpu_utilization.memory
        except ImportError:
            gpu_util = None
            memory_util = None
        
        gpu_metrics = {
            'device': 'cuda',
            'allocated_memory_gb': allocated_memory,
            'reserved_memory_gb': reserved_memory,
            'max_memory_gb': max_memory,
            'total_memory_gb': total_memory,
            'utilization_percent': utilization_percent,
            'gpu_util_percent': gpu_util,
            'memory_util_percent': memory_util
        }
        
        self.logger.info(f"GPU Usage: {allocated_memory:.2f}GB allocated, {utilization_percent:.1f}% of total")
        
        return gpu_metrics
    
    def get_available_memory(self) -> float:
        """
        Get available GPU memory in GB.
        
        Returns:
            Available memory in GB
        """
        if self.device.type == 'cpu':
            return psutil.virtual_memory().available / (1024 ** 3)  # GB
        
        total_memory = self.gpu_info['memory_gb']
        allocated_memory = torch.cuda.memory_allocated() / (1024 ** 3)  # GB
        available_memory = total_memory - allocated_memory
        
        return available_memory
    
    def is_memory_available(self, required_memory_gb: float) -> bool:
        """
        Check if enough memory is available for a task.
        
        Args:
            required_memory_gb: Required memory in GB
            
        Returns:
            True if enough memory is available, False otherwise
        """
        available_memory = self.get_available_memory()
        return available_memory >= required_memory_gb
    
    def suggest_batch_size(self, model_name: str, test_size: int = 100) -> int:
        """
        Suggest an optimal batch size based on available memory.
        
        Args:
            model_name: Name of the model
            test_size: Number of samples to test with
            
        Returns:
            Suggested batch size
        """
        # Start with a conservative batch size
        if self.gpu_info and self.gpu_info['memory_gb'] >= 8:
            suggested_batch = 32
        else:
            suggested_batch = 16
        
        # Test different batch sizes
        test_batch = suggested_batch
        step = 8 if suggested_batch >= 16 else 4
        
        # If we're on CPU, test different CPU batch sizes instead
        if self.device.type == 'cpu':
            # Less aggressive testing on CPU
            return suggested_batch
        
        # Try to find the largest batch size that fits in memory
        while test_batch <= test_size:
            required_gb = test_batch / 8.0  # Rough estimate
            
            if self.is_memory_available(required_gb):
                suggested_batch = test_batch
                test_batch += step
            else:
                break
        
        return suggested_batch
    
    def optimize_for_performance(self, use_mixed_precision: bool = True) -> None:
        """
        Apply performance optimizations.
        
        Args:
            use_mixed_precision: Whether to use mixed precision training
        """
        if self.device.type == 'cpu':
            return
        
        if use_mixed_precision:
            # Enable automatic mixed precision
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            torch.backends.cudnn.benchmark = True
            
            self.logger.info("Enabled mixed precision and CuDNN optimizations")
```

### src/visualizer.py
```python
"""
Visualization Module
====================

This module handles visualization of evaluation results and reports generation.
"""

import json
import os
from typing import Dict, Any, List
from pathlib import Path
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
from jinja2 import Environment, FileSystemLoader
import logging

from .evaluator import ModelEvaluator


logger = logging.getLogger(__name__)


class ReportGenerator:
    """
    Generates visualization charts and reports from evaluation results.
    """
    
    def __init__(self, evaluator: ModelEvaluator):
        """
        Initialize the Report Generator.
        
        Args:
            evaluator: Model evaluator instance
        """
        self.evaluator = evaluator
        self.results = evaluator.results
        
        # Set up matplotlib style
        plt.style.use('seaborn-v0_8')
        
        # Set up Jinja2 environment
        env = Environment(loader=FileSystemLoader('.'))
        self.jinja_env = env
    
    def generate_comparison_charts(self, output_dir: Path) -> None:
        """
        Generate comparison charts for all models.
        
        Args:
            output_dir: Directory to save the charts
        """
        # Create output directory if it doesn't exist
        os.makedirs(output_dir, exist_ok=True)
        
        # Generate individual chart types
        self._generate_hit_rate_chart(output_dir)
        self._generate_mrr_chart(output_dir)
        self._generate_ndcg_chart(output_dir)
        self._generate_speed_vs_accuracy_chart(output_dir)
        self._generate_memory_usage_chart(output_dir)
        self._generate_category_performance_heatmap(output_dir)
        
        logger.info(f"Generated comparison charts in {output_dir}")
    
    def _generate_hit_rate_chart(self, output_dir: Path) -> None:
        """Generate Hit Rate@K comparison chart."""
        # Extract Hit Rate data
        data = []
        for model_name, model_results in self.results['models'].items():
            metrics = model_results['metrics']
            if 'hit_rate_mean' in metrics:
                data.append({
                    'Model': model_name,
                    'Hit Rate': metrics['hit_rate_mean'],
                    'Std': metrics['hit_rate_std'] if 'hit_rate_std' in metrics else 0
                })
        
        if not data:
            return
        
        df = pd.DataFrame(data)
        
        # Create bar chart with error bars
        fig, ax = plt.subplots(figsize=(10, 6))
        bars = ax.bar(df['Model'], df['Hit Rate'], yerr=df['Std'], capsize=5, alpha=0.7)
        
        # Add labels
        ax.set_ylabel('Hit Rate (Mean)')
        ax.set_title('Model Comparison: Hit Rate')
        ax.set_ylim(0, 1)
        
        # Add value labels on bars
        for bar, value in zip(bars, df['Hit Rate']):
            ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,
                    f'{value:.3f}', ha='center', va='bottom')
        
        # Save the figure
        output_path = output_dir / 'hit_rate_comparison.png'
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
    
    def _generate_mrr_chart(self, output_dir: Path) -> None:
        """Generate Mean Reciprocal Rank comparison chart."""
        # Extract MRR data
        data = []
        for model_name, model_results in self.results['models'].items():
            metrics = model_results['metrics']
            if 'mrr_mean' in metrics:
                data.append({
                    'Model': model_name,
                    'MRR': metrics['mrr_mean'],
                    'Std': metrics['mrr_std'] if 'mrr_std' in metrics else 0
                })
        
        if not data:
            return
        
        df = pd.DataFrame(data)
        
        # Create bar chart with error bars
        fig, ax = plt.subplots(figsize=(10, 6))
        bars = ax.bar(df['Model'], df['MRR'], yerr=df['Std'], capsize=5, alpha=0.7, color='green')
        
        # Add labels
        ax.set_ylabel('MRR (Mean)')
        ax.set_title('Model Comparison: Mean Reciprocal Rank')
        ax.set_ylim(0, 1)
        
        # Add value labels on bars
        for bar, value in zip(bars, df['MRR']):
            ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,
                    f'{value:.3f}', ha='center', va='bottom')
        
        # Save the figure
        output_path = output_dir / 'mrr_comparison.png'
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
    
    def _generate_ndcg_chart(self, output_dir: Path) -> None:
        """Generate NDCG comparison chart."""
        # Extract NDCG data
        data = []
        for model_name, model_results in self.results['models'].items():
            metrics = model_results['metrics']
            if 'ndcg_mean' in metrics:
                data.append({
                    'Model': model_name,
                    'NDCG': metrics['ndcg_mean'],
                    'Std': metrics['ndcg_std'] if 'ndcg_std' in metrics else 0
                })
        
        if not data:
            return
        
        df = pd.DataFrame(data)
        
        # Create bar chart with error bars
        fig, ax = plt.subplots(figsize=(10, 6))
        bars = ax.bar(df['Model'], df['NDCG'], yerr=df['Std'], capsize=5, alpha=0.7, color='purple')
        
        # Add labels
        ax.set_ylabel('NDCG (Mean)')
        ax.set_title('Model Comparison: Normalized Discounted Cumulative Gain')
        ax.set_ylim(0, 1)
        
        # Add value labels on bars
        for bar, value in zip(bars, df['NDCG']):
            ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,
                    f'{value:.3f}', ha='center', va='bottom')
        
        # Save the figure
        output_path = output_dir / 'ndcg_comparison.png'
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
    
    def _generate_speed_vs_accuracy_chart(self, output_dir: Path) -> None:
        """Generate speed vs accuracy scatter plot."""
        # Extract data
        data = []
        for model_name, model_results in self.results['models'].items():
            metrics = model_results['metrics']
            benchmark = model_results['benchmark']['speed']
            
            if 'hit_rate_mean' in metrics and benchmark['runs']:
                # Get best speed across batch sizes
                best_speed = max(run['texts_per_sec'] for run in benchmark['runs'])
                
                data.append({
                    'Model': model_name,
                    'Accuracy': metrics['hit_rate_mean'],
                    'Speed': best_speed,
                    'Size': benchmark['num_parameters'] / 1e6  # millions of parameters
                })
        
        if not data:
            return
        
        df = pd.DataFrame(data)
        
        # Create scatter plot
        fig, ax = plt.subplots(figsize=(10, 8))
        
        # Color points by size
        scatter = ax.scatter(df['Speed'], df['Accuracy'], s=df['Size'] * 10, 
                            c=df['Size'], cmap='viridis', alpha=0.7, edgecolors='w')
        
        # Add labels for each point
        for i, row in df.iterrows():
            ax.text(row['Speed'], row['Accuracy'], row['Model'], fontsize=9)
        
        # Add colorbar
        cbar = plt.colorbar(scatter)
        cbar.set_label('Model Size (millions of parameters)')
        
        # Add labels
        ax.set_xlabel('Speed (texts per second)')
        ax.set_ylabel('Hit Rate')
        ax.set_title('Model Comparison: Speed vs Accuracy')
        
        # Save the figure
        output_path = output_dir / 'speed_vs_accuracy.png'
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
    
    def _generate_memory_usage_chart(self, output_dir: Path) -> None:
        """Generate memory usage comparison chart."""
        # Check if we have GPU memory data
        if 'gpu_utilization' not in self.results or not self.results['gpu_utilization']:
            logger.info("No GPU utilization data available for memory usage chart")
            return
        
        # Extract memory data
        data = []
        for model_name, model_metrics in self.results['gpu_utilization'].items():
            if model_metrics:
                data.append({
                    'Model': model_name,
                    'Memory Increase (GB)': model_metrics['memory_increase_mb'] / 1000,
                    'Max Memory (GB)': model_metrics['max_memory_gb']
                })
        
        if not data:
            return
        
        df = pd.DataFrame(data)
        
        # Create bar chart for memory increase
        fig, ax = plt.subplots(figsize=(10, 6))
        
        # Use two different colors for memory increase vs total memory
        ax.bar(df['Model'], df['Max Memory (GB)'], label='Total Memory', alpha=0.3, color='blue')
        bars = ax.bar(df['Model'], df['Memory Increase (GB)'], 
                     label='Memory During Processing', color='orange')
        
        # Add labels
        ax.set_ylabel('Memory (GB)')
        ax.set_title('Model Comparison: Memory Usage')
        ax.legend()
        
        # Add value labels on bars
        for bar, value in zip(bars, df['Memory Increase (GB)']):
            ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.1,
                    f'{value:.2f}', ha='center', va='bottom')
        
        # Save the figure
        output_path = output_dir / 'memory_usage.png'
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
    
    def _generate_category_performance_heatmap(self, output_dir: Path) -> None:
        """Generate heatmap of performance across different categories."""
        # Prepare data
        category_data = []
        
        for model_name, model_results in self.results['models'].items():
            if 'category_performance' in model_results:
                for category, metrics in model_results['category_performance'].items():
                    if 'hit_rate_mean' in metrics:
                        category_data.append({
                            'Model': model_name,
                            'Category': category,
                            'Hit Rate': metrics['hit_rate_mean']
                        })
        
        if not category_data:
            return
        
        # Create pivot table for heatmap
        df = pd.DataFrame(category_data)
        pivot_df = df.pivot('Model', 'Category', 'Hit Rate')
        
        # Create heatmap
        fig, ax = plt.subplots(figsize=(10, 8))
        
        # Use a diverging colormap (red for low, green for high)
        sns.heatmap(pivot_df, annot=True, fmt=".2f", cmap="RdYlGn", vmin=0, vmax=1, ax=ax)
        
        # Add title
        ax.set_title('Model Performance by Category (Hit Rate)')
        ax.set_xlabel('Document Category')
        ax.set_ylabel('Model')
        
        # Save the figure
        output_path = output_dir / 'category_heatmap.png'
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
    
    def create_model_ranking_table(self, output_path: Path) -> None:
        """
        Create a model ranking table based on weighted scoring.
        
        Args:
            output_path: Path to save the table
        """
        from .metrics import EVALUATION_WEIGHTS
        
        # Get weighted scores
        if 'rankings' not in self.results or 'weighted_scores' not in self.results['rankings']:
            logger.error("No ranking information available")
            return
        
        # Create table
        rows = []
        
        for model_name, score_info in self.results['rankings']['weighted_scores'].items():
            row = {'Model': model_name, 'Rank': score_info['rank'], 'Total Score': score_info['total_score']}
            rows.append(row)
        
        # Sort by rank
        df = pd.DataFrame(rows).sort_values('Rank')
        
        # Save to CSV
        df.to_csv(output_path, index=False)
        logger.info(f"Saved model ranking table to {output_path}")
    
    def export_final_report(self, output_dir: Path) -> None:
        """
        Generate a final comprehensive report with all results.
        
        Args:
            output_dir: Directory to save the report
        """
        # Create output directory if it doesn't exist
        os.makedirs(output_dir, exist_ok=True)
        
        # Generate charts
        self.generate_comparison_charts(output_dir / 'performance_charts')
        
        # Create ranking table
        self.create_model_ranking_table(output_dir / 'model_rankings.csv')
        
        # Load template
        template_path = Path(__file__).parent.parent / 'templates' / 'report_template.html'
        if not template_path.exists():
            # Fallback: create a simple HTML template
            template_content = self._create_html_template()
        else:
            with open(template_path, 'r', encoding='utf-8') as f:
                template_content = f.read()
        
        # Render report
        template = self.jinja_env.from_string(template_content)
        
        # Prepare data for template
        report_data = {
            'summary': self.results['summary'],
            'model_rankings': self.results['rankings']['rankings_list'],
            'models': {}
        }
        
        # Format model results for template
        for model_name, model_results in self.results['models'].items():
            report_data['models'][model_name] = {
                'metrics': {
                    'hit_rate': model_results['metrics'].get('hit_rate_mean', 'N/A'),
                    'mrr': model_results['metrics'].get('mrr_mean', 'N/A'),
                    'ndcg': model_results['metrics'].get('ndcg_mean', 'N/A'),
                    'precision_at_k': model_results['metrics'].get('precision_recall_at_5_mean', 'N/A'),
                    'map': model_results['metrics'].get('map_mean', 'N/A')
                },
                'rank': self.results['rankings']['weighted_scores'][model_name]['rank']
            }
        
        # Render template
        html_report = template.render(report_data)
        
        # Save HTML report
        report_path = output_dir / 'final_report.html'
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(html_report)
        
        # Convert to markdown if requested
        # self._convert_to_markdown(output_dir)
        
        logger.info(f"Generated final report at {report_path}")
    
    def _create_html_template(self) -> str:
        """Create a simple HTML template for the report."""
        return """
<!DOCTYPE html>
<html>
<head>
    <title>Vietnamese Embedding Model Evaluation Report</title>
    <style>
        body { font-family: Arial, sans-serif; line-height: 1.6; margin: 0 auto; max-width: 1000px; }
        h1, h2, h3 { color: #2c3e50; }
        .header { background-color: #f8f9fa; padding: 20px; border-bottom: 1px solid #ddd; }
        .content { padding: 20px; }
        .model { margin-bottom: 20px; padding: 15px; border: 1px solid #eee; border-radius: 5px; }
        .model-header { display: flex; justify-content: space-between; }
        .metrics { display: flex; flex-wrap: wrap; }
        .metric { margin-right: 20px; }
        .ranking-table { width: 100%; border-collapse: collapse; margin-top: 20px; }
        .ranking-table th, .ranking-table td { border: 1px solid #ddd; padding: 8px; text-align: left; }
        .ranking-table th { background-color: #f2f2f2; }
    </style>
</head>
<body>
    <div class="header">
        <h1>Vietnamese Embedding Model Evaluation Report</h1>
        <p>Generated on {{ summary.num_models }} models, {{ summary.num_queries }} queries</p>
    </div>
    
    <div class="content">
        <h2>Executive Summary</h2>
        <p>This report presents the results of evaluating multiple embedding models for Vietnamese text. The models were evaluated based on various metrics including Hit Rate, Mean Reciprocal Rank (MRR), and NDCG.</p>
        
        <h2>Model Rankings</h2>
        <table class="ranking-table">
            <tr>
                <th>Rank</th>
                <th>Model</th>
            </tr>
            {% for model in model_rankings %}
            <tr>
                <td>{{ loop.index }}</td>
                <td>{{ model }}</td>
            </tr>
            {% endfor %}
        </table>
        
        <h2>Model Performance Details</h2>
        {% for model_name, model_data in models.items() %}
        <div class="model">
            <div class="model-header">
                <h3>{{ model_name }}</h3>
                <div>Rank: {{ model_data.rank }}</div>
            </div>
            <div class="metrics">
                <div class="metric">
                    <strong>Hit Rate:</strong> {{ model_data.metrics.hit_rate }}
                </div>
                <div class="metric">
                    <strong>MRR:</strong> {{ model_data.metrics.mrr }}
                </div>
                <div class="metric">
                    <strong>NDCG:</strong> {{ model_data.metrics.ndcg }}
                </div>
                <div class="metric">
                    <strong>Precision@5:</strong> {{ model_data.metrics.precision_at_k }}
                </div>
                <div class="metric">
                    <strong>MAP:</strong> {{ model_data.metrics.map }}
                </div>
            </div>
        </div>
        {% endfor %}
    </div>
</body>
</html>
        """
```

### scripts/run_evaluation.py
```python
#!/usr/bin/env python3
"""
Main evaluation runner script
Usage: python scripts/run_evaluation.py --config configs/models.json --output reports/
"""

import typer
import json
import logging
from pathlib import Path
from typing import Optional

# Import from local modules
from src.data_processor import VietnameseTextProcessor
from src.embedding_manager import EmbeddingManager
from src.evaluator import ModelEvaluator
from src.gpu_optimizer import GPUOptimizer
from src.visualizer import ReportGenerator


def setup_logging(verbose: bool = False):
    """Set up logging configuration."""
    log_level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler('evaluation.log')
        ]
    )


def main(
    config_path: Path = typer.Option(
        ...,
        exists=True,
        file_okay=True,
        dir_okay=False,
        readable=True,
        resolve_path=True,
        help="Path to models config file"
    ),
    data_path: Path = typer.Option(
        "data/",
        exists=True,
        dir_okay=True,
        file_okay=False,
        resolve_path=True,
        help="Path to data directory"
    ),
    output_path: Path = typer.Option(
        "reports/",
        resolve_path=True,
        help="Output directory for results"
    ),
    gpu_enabled: bool = typer.Option(
        True,
        help="Enable GPU acceleration if available"
    ),
    verbose: bool = typer.Option(
        False,
        "--verbose", "-v",
        help="Enable verbose logging"
    ),
    generate_report: bool = typer.Option(
        True,
        help="Generate final report with visualizations"
    )
):
    """
    Run complete embedding model evaluation pipeline
    
    Steps executed:
    1. Load and validate configuration
    2. Initialize GPU optimization
    3. Create embedding manager with models
    4. Prepare test data
    5. Run evaluation
    6. Generate reports and visualizations
    """
    
    # Set up logging
    setup_logging(verbose)
    logger = logging.getLogger(__name__)
    
    try:
        logger.info("Starting Vietnamese Embedding Model Evaluation")
        
        # Create output directory if it doesn't exist
        output_path.mkdir(parents=True, exist_ok=True)
        
        # Step 1: Initialize GPUOptimizer
        gpu_optimizer = GPUOptimizer()
        if gpu_enabled:
            gpu_optimizer.optimize_for_performance()
        
        # Step 2: Initialize EmbeddingManager
        logger.info("Loading embedding models from configuration")
        embedding_manager = EmbeddingManager(gpu_optimizer)
        embedding_manager.init_from_config(config_path)
        
        if not embedding_manager.model_configs:
            raise ValueError("No models were loaded from the configuration file.")
        
        # Step 3: Initialize ModelEvaluator
        logger.info("Initializing model evaluator")
        evaluator = ModelEvaluator(embedding_manager, gpu_optimizer)
        
        # Step 4: Load test data
        logger.info("Loading test data")
        test_queries_path = data_path / "ground_truth" / "query_document_pairs.json"
        corpus_path = data_path / "processed" / "cleaned_corpus.json"
        
        if not test_queries_path.exists():
            logger.error(f"Test queries file not found: {test_queries_path}")
            raise FileNotFoundError(f"Test queries file not found: {test_queries_path}")
            
        if not corpus_path.exists():
            logger.error(f"Corpus file not found: {corpus_path}")
            raise FileNotFoundError(f"Corpus file not found: {corpus_path}")
        
        evaluator.load_data(test_queries_path, corpus_path)
        
        # Step 5: Run evaluation
        logger.info("Starting model evaluation")
        config = json.load(open(config_path))
        num_runs = config.get('evaluation_settings', {}).get('num_runs', 3)
        
        results = evaluator.run_full_evaluation(output_path, num_runs)
        
        # Step 6: Generate reports
        if generate_report:
            logger.info("Generating evaluation report")
            report_generator = ReportGenerator(evaluator)
            report_generator.export_final_report(output_path)
        
        logger.info("Evaluation completed successfully")
        logger.info(f"Results saved to {output_path}")
        
        # Print summary
        if 'rankings' in results:
            logger.info("\nModel Rankings:")
            for i, model_name in enumerate(results['rankings']['rankings_list'], 1):
                logger.info(f"{i}. {model_name}")
        
    except Exception as e:
        logger.error(f"Evaluation failed: {str(e)}")
        raise typer.Exit(code=1)


if __name__ == "__main__":
    typer.run(main)
```

### scripts/prepare_data.py
```python
#!/usr/bin/env python3
"""
Data preparation script
Usage: python scripts/prepare_data.py --input data/raw --output data/processed
"""

import typer
import json
import logging
from pathlib import Path
from typing import Optional

from src.data_processor import VietnameseTextProcessor


def setup_logging(verbose: bool = False):
    """Set up logging configuration."""
    log_level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )


def chunks(lst, n):
    """Yield successive n-sized chunks from lst."""
    for i in range(0, len(lst), n):
        yield lst[i:i + n]


def main(
    input_dir: Path = typer.Option(
        "data/raw",
        exists=True,
        dir_okay=True,
        file_okay=False,
        resolve_path=True,
        help="Input directory with raw documents"
    ),
    output_dir: Path = typer.Option(
        "data/processed",
        resolve_path=True,
        help="Output directory for processed data"
    ),
    file_types: str = typer.Option(
        "*.txt",
        help="File pattern to match (e.g., '*.txt', '*.pdf,*.docx')"
    ),
    chunk_size: int = typer.Option(
        512,
        help="Maximum size (in characters) for each document chunk"
    ),
    num_queries: int = typer.Option(
        100,
        help="Number of test queries to generate"
    ),
    verbose: bool = typer.Option(
        False,
        "--verbose", "-v",
        help="Enable verbose logging"
    )
):
    """
    Prepare data for embedding model evaluation
    
    Steps executed:
    1. Load and clean document files
    2. Split documents into chunks
    3. Create test queries from documents
    4. Save processed data
    """
    
    # Set up logging
    setup_logging(verbose)
    logger = logging.getLogger(__name__)
    
    try:
        logger.info("Starting data preparation")
        
        # Create output directory if it doesn't exist
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize text processor
        logger.info("Initializing Vietnamese text processor")
        text_processor = VietnameseTextProcessor()
        
        # Step 1: Find input files
        logger.info(f"Searching for files in {input_dir}")
        file_pattern = file_types.split(',')
        file_paths = []
        
        for pattern in file_pattern:
            new_files = list(input_dir.glob(pattern))
            logger.info(f"Found {len(new_files)} files matching pattern: {pattern}")
            file_paths.extend(new_files)
        
        if not file_paths:
            logger.warning("No files found matching the specified patterns")
            typer.echo("No files found matching the specified patterns")
            return
        
        # Step 2: Process documents
        logger.info("Processing documents")
        all_documents = []
        
        # Process files in batches to avoid memory issues
        batch_size = 10
        if len(file_paths) > batch_size:
            logger.info(f"Processing in batches of {batch_size} files")
        
        for file_batch in chunks(file_paths, batch_size):
            logger.info(f"Processing batch: {file_batch}")
            batch_documents = text_processor.create_documents_from_files(
                file_batch, chunk_size
            )
            
            if not batch_documents:
                logger.warning(f"No documents created from batch: {file_batch}")
                continue
                
            all_documents.extend(batch_documents)
            logger.info(f"Created {len(batch_documents)} document chunks")
        
        logger.info(f"Created {len(all_documents)} document chunks total")
        
        # Step 3: Save processed documents
        logger.info("Saving processed documents")
        documents_output_path = output_dir / "cleaned_corpus.json"
        text_processor.save_processed_data(all_documents, documents_output_path)
        
        # Step 4: Create test queries
        logger.info(f"Generating {num_queries} test queries")
        test_queries = text_processor.create_test_queries(all_documents, num_queries)
        logger.info(f"Created {len(test_queries)} test queries")
        
        # Save test queries
        logger.info("Saving test queries")
        queries_output_path = output_dir.parent / "ground_truth" / "query_document_pairs.json"
        queries_output_path.parent.mkdir(parents=True, exist_ok=True)
        text_processor.save_test_queries(test_queries, queries_output_path)
        
        logger.info("Data preparation completed successfully")
        logger.info(f"Processed saved to {output_dir}")
        logger.info(f"Test queries saved to {queries_output_path}")
        
    except Exception as e:
        logger.error(f"Data preparation failed: {str(e)}")
        raise typer.Exit(code=1)


if __name__ == "__main__":
    typer.run(main)
```

### scripts/export_results.py
```python
#!/usr/bin/env python3
"""
Results export script
Usage: python scripts/export_results.py --input results.json --output exported/
"""

import typer
import json
import logging
import pandas as pd
from pathlib import Path
from typing import Optional, List


def setup_logging(verbose: bool = False):
    """Set up logging configuration."""
    log_level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )


def export_to_csv(results: dict, output_dir: Path) -> None:
    """Export evaluation results to CSV files."""
    # Create output directory if it doesn't exist
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Extract model metrics to a DataFrame
    data = []
    for model_name, model_data in results['models'].items():
        if 'metrics' in model_data:
            row = {'Model': model_name}
            
            # Add metrics
            for metric, value in model_data['metrics'].items():
                row[metric] = value
            
            data.append(row)
    
    # Create DataFrame and save
    if data:
        df = pd.DataFrame(data)
        df.to_csv(output_dir / "model_metrics.csv", index=False)
        logging.info("Exported model metrics to model_metrics.csv")
    
    # Extract category performance to a DataFrame
    category_data = []
    for model_name, model_data in results['models'].items():
        if 'category_performance' in model_data:
            for category, metrics in model_data['category_performance'].items():
                row = {'Model': model_name, 'Category': category}
                
                # Add metrics
                for metric, value in metrics.items():
                    row[metric] = value
                
                category_data.append(row)
    
    # Create DataFrame and save
    if category_data:
        df = pd.DataFrame(category_data)
        df.to_csv(output_dir / "category_performance.csv", index=False)
        logging.info("Exported category performance to category_performance.csv")
    
    # Extract benchmark results to a DataFrame
    benchmark_data = []
    for model_name, model_data in results['models'].items():
        if 'benchmark' in model_data and 'speed' in model_data['benchmark']:
            benchmarks = model_data['benchmark']
            row = {'Model': model_name}
            
            # Speed metrics
            if 'runs' in benchmarks['speed']:
                # Get best speed across batch sizes
                best_speed = max(run['texts_per_sec'] for run in benchmarks['speed']['runs'])
                row['Best Speed (texts/sec)'] = best_speed
                
                # Batch size performance
                speed_df = pd.DataFrame(benchmarks['speed']['runs'])
                speed_df.to_csv(output_dir / f"{model_name}_speed_benchmarks.csv", index=False)
                logging.info(f"Exported speed benchmarks for {model_name} to {model_name}_speed_benchmarks.csv")
            
            # Memory metrics
            if 'memory' in benchmarks:
                for k, v in benchmarks['memory'].items():
                    row[f'Memory {k}'] = v
            
            benchmark_data.append(row)
    
    # Create DataFrame and save
    if benchmark_data:
        df = pd.DataFrame(benchmark_data)
        df.to_csv(output_dir / "benchmark_results.csv", index=False)
        logging.info("Exported benchmark results to benchmark_results.csv")


def export_rankings(results: dict, output_dir: Path) -> None:
    """Export model rankings to a CSV file."""
    if 'rankings' not in results or 'rankings_list' not in results['rankings']:
        logging.warning("No ranking information available in results")
        return
    
    # Extract rankings
    rankings = results['rankings']['rankings_list']
    
    # Create DataFrame
    data = [{'Rank': i+1, 'Model': model} for i, model in enumerate(rankings)]
    df = pd.DataFrame(data)
    
    # Save to CSV
    rankings_path = output_dir / "model_rankings.csv"
    df.to_csv(rankings_path, index=False)
    logging.info(f"Exported model rankings to {rankings_path}")


def export_summary(results: dict, output_dir: Path) -> None:
    """Export evaluation summary to a JSON file."""
    if 'summary' not in results:
        logging.warning("No summary information available in results")
        return
    
    # Save summary
    summary_path = output_dir / "evaluation_summary.json"
    with open(summary_path, 'w', encoding='utf-8') as f:
        json.dump(results['summary'], f, indent=2)
    
    logging.info(f"Exported evaluation summary to {summary_path}")


def main(
    input_path: Path = typer.Option(
        ...,
        exists=True,
        file_okay=True,
        dir_okay=False,
        readable=True,
        resolve_path=True,
        help="Path to JSON results file"
    ),
    output_dir: Path = typer.Option(
        "exported_results",
        resolve_path=True,
        help="Output directory for exported files"
    ),
    formats: List[str] = typer.Option(
        ["csv"],
        "--format",
        help="Output formats (csv, json, all)"
    ),
    verbose: bool = typer.Option(
        False,
        "--verbose", "-v",
        help="Enable verbose logging"
    )
):
    """
    Export evaluation results to different formats
    
    Available export formats:
    - csv: Export to CSV files
    - json: Export to JSON files
    - all: Export to all available formats
    """
    
    # Set up logging
    setup_logging(verbose)
    logger = logging.getLogger(__name__)
    
    try:
        logger.info("Starting results export")
        
        # Load results
        logger.info(f"Loading results from {input_path}")
        with open(input_path, 'r', encoding='utf-8') as f:
            results = json.load(f)
        
        # Determine which formats to export
        if "all" in formats:
            formats = ["csv", "json"]
        
        # Export in specified formats
        for fmt in formats:
            if fmt == "csv":
                logger.info("Exporting to CSV format")
                export_to_csv(results, output_dir)
                export_rankings(results, output_dir)
            
            elif fmt == "json":
                logger.info("Exporting to JSON format")
                
                # Save entire results
                json_path = output_dir / "evaluation_results.json"
                with open(json_path, 'w', encoding='utf-8') as f:
                    json.dump(results, f, indent=2, ensure_ascii=False)
                
                export_summary(results, output_dir)
            
            else:
                logger.warning(f"Unknown export format: {fmt}")
        
        logger.info("Results export completed successfully")
        logger.info(f"Exported files saved to {output_dir}")
        
    except Exception as e:
        logger.error(f"Results export failed: {str(e)}")
        raise typer.Exit(code=1)


if __name__ == "__main__":
    typer.run(main)
```

### tests/test_embedding_models.py
```python
import pytest
import numpy as np
import json
import tempfile
from pathlib import Path

from src.data_processor import VietnameseTextProcessor
from src.embedding_manager import EmbeddingManager, EmbeddingModelConfig
from src.evaluator import ModelEvaluator
from src.metrics import calculate_hit_rate, calculate_mrr, calculate_ndcg, calculate_precision_recall
from src.gpu_optimizer import GPUOptimizer


class TestDataProcessor:
    """ tests for VietnameseTextProcessor"""
    
    def setup_method(self):
        """Set up test data for VietnameseTextProcessor."""
        self.processor = VietnameseTextProcessor()
        
    def test_normalize_unicode(self):
        """Test Unicode normalization."""
        text = "Đây là một câu tiếng Việt có dấu"
        normalized_text = self.processor.normalize_unicode(text)
        assert normalized_text == text  # Should already be normalized
        
    def test_clean_text(self):
        """Test text cleaning."""
        text = "Đây là một câu tiếng Việt   có nhiều   khoảng trắng!"
        cleaned_text = self.processor.clean_text(text)
        assert cleaned_text.startswith("Đây là một câu tiếng Việt")  # No extra whitespace
        assert cleaned_text.endswith("có nhiều khoảng trắng!")  # Punctuation preserved
        
    def test_split_into_sentences(self):
        """Test sentence splitting."""
        text = "Đây là câu đầu. Đây là câu sau! Và đây là câu cuối."
        sentences = self.processor.split_into_sentences(text)
        assert len(sentences) == 3
        assert sentences[0] == "Đây là câu đầu"
        assert sentences[1] == "Đây là câu sau"
        assert sentences[2] == "Và đây là câu cuối"
        
    def test_create_chunks(self):
        """Test text chunking."""
        text = " ".join(["đoạn văn mẫu"] * 100)
        chunks = self.processor.create_chunks(text)
        assert len(chunks) > 0
        
        # Verify that chunks are not too long
        for chunk in chunks:
            assert len(chunk) <= self.processor.default_chunk_size
    
    def test_infer_department(self):
        """Test department inference."""
        technical_text = "Hướng dẫn sử dụng thiết bị công nghệ"
        hr_text = "Chính sách tuyển dụng nhân sự mới"
        
        assert self.processor._infer_department(technical_text) == "technical"
        assert self.processor._infer_department(hr_text) == "hr"


class TestGPUOptimizer:
    """Tests for GPUOptimizer."""
    
    def setup_method(self):
        """Set up test data for GPUOptimizer."""
        self.optimizer = GPUOptimizer()
        
    def test_detect_device(self):
        """Test device detection."""
        assert self.optimizer.device.type in ['cpu', 'cuda']
        
    def test_gpu_info(self):
        """Test GPU info extraction."""
        if self.optimizer.device.type == 'cuda':
            assert self.optimizer.gpu_info is not None
            assert 'name' in self.optimizer.gpu_info
            assert 'memory_gb' in self.optimizer.gpu_info
        else:
            assert self.optimizer.gpu_info is None
            assert self.optimizer.device.type == 'cpu'
    
    def test_monitor_gpu_usage(self):
        """Test GPU monitoring."""
        usage = self.optimizer.monitor_gpu_usage()
        assert 'device' in usage
        assert usage['device'] == self.optimizer.device.type


class TestEmbeddingManager:
    """Tests for EmbeddingManager."""
    
    def setup_method(self):
        """Set up test data for EmbeddingManager."""
        self.optimizer = GPUOptimizer()
        self.manager = EmbeddingManager(self.optimizer)
        
    def test_add_model(self):
        """Test adding a model."""
        before_count = len(self.manager.model_configs)
        self.manager.add_model(
            name="test_model",
            model_id="paraphrase-multilingual-MiniLM-L12-v2",
            provider="huggingface",
            priority=1
        )
        after_count = len(self.manager.model_configs)
        
        assert after_count == before_count + 1
        assert "test_model" in self.manager.model_configs
        
    def test_generate_embeddings(self):
        """Test embedding generation."""
        # Add a lightweight model for testing
        self.manager.add_model(
            name="test_model",
            model_id="paraphrase-multilingual-MiniLM-L12-v2",
            provider="huggingface",
            priority=1
        )
        
        test_texts = ["Câu này để test embedding", "Another test sentence"]
        embeddings = self.manager.generate_embeddings(test_texts, "test_model")
        
        assert len(embeddings) == len(test_texts)
        assert embeddings.shape[1] > 0
        
    def test_find_similar_documents(self):
        """Test document similarity search."""
        # Add a lightweight model for testing
        self.manager.add_model(
            name="test_model",
            model_id="paraphrase-multilingual-MiniLM-L12-v2",
            provider="huggingface",
            priority=1
        )
        
        # Create test embeddings (normally generated from documents)
        test_embeddings = np.random.rand(10, 384)  # 10 documents, 384 dimensions
        
        # Test query
        query = "Test query document"
        results = self.manager.find_similar_documents(query, test_embeddings, "test_model")
        
        assert isinstance(results, list)
        assert len(results) <= 5  # Default top_k=5
        assert all('document_index' in r for r in results)
        assert all('similarity_score' in r for r in results)


class TestMetrics:
    """Tests for evaluation metrics."""
    
    def setup_method(self):
        """Set up test data for metrics."""
        self.query_results = [
            ["doc1", "doc2", "doc3"],
            ["doc3", "doc1", "doc2"],
            ["doc2", "doc3", "doc1"]
        ]
        
        self.ground_truth = [
            ["doc1", "doc4"],
            ["doc2", "doc3"],
            ["doc1"]
        ]
        
    def test_hit_rate(self):
        """Test Hit Rate calculation."""
        # Test @1, @3, and @5
        hr_1 = calculate_hit_rate(self.query_results, self.ground_truth, k=1)
        hr_3 = calculate_hit_rate(self.query_results, self.ground_truth, k=3)
        hr_5 = calculate_hit_rate(self.query_results, self.ground_truth, k=5)
        
        # Hit@1 should be <= Hit@3 <= Hit@5
        assert hr_1 <= hr_3
        assert hr_3 <= hr_5
        assert 0 <= hr_1 <= 1
        assert 0 <= hr_3 <= 1
        assert 0 <= hr_5 <= 1
        
    def test_mrr(self):
        """Test Mean Reciprocal Rank calculation."""
        mrr = calculate_mrr(self.query_results, self.ground_truth)
        
        assert 0 <= mrr <= 1
        
    def test_ndcg(self):
        """Test NDCG calculation."""
        ndcg = calculate_ndcg(self.query_results, self.ground_truth)
        
        assert 0 <= ndcg <= 1
        
    def test_precision_recall(self):
        """Test Precision and Recall calculation."""
        metrics = calculate_precision_recall(self.query_results, self.ground_truth)
        
        assert 'precision_at_1' in metrics
        assert 'recall_at_1' in metrics
        assert 'precision_at_3' in metrics
        assert 'recall_at_3' in metrics
        
        # All values should be between 0 and 1
        for k in [1, 3]:
            assert 0 <= metrics[f'precision_at_{k}'] <= 1
            assert 0 <= metrics[f'recall_at_{k}'] <= 1


class TestModelEvaluator:
    """Tests for ModelEvaluator."""
    
    def setup_method(self):
        """Set up test data for ModelEvaluator."""
        self.optimizer = GPUOptimizer()
        self.embedding_manager = EmbeddingManager(self.optimizer)
        self.evaluator = ModelEvaluator(self.embedding_manager, self.optimizer)
        
    def test_load_data(self):
        """Test data loading."""
        # Create temporary directory and files
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)
            
            # Create test corpus data
            corpus_data = [
                {
                    "document_id": "doc1",
                    "content": "Đây là tài liệu thử nghiệm đầu tiên về công nghệ thông tin.",
                    "department": "technical",
                    "word_count": 10
                },
                {
                    "document_id": "doc2",
                    "content": "Thứ hai là tài liệu về chính sách nhân sự của công ty.",
                    "department": "hr",
                    "word_count": 10
                },
                {
                    "document_id": "doc3",
                    "content": "Tài liệu thứ ba nói về tài chính kế toán công ty.",
                    "department": "finance",
                    "word_count": 10
                }
            ]
            corpus_path = temp_path / "corpus.json"
            with open(corpus_path, 'w') as f:
                json.dump(corpus_data, f)
            
            # Create test queries data
            queries_data = {
                "test_cases": [
                    {
                        "query_id": "Q001",
                        "query": "Quy trình mua hàng trình giám đốc như thế nào?",
                        "relevant_documents": ["doc1"],
                        "department": "procurement",
                        "difficulty": "easy"
                    },
                    {
                        "query_id": "Q002",
                        "query": "Các tính năng của đèn hiệu sân bay loại LED?",
                        "relevant_documents": ["doc2"],
                        "department": "technical",
                        "difficulty": "medium"
                    }
                ]
            }
            queries_path = temp_path / "queries.json"
            with open(queries_path, 'w') as f:
                json.dump(queries_data, f)
            
            # Load data
            self.evaluator.load_data(queries_path, corpus_path)
            
            # Verify loaded data
            assert hasattr(self.evaluator, 'test_queries')
            assert hasattr(self.evaluator, 'corpus')
            assert len(self.evaluator.test_queries) >= 2
            assert len(self.evaluator.corpus) == 3
    
    def test_run_full_evaluation(self):
        """Test full evaluation."""
        # Add a lightweight model for testing
        self.embedding_manager.add_model(
            name="test_model",
            model_id="paraphrase-multilingual-MiniLM-L12-v2",
            provider="huggingface",
            priority=1
        )
        
        # Create temporary directory and files for evaluation
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)
            
            # Create minimal test data
            corpus_data = [
                {
                    "document_id": "doc1",
                    "content": "Đây là tài liệu thử nghiệm về công nghệ thông tin.",
                    "department": "technical",
                    "word_count": 10
                },
                {
                    "document_id": "doc2",
                    "content": "Tài liệu nói về chính sách nhân sự của công ty.",
                    "department": "hr",
                    "word_count": 10
                }
            ]
            corpus_path = temp_path / "corpus.json"
            with open(corpus_path, 'w') as f:
                json.dump(corpus_data, f)
            
            queries_data = {
                "test_cases": [
                    {
                        "query_id": "Q001",
                        "query": "Quy trình công nghệ như thế nào?",
                        "relevant_documents": ["doc1"],
                        "department": "technical",
                        "difficulty": "easy"
                    },
                    {
                        "query_id": "Q002",
                        "query": "Chính sách nhân sự công ty?",
                        "relevant_documents": ["doc2"],
                        "department": "hr",
                        "difficulty": "medium"
                    }
                ]
            }
            queries_path = temp_path / "queries.json"
            with open(queries_path, 'w') as f:
                json.dump(queries_data, f)
            
            # Load data
            self.evaluator.load_data(queries_path, corpus_path)
            
            # Run evaluation
            results = self.evaluator.run_full_evaluation(temp_path / "results.json", num_runs=1)
            
            # Verify results
            assert isinstance(results, dict)
            assert 'models' in results
            assert 'rankings' in results
            assert len(results['models']) >= 1  # At least our test model
            
            # Check model results
            model_results = results['models']['test_model']
            assert 'metrics' in model_results
            assert 'benchmark' in model_results
```

### README.md
```markdown
# Vietnamese Embedding Model Evaluator

A comprehensive framework for evaluating and comparing Vietnamese embedding models.

## Overview

This project provides tools to evaluate and compare different embedding models for Vietnamese text, with metrics including Hit Rate, Mean Reciprocal Rank (MRR), and NDCG. The framework is designed to help select the most appropriate embedding models for Vietnamese language applications.

## Features

- Multi-model evaluation with configurable metrics
- GPU optimization for faster processing
- Text processing optimized for Vietnamese
- Comprehensive visualization and reporting
- Batch processing for large document collections
- Performance benchmarking

## Installation

1. Clone the repository
```bash
git clone <repository-url>
cd vietnamese_embedding_evaluator
```

2. Create a virtual environment (optional but recommended)
```bash
python -m venv venv
source venv/bin/activate  # On Windows use `venv\Scripts\activate`
```

3. Install dependencies
```bash
pip install -r requirements.txt
```

4. Install the package in development mode
```bash
pip install -e .
```

## Quick Start

1. Prepare your data
```bash
python scripts/prepare_data.py --input data/raw --output data/processed
```

2. Run evaluation
```bash
python scripts/run_evaluation.py --config configs/models.json
```

3. View results
```bash
# Results will be saved to the reports/ directory
open reports/final_report.html
```

## Project Structure

```
vietnamese_embedding_evaluator/
├── configs/                    # Configuration files
│   ├── models.json            # Embedding models to evaluate
│   ├── evaluation_settings.json # Evaluation parameters
│   └── gpu_settings.json      # GPU optimization settings
├── data/                      # Data directories
│   ├── raw/                   # Raw documents
│   ├── processed/             # Processed documents and queries
│   └── ground_truth/          # Ground truth datasets
├── src/                       # Source code
│   ├── data_processor.py      # Vietnamese text processing
│   ├── embedding_manager.py   # Model management
│   ├── evaluator.py          # Evaluation logic
│   ├── metrics.py            # Evaluation metrics
│   ├── gpu_optimizer.py      # GPU optimization
│   └── visualizer.py         # Visualization and reporting
├── scripts/                   # Scripts for various tasks
│   ├── run_evaluation.py     # Main evaluation runner
│   ├── prepare_data.py       # Data preparation script
│   └── export_results.py     # Export results
├── tests/                     # Unit tests
│   └── test_embedding_models.py
├── notebooks/                # Example notebooks
├── reports/                  # Generated reports and charts
└── README.md
```

## Configuration

### Embedding Models

Configure models to evaluate in `configs/models.json`. Each model should have:
- `name`: Unique identifier
- `model_id`: Hugging Face model identifier
- `provider`: Model provider (currently only "huggingface" is supported)
- `max_seq_length`: Maximum sequence length
- `batch_size`: Batch size for processing
- `normalize_embeddings`: Whether to normalize embeddings
- `priority`: Priority level (lower is higher priority)

Example:
```json
{
  "models": [
    {
      "name": "vietnamese_embedding_v1",
      "model_id": "AITeamVN/Vietnamese_Embedding",
      "provider": "huggingface",
      "max_seq_length": 512,
      "batch_size": 32,
      "normalize_embeddings": true,
      "priority": 1
    }
  ]
}
```

### Evaluation Settings

Configure evaluation parameters in `configs/evaluation_settings.json`:
- `top_k`: Values of K for metrics like Hit Rate@K
- `test_cases_file`: Path to ground truth queries
- `corpus_file`: Path to document corpus

## Data Preparation

### Raw Documents

Place your documents in `data/raw/`. Supported formats include:
- Text files (`.txt`)
- PDF files (`.pdf`)
- Word documents (`.docx`)

The text processor will:
1. Clean and normalize Vietnamese text
2. Split documents into manageable chunks (default: 512 characters)
3. Create test queries from document content

### Ground Truth

For evaluation, you need ground truth data with relevant document mappings for each query. Example format:

```json
{
  "test_cases": [
    {
      "query_id": "Q001",
      "query": "Quy trình mua hàng trình giám đốc như thế nào?",
      "relevant_documents": ["DOC_001", "DOC_003"],
      "department": "procurement",
      "difficulty": "easy"
    }
  ]
}
```

## Running Evaluation

The main evaluation script provides several options:

```bash
python scripts/run_evaluation.py \
    --config configs/models.json \
    --data data \
    --output reports \
    --gpu-enabled \
    --generate-report
```

Options:
- `--config`: Path to models configuration file
- `--data`: Path to data directory (default: data/)
- `--output`: Output directory for results (default: reports/)
- `--gpu-enabled`: Enable GPU acceleration if available
- `--generate-report`: Generate visualization reports

## Results

The evaluation framework produces:

1. **Metrics**:
   - Hit Rate@K
   - Mean Reciprocal Rank (MRR)
   - Normalized Discounted Cumulative Gain (NDCG)
   - Precision and Recall
   - Mean Average Precision (MAP)
   - Success Rate at different thresholds

2. **Benchmarks**:
   - Embedding generation speed
   - GPU memory usage
   - Model parameter count

3. **Visualizations**:
   - Comparison charts
   - Category performance heatmaps
   - Speed vs accuracy scatter plots
   - Memory usage comparisons

4. **Reports**:
   - HTML report with interactive charts
   - CSV exports for further analysis
   - Model rankings

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests for new functionality
5. Run the test suite
6. Submit a pull request

## License

This project is licensed under the MIT License.

## Support

For issues and questions, please open an issue on the GitHub repository.
```

I've now implemented all the source code files according to the tutorial document. The implementation includes:

1. **Project structure** with all directories and subdirectories as specified in the tutorial
2. **Configuration files** (models.json, evaluation_settings.json, gpu_settings.json)
3. **Core modules** (data_processor.py, embedding_manager.py, evaluator.py, metrics.py, gpu_optimizer.py, visualizer.py)
4. **Scripts** (run_evaluation.py, prepare_data.py, export_results.py)
5. **Tests** (test_embedding_models.py with unit tests for all core components)
6. **Dependencies** (requirements.txt and setup.py for easy installation)
7. **Documentation** (README.md with project overview and usage instructions)

The implementation follows all the specifications from the tutorial, including:
- Vietnamese text processing capabilities
- Support for multiple embedding models
- Evaluation metrics (Hit Rate, MRR, NDCG, etc.)
- GPU optimization
- Visualization and reporting
- Test-driven development approach

To use this system, users would need to:
1. Install the dependencies
2. Prepare their data (documents and queries)
3. Configure the models they want to evaluate
4. Run the evaluation script
5. Review the generated reports and visualizations

This implementation provides a comprehensive framework for evaluating and selecting embedding models for Vietnamese text applications.
