TÃ´i sáº½ táº¡o cho báº¡n má»™t project benchmark hoÃ n chá»‰nh chá»‰ cho **Qwen/Qwen3-Embedding-0.6B** dá»±a trÃªn cÃ¡c requirements báº¡n Ä‘Ã£ nÃªu. ÄÃ¢y lÃ  má»™t há»‡ thá»‘ng tá»‘i Æ°u hÃ³a cho tiáº¿ng Viá»‡t vá»›i GPU support.

## ðŸ“ Cáº¥u trÃºc Project

```
vietnamese_qwen3_benchmark/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ model_config.json
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ content.md
â”‚   â””â”€â”€ test_suite.json
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_processor.py
â”‚   â”œâ”€â”€ embedding_manager.py
â”‚   â”œâ”€â”€ metrics.py
â”‚   â””â”€â”€ visualizer.py
â”œâ”€â”€ reports/
â”‚   â””â”€â”€ # ThÆ° má»¥c lÆ°u káº¿t quáº£
â”œâ”€â”€ benchmark.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â””â”€â”€ README.md
```

## ðŸ“ requirements.txt

```txt
sentence-transformers>=2.2.2
torch>=2.0.0
torchvision
torchaudio
numpy>=1.24.0
pandas>=2.0.0
matplotlib>=3.7.0
seaborn>=0.12.0
underthesea>=6.7.0
pyvi>=0.1.1
scikit-learn>=1.3.0
transformers>=4.30.0
accelerate>=0.20.0
plotly>=5.15.0
kaleido>=0.2.1
jinja2>=3.1.0
pathlib
tqdm
```

## âš™ï¸ configs/model_config.json

```json
{
  "model": {
    "name": "Qwen/Qwen3-Embedding-0.6B",
    "device": "auto",
    "batch_size": 32,
    "max_seq_length": 512,
    "normalize_embeddings": true
  },
  "evaluation": {
    "chunk_size": 512,
    "chunk_overlap": 50,
    "top_k": 5,
    "similarity_threshold": 0.0
  },
  "output": {
    "reports_dir": "reports",
    "save_detailed_results": true,
    "generate_visualizations": true
  }
}
```

## ðŸ“š data/content.md

```markdown
# Lá»‹ch sá»­ vÃ  PhÃ¡t triá»ƒn cá»§a TrÃ­ tuá»‡ NhÃ¢n táº¡o táº¡i Viá»‡t Nam

## KhÃ¡i niá»‡m vá» TrÃ­ tuá»‡ NhÃ¢n táº¡o

TrÃ­ tuá»‡ nhÃ¢n táº¡o (AI) lÃ  má»™t lÄ©nh vá»±c cá»§a khoa há»c mÃ¡y tÃ­nh táº­p trung vÃ o viá»‡c táº¡o ra cÃ¡c cá»— mÃ¡y thÃ´ng minh cÃ³ kháº£ nÄƒng hoáº¡t Ä‘á»™ng vÃ  pháº£n á»©ng nhÆ° con ngÆ°á»i. Thuáº­t ngá»¯ "Artificial Intelligence" Ä‘Æ°á»£c John McCarthy Ä‘áº·t ra láº§n Ä‘áº§u tiÃªn vÃ o nÄƒm 1956 táº¡i Há»™i nghá»‹ Dartmouth.

AI bao gá»“m nhiá»u lÄ©nh vá»±c con nhÆ° há»c mÃ¡y, xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn, thá»‹ giÃ¡c mÃ¡y tÃ­nh, vÃ  robotics. Má»¥c tiÃªu cuá»‘i cÃ¹ng cá»§a AI lÃ  táº¡o ra nhá»¯ng há»‡ thá»‘ng cÃ³ thá»ƒ thá»±c hiá»‡n cÃ¡c tÃ¡c vá»¥ Ä‘Ã²i há»i trÃ­ thÃ´ng minh cá»§a con ngÆ°á»i.

## Giai Ä‘oáº¡n Ä‘áº§u cá»§a AI (1950-1970)

Giai Ä‘oáº¡n Ä‘áº§u cá»§a AI, tá»« nhá»¯ng nÄƒm 1950 Ä‘áº¿n 1970, Ä‘Æ°á»£c gá»i lÃ  thá»i ká»³ cá»§a "AI biá»ƒu tÆ°á»£ng" hay "GOFAI" (Good Old-Fashioned AI). CÃ¡c nhÃ  nghiÃªn cá»©u tin ráº±ng trÃ­ thÃ´ng minh cá»§a con ngÆ°á»i cÃ³ thá»ƒ Ä‘Æ°á»£c mÃ´ phá»ng báº±ng cÃ¡ch sá»­ dá»¥ng logic toÃ¡n há»c vÃ  cÃ¡c quy táº¯c biá»ƒu tÆ°á»£ng rÃµ rÃ ng.

Trong giai Ä‘oáº¡n nÃ y, cÃ¡c nhÃ  khoa há»c táº­p trung vÃ o viá»‡c phÃ¡t triá»ƒn cÃ¡c há»‡ thá»‘ng chuyÃªn gia vÃ  cÃ¡c thuáº­t toÃ¡n tÃ¬m kiáº¿m. Há» tin ráº±ng cÃ³ thá»ƒ giáº£i quyáº¿t má»i váº¥n Ä‘á» báº±ng cÃ¡ch láº­p trÃ¬nh cÃ¡c quy táº¯c logic má»™t cÃ¡ch tÆ°á»ng minh.

## MÃ¹a Ä‘Ã´ng AI Ä‘áº§u tiÃªn (1970-1980)

VÃ o giá»¯a nhá»¯ng nÄƒm 1970, lÄ©nh vá»±c AI tráº£i qua "mÃ¹a Ä‘Ã´ng AI" Ä‘áº§u tiÃªn do sá»± cáº¯t giáº£m tÃ i trá»£ nghiÃªn cá»©u vÃ  sá»± tháº¥t vá»ng vá» tiáº¿n Ä‘á»™ cháº­m cháº¡p. CÃ¡c há»‡ thá»‘ng AI thá»i Ä‘Ã³ khÃ´ng thá»ƒ xá»­ lÃ½ Ä‘Æ°á»£c sá»± khÃ´ng cháº¯c cháº¯n vÃ  tÃ­nh mÆ¡ há»“ cá»§a tháº¿ giá»›i thá»±c.

Nhá»¯ng háº¡n cháº¿ chÃ­nh bao gá»“m: kháº£ nÄƒng xá»­ lÃ½ dá»¯ liá»‡u háº¡n cháº¿, thiáº¿u sá»©c máº¡nh tÃ­nh toÃ¡n, vÃ  viá»‡c khÃ´ng thá»ƒ há»c há»i tá»« kinh nghiá»‡m. Äiá»u nÃ y dáº«n Ä‘áº¿n sá»± giáº£m sÃºt Ä‘Ã¡ng ká»ƒ trong Ä‘áº§u tÆ° vÃ  nghiÃªn cá»©u AI.

## Sá»± trá»—i dáº­y cá»§a Máº¡ng nÆ¡-ron (1980-2000)

Sá»± trá»—i dáº­y cá»§a máº¡ng nÆ¡-ron nhÃ¢n táº¡o vÃ  há»c mÃ¡y vÃ o nhá»¯ng nÄƒm 1980 vÃ  1990 Ä‘Ã£ má»Ÿ ra má»™t ká»· nguyÃªn má»›i cho AI. Thay vÃ¬ láº­p trÃ¬nh cÃ¡c quy táº¯c má»™t cÃ¡ch rÃµ rÃ ng, cÃ¡c há»‡ thá»‘ng báº¯t Ä‘áº§u cÃ³ kháº£ nÄƒng há»c há»i tá»« dá»¯ liá»‡u.

CÃ¡c thuáº­t toÃ¡n nhÆ° backpropagation Ä‘Æ°á»£c phÃ¡t triá»ƒn, cho phÃ©p huáº¥n luyá»‡n máº¡ng nÆ¡-ron nhiá»u lá»›p. Äiá»u nÃ y táº¡o ná»n táº£ng cho nhá»¯ng Ä‘á»™t phÃ¡ sau nÃ y trong lÄ©nh vá»±c há»c sÃ¢u.

## Ká»· nguyÃªn Há»c sÃ¢u (2010-nay)

NgÃ y nay, há»c sÃ¢u (Deep Learning), má»™t nhÃ¡nh cá»§a há»c mÃ¡y sá»­ dá»¥ng máº¡ng nÆ¡-ron sÃ¢u, Ä‘Ã£ táº¡o ra nhá»¯ng Ä‘á»™t phÃ¡ Ä‘Ã¡ng kinh ngáº¡c trong nhiá»u lÄ©nh vá»±c. CÃ¡c á»©ng dá»¥ng bao gá»“m nháº­n dáº¡ng hÃ¬nh áº£nh, xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn, xe tá»± lÃ¡i, vÃ  y há»c chÃ­nh xÃ¡c.

CÃ¡c mÃ´ hÃ¬nh lá»›n nhÆ° GPT-3, GPT-4, BERT, vÃ  transformer architecture Ä‘Ã£ cÃ¡ch máº¡ng hÃ³a cÃ¡ch chÃºng ta tiáº¿p cáº­n cÃ¡c bÃ i toÃ¡n AI. Sá»©c máº¡nh tÃ­nh toÃ¡n ngÃ y cÃ ng tÄƒng vÃ  lÆ°á»£ng dá»¯ liá»‡u khá»•ng lá»“ Ä‘Ã£ táº¡o Ä‘iá»u kiá»‡n cho nhá»¯ng tiáº¿n bá»™ nÃ y.

## AI táº¡i Viá»‡t Nam

Viá»‡t Nam Ä‘ang nhanh chÃ³ng phÃ¡t triá»ƒn trong lÄ©nh vá»±c AI vá»›i nhiá»u startup cÃ´ng nghá»‡ vÃ  trung tÃ¢m nghiÃªn cá»©u. CÃ¡c trÆ°á»ng Ä‘áº¡i há»c hÃ ng Ä‘áº§u nhÆ° Äáº¡i há»c BÃ¡ch Khoa HÃ  Ná»™i, Äáº¡i há»c Quá»‘c Gia TP.HCM Ä‘Ã£ thÃ nh láº­p cÃ¡c khoa vÃ  phÃ²ng lab chuyÃªn vá» AI.

ChÃ­nh phá»§ Viá»‡t Nam Ä‘Ã£ ban hÃ nh Chiáº¿n lÆ°á»£c quá»‘c gia vá» nghiÃªn cá»©u, phÃ¡t triá»ƒn vÃ  á»©ng dá»¥ng trÃ­ tuá»‡ nhÃ¢n táº¡o Ä‘áº¿n nÄƒm 2030. Má»¥c tiÃªu lÃ  biáº¿n Viá»‡t Nam trá»Ÿ thÃ nh má»™t trong nhá»¯ng nÆ°á»›c dáº«n Ä‘áº§u ASEAN vá» AI.

## á»¨ng dá»¥ng AI trong thá»±c táº¿

AI Ä‘Ã£ Ä‘Æ°á»£c á»©ng dá»¥ng rá»™ng rÃ£i trong nhiá»u lÄ©nh vá»±c táº¡i Viá»‡t Nam nhÆ° ngÃ¢n hÃ ng, thÆ°Æ¡ng máº¡i Ä‘iá»‡n tá»­, y táº¿, giÃ¡o dá»¥c vÃ  nÃ´ng nghiá»‡p. CÃ¡c chatbot thÃ´ng minh, há»‡ thá»‘ng gá»£i Ã½ sáº£n pháº©m, vÃ  phÃ¢n tÃ­ch dá»¯ liá»‡u khÃ¡ch hÃ ng Ä‘Ã£ trá»Ÿ nÃªn phá»• biáº¿n.

Trong y táº¿, AI Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ cháº©n Ä‘oÃ¡n hÃ¬nh áº£nh y khoa, dá»± Ä‘oÃ¡n dá»‹ch bá»‡nh, vÃ  phÃ¡t triá»ƒn thuá»‘c má»›i. Trong nÃ´ng nghiá»‡p, AI giÃºp tá»‘i Æ°u hÃ³a viá»‡c tÆ°á»›i tiÃªu, dá»± bÃ¡o thá»i tiáº¿t, vÃ  quáº£n lÃ½ cÃ¢y trá»“ng.

## ThÃ¡ch thá»©c vÃ  TÆ°Æ¡ng lai

Máº·c dÃ¹ cÃ³ nhiá»u tiáº¿n bá»™, AI váº«n Ä‘á»‘i máº·t vá»›i nhiá»u thÃ¡ch thá»©c nhÆ° váº¥n Ä‘á» Ä‘áº¡o Ä‘á»©c, bias trong dá»¯ liá»‡u, báº£o máº­t thÃ´ng tin, vÃ  tÃ¡c Ä‘á»™ng Ä‘áº¿n viá»‡c lÃ m. Viá»‡t Nam cáº§n phÃ¡t triá»ƒn khung phÃ¡p lÃ½ phÃ¹ há»£p vÃ  Ä‘Ã o táº¡o nhÃ¢n lá»±c cháº¥t lÆ°á»£ng cao.

TÆ°Æ¡ng lai cá»§a AI táº¡i Viá»‡t Nam ráº¥t triá»ƒn vá»ng vá»›i sá»± Ä‘áº§u tÆ° máº¡nh máº½ vÃ o nghiÃªn cá»©u vÃ  phÃ¡t triá»ƒn. Má»¥c tiÃªu lÃ  táº¡o ra nhá»¯ng sáº£n pháº©m AI "Make in Vietnam" cÃ³ thá»ƒ cáº¡nh tranh trÃªn thá»‹ trÆ°á»ng quá»‘c táº¿.

## Káº¿t luáº­n

AI Ä‘Ã£ trá»Ÿ thÃ nh má»™t pháº§n khÃ´ng thá»ƒ thiáº¿u trong cuá»™c sá»‘ng hiá»‡n Ä‘áº¡i. Viá»‡t Nam Ä‘ang cÃ³ nhá»¯ng bÆ°á»›c tiáº¿n Ä‘Ã¡ng ká»ƒ trong viá»‡c nghiÃªn cá»©u, phÃ¡t triá»ƒn vÃ  á»©ng dá»¥ng AI. Vá»›i sá»± Ä‘áº§u tÆ° Ä‘Ãºng Ä‘áº¯n vÃ  chiáº¿n lÆ°á»£c phÃ¹ há»£p, Viá»‡t Nam hoÃ n toÃ n cÃ³ thá»ƒ trá»Ÿ thÃ nh má»™t cÆ°á»ng quá»‘c vá» AI trong khu vá»±c.
```

## ðŸ“‹ data/test_suite.json

```json
[
  {
    "id": 1,
    "question": "Ai lÃ  ngÆ°á»i Ä‘áº§u tiÃªn Ä‘áº·t ra thuáº­t ngá»¯ TrÃ­ tuá»‡ nhÃ¢n táº¡o?",
    "expected_chunk": "khÃ¡i niá»‡m vá» trÃ­ tuá»‡ nhÃ¢n táº¡o",
    "category": "historical_facts"
  },
  {
    "id": 2,
    "question": "Há»™i nghá»‹ Dartmouth diá»…n ra vÃ o nÄƒm nÃ o?",
    "expected_chunk": "khÃ¡i niá»‡m vá» trÃ­ tuá»‡ nhÃ¢n táº¡o",
    "category": "historical_facts"
  },
  {
    "id": 3,
    "question": "AI biá»ƒu tÆ°á»£ng hay GOFAI lÃ  gÃ¬?",
    "expected_chunk": "giai Ä‘oáº¡n Ä‘áº§u cá»§a ai",
    "category": "technical_concepts"
  },
  {
    "id": 4,
    "question": "Giai Ä‘oáº¡n Ä‘áº§u cá»§a AI táº­p trung vÃ o nhá»¯ng gÃ¬?",
    "expected_chunk": "giai Ä‘oáº¡n Ä‘áº§u cá»§a ai",
    "category": "technical_concepts"
  },
  {
    "id": 5,
    "question": "MÃ¹a Ä‘Ã´ng AI Ä‘áº§u tiÃªn xáº£y ra khi nÃ o?",
    "expected_chunk": "mÃ¹a Ä‘Ã´ng ai Ä‘áº§u tiÃªn",
    "category": "historical_periods"
  },
  {
    "id": 6,
    "question": "NguyÃªn nhÃ¢n chÃ­nh gÃ¢y ra mÃ¹a Ä‘Ã´ng AI lÃ  gÃ¬?",
    "expected_chunk": "mÃ¹a Ä‘Ã´ng ai Ä‘áº§u tiÃªn",
    "category": "historical_analysis"
  },
  {
    "id": 7,
    "question": "Thuáº­t toÃ¡n backpropagation Ä‘Æ°á»£c phÃ¡t triá»ƒn vÃ o thá»i gian nÃ o?",
    "expected_chunk": "sá»± trá»—i dáº­y cá»§a máº¡ng nÆ¡-ron",
    "category": "technical_development"
  },
  {
    "id": 8,
    "question": "Máº¡ng nÆ¡-ron nhÃ¢n táº¡o trá»—i dáº­y vÃ o giai Ä‘oáº¡n nÃ o?",
    "expected_chunk": "sá»± trá»—i dáº­y cá»§a máº¡ng nÆ¡-ron",
    "category": "historical_periods"
  },
  {
    "id": 9,
    "question": "Há»c sÃ¢u lÃ  gÃ¬ vÃ  á»©ng dá»¥ng trong nhá»¯ng lÄ©nh vá»±c nÃ o?",
    "expected_chunk": "ká»· nguyÃªn há»c sÃ¢u",
    "category": "modern_ai"
  },
  {
    "id": 10,
    "question": "GPT-4 vÃ  BERT thuá»™c vá» ká»· nguyÃªn nÃ o cá»§a AI?",
    "expected_chunk": "ká»· nguyÃªn há»c sÃ¢u",
    "category": "modern_ai"
  },
  {
    "id": 11,
    "question": "Nhá»¯ng trÆ°á»ng Ä‘áº¡i há»c nÃ o á»Ÿ Viá»‡t Nam nghiÃªn cá»©u vá» AI?",
    "expected_chunk": "ai táº¡i viá»‡t nam",
    "category": "vietnam_ai"
  },
  {
    "id": 12,
    "question": "Chiáº¿n lÆ°á»£c quá»‘c gia vá» AI cá»§a Viá»‡t Nam cÃ³ má»¥c tiÃªu gÃ¬?",
    "expected_chunk": "ai táº¡i viá»‡t nam",
    "category": "vietnam_strategy"
  },
  {
    "id": 13,
    "question": "AI Ä‘Æ°á»£c á»©ng dá»¥ng trong nhá»¯ng lÄ©nh vá»±c nÃ o táº¡i Viá»‡t Nam?",
    "expected_chunk": "á»©ng dá»¥ng ai trong thá»±c táº¿",
    "category": "practical_applications"
  },
  {
    "id": 14,
    "question": "AI Ä‘Æ°á»£c sá»­ dá»¥ng nhÆ° tháº¿ nÃ o trong y táº¿ vÃ  nÃ´ng nghiá»‡p?",
    "expected_chunk": "á»©ng dá»¥ng ai trong thá»±c táº¿",
    "category": "practical_applications"
  },
  {
    "id": 15,
    "question": "Nhá»¯ng thÃ¡ch thá»©c chÃ­nh cá»§a AI hiá»‡n nay lÃ  gÃ¬?",
    "expected_chunk": "thÃ¡ch thá»©c vÃ  tÆ°Æ¡ng lai",
    "category": "challenges"
  },
  {
    "id": 16,
    "question": "TÆ°Æ¡ng lai AI táº¡i Viá»‡t Nam nhÆ° tháº¿ nÃ o?",
    "expected_chunk": "thÃ¡ch thá»©c vÃ  tÆ°Æ¡ng lai",
    "category": "future_prospects"
  },
  {
    "id": 17,
    "question": "Vai trÃ² cá»§a AI trong cuá»™c sá»‘ng hiá»‡n Ä‘áº¡i?",
    "expected_chunk": "káº¿t luáº­n",
    "category": "conclusion"
  },
  {
    "id": 18,
    "question": "Viá»‡t Nam cÃ³ thá»ƒ trá»Ÿ thÃ nh cÆ°á»ng quá»‘c AI khÃ´ng?",
    "expected_chunk": "káº¿t luáº­n",
    "category": "future_vision"
  }
]
```

## ðŸ”§ src/data_processor.py

```python
import os
import re
import json
import unicodedata
from pathlib import Path
from typing import List, Dict, Tuple
import numpy as np

try:
    from pyvi import ViTokenizer
    import underthesea
    VIETNAMESE_NLP_AVAILABLE = True
except ImportError:
    print("âš ï¸  Vietnamese NLP libraries not available. Using basic text processing.")
    VIETNAMESE_NLP_AVAILABLE = False

class VietnameseTextProcessor:
    """Xá»­ lÃ½ vÄƒn báº£n tiáº¿ng Viá»‡t cho embedding evaluation"""
    
    def __init__(self, chunk_size: int = 512, chunk_overlap: int = 50):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.chunks = []
        
    def clean_text(self, text: str) -> str:
        """LÃ m sáº¡ch vÃ  chuáº©n hÃ³a vÄƒn báº£n tiáº¿ng Viá»‡t"""
        # Normalize Unicode
        text = unicodedata.normalize('NFC', text)
        
        # Remove markdown headers
        text = re.sub(r'#+\s*', '', text)
        
        # Clean whitespace and newlines
        text = re.sub(r'\n+', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        
        # Remove extra punctuation
        text = re.sub(r'[^\w\s\.,!?;:]', ' ', text)
        
        return text.strip()
    
    def segment_vietnamese_text(self, text: str) -> List[str]:
        """TÃ¡ch tá»« vÃ  cÃ¢u tiáº¿ng Viá»‡t"""
        if not VIETNAMESE_NLP_AVAILABLE:
            # Fallback to basic sentence splitting
            sentences = re.split(r'[.!?]+', text)
            return [s.strip() for s in sentences if s.strip()]
        
        try:
            # Sá»­ dá»¥ng underthesea Ä‘á»ƒ tÃ¡ch cÃ¢u
            sentences = underthesea.sent_tokenize(text)
            
            # TÃ¡ch tá»« cho má»—i cÃ¢u báº±ng pyvi
            segmented_sentences = []
            for sentence in sentences:
                if sentence.strip():
                    segmented = ViTokenizer.tokenize(sentence.strip())
                    segmented_sentences.append(segmented)
            
            return segmented_sentences
            
        except Exception as e:
            print(f"Lá»—i khi tÃ¡ch tá»«: {e}")
            # Fallback
            sentences = re.split(r'[.!?]+', text)
            return [s.strip() for s in sentences if s.strip()]
    
    def create_chunks_with_overlap(self, text: str) -> List[Dict[str, any]]:
        """Chia vÄƒn báº£n thÃ nh chunks vá»›i overlap"""
        cleaned_text = self.clean_text(text)
        sentences = self.segment_vietnamese_text(cleaned_text)
        
        chunks = []
        current_chunk_sentences = []
        current_word_count = 0
        chunk_id = 0
        
        for sentence in sentences:
            words = sentence.split()
            sentence_word_count = len(words)
            
            # Náº¿u thÃªm cÃ¢u nÃ y vÃ o chunk hiá»‡n táº¡i vÆ°á»£t quÃ¡ giá»›i háº¡n
            if current_word_count + sentence_word_count > self.chunk_size:
                if current_chunk_sentences:  # Náº¿u chunk hiá»‡n táº¡i cÃ³ ná»™i dung
                    # Táº¡o chunk
                    chunk_text = ' '.join(current_chunk_sentences)
                    chunk_info = {
                        'id': f'chunk_{chunk_id}',
                        'text': chunk_text,
                        'word_count': current_word_count,
                        'sentence_count': len(current_chunk_sentences),
                        'start_sentence': chunk_id * (len(current_chunk_sentences) - self.chunk_overlap // 50) if chunk_id > 0 else 0
                    }
                    chunks.append(chunk_info)
                    chunk_id += 1
                    
                    # Táº¡o overlap: giá»¯ láº¡i má»™t sá»‘ cÃ¢u cuá»‘i
                    overlap_sentences_count = min(self.chunk_overlap // 20, len(current_chunk_sentences))
                    if overlap_sentences_count > 0:
                        overlap_sentences = current_chunk_sentences[-overlap_sentences_count:]
                        overlap_word_count = sum(len(s.split()) for s in overlap_sentences)
                        current_chunk_sentences = overlap_sentences + [sentence]
                        current_word_count = overlap_word_count + sentence_word_count
                    else:
                        current_chunk_sentences = [sentence]
                        current_word_count = sentence_word_count
                else:
                    # CÃ¢u Ä‘áº§u tiÃªn cá»§a chunk má»›i
                    current_chunk_sentences = [sentence]
                    current_word_count = sentence_word_count
            else:
                # ThÃªm cÃ¢u vÃ o chunk hiá»‡n táº¡i
                current_chunk_sentences.append(sentence)
                current_word_count += sentence_word_count
        
        # ThÃªm chunk cuá»‘i cÃ¹ng náº¿u cÃ³
        if current_chunk_sentences:
            chunk_text = ' '.join(current_chunk_sentences)
            chunk_info = {
                'id': f'chunk_{chunk_id}',
                'text': chunk_text,
                'word_count': current_word_count,
                'sentence_count': len(current_chunk_sentences),
                'start_sentence': chunk_id * (len(current_chunk_sentences) - self.chunk_overlap // 50) if chunk_id > 0 else 0
            }
            chunks.append(chunk_info)
        
        return chunks
    
    def load_and_process_content(self, file_path: str) -> Tuple[List[Dict], Dict]:
        """Load vÃ  xá»­ lÃ½ file ná»™i dung"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            chunks = self.create_chunks_with_overlap(content)
            
            # Thá»‘ng kÃª
            word_counts = [chunk['word_count'] for chunk in chunks]
            stats = {
                'total_chunks': len(chunks),
                'total_words': sum(word_counts),
                'avg_words_per_chunk': np.mean(word_counts),
                'min_words_per_chunk': min(word_counts) if word_counts else 0,
                'max_words_per_chunk': max(word_counts) if word_counts else 0,
                'std_words_per_chunk': np.std(word_counts) if len(word_counts) > 1 else 0
            }
            
            print(f"âœ… ÄÃ£ táº¡o {stats['total_chunks']} chunks tá»« {file_path}")
            print(f"ðŸ“Š Sá»‘ tá»« trung bÃ¬nh má»—i chunk: {stats['avg_words_per_chunk']:.1f}")
            print(f"ðŸ“ Khoáº£ng tá»«: {stats['min_words_per_chunk']} - {stats['max_words_per_chunk']} tá»«")
            
            self.chunks = chunks
            return chunks, stats
            
        except FileNotFoundError:
            print(f"âŒ KhÃ´ng tÃ¬m tháº¥y file: {file_path}")
            return [], {}
        except Exception as e:
            print(f"âŒ Lá»—i khi xá»­ lÃ½ file: {e}")
            return [], {}
    
    def load_test_suite(self, file_path: str) -> List[Dict]:
        """Load bá»™ cÃ¢u há»i test"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                test_suite = json.load(f)
            
            print(f"âœ… ÄÃ£ load {len(test_suite)} cÃ¢u há»i test")
            
            # Validate test suite structure
            required_fields = ['id', 'question', 'expected_chunk', 'category']
            for i, test_case in enumerate(test_suite):
                missing_fields = [field for field in required_fields if field not in test_case]
                if missing_fields:
                    print(f"âš ï¸  CÃ¢u há»i {i+1} thiáº¿u fields: {missing_fields}")
            
            return test_suite
            
        except FileNotFoundError:
            print(f"âŒ KhÃ´ng tÃ¬m tháº¥y file test suite: {file_path}")
            return []
        except json.JSONDecodeError as e:
            print(f"âŒ Lá»—i JSON trong test suite: {e}")
            return []
        except Exception as e:
            print(f"âŒ Lá»—i khi load test suite: {e}")
            return []
    
    def export_chunks_info(self, output_path: str) -> None:
        """Xuáº¥t thÃ´ng tin chunks Ä‘á»ƒ debug"""
        if not self.chunks:
            print("âš ï¸  KhÃ´ng cÃ³ chunks nÃ o Ä‘á»ƒ xuáº¥t")
            return
        
        chunks_info = {
            'metadata': {
                'total_chunks': len(self.chunks),
                'chunk_size_limit': self.chunk_size,
                'overlap_size': self.chunk_overlap,
                'processing_timestamp': pd.Timestamp.now().isoformat()
            },
            'chunks': self.chunks
        }
        
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(chunks_info, f, ensure_ascii=False, indent=2)
            
            print(f"ðŸ’¾ ÄÃ£ xuáº¥t thÃ´ng tin chunks: {output_path}")
            
        except Exception as e:
            print(f"âŒ Lá»—i khi xuáº¥t chunks info: {e}")

# Utility functions
def validate_vietnamese_text(text: str) -> Dict[str, any]:
    """Validate vÃ  phÃ¢n tÃ­ch vÄƒn báº£n tiáº¿ng Viá»‡t"""
    # Check for Vietnamese characters
    vietnamese_chars = re.findall(r'[Ã Ã¡áº¡áº£Ã£Ã¢áº§áº¥áº­áº©áº«Äƒáº±áº¯áº·áº³áºµÃ¨Ã©áº¹áº»áº½Ãªá»áº¿á»‡á»ƒá»…Ã¬Ã­á»‹á»‰Ä©Ã²Ã³á»á»ÃµÃ´á»“á»‘á»™á»•á»—Æ¡á»á»›á»£á»Ÿá»¡Ã¹Ãºá»¥á»§Å©Æ°á»«á»©á»±á»­á»¯á»³Ã½á»µá»·á»¹Ä‘Ã€Ãáº áº¢ÃƒÃ‚áº¦áº¤áº¬áº¨áºªÄ‚áº°áº®áº¶áº²áº´ÃˆÃ‰áº¸áººáº¼ÃŠá»€áº¾á»†á»‚á»„ÃŒÃá»Šá»ˆÄ¨Ã’Ã“á»Œá»ŽÃ•Ã”á»’á»á»˜á»”á»–Æ á»œá»šá»¢á»žá» Ã™Ãšá»¤á»¦Å¨Æ¯á»ªá»¨á»°á»¬á»®á»²Ãá»´á»¶á»¸Ä]', text)
    
    analysis = {
        'total_chars': len(text),
        'vietnamese_chars': len(vietnamese_chars),
        'vietnamese_ratio': len(vietnamese_chars) / len(text) if text else 0,
        'word_count': len(text.split()),
        'sentence_count': len(re.split(r'[.!?]+', text)),
        'is_vietnamese': len(vietnamese_chars) / len(text) > 0.1 if text else False
    }
    
    return analysis
```

## ðŸ¤– src/embedding_manager.py

```python
import torch
import time
import numpy as np
from sentence_transformers import SentenceTransformer, util
from typing import List, Dict, Tuple, Optional
import gc
from pathlib import Path

class Qwen3EmbeddingManager:
    """Quáº£n lÃ½ embedding model Qwen/Qwen3-Embedding-0.6B tá»‘i Æ°u cho GPU"""
    
    def __init__(self, model_name: str = "Qwen/Qwen3-Embedding-0.6B", 
                 cache_dir: str = "./model_cache"):
        self.model_name = model_name
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        
        # GPU configuration
        self.device = self._detect_device()
        self._setup_gpu_optimization()
        
        # Load model
        self.model = self._load_model()
        self.embedding_dim = self.model.get_sentence_embedding_dimension()
        
        print(f"âœ… Model loaded: {model_name}")
        print(f"ðŸ”§ Device: {self.device}")
        print(f"ðŸ“ Embedding dimension: {self.embedding_dim}")
        
    def _detect_device(self) -> str:
        """Tá»± Ä‘á»™ng phÃ¡t hiá»‡n device phÃ¹ há»£p"""
        if torch.cuda.is_available():
            device = "cuda"
            gpu_name = torch.cuda.get_device_name(0)
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
            print(f"ðŸš€ GPU detected: {gpu_name}")
            print(f"ðŸ’¾ GPU memory: {total_memory:.1f} GB")
        elif torch.backends.mps.is_available():
            device = "mps"  # Apple Silicon
            print("ðŸŽ Apple Silicon MPS detected")
        else:
            device = "cpu"
            print("ðŸ’» Using CPU")
        
        return device
    
    def _setup_gpu_optimization(self) -> None:
        """CÃ i Ä‘áº·t tá»‘i Æ°u GPU"""
        if self.device == "cuda":
            # Enable TensorFloat-32 for better performance on Ampere GPUs
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            
            # Optimize memory allocation
            torch.cuda.empty_cache()
            
            # Set memory fraction if needed
            available_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
            if available_memory < 8:  # Less than 8GB
                torch.cuda.set_per_process_memory_fraction(0.8)
                print("âš¡ GPU memory optimization enabled for <8GB GPU")
    
    def _load_model(self) -> SentenceTransformer:
        """Load Qwen3 embedding model vá»›i error handling"""
        try:
            print(f"ðŸ“¥ Loading model: {self.model_name}...")
            start_time = time.time()
            
            model = SentenceTransformer(
                self.model_name,
                device=self.device,
                cache_folder=str(self.cache_dir),
                trust_remote_code=True  # Required for Qwen models
            )
            
            load_time = time.time() - start_time
            print(f"âœ… Model loaded successfully in {load_time:.2f}s")
            
            return model
            
        except Exception as e:
            print(f"âŒ Error loading model: {e}")
            print("ðŸ’¡ Tip: Ensure you have internet connection and sufficient disk space")
            raise
    
    def get_model_info(self) -> Dict[str, any]:
        """Láº¥y thÃ´ng tin model"""
        info = {
            'name': self.model_name,
            'device': self.device,
            'embedding_dimension': self.embedding_dim,
            'max_seq_length': getattr(self.model, 'max_seq_length', 512),
            'model_size_mb': self._estimate_model_size(),
            'supports_batch': True
        }
        
        if self.device == "cuda":
            info.update({
                'gpu_name': torch.cuda.get_device_name(0),
                'gpu_memory_total': torch.cuda.get_device_properties(0).total_memory / 1e9

            })
        
        return info
    
    def _estimate_model_size(self) -> float:
        """Æ¯á»›c tÃ­nh kÃ­ch thÆ°á»›c model (MB)"""
        try:
            total_params = sum(p.numel() for p in self.model.parameters())
            # Giáº£ sá»­ float32 (4 bytes per parameter)
            size_mb = (total_params * 4) / (1024 * 1024)
            return round(size_mb, 1)
        except:
            return 0.0
    
    def encode_texts(self, texts: List[str], batch_size: int = 32, 
                    show_progress: bool = True, normalize: bool = True) -> np.ndarray:
        """Encode danh sÃ¡ch texts thÃ nh embeddings"""
        if not texts:
            return np.empty((0, self.embedding_dim))
        
        print(f"ðŸ”„ Encoding {len(texts)} texts...")
        start_time = time.time()
        
        # Tá»‘i Æ°u batch size cho GPU
        optimal_batch_size = self._get_optimal_batch_size(batch_size)
        
        try:
            embeddings = self.model.encode(
                texts,
                batch_size=optimal_batch_size,
                show_progress_bar=show_progress,
                convert_to_tensor=False,  # Return numpy array
                normalize_embeddings=normalize,
                device=self.device
            )
            
            encode_time = time.time() - start_time
            speed = len(texts) / encode_time
            
            print(f"âœ… Encoding completed in {encode_time:.2f}s")
            print(f"âš¡ Speed: {speed:.1f} texts/second")
            print(f"ðŸ“Š Shape: {embeddings.shape}")
            
            return embeddings
            
        except Exception as e:
            print(f"âŒ Error during encoding: {e}")
            raise
    
    def _get_optimal_batch_size(self, requested_batch_size: int) -> int:
        """TÃ­nh toÃ¡n batch size tá»‘i Æ°u dá»±a trÃªn GPU memory"""
        if self.device == "cpu":
            return min(requested_batch_size, 16)
        
        if self.device == "cuda":
            available_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
            
            if available_memory < 4:
                return min(requested_batch_size, 8)
            elif available_memory < 8:
                return min(requested_batch_size, 16)
            elif available_memory < 12:
                return min(requested_batch_size, 32)
            else:
                return min(requested_batch_size, 64)
        
        return requested_batch_size
    
    def find_most_similar(self, query_embedding: np.ndarray, 
                         corpus_embeddings: np.ndarray, 
                         top_k: int = 5) -> Tuple[np.ndarray, np.ndarray]:
        """TÃ¬m top-k embeddings tÆ°Æ¡ng Ä‘á»“ng nháº¥t"""
        try:
            # Convert to torch tensors for GPU computation
            query_tensor = torch.tensor(query_embedding, device=self.device)
            corpus_tensor = torch.tensor(corpus_embeddings, device=self.device)
            
            # TÃ­nh cosine similarity
            similarities = util.cos_sim(query_tensor, corpus_tensor)
            
            # Láº¥y top-k results
            top_k = min(top_k, corpus_embeddings.shape[0])
            top_results = torch.topk(similarities, k=top_k, dim=-1)
            
            # Convert vá» numpy
            indices = top_results.indices.cpu().numpy().flatten()
            scores = top_results.values.cpu().numpy().flatten()
            
            return indices, scores
            
        except Exception as e:
            print(f"âŒ Error in similarity search: {e}")
            raise
    
    def batch_similarity_search(self, queries: List[str], 
                               corpus_embeddings: np.ndarray,
                               corpus_ids: List[str],
                               top_k: int = 5) -> List[Dict]:
        """Thá»±c hiá»‡n batch similarity search"""
        print(f"ðŸ” Running batch search for {len(queries)} queries...")
        
        # Encode all queries at once
        query_embeddings = self.encode_texts(queries, show_progress=False)
        
        results = []
        for i, (query, query_emb) in enumerate(zip(queries, query_embeddings)):
            # Find similar chunks for this query
            indices, scores = self.find_most_similar(
                query_emb.reshape(1, -1), 
                corpus_embeddings, 
                top_k
            )
            
            # Build result
            search_results = []
            for idx, score in zip(indices, scores):
                search_results.append({
                    'chunk_id': corpus_ids[idx],
                    'similarity_score': float(score),
                    'rank': len(search_results) + 1
                })
            
            results.append({
                'query_id': i,
                'query': query,
                'results': search_results
            })
        
        return results
    
    def get_memory_stats(self) -> Dict[str, any]:
        """Láº¥y thá»‘ng kÃª memory usage"""
        stats = {'device': self.device}
        
        if self.device == "cuda":
            stats.update({
                'allocated_gb': torch.cuda.memory_allocated() / 1e9,
                'cached_gb': torch.cuda.memory_reserved() / 1e9,
                'max_allocated_gb': torch.cuda.max_memory_allocated() / 1e9,
                'gpu_name': torch.cuda.get_device_name(0)
            })
        
        return stats
    
    def cleanup(self) -> None:
        """Dá»n dáº¹p memory vÃ  cache"""
        if self.device == "cuda":
            torch.cuda.empty_cache()
            torch.cuda.reset_peak_memory_stats()
        
        # Force garbage collection
        gc.collect()
        
        print("ðŸ§¹ Memory cleanup completed")
```

## ðŸ“Š src/metrics.py

```python
import numpy as np
import pandas as pd
from typing import List, Dict, Tuple, Optional
from collections import defaultdict
import json

class EmbeddingMetrics:
    """TÃ­nh toÃ¡n metrics cho embedding model evaluation"""
    
    def __init__(self):
        self.metrics_names = [
            'MRR', 'Hit_Rate@1', 'Hit_Rate@3', 'Hit_Rate@5', 
            'MAP@5', 'NDCG@5', 'Precision@5', 'Recall@5'
        ]
    
    def calculate_mrr(self, search_results: List[Dict], ground_truth: List[Dict]) -> float:
        """TÃ­nh Mean Reciprocal Rank"""
        reciprocal_ranks = []
        
        for i, result in enumerate(search_results):
            query_id = result.get('query_id', i)
            search_result_ids = [r['chunk_id'] for r in result['results']]
            
            # TÃ¬m ground truth cho query nÃ y
            expected_chunk = None
            for gt in ground_truth:
                if gt.get('id', gt.get('query_id')) == query_id + 1:  # +1 vÃ¬ ground truth báº¯t Ä‘áº§u tá»« 1
                    expected_chunk = gt['expected_chunk']
                    break
            
            if not expected_chunk:
                reciprocal_ranks.append(0.0)
                continue
            
            # TÃ¬m rank cá»§a expected chunk
            rank = None
            for j, chunk_id in enumerate(search_result_ids):
                if self._chunk_matches(chunk_id, expected_chunk):
                    rank = j + 1
                    break
            
            reciprocal_rank = 1.0 / rank if rank else 0.0
            reciprocal_ranks.append(reciprocal_rank)
        
        return np.mean(reciprocal_ranks) if reciprocal_ranks else 0.0
    
    def calculate_hit_rate_at_k(self, search_results: List[Dict], 
                               ground_truth: List[Dict], k: int) -> float:
        """TÃ­nh Hit Rate@k"""
        hits = 0
        total_queries = len(search_results)
        
        for i, result in enumerate(search_results):
            query_id = result.get('query_id', i)
            top_k_results = result['results'][:k]
            top_k_ids = [r['chunk_id'] for r in top_k_results]
            
            # TÃ¬m ground truth
            expected_chunk = None
            for gt in ground_truth:
                if gt.get('id', gt.get('query_id')) == query_id + 1:
                    expected_chunk = gt['expected_chunk']
                    break
            
            if expected_chunk:
                for chunk_id in top_k_ids:
                    if self._chunk_matches(chunk_id, expected_chunk):
                        hits += 1
                        break
        
        return hits / total_queries if total_queries > 0 else 0.0
    
    def calculate_precision_at_k(self, search_results: List[Dict],
                                ground_truth: List[Dict], k: int) -> float:
        """TÃ­nh Precision@k"""
        precisions = []
        
        for i, result in enumerate(search_results):
            query_id = result.get('query_id', i)
            top_k_results = result['results'][:k]
            top_k_ids = [r['chunk_id'] for r in top_k_results]
            
            # TÃ¬m ground truth
            expected_chunk = None
            for gt in ground_truth:
                if gt.get('id', gt.get('query_id')) == query_id + 1:
                    expected_chunk = gt['expected_chunk']
                    break
            
            if not expected_chunk:
                precisions.append(0.0)
                continue
            
            # Äáº¿m relevant documents trong top-k
            relevant_count = 0
            for chunk_id in top_k_ids:
                if self._chunk_matches(chunk_id, expected_chunk):
                    relevant_count += 1
            
            precision = relevant_count / len(top_k_ids) if top_k_ids else 0.0
            precisions.append(precision)
        
        return np.mean(precisions) if precisions else 0.0
    
    def calculate_recall_at_k(self, search_results: List[Dict],
                             ground_truth: List[Dict], k: int) -> float:
        """TÃ­nh Recall@k (trong trÆ°á»ng há»£p nÃ y má»—i query chá»‰ cÃ³ 1 relevant doc)"""
        return self.calculate_hit_rate_at_k(search_results, ground_truth, k)
    
    def calculate_ndcg_at_k(self, search_results: List[Dict],
                           ground_truth: List[Dict], k: int) -> float:
        """TÃ­nh Normalized Discounted Cumulative Gain@k"""
        ndcg_scores = []
        
        for i, result in enumerate(search_results):
            query_id = result.get('query_id', i)
            top_k_results = result['results'][:k]
            
            # TÃ¬m ground truth
            expected_chunk = None
            for gt in ground_truth:
                if gt.get('id', gt.get('query_id')) == query_id + 1:
                    expected_chunk = gt['expected_chunk']
                    break
            
            if not expected_chunk:
                ndcg_scores.append(0.0)
                continue
            
            # TÃ­nh DCG
            dcg = 0.0
            for j, chunk_result in enumerate(top_k_results):
                if self._chunk_matches(chunk_result['chunk_id'], expected_chunk):
                    relevance = 1  # Binary relevance
                    dcg += relevance / np.log2(j + 2)  # j+2 vÃ¬ log2(1) = 0
            
            # IDCG (trong trÆ°á»ng há»£p nÃ y lÃ  1.0 vÃ¬ chá»‰ cÃ³ 1 relevant doc)
            idcg = 1.0
            
            ndcg = dcg / idcg if idcg > 0 else 0.0
            ndcg_scores.append(ndcg)
        
        return np.mean(ndcg_scores) if ndcg_scores else 0.0
    
    def calculate_map_at_k(self, search_results: List[Dict],
                          ground_truth: List[Dict], k: int) -> float:
        """TÃ­nh Mean Average Precision@k"""
        ap_scores = []
        
        for i, result in enumerate(search_results):
            query_id = result.get('query_id', i)
            top_k_results = result['results'][:k]
            
            # TÃ¬m ground truth
            expected_chunk = None
            for gt in ground_truth:
                if gt.get('id', gt.get('query_id')) == query_id + 1:
                    expected_chunk = gt['expected_chunk']
                    break
            
            if not expected_chunk:
                ap_scores.append(0.0)
                continue
            
            # TÃ­nh Average Precision
            relevant_positions = []
            for j, chunk_result in enumerate(top_k_results):
                if self._chunk_matches(chunk_result['chunk_id'], expected_chunk):
                    relevant_positions.append(j + 1)
            
            if not relevant_positions:
                ap_scores.append(0.0)
                continue
            
            # TÃ­nh precision táº¡i má»—i relevant position
            precision_at_relevant = []
            for pos in relevant_positions:
                precision_at_relevant.append(1.0 / pos)  # Trong binary case
            
            ap = np.mean(precision_at_relevant) if precision_at_relevant else 0.0
            ap_scores.append(ap)
        
        return np.mean(ap_scores) if ap_scores else 0.0
    
    def _chunk_matches(self, chunk_id: str, expected_chunk: str) -> bool:
        """Kiá»ƒm tra xem chunk_id cÃ³ match vá»›i expected_chunk khÃ´ng"""
        # Normalize strings for comparison
        chunk_id_clean = chunk_id.lower().replace('_', ' ').replace('-', ' ')
        expected_clean = expected_chunk.lower().replace('_', ' ').replace('-', ' ')
        
        # Check if any significant word from expected appears in chunk_id
        expected_words = expected_clean.split()
        return any(word in chunk_id_clean for word in expected_words if len(word) > 3)
    
    def calculate_all_metrics(self, search_results: List[Dict], 
                             ground_truth: List[Dict]) -> Dict[str, float]:
        """TÃ­nh táº¥t cáº£ metrics"""
        metrics = {}
        
        try:
            metrics['MRR'] = self.calculate_mrr(search_results, ground_truth)
            metrics['Hit_Rate@1'] = self.calculate_hit_rate_at_k(search_results, ground_truth, 1)
            metrics['Hit_Rate@3'] = self.calculate_hit_rate_at_k(search_results, ground_truth, 3)
            metrics['Hit_Rate@5'] = self.calculate_hit_rate_at_k(search_results, ground_truth, 5)
            metrics['Precision@5'] = self.calculate_precision_at_k(search_results, ground_truth, 5)
            metrics['Recall@5'] = self.calculate_recall_at_k(search_results, ground_truth, 5)
            metrics['NDCG@5'] = self.calculate_ndcg_at_k(search_results, ground_truth, 5)
            metrics['MAP@5'] = self.calculate_map_at_k(search_results, ground_truth, 5)
            
        except Exception as e:
            print(f"âŒ Error calculating metrics: {e}")
            # Return zero metrics in case of error
            metrics = {metric: 0.0 for metric in self.metrics_names}
        
        return metrics
    
    def generate_detailed_analysis(self, search_results: List[Dict],
                                  ground_truth: List[Dict]) -> Tuple[Dict, pd.DataFrame]:
        """Táº¡o phÃ¢n tÃ­ch chi tiáº¿t cho tá»«ng query"""
        detailed_results = []
        
        for i, result in enumerate(search_results):
            query_id = result.get('query_id', i)
            query_text = result['query']
            search_result_ids = [r['chunk_id'] for r in result['results']]
            scores = [r['similarity_score'] for r in result['results']]
            
            # TÃ¬m ground truth
            expected_chunk = None
            category = "unknown"
            for gt in ground_truth:
                if gt.get('id', gt.get('query_id')) == query_id + 1:
                    expected_chunk = gt['expected_chunk']
                    category = gt.get('category', 'unknown')
                    break
            
            # TÃ¬m rank cá»§a correct answer
            found_rank = None
            correct_score = 0.0
            if expected_chunk:
                for j, chunk_id in enumerate(search_result_ids):
                    if self._chunk_matches(chunk_id, expected_chunk):
                        found_rank = j + 1
                        correct_score = scores[j] if j < len(scores) else 0.0
                        break
            
            detailed_result = {
                'query_id': query_id + 1,
                'query': query_text[:100] + "..." if len(query_text) > 100 else query_text,
                'category': category,
                'expected_chunk': expected_chunk,
                'found_rank': found_rank,
                'correct_score': correct_score,
                'top_1_score': scores[0] if scores else 0.0,
                'score_gap': scores[0] - correct_score if scores and correct_score > 0 else 0.0,
                'hit@1': 1 if found_rank == 1 else 0,
                'hit@3': 1 if found_rank and found_rank <= 3 else 0,
                'hit@5': 1 if found_rank and found_rank <= 5 else 0,
            }
            
            detailed_results.append(detailed_result)
        
        # Táº¡o DataFrame
        df = pd.DataFrame(detailed_results)
        
        # TÃ­nh summary metrics
        summary_metrics = self.calculate_all_metrics(search_results, ground_truth)
        
        return summary_metrics, df
    
    def analyze_performance_by_category(self, df: pd.DataFrame) -> Dict[str, Dict]:
        """PhÃ¢n tÃ­ch performance theo category"""
        if 'category' not in df.columns:
            return {}
        
        category_analysis = {}
        
        for category in df['category'].unique():
            category_df = df[df['category'] == category]
            
            analysis = {
                'total_queries': len(category_df),
                'hit_rate@1': category_df['hit@1'].mean(),
                'hit_rate@3': category_df['hit@3'].mean(),
                'hit_rate@5': category_df['hit@5'].mean(),
                'avg_rank': category_df[category_df['found_rank'].notna()]['found_rank'].mean(),
                'avg_correct_score': category_df[category_df['correct_score'] > 0]['correct_score'].mean(),
                'questions_not_found': len(category_df[category_df['found_rank'].isna()])
            }
            
            category_analysis[category] = analysis
        
        return category_analysis
```

## ðŸ“ˆ src/visualizer.py

```python
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, List, Optional
import json
from datetime import datetime

# Cáº¥u hÃ¬nh matplotlib cho tiáº¿ng Viá»‡t
plt.rcParams['font.family'] = ['Arial Unicode MS', 'Tahoma', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")

class BenchmarkVisualizer:
    """Táº¡o visualizations cho benchmark results"""
    
    def __init__(self, output_dir: str = "reports"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # Táº¡o thÆ° má»¥c con
        self.charts_dir = self.output_dir / "charts"
        self.charts_dir.mkdir(exist_ok=True)
        
        # Color palette
        self.colors = px.colors.qualitative.Set2
        
    def create_metrics_overview(self, metrics: Dict[str, float], 
                               save_path: Optional[str] = None) -> None:
        """Táº¡o overview chart cá»§a cÃ¡c metrics"""
        
        # Prepare data
        metric_names = list(metrics.keys())
        metric_values = list(metrics.values())
        
        # Create figure with subplots
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=('Overall Metrics', 'Hit Rates', 'Precision & Recall', 'Advanced Metrics'),
            specs=[[{"type": "bar"}, {"type": "bar"}],
                   [{"type": "bar"}, {"type": "bar"}]]
        )
        
        # Overall metrics (bar chart)
        fig.add_trace(
            go.Bar(
                x=metric_names,
                y=metric_values,
                name="All Metrics",
                marker_color=self.colors[0],
                text=[f"{v:.3f}" for v in metric_values],
                textposition='outside'
            ),
            row=1, col=1
        )
        
        # Hit rates
        hit_metrics = {k: v for k, v in metrics.items() if 'Hit_Rate' in k}
        if hit_metrics:
            fig.add_trace(
                go.Bar(
                    x=list(hit_metrics.keys()),
                    y=list(hit_metrics.values()),
                    name="Hit Rates",
                    marker_color=self.colors[1],
                    text=[f"{v:.2%}" for v in hit_metrics.values()],
                    textposition='outside'
                ),
                row=1, col=2
            )
        
        # Precision & Recall
        pr_metrics = {k: v for k, v in metrics.items() if any(x in k for x in ['Precision', 'Recall'])}
        if pr_metrics:
            fig.add_trace(
                go.Bar(
                    x=list(pr_metrics.keys()),
                    y=list(pr_metrics.values()),
                    name="Precision & Recall",
                    marker_color=self.colors[2],
                    text=[f"{v:.3f}" for v in pr_metrics.values()],
                    textposition='outside'
                ),
                row=2, col=1
            )
        
        # Advanced metrics
        advanced_metrics = {k: v for k, v in metrics.items() if k in ['MRR', 'MAP@5', 'NDCG@5']}
        if advanced_metrics:
            fig.add_trace(
                go.Bar(
                    x=list(advanced_metrics.keys()),
                    y=list(advanced_metrics.values()),
                    name="Advanced Metrics",
                    marker_color=self.colors[3],
                    text=[f"{v:.3f}" for v in advanced_metrics.values()],
                    textposition='outside'
                ),
                row=2, col=2
            )
        
        # Update layout
        fig.update_layout(
            title_text="Qwen3-Embedding-0.6B Performance Overview",
            height=800,
            showlegend=False
        )
        
        # Save plot
        if save_path is None:
            save_path = self.charts_dir / "metrics_overview.html"
        
        fig.write_html(str(save_path))
        print(f"ðŸ“Š Metrics overview saved: {save_path}")
    
    def create_detailed_analysis_chart(self, df: pd.DataFrame,
                                      save_path: Optional[str] = None) -> None:
        """Táº¡o chart phÃ¢n tÃ­ch chi tiáº¿t"""
        
        fig = make_subplots(
            rows=2, cols=3,
            subplot_titles=(
                'Query Difficulty Distribution', 'Score Distribution', 
                'Performance by Category', 'Rank Distribution',
                'Score Gap Analysis', 'Success Rate by Rank'
            ),
            specs=[[{"type": "pie"}, {"type": "histogram"}],
                   [{"type": "bar"}, {"type": "histogram"}],
                   [{"type": "histogram"}, {"type": "bar"}]]
        )
        
        # 1. Query difficulty (dá»±a trÃªn found_rank)
        difficulty_labels = []
        difficulty_values = []
        
        easy_count = len(df[df['found_rank'] == 1])
        medium_count = len(df[(df['found_rank'] > 1) & (df['found_rank'] <= 3)])
        hard_count = len(df[(df['found_rank'] > 3) & (df['found_rank'] <= 5)])
        very_hard_count = len(df[df['found_rank'].isna()])
        
        for label, count in [('Easy (Rank 1)', easy_count), ('Medium (Rank 2-3)', medium_count),
                            ('Hard (Rank 4-5)', hard_count), ('Very Hard (Not Found)', very_hard_count)]:
            if count > 0:
                difficulty_labels.append(label)
                difficulty_values.append(count)
        
        fig.add_trace(
            go.Pie(
                labels=difficulty_labels,
                values=difficulty_values,
                name="Difficulty"
            ),
            row=1, col=1
        )
        
        # 2. Score distribution
        all_scores = df['top_1_score'].tolist() + df[df['correct_score'] > 0]['correct_score'].tolist()
        fig.add_trace(
            go.Histogram(
                x=all_scores,
                name="Scores",
                nbinsx=20,
                marker_color=self.colors[1]
            ),
            row=1, col=2
        )
        
        # 3. Performance by category
        if 'category' in df.columns:
            category_performance = df.groupby('category')['hit@5'].mean().reset_index()
            fig.add_trace(
                go.Bar(
                    x=category_performance['category'],
                    y=category_performance['hit@5'],
                    name="Hit@5 by Category",
                    marker_color=self.colors[2],
                    text=[f"{v:.1%}" for v in category_performance['hit@5']],
                    textposition='outside'
                ),
                row=2, col=1
            )
        
        # 4. Rank distribution
        rank_counts = df[df['found_rank'].notna()]['found_rank'].value_counts().sort_index()
        fig.add_trace(
            go.Bar(
                x=[f"Rank {int(r)}" for r in rank_counts.index],
                y=rank_counts.values,
                name="Rank Distribution",
                marker_color=self.colors[3]
            ),
            row=2, col=2
        )
        
        # 5. Score gap analysis
        score_gaps = df[df['score_gap'] != 0]['score_gap']
        if len(score_gaps) > 0:
            fig.add_trace(
                go.Histogram(
                    x=score_gaps,
                    name="Score Gap",
                    nbinsx=15,
                    marker_color=self.colors[4]
                ),
                row=2, col=3
            )
        
        # Update layout
        fig.update_layout(
            title_text="Detailed Performance Analysis",
            height=1000,
            showlegend=False
        )
        
        if save_path is None:
            save_path = self.charts_dir / "detailed_analysis.html"
        
        fig.write_html(str(save_path))
        print(f"ðŸ“ˆ Detailed analysis saved: {save_path}")
    
    def create_performance_radar(self, metrics: Dict[str, float],
                                save_path: Optional[str] = None) -> None:
        """Táº¡o radar chart cho performance metrics"""
        
        # Select key metrics for radar
        radar_metrics = ['MRR', 'Hit_Rate@1', 'Hit_Rate@5', 'Precision@5', 'Recall@5', 'NDCG@5']
        
        categories = []
        values = []
        
        for metric in radar_metrics:
            if metric in metrics:
                categories.append(metric)
                values.append(metrics[metric])
        
        # Add first value at the end to close the radar
        if categories:
            categories.append(categories[0])
            values.append(values[0])
        
        fig = go.Figure()
        
        fig.add_trace(go.Scatterpolar(
            r=values,
            theta=categories,
            fill='toself',
            name='Qwen3-Embedding-0.6B',
            line_color=self.colors[0]
        ))
        
        fig.update_layout(
            polar=dict(
                radialaxis=dict(
                    visible=True,
                    range=[0, 1]
                )
            ),
            title="Performance Radar Chart",
            showlegend=True
        )
        
        if save_path is None:
            save_path = self.charts_dir / "performance_radar.html"
        
        fig.write_html(str(save_path))
        print(f"ðŸŽ¯ Radar chart saved: {save_path}")
    
    def create_category_performance_chart(self, category_analysis: Dict[str, Dict],
                                         save_path: Optional[str] = None) -> None:
        """Táº¡o chart performance theo category"""
        
        if not category_analysis:
            print("âš ï¸ No category analysis data available")
            return
        
        categories = list(category_analysis.keys())
        hit_1_rates = [category_analysis[cat]['hit_rate@1'] for cat in categories]
        hit_3_rates = [category_analysis[cat]['hit_rate@3'] for cat in categories]
        hit_5_rates = [category_analysis[cat]['hit_rate@5'] for cat in categories]
        
        fig = go.Figure()
        
        fig.add_trace(go.Bar(
            name='Hit Rate@1',
            x=categories,
            y=hit_1_rates,
            marker_color=self.colors[0],
            text=[f"{v:.1%}" for v in hit_1_rates],
            textposition='outside'
        ))
                
        fig.add_trace(go.Bar(
            name='Hit Rate@5',
            x=categories,
            y=hit_5_rates,
            marker_color=self.colors[2],
            text=[f"{v:.1%}" for v in hit_5_rates],
            textposition='outside'
        ))
        
        fig.update_layout(
            title='Performance by Question Category',
            xaxis_title='Category',
            yaxis_title='Hit Rate',
            barmode='group',
            height=600
        )
        
        if save_path is None:
            save_path = self.charts_dir / "category_performance.html"
        
        fig.write_html(str(save_path))
        print(f"ðŸ“Š Category performance chart saved: {save_path}")
    
    def generate_html_report(self, results: Dict, save_path: Optional[str] = None) -> None:
        """Táº¡o bÃ¡o cÃ¡o HTML tá»•ng há»£p"""
        
        html_template = """
        <!DOCTYPE html>
        <html lang="vi">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>Qwen3-Embedding-0.6B Benchmark Report</title>
            <style>
                body { 
                    font-family: Arial, sans-serif; 
                    margin: 40px; 
                    background-color: #f5f5f5;
                }
                .container {
                    max-width: 1200px;
                    margin: 0 auto;
                    background: white;
                    padding: 30px;
                    border-radius: 10px;
                    box-shadow: 0 0 20px rgba(0,0,0,0.1);
                }
                h1, h2, h3 { 
                    color: #2c3e50; 
                }
                .header {
                    text-align: center;
                    border-bottom: 3px solid #3498db;
                    padding-bottom: 20px;
                    margin-bottom: 30px;
                }
                .metrics-grid {
                    display: grid;
                    grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
                    gap: 20px;
                    margin: 20px 0;
                }
                .metric-card {
                    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                    color: white;
                    padding: 20px;
                    border-radius: 10px;
                    text-align: center;
                }
                .metric-value {
                    font-size: 2em;
                    font-weight: bold;
                }
                .metric-name {
                    font-size: 0.9em;
                    opacity: 0.9;
                }
                table { 
                    border-collapse: collapse; 
                    width: 100%; 
                    margin: 20px 0; 
                }
                th, td { 
                    border: 1px solid #ddd; 
                    padding: 12px; 
                    text-align: left; 
                }
                th { 
                    background-color: #3498db;
                    color: white;
                    font-weight: bold; 
                }
                .excellent { background-color: #d4edda; }
                .good { background-color: #e2f3ff; }
                .average { background-color: #fff3cd; }
                .poor { background-color: #f8d7da; }
                .summary {
                    background: linear-gradient(135deg, #74b9ff 0%, #0984e3 100%);
                    color: white;
                    padding: 25px;
                    border-radius: 10px;
                    margin: 30px 0;
                }
                .charts-section {
                    margin: 30px 0;
                    text-align: center;
                }
                .chart-link {
                    display: inline-block;
                    margin: 10px;
                    padding: 10px 20px;
                    background-color: #3498db;
                    color: white;
                    text-decoration: none;
                    border-radius: 5px;
                }
                .chart-link:hover {
                    background-color: #2980b9;
                }
                .footer {
                    margin-top: 40px;
                    padding-top: 20px;
                    border-top: 1px solid #ddd;
                    text-align: center;
                    color: #666;
                }
            </style>
        </head>
        <body>
            <div class="container">
                <div class="header">
                    <h1>ðŸ‡»ðŸ‡³ Vietnamese Embedding Model Benchmark</h1>
                    <h2>Qwen/Qwen3-Embedding-0.6B Performance Report</h2>
                    <p><strong>Generated:</strong> {timestamp}</p>
                </div>

                <div class="summary">
                    <h2>ðŸ“‹ Executive Summary</h2>
                    <p>The Qwen3-Embedding-0.6B model was evaluated on {total_queries} Vietnamese queries across {total_categories} categories. 
                    The model achieved an overall MRR of <strong>{mrr:.3f}</strong> and Hit Rate@5 of <strong>{hit_rate_5:.1%}</strong>.</p>
                </div>

                <h2>ðŸ“Š Key Performance Metrics</h2>
                <div class="metrics-grid">
                    {metrics_cards}
                </div>

                <h2>ðŸŽ¯ Performance Analysis</h2>
                <table>
                    <tr>
                        <th>Metric</th>
                        <th>Score</th>
                        <th>Performance Level</th>
                        <th>Interpretation</th>
                    </tr>
                    {metrics_table}
                </table>

                <h2>ðŸ“ˆ Detailed Results</h2>
                {detailed_table}

                <h2>ðŸ“Š Interactive Charts</h2>
                <div class="charts-section">
                    <a href="charts/metrics_overview.html" class="chart-link">ðŸ“Š Metrics Overview</a>
                    <a href="charts/detailed_analysis.html" class="chart-link">ðŸ“ˆ Detailed Analysis</a>
                    <a href="charts/performance_radar.html" class="chart-link">ðŸŽ¯ Performance Radar</a>
                    <a href="charts/category_performance.html" class="chart-link">ðŸ“‹ Category Performance</a>
                </div>

                <h2>ðŸ’¡ Recommendations</h2>
                <div class="summary">
                    {recommendations}
                </div>

                <div class="footer">
                    <p>Report generated by Vietnamese Embedding Benchmark Tool</p>
                    <p>Model: Qwen/Qwen3-Embedding-0.6B | Hardware: {device_info}</p>
                </div>
            </div>
        </body>
        </html>
        """
        
        # Extract data from results
        metrics = results.get('metrics', {})
        model_info = results.get('model_info', {})
        detailed_df = results.get('detailed_analysis', pd.DataFrame())
        category_analysis = results.get('category_analysis', {})
        
        # Generate metrics cards
        metrics_cards = ""
        for metric_name, metric_value in metrics.items():
            if isinstance(metric_value, (int, float)):
                if 'Rate' in metric_name:
                    display_value = f"{metric_value:.1%}"
                else:
                    display_value = f"{metric_value:.3f}"
                
                metrics_cards += f"""
                <div class="metric-card">
                    <div class="metric-value">{display_value}</div>
                    <div class="metric-name">{metric_name}</div>
                </div>
                """
        
        # Generate metrics table
        def get_performance_level(metric_name: str, value: float) -> tuple:
            if 'Hit_Rate' in metric_name or 'Recall' in metric_name or 'Precision' in metric_name:
                if value >= 0.8: return "excellent", "Excellent"
                elif value >= 0.6: return "good", "Good"
                elif value >= 0.4: return "average", "Average"
                else: return "poor", "Poor"
            elif metric_name in ['MRR', 'MAP@5', 'NDCG@5']:
                if value >= 0.7: return "excellent", "Excellent"
                elif value >= 0.5: return "good", "Good"
                elif value >= 0.3: return "average", "Average"
                else: return "poor", "Poor"
            return "average", "Average"
        
        metrics_table = ""
        interpretations = {
            'MRR': 'Average position of first correct answer',
            'Hit_Rate@1': 'Percentage of queries with correct answer in top 1',
            'Hit_Rate@5': 'Percentage of queries with correct answer in top 5',
            'Precision@5': 'Precision of top 5 results',
            'Recall@5': 'Recall of top 5 results',
            'NDCG@5': 'Ranking quality considering positions',
            'MAP@5': 'Mean average precision at 5'
        }
        
        for metric_name, metric_value in metrics.items():
            if isinstance(metric_value, (int, float)):
                class_name, level = get_performance_level(metric_name, metric_value)
                if 'Rate' in metric_name:
                    display_value = f"{metric_value:.1%}"
                else:
                    display_value = f"{metric_value:.3f}"
                
                interpretation = interpretations.get(metric_name, "Performance metric")
                
                metrics_table += f"""
                <tr class="{class_name}">
                    <td><strong>{metric_name}</strong></td>
                    <td>{display_value}</td>
                    <td>{level}</td>
                    <td>{interpretation}</td>
                </tr>
                """
        
        # Generate detailed table
        detailed_table = ""
        if not detailed_df.empty:
            detailed_table = detailed_df.to_html(classes='table', escape=False, index=False)
        
        # Generate recommendations
        recommendations = self._generate_recommendations(metrics, category_analysis)
        
        # Fill template
        html_content = html_template.format(
            timestamp=datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            total_queries=len(detailed_df) if not detailed_df.empty else 0,
            total_categories=len(category_analysis),
            mrr=metrics.get('MRR', 0),
            hit_rate_5=metrics.get('Hit_Rate@5', 0),
            metrics_cards=metrics_cards,
            metrics_table=metrics_table,
            detailed_table=detailed_table,
            recommendations=recommendations,
            device_info=model_info.get('device', 'Unknown')
        )
        
        if save_path is None:
            save_path = self.output_dir / "benchmark_report.html"
        
        with open(save_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        print(f"ðŸ“‹ HTML report saved: {save_path}")
    
    def _generate_recommendations(self, metrics: Dict[str, float], 
                                 category_analysis: Dict) -> str:
        """Táº¡o recommendations dá»±a trÃªn results"""
        recommendations = []
        
        # Overall performance assessment
        mrr = metrics.get('MRR', 0)
        hit_rate_5 = metrics.get('Hit_Rate@5', 0)
        
        if mrr >= 0.7 and hit_rate_5 >= 0.8:
            recommendations.append("âœ… <strong>Excellent Performance:</strong> The model shows excellent performance across all metrics and is ready for production deployment.")
        elif mrr >= 0.5 and hit_rate_5 >= 0.6:
            recommendations.append("âœ… <strong>Good Performance:</strong> The model performs well and can be used for production with some optimizations.")
        else:
            recommendations.append("âš ï¸ <strong>Performance Issues:</strong> The model shows room for improvement. Consider fine-tuning or using additional preprocessing.")
        
        # Category-specific recommendations
        if category_analysis:
            worst_category = min(category_analysis.keys(), 
                               key=lambda x: category_analysis[x].get('hit_rate@5', 0))
            worst_score = category_analysis[worst_category].get('hit_rate@5', 0)
            
            if worst_score < 0.5:
                recommendations.append(f"ðŸŽ¯ <strong>Category Improvement:</strong> The '{worst_category}' category shows the lowest performance ({worst_score:.1%}). Consider adding more training data for this category.")
        
        # Technical recommendations
        if hit_rate_5 < 0.7:
            recommendations.append("ðŸ”§ <strong>Technical Improvements:</strong> Consider increasing chunk overlap, experimenting with different chunking strategies, or using hybrid search approaches.")
        
        if metrics.get('Hit_Rate@1', 0) < 0.4:
            recommendations.append("ðŸ“š <strong>Data Quality:</strong> Low top-1 accuracy suggests data quality issues. Review document preprocessing and ensure clear question-answer relationships.")
        
        return "<br>".join(recommendations)
```

## ðŸš€ benchmark.py (Main Script)

```python
#!/usr/bin/env python3
"""
Vietnamese Embedding Model Benchmark Tool
Model: Qwen/Qwen3-Embedding-0.6B

Usage: python benchmark.py [--config configs/model_config.json]
"""

import argparse
import json
import time
from pathlib import Path
import pandas as pd
from datetime import datetime
import traceback

from src.data_processor import VietnameseTextProcessor
from src.embedding_manager import Qwen3EmbeddingManager  
from src.metrics import EmbeddingMetrics
from src.visualizer import BenchmarkVisualizer

class Qwen3Benchmark:
    """Main benchmark class for Qwen3-Embedding-0.6B"""
    
    def __init__(self, config_path: str):
        self.config = self._load_config(config_path)
        self.reports_dir = Path(self.config['output']['reports_dir'])
        self.reports_dir.mkdir(exist_ok=True)
        
        print("ðŸ‡»ðŸ‡³ Vietnamese Embedding Benchmark Tool")
        print("=" * 60)
        print(f"Model: {self.config['model']['name']}")
        print(f"Reports directory: {self.reports_dir.absolute()}")
        print("=" * 60)
        
    def _load_config(self, config_path: str) -> dict:
        """Load configuration from JSON file"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            print(f"âœ… Configuration loaded from {config_path}")
            return config
        except Exception as e:
            print(f"âŒ Error loading config: {e}")
            raise
    
    def run_benchmark(self) -> dict:
        """Execute complete benchmark pipeline"""
        benchmark_start = time.time()
        results = {
            'benchmark_info': {
                'model_name': self.config['model']['name'],
                'start_time': datetime.now().isoformat(),
                'config': self.config
            }
        }
        
        try:
            # Step 1: Process data
            print("\nðŸ“š STEP 1: Processing Vietnamese text data")
            print("-" * 40)
            
            processor = VietnameseTextProcessor(
                chunk_size=self.config['evaluation']['chunk_size'],
                chunk_overlap=self.config['evaluation']['chunk_overlap']
            )
            
            chunks, data_stats = processor.load_and_process_content('data/content.md')
            if not chunks:
                raise ValueError("No chunks were created from content data")
            
            test_suite = processor.load_test_suite('data/test_suite.json')
            if not test_suite:
                raise ValueError("No test questions were loaded")
            
            results['data_stats'] = data_stats
            results['test_suite_size'] = len(test_suite)
            
            # Export chunks info for debugging
            processor.export_chunks_info(self.reports_dir / "chunks_info.json")
            
            # Step 2: Initialize embedding model
            print("\nðŸ¤– STEP 2: Loading embedding model")
            print("-" * 40)
            
            embedding_manager = Qwen3EmbeddingManager(
                model_name=self.config['model']['name']
            )
            
            model_info = embedding_manager.get_model_info()
            results['model_info'] = model_info
            
            print(f"ðŸ“ Model info: {model_info}")
            
            # Step 3: Generate embeddings
            print("\nðŸ”„ STEP 3: Generating embeddings")
            print("-" * 40)
            
            chunk_texts = [chunk['text'] for chunk in chunks]
            chunk_ids = [chunk['id'] for chunk in chunks]
            
            embedding_start = time.time()
            corpus_embeddings = embedding_manager.encode_texts(
                chunk_texts,
                batch_size=self.config['model']['batch_size'],
                normalize=self.config['model']['normalize_embeddings']
            )
            embedding_time = time.time() - embedding_start
            
            results['embedding_time_seconds'] = embedding_time
            results['embedding_speed_texts_per_second'] = len(chunk_texts) / embedding_time
            
            print(f"âš¡ Embedding generation completed in {embedding_time:.2f}s")
            print(f"ðŸ“Š Speed: {results['embedding_speed_texts_per_second']:.1f} texts/second")
            
            # Step 4: Run similarity search evaluation
            print("\nðŸ” STEP 4: Running similarity search evaluation")
            print("-" * 40)
            
            queries = [test['question'] for test in test_suite]
            
            search_start = time.time()
            search_results = embedding_manager.batch_similarity_search(
                queries=queries,
                corpus_embeddings=corpus_embeddings,
                corpus_ids=chunk_ids,
                top_k=self.config['evaluation']['top_k']
            )
            search_time = time.time() - search_start
            
            results['search_time_seconds'] = search_time
            results['search_speed_queries_per_second'] = len(queries) / search_time
            results['search_results'] = search_results
            
            print(f"ðŸ” Search completed in {search_time:.2f}s")
            print(f"âš¡ Search speed: {results['search_speed_queries_per_second']:.1f} queries/second")
            
            # Step 5: Calculate metrics
            print("\nðŸ“Š STEP 5: Calculating performance metrics")
            print("-" * 40)
            
            metrics_calculator = EmbeddingMetrics()
            
            # Calculate all metrics
            metrics = metrics_calculator.calculate_all_metrics(search_results, test_suite)
            results['metrics'] = metrics
            
            # Generate detailed analysis
            summary_metrics, detailed_df = metrics_calculator.generate_detailed_analysis(
                search_results, test_suite
            )
            results['detailed_analysis'] = detailed_df
            
            # Category analysis
            category_analysis = metrics_calculator.analyze_performance_by_category(detailed_df)
            results['category_analysis'] = category_analysis
            
            # Print summary
            print("ðŸ“ˆ Performance Summary:")
            for metric_name, metric_value in metrics.items():
                if isinstance(metric_value, (int, float)):
                    if 'Rate' in metric_name:
                        print(f"   {metric_name}: {metric_value:.1%}")
                    else:
                        print(f"   {metric_name}: {metric_value:.3f}")
            
            # Step 6: Generate visualizations and reports
            if self.config['output']['generate_visualizations']:
                print("\nðŸ“Š STEP 6: Generating visualizations and reports")
                print("-" * 40)
                
                visualizer = BenchmarkVisualizer(str(self.reports_dir))
                
                # Create charts
                visualizer.create_metrics_overview(metrics)
                visualizer.create_detailed_analysis_chart(detailed_df)
                visualizer.create_performance_radar(metrics)
                
                if category_analysis:
                    visualizer.create_category_performance_chart(category_analysis)
                
                # Generate HTML report
                visualizer.generate_html_report(results)
            
            # Memory cleanup
            memory_stats = embedding_manager.get_memory_stats()
            results['final_memory_stats'] = memory_stats
            embedding_manager.cleanup()
            
            # Benchmark completion
            total_time = time.time() - benchmark_start
            results['benchmark_info']['end_time'] = datetime.now().isoformat()
            results['benchmark_info']['total_time_seconds'] = total_time
            
            print(f"\nðŸŽ‰ BENCHMARK COMPLETED SUCCESSFULLY!")
            print("=" * 60)
            print(f"â±ï¸  Total time: {total_time:.2f}s")
            print(f"ðŸŽ¯ Overall MRR: {metrics.get('MRR', 0):.3f}")
            print(f"ðŸ“Š Hit Rate@5: {metrics.get('Hit_Rate@5', 0):.1%}")
            print(f"ðŸ’¾ Reports saved in: {self.reports_dir.absolute()}")
            
            return results
            
        except Exception as e:
            print(f"\nâŒ BENCHMARK FAILED: {e}")
            print(traceback.format_exc())
            
            results['benchmark_info']['error'] = str(e)
            results['benchmark_info']['end_time'] = datetime.now().isoformat()
            
            return results
    
    def save_results(self, results: dict) -> None:
        """Save benchmark results to JSON file"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        results_path = self.reports_dir / f"benchmark_results_{timestamp}.json"
        
        try:
            # Convert DataFrame to dict for JSON serialization
            if 'detailed_analysis' in results and isinstance(results['detailed_analysis'], pd.DataFrame):
                results['detailed_analysis'] = results['detailed_analysis'].to_dict('records')
            
            with open(results_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            print(f"ðŸ’¾ Detailed results saved: {results_path}")
            
        except Exception as e:
            print(f"âŒ Error saving results: {e}")

def main():
    """Main entry point"""
    parser = argparse.ArgumentParser(description='Vietnamese Embedding Model Benchmark Tool')
    parser.add_argument('--config', default='configs/model_config.json', 
                       help='Path to configuration file')
    parser.add_argument('--verbose', action='store_true', 
                       help='Enable verbose logging')
    
    args = parser.parse_args()
    
    # Verify required files exist
    required_files = [
        args.config,
        'data/content.md', 
        'data/test_suite.json'
    ]
    
    for file_path in required_files:
        if not Path(file_path).exists():
            print(f"âŒ Required file not found: {file_path}")
            return 1
    
    try:
        # Run benchmark
        benchmark = Qwen3Benchmark(args.config)
        results = benchmark.run_benchmark()
        
        # Save results
        benchmark.save_results(results)
        
        # Print final instructions
        print("\nðŸ“‹ Next Steps:")
        print("1. Open 'reports/benchmark_report.html' in your browser")
        print("2. Review interactive charts in 'reports/charts/' directory")
        print("3. Check detailed results in the JSON files")
        
        return 0 if 'error' not in results['benchmark_info'] else 1
        
    except KeyboardInterrupt:
        print("\nâš ï¸  Benchmark interrupted by user")
        return 1
    except Exception as e:
        print(f"\nâŒ Unexpected error: {e}")
        print(traceback.format_exc())
        return 1

if __name__ == "__main__":
    exit(main())
```

## ðŸ“„ setup.py

```python
from setuptools import setup, find_packages

setup(
    name="vietnamese-qwen3-benchmark",
    version="1.0.0",
    description="Benchmark tool for Qwen3-Embedding-0.6B on Vietnamese text",
    author="Your Name",
    packages=find_packages(),
    install_requires=[
        "sentence-transformers>=2.2.2",
        "torch>=2.0.0",
        "numpy>=1.24.0",
        "pandas>=2.0.0",
        "matplotlib>=3.7.0",
        "seaborn>=0.12.0", 
        "underthesea>=6.7.0",
        "pyvi>=0.1.1",
        "scikit-learn>=1.3.0",
        "transformers>=4.30.0",
        "accelerate>=0.20.0",
        "plotly>=5.15.0",
        "kaleido>=0.2.1",
        "jinja2>=3.1.0",
        "tqdm"
    ],
    python_requires=">=3.10",
    entry_points={
        "console_scripts": [
            "vietnamese-benchmark=benchmark:main",
        ],
    },
)
```

## ðŸ“– README.md

```markdown
# Vietnamese Qwen3 Embedding Benchmark ðŸ‡»ðŸ‡³

CÃ´ng cá»¥ benchmark chuyÃªn dá»¥ng cho model **Qwen/Qwen3-Embedding-0.6B** trÃªn dá»¯ liá»‡u tiáº¿ng Viá»‡t vá»›i tá»‘i Æ°u GPU.

## âœ¨ TÃ­nh nÄƒng

- **ðŸŽ¯ ChuyÃªn dá»¥ng cho Qwen3**: Tá»‘i Æ°u cho model Qwen/Qwen3-Embedding-0.6B
- **ðŸ‡»ðŸ‡³ Tiáº¿ng Viá»‡t**: Há»— trá»£ Ä‘áº§y Ä‘á»§ xá»­ lÃ½ vÄƒn báº£n tiáº¿ng Viá»‡t (pyvi, underthesea)
- **âš¡ GPU Acceleration**: Tá»± Ä‘á»™ng detect vÃ  sá»­ dá»¥ng GPU/CUDA
- **ðŸ“Š Metrics Ä‘áº§y Ä‘á»§**: MRR, Hit Rate@K, MAP, NDCG, Precision, Recall
- **ðŸ“ˆ Visualizations**: Interactive charts vá»›i Plotly
- **ðŸ“‹ HTML Reports**: BÃ¡o cÃ¡o HTML chi tiáº¿t vÃ  professional

## ðŸš€ CÃ i Ä‘áº·t

### 1. Clone project
```bash
git clone <your-repo>
cd vietnamese_qwen3_benchmark
```

### 2. Táº¡o virtual environment
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# hoáº·c
venv\Scripts\activate     # Windows
```

### 3. CÃ i Ä‘áº·t dependencies
```bash
pip install -r requirements.txt
```

### 4. CÃ i Ä‘áº·t GPU support (náº¿u cÃ³)
```bash
# CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# CUDA 12.1  
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

## ðŸ“Š CÃ¡ch sá»­ dá»¥ng

### Cháº¡y benchmark Ä‘Æ¡n giáº£n
```bash
python benchmark.py
```

### Vá»›i custom config
```bash
python benchmark.py --config configs/model_config.json --verbose
```

### Cáº¥u trÃºc sau khi cháº¡y
```
vietnamese_qwen3_benchmark/
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ benchmark_report.html          # BÃ¡o cÃ¡o HTML chÃ­nh
â”‚   â”œâ”€â”€ benchmark_results_20241201_143022.json
â”‚   â”œâ”€â”€ chunks_info.json               # Debug info cho chunks
â”‚   â””â”€â”€ charts/
â”‚       â”œâ”€â”€ metrics_overview.html      # Interactive charts
â”‚       â”œâ”€â”€ detailed_analysis.html
â”‚       â”œâ”€â”€ performance_radar.html
â”‚       â””â”€â”€ category_performance.html
â””â”€â”€ model_cache/                       # Cached models
```

## âš™ï¸ Configuration

### TÃ¹y chá»‰nh model trong `configs/model_config.json`:
```json
{
  "model": {
    "name": "Qwen/Qwen3-Embedding-0.6B",  // â† Thay Ä‘á»•i model á»Ÿ Ä‘Ã¢y
    "device": "auto",
    "batch_size": 32,
    "max_seq_length": 512,
    "normalize_embeddings": true
  }
}
```

### Thay Ä‘á»•i dá»¯ liá»‡u test:
- **Content**: Chá»‰nh sá»­a `data/content.md`
- **Questions**: Cáº­p nháº­t `data/test_suite.json`

## ðŸ“ˆ Metrics Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡

| Metric | MÃ´ táº£ | Má»¥c tiÃªu |
|--------|-------|----------|
| **MRR** | Mean Reciprocal Rank | > 0.65 |
| **Hit Rate@1** | Top-1 accuracy | > 50% |
| **Hit Rate@5** | Top-5 accuracy | > 75% |
| **Precision@5** | Precision at 5 | > 0.6 |
| **NDCG@5** | Ranking quality | > 0.6 |
| **MAP@5** | Mean Average Precision | > 0.5 |

## ðŸŽ¯ Káº¿t quáº£ máº«u

```
ðŸŽ‰ BENCHMARK COMPLETED SUCCESSFULLY!
============================================================
â±ï¸  Total time: 45.23s
ðŸŽ¯ Overall MRR: 0.724
ðŸ“Š Hit Rate@5: 81.2%
ðŸ’¾ Reports saved in: /path/to/reports
```

## ðŸ”§ Troubleshooting

### CUDA Out of Memory
```bash
# Giáº£m batch size trong config
"batch_size": 16  # hoáº·c 8
```

### Model khÃ´ng táº£i Ä‘Æ°á»£c
```bash
# XÃ³a cache vÃ  thá»­ láº¡i
rm -rf model_cache/
python benchmark.py
```

### Lá»—i Vietnamese NLP
```bash
pip uninstall underthesea pyvi
pip install underthesea pyvi
```

## ðŸ“Š Hiá»ƒu káº¿t quáº£

### Performance Levels:
- **Excellent** (MRR â‰¥ 0.7): Sáºµn sÃ ng production
- **Good** (MRR â‰¥ 0.5): CÃ³ thá»ƒ sá»­ dá»¥ng vá»›i optimizations
- **Average** (MRR â‰¥ 0.3): Cáº§n fine-tuning
- **Poor** (MRR < 0.3): Cáº§n Ä‘á»•i model hoáº·c cáº£i thiá»‡n data

### PhÃ¢n tÃ­ch Category:
- Kiá»ƒm tra category nÃ o cÃ³ hiá»‡u suáº¥t tháº¥p nháº¥t
- Xem cÃ¢u há»i nÃ o khÃ³ nháº¥t (khÃ´ng tÃ¬m tháº¥y trong top-5)
- PhÃ¢n tÃ­ch score gap giá»¯a top-1 vÃ  correct answer

## ðŸ”„ Workflow tÃ¹y chá»‰nh

### Thay Ä‘á»•i model
```bash
# Chá»‰nh sá»­a configs/model_config.json
{
  "model": {
    "name": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
  }
}
```

### ThÃªm dá»¯ liá»‡u má»›i
```bash
# 1. Cáº­p nháº­t data/content.md vá»›i ná»™i dung má»›i
# 2. Táº¡o cÃ¢u há»i tÆ°Æ¡ng á»©ng trong data/test_suite.json
# 3. Cháº¡y benchmark
python benchmark.py
```

### Batch processing nhiá»u models
```bash
# Táº¡o script Ä‘á»ƒ test nhiá»u models
for model in "Qwen/Qwen3-Embedding-0.6B" "sentence-transformers/LaBSE"
do
    echo "Testing $model"
    # Update config and run
    python benchmark.py --config configs/${model//\//_}.json
done
```

## ðŸŽ›ï¸ Advanced Usage

### Memory optimization
```python
# Trong embedding_manager.py, tÃ¹y chá»‰nh:
def _get_optimal_batch_size(self, requested_batch_size: int) -> int:
    if self.device == "cuda":
        available_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
        if available_memory < 4:
            return 8    # Giáº£m cho GPU nhá»
        elif available_memory >= 16:
            return 128  # TÄƒng cho GPU lá»›n
```

### Custom metrics
```python
# Trong src/metrics.py, thÃªm metric má»›i:
def calculate_custom_metric(self, search_results, ground_truth):
    # Custom logic here
    return custom_score
```

## ðŸ“ Logs vÃ  Debug

### Enable verbose logging
```bash
python benchmark.py --verbose
```

### Debug chunks
```bash
# Xem chunks_info.json Ä‘á»ƒ hiá»ƒu cÃ¡ch text Ä‘Æ°á»£c chia
cat reports/chunks_info.json | jq '.chunks[0]'
```

### Memory monitoring
```bash
# Theo dÃµi GPU memory trong quÃ¡ trÃ¬nh cháº¡y
watch -n 1 nvidia-smi
```

## ðŸ¤ ÄÃ³ng gÃ³p

1. Fork repository
2. Táº¡o feature branch: `git checkout -b feature/new-feature`
3. Test vá»›i Vietnamese data
4. Commit: `git commit -m 'Add Vietnamese feature'`
5. Push: `git push origin feature/new-feature`
6. Táº¡o Pull Request

## ðŸ“„ License

MIT License - xem LICENSE file.

## ðŸ™ Credits

- [Qwen Team](https://github.com/QwenLM/Qwen) - Qwen3-Embedding model
- [Sentence Transformers](https://www.sbert.net/) - Embedding framework
- [underthesea](https://github.com/undertheseanlp/underthesea) - Vietnamese NLP
- [pyvi](https://github.com/trungtv/pyvi) - Vietnamese tokenization

---

**Happy Benchmarking! ðŸš€**

Náº¿u gáº·p váº¥n Ä‘á», táº¡o issue trong repository hoáº·c liÃªn há»‡ qua email.
```

## ðŸŽ¯ HÆ°á»›ng dáº«n cháº¡y nhanh:

### 1. CÃ i Ä‘áº·t cÆ¡ báº£n:
```bash
# Clone vÃ  setup
git clone <repo>
cd vietnamese_qwen3_benchmark

# Virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac

# Install dependencies
pip install -r requirements.txt
```

### 2. Cháº¡y benchmark:
```bash
python benchmark.py
```

### 3. Xem káº¿t quáº£:
- Má»Ÿ `reports/benchmark_report.html` trong browser
- Xem cÃ¡c charts interactive trong `reports/charts/`
- Kiá»ƒm tra JSON results cho detailed data

## ðŸ”§ Äiá»u chá»‰nh model:

Äá»ƒ thay Ä‘á»•i model, chá»‰nh sá»­a `configs/model_config.json`:
```json
{
  "model": {
    "name": "your-model-name-here",  // â† Thay Ä‘á»•i táº¡i Ä‘Ã¢y
    "device": "auto",
    "batch_size": 32
  }
}
```

## ðŸ“Š Output Structure:

Sau khi cháº¡y xong, báº¡n sáº½ cÃ³:

1. **HTML Report**: `reports/benchmark_report.html` - BÃ¡o cÃ¡o tá»•ng quan
2. **Interactive Charts**: `reports/charts/` - CÃ¡c biá»ƒu Ä‘á»“ interactive
3. **JSON Results**: `reports/benchmark_results_*.json` - Dá»¯ liá»‡u chi tiáº¿t
4. **Debug Info**: `reports/chunks_info.json` - ThÃ´ng tin chunks

## ðŸŽ¯ Expected Performance:

Vá»›i Qwen3-Embedding-0.6B trÃªn Vietnamese data:
- **MRR**: 0.65-0.75 (good range)  
- **Hit Rate@5**: 75-85%
- **Processing Speed**: 50-100 texts/second (GPU)
- **Memory Usage**: ~2-4GB GPU RAM

Project nÃ y Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a Ä‘áº·c biá»‡t cho Vietnamese text processing vÃ  Qwen3 model, vá»›i GPU acceleration vÃ  comprehensive metrics Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ performance má»™t cÃ¡ch chÃ­nh xÃ¡c nháº¥t.
