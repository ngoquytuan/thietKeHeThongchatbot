Nếu đã đủ thông tin hãy giúp tôi viết tài liệu thiết kế module dưới đây.
Tài liệu này cung cấp roadmap đầy đủ để implement FR-03.3 mà không cần code cụ thể nhưng 
có các bước làm cụ thể như chuẩn bị môi trường PC thế nào, phần mềm gì?
 để Team kỹ thuật có thể follow step-by-step guide này để thiết kế tối ưu cho hệ thống chatbot
 và có mermaidchart luồng công việc, điểm quay đầu khi bị fail.

3.3 Module Xử lý Dữ liệu (FR-03)
**FR-03.3 - Pipeline Nạp Dữ liệu (Data Ingestion)**
- Tự động chunking tài liệu (size: 500-1000 tokens)
- Tạo embeddings cho từng chunk
- Lưu trữ đồng bộ vào Vector DB và Relational DB
- Hỗ trợ batch processing và real-time ingestion
---
# TÀI LIỆU THIẾT KẾ MODULE: DATA INGESTION PIPELINE
## FR-03.3 - Pipeline Nạp Dữ liệu (Data Ingestion)

---

**Phiên bản:** 1.0  
**Ngày:** 31/08/2025  
**Module:** FR-03.3 Data Ingestion Pipeline  
**Độ ưu tiên:** HIGH (Critical Path)  

---

## 1. TỔNG QUAN MODULE

### 1.1 Mục tiêu
Xây dựng pipeline tự động để xử lý và nạp dữ liệu tài liệu vào hệ thống, bao gồm chunking, embedding generation, và lưu trữ đồng bộ vào cả Vector Database và Relational Database.

### 1.2 Chức năng chính
- **Automatic Document Chunking**: Tách tài liệu thành các chunk 500-1000 tokens
- **Embedding Generation**: Tạo vector embeddings cho mỗi chunk
- **Dual Storage**: Lưu trữ đồng bộ vào Vector DB và PostgreSQL
- **Batch & Real-time Processing**: Hỗ trợ cả xử lý hàng loạt và thời gian thực
- **Error Handling**: Xử lý lỗi và retry mechanism
- **Progress Tracking**: Theo dõi tiến độ xử lý

### 1.3 Input/Output
- **Input**: Tài liệu đã qua Quality Control (từ FR-03.2)
- **Output**: Dữ liệu đã được index trong Vector DB và metadata trong PostgreSQL

---

## 2. YÊU CẦU HỆ THỐNG & CHUẨN BỊ MÔI TRƯỜNG

### 2.1 Hardware Requirements

| Component | Minimum | Recommended | Lý do |
|-----------|---------|-------------|-------|
| **CPU** | 8 cores | 16+ cores | Parallel processing |
| **RAM** | 16GB | 32GB+ | Large document processing |
| **Storage** | 100GB SSD | 500GB+ NVMe SSD | Fast I/O operations |
| **GPU** | Optional | RTX 4090 / Tesla V100 | Embedding acceleration (nếu dùng local models) |
| **Network** | 100Mbps | 1Gbps+ | API calls & data transfer |

### 2.2 Software Environment Setup

#### 2.2.1 Operating System
```bash
# Recommended: Ubuntu 22.04 LTS hoặc CentOS Stream 9
# Alternative: Windows 11 với WSL2
```

#### 2.2.2 Python Environment
```bash
# Step 1: Install Python 3.9+
sudo apt update && sudo apt install python3.9 python3.9-pip python3.9-venv

# Step 2: Create virtual environment
python3.9 -m venv data_ingestion_env
source data_ingestion_env/bin/activate

# Step 3: Upgrade pip
pip install --upgrade pip setuptools wheel
```

#### 2.2.3 Required Python Packages
```bash
# Core dependencies
pip install fastapi==0.104.1
pip install uvicorn[standard]==0.24.0
pip install pydantic==2.4.2
pip install python-multipart==0.0.6

# Document processing
pip install langchain==0.0.335
pip install langchain-community==0.0.5
pip install tiktoken==0.5.1
pip install pypdf2==3.0.1
pip install python-docx==1.1.0
pip install python-pptx==0.6.23

# Embedding & Vector DB
pip install openai==1.3.5
pip install chromadb==0.4.15
pip install sentence-transformers==2.2.2
pip install faiss-cpu==1.7.4  # hoặc faiss-gpu nếu có GPU

# Database
pip install psycopg2-binary==2.9.7
pip install sqlalchemy==2.0.23
pip install alembic==1.12.1

# Utilities
pip install redis==5.0.1
pip install celery==5.3.4
pip install pika==1.3.2  # RabbitMQ
pip install python-dotenv==1.0.0
pip install loguru==0.7.2
```

#### 2.2.4 Database Setup
```bash
# PostgreSQL installation
sudo apt install postgresql postgresql-contrib
sudo systemctl start postgresql
sudo systemctl enable postgresql

# Create database và user
sudo -u postgres psql
CREATE DATABASE knowledge_assistant;
CREATE USER ingestion_user WITH PASSWORD 'secure_password_123';
GRANT ALL PRIVILEGES ON DATABASE knowledge_assistant TO ingestion_user;
\q

# Redis installation (cho queue management)
sudo apt install redis-server
sudo systemctl start redis-server
sudo systemctl enable redis-server
```

#### 2.2.5 Vector Database Setup
```bash
# Option 1: ChromaDB (Recommended cho development)
# Đã cài trong Python packages

# Option 2: Weaviate (cho production)
docker run -d \
  --name weaviate \
  -p 8080:8080 \
  -e QUERY_DEFAULTS_LIMIT=25 \
  -e AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED=true \
  -e PERSISTENCE_DATA_PATH='/var/lib/weaviate' \
  semitechnologies/weaviate:latest

# Option 3: FAISS (local, high performance)
# Đã cài trong Python packages
```

#### 2.2.6 Message Queue Setup
```bash
# RabbitMQ cho task queue
sudo apt install rabbitmq-server
sudo systemctl start rabbitmq-server
sudo systemctl enable rabbitmq-server

# Create user và virtual host
sudo rabbitmqctl add_user ingestion_worker secure_password_456
sudo rabbitmqctl set_user_tags ingestion_worker administrator
sudo rabbitmqctl add_vhost ingestion_vhost
sudo rabbitmqctl set_permissions -p ingestion_vhost ingestion_worker ".*" ".*" ".*"
```

### 2.3 Development Tools
```bash
# Code quality tools
pip install black==23.10.1
pip install flake8==6.1.0
pip install mypy==1.7.1
pip install pytest==7.4.3
pip install pytest-asyncio==0.21.1

# Monitoring tools
pip install prometheus-client==0.19.0
pip install structlog==23.2.0
```

---

## 3. KIẾN TRÚC MODULE CHI TIẾT

### 3.1 Architecture Overview

```mermaid
graph TB
    %% Input Layer
    subgraph "📥 INPUT LAYER"
        FileUpload[📄 File Upload API]
        BatchImporter[📦 Batch Importer]
        RealtimeStream[⚡ Real-time Stream]
    end
    
    %% Queue Management
    subgraph "🔄 QUEUE MANAGEMENT"
        TaskQueue[📋 Task Queue<br/>RabbitMQ/Celery]
        PriorityQueue[⭐ Priority Queue<br/>High/Medium/Low]
        DLQ[☠️ Dead Letter Queue<br/>Failed Tasks]
    end
    
    %% Processing Core
    subgraph "⚙️ PROCESSING CORE"
        subgraph "Worker Pool"
            Worker1[👷 Worker 1<br/>Document Processing]
            Worker2[👷 Worker 2<br/>Embedding Generation]
            Worker3[👷 Worker 3<br/>Storage Operations]
            WorkerN[👷 Worker N<br/>Scalable Workers]
        end
        
        subgraph "Processing Steps"
            DocValidator[✅ Document Validator]
            ChunkProcessor[✂️ Chunk Processor]
            EmbeddingGen[🔢 Embedding Generator]
            StorageManager[💾 Storage Manager]
        end
    end
    
    %% Storage Layer
    subgraph "🗄️ STORAGE LAYER"
        VectorDB[(🔢 Vector Database<br/>ChromaDB/Weaviate)]
        PostgreSQL[(🐘 PostgreSQL<br/>Metadata Storage)]
        Redis[(🔴 Redis<br/>Cache & Sessions)]
    end
    
    %% Monitoring & Control
    subgraph "📊 MONITORING"
        ProgressTracker[📊 Progress Tracker]
        ErrorHandler[🚨 Error Handler]
        MetricsCollector[📈 Metrics Collector]
        Alerting[🔔 Alerting System]
    end
    
    %% External Services
    EmbeddingAPI[🌐 Embedding API<br/>OpenAI/HuggingFace]
    
    %% Flow connections
    FileUpload --> TaskQueue
    BatchImporter --> TaskQueue
    RealtimeStream --> PriorityQueue
    
    TaskQueue --> Worker1
    PriorityQueue --> Worker2
    TaskQueue --> Worker3
    TaskQueue --> WorkerN
    
    Worker1 --> DocValidator
    Worker2 --> ChunkProcessor
    Worker3 --> EmbeddingGen
    WorkerN --> StorageManager
    
    DocValidator --> ChunkProcessor
    ChunkProcessor --> EmbeddingGen
    EmbeddingGen --> StorageManager
    
    EmbeddingGen -.-> EmbeddingAPI
    
    StorageManager --> VectorDB
    StorageManager --> PostgreSQL
    StorageManager --> Redis
    
    %% Monitoring connections
    Worker1 -.-> ProgressTracker
    Worker2 -.-> ErrorHandler
    Worker3 -.-> MetricsCollector
    ErrorHandler --> DLQ
    MetricsCollector --> Alerting
    
    %% Styling
    classDef input fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef queue fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef processing fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef storage fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef monitoring fill:#e0f2f1,stroke:#00796b,stroke-width:2px
    classDef external fill:#ffebee,stroke:#c62828,stroke-width:2px
    
    class FileUpload,BatchImporter,RealtimeStream input
    class TaskQueue,PriorityQueue,DLQ queue
    class Worker1,Worker2,Worker3,WorkerN,DocValidator,ChunkProcessor,EmbeddingGen,StorageManager processing
    class VectorDB,PostgreSQL,Redis storage
    class ProgressTracker,ErrorHandler,MetricsCollector,Alerting monitoring
    class EmbeddingAPI external
```

### 3.2 Data Flow với Error Handling

```mermaid
graph TB
    %% Start
    Start([🚀 START<br/>Document Input])
    
    %% Main Processing Steps
    Validate{✅ Validate Document<br/>Format, Size, Content}
    PreProcess[🧹 Pre-process<br/>Clean & Normalize]
    Chunk[✂️ Chunking<br/>500-1000 tokens]
    ValidateChunks{🔍 Validate Chunks<br/>Quality Check}
    
    %% Embedding Process
    GenerateEmbedding[🔢 Generate Embeddings<br/>API Call/Local Model]
    EmbeddingSuccess{✅ Embedding Success?}
    
    %% Storage Process
    PrepareStorage[📦 Prepare Storage Data<br/>Vector + Metadata]
    VectorStore[💾 Store in Vector DB]
    VectorSuccess{✅ Vector Store Success?}
    MetadataStore[🗃️ Store in PostgreSQL]
    MetadataSuccess{✅ Metadata Success?}
    
    %% Success Path
    UpdateProgress[📊 Update Progress]
    CacheResult[⚡ Cache Results]
    NotifyComplete[📢 Notify Completion]
    Success([✅ SUCCESS<br/>Document Processed])
    
    %% Error Handling Points
    ValidationError[❌ Validation Error]
    ChunkError[❌ Chunking Error] 
    EmbeddingError[❌ Embedding Error]
    VectorStoreError[❌ Vector Store Error]
    MetadataError[❌ Metadata Error]
    
    %% Retry Logic
    RetryDecision{🔄 Retry?<br/>Count < Max}
    IncrementRetry[➕ Increment Retry Count]
    ExponentialBackoff[⏳ Exponential Backoff<br/>Wait 2^n seconds]
    
    %% Dead Letter Queue
    DeadLetter[☠️ Send to DLQ<br/>Manual Review Required]
    LogError[📝 Log Error Details]
    NotifyAdmin[🚨 Notify Administrator]
    
    %% Rollback Process
    RollbackVector[↩️ Rollback Vector Store]
    RollbackMetadata[↩️ Rollback Metadata]
    CleanupTemp[🧹 Cleanup Temp Files]
    
    %% Main Flow
    Start --> Validate
    Validate -->|Valid| PreProcess
    Validate -->|Invalid| ValidationError
    
    PreProcess --> Chunk
    Chunk --> ValidateChunks
    ValidateChunks -->|Valid| GenerateEmbedding
    ValidateChunks -->|Invalid| ChunkError
    
    GenerateEmbedding --> EmbeddingSuccess
    EmbeddingSuccess -->|Success| PrepareStorage
    EmbeddingSuccess -->|Failure| EmbeddingError
    
    PrepareStorage --> VectorStore
    VectorStore --> VectorSuccess
    VectorSuccess -->|Success| MetadataStore
    VectorSuccess -->|Failure| VectorStoreError
    
    MetadataStore --> MetadataSuccess
    MetadataSuccess -->|Success| UpdateProgress
    MetadataSuccess -->|Failure| MetadataError
    
    UpdateProgress --> CacheResult
    CacheResult --> NotifyComplete
    NotifyComplete --> Success
    
    %% Error Flows
    ValidationError --> RetryDecision
    ChunkError --> RetryDecision
    EmbeddingError --> RetryDecision
    VectorStoreError --> RetryDecision
    MetadataError --> RollbackVector
    
    %% Retry Logic Flow
    RetryDecision -->|Yes, Retry < 3| IncrementRetry
    RetryDecision -->|No, Max Retries| DeadLetter
    
    IncrementRetry --> ExponentialBackoff
    ExponentialBackoff --> Validate
    
    %% Dead Letter Flow
    DeadLetter --> LogError
    LogError --> NotifyAdmin
    
    %% Rollback Flow
    RollbackVector --> RollbackMetadata
    RollbackMetadata --> CleanupTemp
    CleanupTemp --> RetryDecision
    
    %% Styling
    classDef startEnd fill:#e3f2fd,stroke:#1976d2,stroke-width:3px
    classDef process fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef decision fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef error fill:#ffebee,stroke:#c62828,stroke-width:2px
    classDef retry fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef rollback fill:#e0f2f1,stroke:#00796b,stroke-width:2px
    
    class Start,Success startEnd
    class PreProcess,Chunk,GenerateEmbedding,PrepareStorage,VectorStore,MetadataStore,UpdateProgress,CacheResult,NotifyComplete process
    class Validate,ValidateChunks,EmbeddingSuccess,VectorSuccess,MetadataSuccess,RetryDecision decision
    class ValidationError,ChunkError,EmbeddingError,VectorStoreError,MetadataError,DeadLetter,LogError,NotifyAdmin error
    class IncrementRetry,ExponentialBackoff retry
    class RollbackVector,RollbackMetadata,CleanupTemp rollback
```

---

## 4. IMPLEMENTATION ROADMAP

### 4.1 Phase 1: Core Infrastructure Setup (Tuần 1-2)

#### 4.1.1 Development Environment
```bash
# Step 1: Project structure setup
mkdir data_ingestion_pipeline
cd data_ingestion_pipeline
mkdir -p {src,tests,config,logs,temp,docs}
mkdir -p src/{api,core,models,services,utils}

# Step 2: Initialize git repository
git init
echo "venv/\n*.pyc\n__pycache__/\n.env\nlogs/\ntemp/" > .gitignore

# Step 3: Create configuration files
```

#### 4.1.2 Configuration Setup
Tạo file `config/settings.py`:
```python
# Environment variables và configuration
# Database connections
# API keys và endpoints
# Processing parameters
# Retry policies
```

#### 4.1.3 Database Schema Design
```sql
-- PostgreSQL schema cho metadata storage
-- Tables: documents, chunks, processing_jobs, error_logs
-- Indexes cho performance optimization
```

#### 4.1.4 Basic API Framework
```python
# FastAPI application setup
# Health check endpoints
# Basic CRUD operations
# Authentication middleware
```

### 4.2 Phase 2: Document Processing Core (Tuần 3-4)

#### 4.2.1 Document Validator Implementation
```python
# File format validation
# Size limits checking
# Content security scanning
# Metadata extraction
```

**Key Implementation Points:**
- Support formats: PDF, DOCX, TXT, HTML, Markdown
- Max file size: 50MB per document
- Virus scanning integration
- Duplicate detection based on hash

#### 4.2.2 Chunk Processor Development
```python
# Smart chunking algorithms
# Token counting với tiktoken
# Overlap handling
# Chunk quality validation
```

**Chunking Strategy:**
```mermaid
graph LR
    Document[📄 Document] --> TextExtract[📝 Text Extraction]
    TextExtract --> SentenceSplit[📏 Sentence Splitting]
    SentenceSplit --> TokenCount[🔢 Token Counting]
    TokenCount --> ChunkDecision{📊 Chunk Size OK?<br/>500-1000 tokens}
    ChunkDecision -->|Too Small| Merge[➕ Merge with Next]
    ChunkDecision -->|Too Large| Split[✂️ Split at Sentence Boundary]
    ChunkDecision -->|Perfect| ValidChunk[✅ Valid Chunk]
    Merge --> TokenCount
    Split --> TokenCount
    ValidChunk --> AddOverlap[🔗 Add Overlap<br/>50 tokens]
    AddOverlap --> FinalChunk[📦 Final Chunk]
    
    classDef process fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef decision fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef result fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    
    class Document,TextExtract,SentenceSplit,TokenCount,Merge,Split,AddOverlap process
    class ChunkDecision decision
    class ValidChunk,FinalChunk result
```

#### 4.2.3 Error Handling Framework
```python
# Custom exception classes
# Retry mechanisms với exponential backoff
# Dead letter queue implementation
# Comprehensive logging
```

### 4.3 Phase 3: Embedding & Storage Integration (Tuần 5-6)

#### 4.3.1 Embedding Generator
```python
# Multiple embedding model support
# Batch processing optimization
# Rate limiting cho API calls
# Local model fallback
```

**Embedding Models Priority:**
1. **OpenAI text-embedding-ada-002** (Primary)
2. **HuggingFace sentence-transformers** (Fallback)
3. **Local model** (Offline mode)

#### 4.3.2 Vector Database Integration
```python
# ChromaDB/Weaviate client implementation
# Batch insert optimization
# Index management
# Query performance tuning
```

#### 4.3.3 Dual Storage Manager
```python
# Transaction management
# Consistency guarantees
# Rollback mechanisms
# Performance optimization
```

**Storage Transaction Flow:**

# Update 20/Sep/2025

```mermaid
sequenceDiagram
    participant Client
    participant StorageManager
    participant VectorDB
    participant PostgreSQL  
    participant Cache
    participant ErrorHandler
    
    Client->>StorageManager: Store Document Chunks
    StorageManager->>StorageManager: Validate Input Data
    
    alt Validation Success
        StorageManager->>StorageManager: Begin Transaction
        StorageManager->>VectorDB: Store Embeddings
        VectorDB-->>StorageManager: Success/Failure
        
        alt Vector Store Success
            StorageManager->>PostgreSQL: Store Metadata
            PostgreSQL-->>StorageManager: Success/Failure
            
            alt Metadata Store Success
                StorageManager->>Cache: Update Cache
                Cache-->>StorageManager: Cache Updated
                StorageManager->>StorageManager: Commit Transaction
                StorageManager-->>Client: Success Response
            else Metadata Store Failure
                StorageManager->>VectorDB: Rollback Vector Store
                StorageManager->>ErrorHandler: Log Error + Retry Logic
                StorageManager->>StorageManager: Rollback Transaction
                StorageManager-->>Client: Error Response
            end
        else Vector Store Failure
            StorageManager->>ErrorHandler: Log Error + Retry Logic
            StorageManager->>StorageManager: Rollback Transaction
            StorageManager-->>Client: Error Response
        end
    else Validation Failure
        StorageManager->>ErrorHandler: Log Validation Error
        StorageManager-->>Client: Validation Error Response
    end
```


```mermaid
sequenceDiagram
    participant Client
    participant StorageManager
    participant VectorDB
    participant PostgreSQL
    participant Cache
    
    Client->>StorageManager: Store Document Chunks
    StorageManager->>StorageManager: Begin Transaction
    
    StorageManager->>VectorDB: Store Embeddings
    VectorDB-->>StorageManager: Success/Failure
    
    alt Vector Store Success
        StorageManager->>PostgreSQL: Store Metadata
        PostgreSQL-->>StorageManager: Success/Failure
        
        alt Metadata Store Success
            StorageManager->>Cache: Update Cache
            StorageManager->>StorageManager: Commit Transaction
            StorageManager-->>Client: Success Response
        else Metadata Store Failure
            StorageManager->>VectorDB: Rollback Vector Store
            StorageManager->>StorageManager: Rollback Transaction
            StorageManager-->>Client: Error Response
        end
    else Vector Store Failure
        StorageManager->>StorageManager: Rollback Transaction
        StorageManager-->>Client: Error Response
    end
```

### 4.4 Phase 4: Queue Management & Workers (Tuần 7-8)

#### 4.4.1 Task Queue Implementation
```python
# Celery workers setup
# Task routing strategies
# Priority queue management
# Monitoring dashboard
```

#### 4.4.2 Worker Pool Management
```python
# Dynamic scaling
# Load balancing
# Health monitoring
# Graceful shutdown
```

#### 4.4.3 Real-time Processing
```python
# WebSocket connections
# Event streaming
# Progress notifications
# Live status updates
```

### 4.5 Phase 5: Monitoring & Optimization (Tuần 9-10)

#### 4.5.1 Metrics Collection
```python
# Prometheus metrics
# Custom KPIs tracking
# Performance benchmarking
# Resource utilization monitoring
```

#### 4.5.2 Performance Optimization
```python
# Batch processing tuning
# Memory management
# Connection pooling
# Caching strategies
```

#### 4.5.3 Alerting System
```python
# Error threshold alerts
# Performance degradation warnings
# Resource exhaustion notifications
# SLA breach alarms
```

---

## 5. CONFIGURATION & PARAMETERS

### 5.1 Environment Configuration (.env)
```bash
# Database Configuration
DATABASE_URL=postgresql://ingestion_user:secure_password_123@localhost:5432/knowledge_assistant
REDIS_URL=redis://localhost:6379/0

# Vector Database
VECTOR_DB_TYPE=chromadb  # chromadb, weaviate, faiss
CHROMA_HOST=localhost
CHROMA_PORT=8000

# Embedding Service
EMBEDDING_PROVIDER=openai  # openai, huggingface, local
OPENAI_API_KEY=your_openai_api_key_here
HUGGINGFACE_API_TOKEN=your_hf_token_here

# Processing Parameters
MAX_CHUNK_SIZE=1000
MIN_CHUNK_SIZE=500
CHUNK_OVERLAP=50
MAX_FILE_SIZE_MB=50
SUPPORTED_FORMATS=pdf,docx,txt,html,md

# Queue Configuration
CELERY_BROKER_URL=pyamqp://ingestion_worker:secure_password_456@localhost:5672/ingestion_vhost
CELERY_RESULT_BACKEND=redis://localhost:6379/1
MAX_WORKERS=4
TASK_TIMEOUT=3600

# Retry Policy
MAX_RETRIES=3
RETRY_BACKOFF_FACTOR=2
RETRY_JITTER=True

# Monitoring
ENABLE_METRICS=true
METRICS_PORT=8000
LOG_LEVEL=INFO
```

### 5.2 Processing Parameters
```python
CHUNKING_CONFIG = {
    "strategy": "semantic_aware",  # semantic_aware, fixed_size, sentence_based
    "target_size": 750,  # tokens
    "size_range": (500, 1000),
    "overlap_tokens": 50,
    "preserve_sentences": True,
    "max_chunk_overlap": 0.1  # 10% overlap maximum
}

EMBEDDING_CONFIG = {
    "batch_size": 32,
    "rate_limit": 3000,  # requests per minute
    "timeout": 30,  # seconds
    "retry_on_rate_limit": True,
    "normalize_vectors": True
}

STORAGE_CONFIG = {
    "vector_db_batch_size": 100,
    "postgresql_batch_size": 50,
    "transaction_timeout": 300,  # 5 minutes
    "enable_compression": True
}
```

---

## 6. TESTING & VALIDATION STRATEGY

### 6.1 Unit Testing Checklist
- [ ] Document validation logic
- [ ] Chunking algorithms accuracy
- [ ] Embedding generation consistency
- [ ] Storage transaction integrity
- [ ] Error handling completeness
- [ ] Retry mechanism reliability

### 6.2 Integration Testing Scenarios
- [ ] End-to-end document processing
- [ ] Database connectivity và transactions
- [ ] External API integration (OpenAI, HuggingFace)
- [ ] Queue management và worker coordination
- [ ] Error recovery và rollback processes

### 6.3 Performance Testing
```python
# Load testing parameters
TEST_SCENARIOS = {
    "single_document": {
        "file_size": "1MB",
        "expected_time": "< 30 seconds",
        "success_rate": "> 99%"
    },
    "batch_processing": {
        "document_count": 100,
        "concurrent_workers": 4,
        "expected_throughput": "> 10 docs/minute",
        "memory_usage": "< 4GB"
    },
    "stress_test": {
        "document_count": 1000,
        "concurrent_requests": 20,
        "duration": "1 hour",
        "error_rate": "< 1%"
    }
}
```

### 6.4 Quality Assurance
```python
# Validation metrics
QUALITY_METRICS = {
    "chunk_size_compliance": "> 95%",  # chunks trong range 500-1000 tokens
    "embedding_consistency": "> 99%",  # same input → same embedding
    "storage_integrity": "100%",       # vector-metadata consistency
    "processing_speed": "< 60s per MB", # throughput requirement
    "error_recovery": "> 95%"          # successful recovery rate
}
```

---

## 7. DEPLOYMENT & OPERATIONS

### 7.1 Docker Configuration
```dockerfile
# Dockerfile cho production deployment
FROM python:3.9-slim

WORKDIR /app

# System dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    postgresql-client \
    && rm -rf /var/lib/apt/lists/*

# Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Application code
COPY src/ ./src/
COPY config/ ./config/

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD python -c "import requests; requests.get('http://localhost:8000/health')"

CMD ["uvicorn", "src.api.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### 7.2 Kubernetes Deployment
```yaml
# k8s deployment configuration
# Service definitions
# ConfigMaps và Secrets
# Horizontal Pod Autoscaler
```

### 7.3 Monitoring Setup
```yaml
# Prometheus monitoring rules
# Grafana dashboards
# Alert manager configuration
# Log aggregation setup
```

---

## 8. TROUBLESHOOTING GUIDE

### 8.1 Common Issues & Solutions

#### 8.1.1 Performance Issues
```bash
# Symptom: Slow processing
# Check: Worker utilization, database connections, API rate limits
# Solution: Scale workers, optimize batch sizes, implement caching

# Diagnostic commands:
docker stats
kubectl top pods
redis-cli info memory
```

#### 8.1.2 Storage Issues
```bash
# Symptom: Storage inconsistency
# Check: Transaction logs, rollback triggers
# Solution: Manual cleanup scripts, data integrity validation

# Recovery commands:
python scripts/validate_storage_integrity.py
python scripts/cleanup_orphaned_vectors.py
```

#### 8.1.3 Memory Issues
```bash
# Symptom: Out of memory errors
# Check: Document sizes, batch processing parameters
# Solution: Reduce batch sizes, implement streaming processing

# Memory monitoring:
free -h
ps aux --sort=-%mem | head
docker system df
```

### 8.2 Emergency Procedures

#### 8.2.1 System Recovery
```bash
# 1. Stop all workers
kubectl scale deployment ingestion-workers --replicas=0

# 2. Clear processing queues
redis-cli FLUSHDB

# 3. Validate data integrity
python scripts/data_integrity_check.py

# 4. Restart with reduced capacity
kubectl scale deployment ingestion-workers --replicas=2
```

#### 8.2.2 Data Recovery
```bash
# 1. Identify failed documents
SELECT * FROM processing_jobs WHERE status = 'failed';

# 2. Reprocess failed documents
python scripts/reprocess_failed_documents.py --job-ids 1,2,3

# 3. Verify completion
python scripts/verify_processing_completion.py
```

---

## 9. SUCCESS CRITERIA & VALIDATION

### 9.1 Functional Requirements Validation
- [ ] **Chunking Accuracy**: 95%+ chunks trong range 500-1000 tokens
- [ ] **Embedding Generation**: 99%+ success rate với retry
- [ ] **Dual Storage**: 100% consistency giữa Vector DB và PostgreSQL
- [ ] **Batch Processing**: Xử lý được 100+ documents đồng thời
- [ ] **Real-time Processing**: Response time < 60 seconds cho document < 5MB

### 9.2 Performance Benchmarks
- [ ] **Throughput**: > 10 documents/minute với 4 workers
- [ ] **Memory Usage**: < 4GB peak memory với batch size 50
- [ ] **Storage Efficiency**: < 10% overhead cho metadata storage
- [ ] **API Response**: < 5 seconds cho status queries
- [ ] **Error Rate**: < 1% error rate under normal load

### 9.3 Reliability Metrics
- [ ] **Uptime**: > 99.5% availability during business hours
- [ ] **Data Integrity**: 100% consistency between vector và metadata
- [ ] **Recovery Time**: < 5 minutes recovery từ system failure
- [ ] **Error Handling**: 95%+ successful retry rate
- [ ] **Monitoring Coverage**: 100% critical paths monitored

### 9.4 Acceptance Testing
```python
# Final acceptance test suite
def test_end_to_end_processing():
    # Upload 50 diverse documents
    # Verify all processing stages
    # Validate storage consistency
    # Check performance metrics
    # Test error scenarios
    pass

def test_production_readiness():
    # Load testing
    # Security scanning
    # Disaster recovery
    # Monitoring validation
    pass
```

---

## 10. NEXT STEPS & INTEGRATION

### 10.1 Integration với Other Modules
- **FR-03.1**: Receive validated documents từ Raw-to-Clean converter
- **FR-03.2**: Interface với Quality Control system
- **FR-04**: Provide processed data cho RAG Core Engine
- **System Monitoring**: Integration với central monitoring system

### 10.2 Future Enhancements

#### 10.2.1 Phase 2 Features (Month 4-6)
```python
FUTURE_FEATURES = {
    "advanced_chunking": {
        "semantic_chunking": "Chunk based on semantic boundaries",
        "topic_modeling": "Group related chunks by topic",
        "hierarchical_chunking": "Multi-level chunk relationships"
    },
    "embedding_optimization": {
        "fine_tuned_models": "Domain-specific embeddings",
        "multi_modal_support": "Images + text embeddings",
        "embedding_compression": "Reduce storage footprint"
    },
    "processing_intelligence": {
        "auto_quality_scoring": "ML-based quality assessment",
        "duplicate_detection": "Advanced similarity detection",
        "content_categorization": "Auto-tagging by content type"
    }
}
```

#### 10.2.2 Scalability Improvements
- **Horizontal Scaling**: Auto-scaling workers based on queue depth
- **Geographic Distribution**: Multi-region processing nodes
- **Storage Optimization**: Tiered storage cho hot/warm/cold data
- **Edge Processing**: Local processing nodes cho remote offices

#### 10.2.3 Advanced Monitoring
- **Predictive Analytics**: Forecast processing bottlenecks
- **Cost Optimization**: Track và optimize API usage costs
- **User Behavior Analysis**: Processing pattern insights
- **Anomaly Detection**: Automatic detection of unusual patterns

### 10.3 Integration Roadmap

#### 10.3.1 Module Dependencies
```mermaid
graph TB
    subgraph "Pre-requisites (Must Complete First)"
        FR031[FR-03.1<br/>Raw-to-Clean<br/>Data Tools]
        FR032[FR-03.2<br/>Quality Control<br/>System]
        DB[Database Schema<br/>Design & Setup]
        Auth[Authentication<br/>Service]
    end
    
    subgraph "Current Module"
        FR033[FR-03.3<br/>Data Ingestion<br/>Pipeline]
    end
    
    subgraph "Downstream Consumers"
        FR041[FR-04.1<br/>Document Retrieval<br/>System]
        FR042[FR-04.2<br/>Context Builder<br/>Service]
        Analytics[Analytics<br/>Dashboard]
        Search[Search<br/>Interface]
    end
    
    FR031 --> FR033
    FR032 --> FR033
    DB --> FR033
    Auth --> FR033
    
    FR033 --> FR041
    FR033 --> FR042
    FR033 --> Analytics
    FR033 --> Search
    
    classDef prereq fill:#ffebee,stroke:#c62828,stroke-width:2px
    classDef current fill:#e8f5e8,stroke:#388e3c,stroke-width:3px
    classDef consumer fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    
    class FR031,FR032,DB,Auth prereq
    class FR033 current
    class FR041,FR042,Analytics,Search consumer
```

#### 10.3.2 API Integration Points
```python
# Input APIs (upstream)
UPSTREAM_APIS = {
    "quality_control": {
        "endpoint": "/api/v1/quality/approved-documents",
        "method": "GET",
        "description": "Fetch documents ready for processing"
    },
    "document_upload": {
        "endpoint": "/api/v1/documents/upload",
        "method": "POST", 
        "description": "Receive new documents for processing"
    }
}

# Output APIs (downstream)
DOWNSTREAM_APIS = {
    "retrieval_service": {
        "endpoint": "/api/v1/ingestion/status",
        "method": "GET",
        "description": "Check if document is ready for retrieval"
    },
    "search_indexing": {
        "webhook": "/api/v1/search/reindex",
        "trigger": "on_document_processed",
        "description": "Notify search service of new content"
    }
}
```

---

## 11. RISK MANAGEMENT & MITIGATION

### 11.1 Technical Risks

#### 11.1.1 High Severity Risks
| Risk | Probability | Impact | Mitigation Strategy |
|------|-------------|--------|-------------------|
| **Embedding API Rate Limits** | High | High | Implement multiple providers, local model fallback, intelligent retry |
| **Vector DB Performance Degradation** | Medium | High | Connection pooling, read replicas, query optimization |
| **Memory Exhaustion** | Medium | High | Streaming processing, memory monitoring, auto-scaling |
| **Data Corruption** | Low | Critical | ACID transactions, data validation, automated backups |

#### 11.1.2 Mitigation Implementation
```python
# Rate limiting mitigation
RATE_LIMIT_STRATEGY = {
    "primary_provider": "openai",
    "fallback_providers": ["huggingface", "local_model"],
    "rate_limit_detection": "automatic",
    "backoff_strategy": "exponential_jitter",
    "circuit_breaker": {
        "failure_threshold": 5,
        "recovery_timeout": 60,
        "half_open_max_calls": 3
    }
}

# Memory management
MEMORY_MANAGEMENT = {
    "max_memory_usage": "80%",
    "gc_trigger_threshold": "70%",
    "streaming_threshold": "50MB",
    "batch_size_adjustment": "dynamic"
}
```

### 11.2 Operational Risks

#### 11.2.1 Business Continuity
```python
CONTINUITY_PLAN = {
    "disaster_scenarios": {
        "database_failure": {
            "rpo": "15 minutes",  # Recovery Point Objective
            "rto": "1 hour",      # Recovery Time Objective
            "backup_frequency": "every 15 minutes",
            "failover_process": "automatic"
        },
        "api_provider_outage": {
            "detection_time": "< 30 seconds",
            "fallback_activation": "< 5 minutes", 
            "manual_intervention": "if all providers down"
        },
        "infrastructure_failure": {
            "k8s_auto_recovery": "automatic",
            "cross_region_failover": "manual",
            "data_replication": "synchronous"
        }
    }
}
```

### 11.3 Security Risks
```python
SECURITY_MEASURES = {
    "data_protection": {
        "encryption_at_rest": "AES-256",
        "encryption_in_transit": "TLS 1.3",
        "key_management": "HashiCorp Vault",
        "data_masking": "PII detection"
    },
    "access_control": {
        "authentication": "JWT + mTLS",
        "authorization": "RBAC",
        "audit_logging": "comprehensive",
        "session_management": "secure"
    },
    "vulnerability_management": {
        "dependency_scanning": "daily",
        "container_scanning": "on build",
        "penetration_testing": "quarterly",
        "security_reviews": "monthly"
    }
}
```

---

## 12. MAINTENANCE & SUPPORT

### 12.1 Operational Procedures

#### 12.1.1 Daily Operations
```bash
#!/bin/bash
# daily_health_check.sh

echo "=== Daily Data Ingestion Health Check ==="

# Check system resources
echo "1. System Resources:"
kubectl top nodes
kubectl top pods -n ingestion

# Check queue depths
echo "2. Queue Status:"
redis-cli llen ingestion:queue
redis-cli llen ingestion:failed

# Check processing statistics
echo "3. Processing Stats (Last 24h):"
psql -c "SELECT 
    COUNT(*) as total_processed,
    AVG(processing_time_seconds) as avg_processing_time,
    COUNT(*) FILTER (WHERE status = 'failed') as failed_count
FROM processing_jobs 
WHERE created_at > NOW() - INTERVAL '24 hours';"

# Check storage utilization
echo "4. Storage Utilization:"
df -h /data
psql -c "SELECT pg_size_pretty(pg_database_size('knowledge_assistant'));"

# Alert if issues found
if [ $failed_count -gt 10 ]; then
    echo "ALERT: High failure rate detected!"
    # Send notification to ops team
fi
```

#### 12.1.2 Weekly Maintenance
```bash
#!/bin/bash
# weekly_maintenance.sh

echo "=== Weekly Maintenance Tasks ==="

# Database maintenance
echo "1. Database Optimization:"
psql -c "VACUUM ANALYZE;"
psql -c "REINDEX DATABASE knowledge_assistant;"

# Vector DB optimization
echo "2. Vector Database Maintenance:"
python scripts/optimize_vector_indexes.py
python scripts/cleanup_orphaned_embeddings.py

# Log rotation và cleanup
echo "3. Log Cleanup:"
find /var/log/ingestion -name "*.log" -mtime +7 -delete
docker system prune -f

# Performance analysis
echo "4. Performance Analysis:"
python scripts/generate_performance_report.py --period week

# Security scan
echo "5. Security Scan:"
docker run --rm -v $(pwd):/app security-scanner:latest
```

### 12.2 Troubleshooting Runbook

#### 12.2.1 Common Issues Diagnostic Tree
```mermaid
graph TB
    Issue[🚨 Processing Issue Reported]
    
    CheckQueue{📋 Check Queue Status}
    CheckWorkers{👷 Check Worker Status}
    CheckDB{🗄️ Check Database Status}
    CheckAPI{🌐 Check External APIs}
    
    QueueFull[📬 Queue Full<br/>Scale Workers]
    QueueEmpty[📭 Queue Empty<br/>Check Upstream]
    
    WorkersDown[❌ Workers Down<br/>Restart Services]
    WorkersOverloaded[⚡ Workers Overloaded<br/>Scale Up]
    
    DBConnectionError[🔌 DB Connection Error<br/>Check Network/Credentials]
    DBPerformanceIssue[🐌 DB Performance Issue<br/>Check Locks/Indexes]
    
    APIRateLimit[⏱️ API Rate Limit<br/>Switch Provider/Wait]
    APIDown[❌ API Down<br/>Use Fallback]
    
    Issue --> CheckQueue
    Issue --> CheckWorkers  
    Issue --> CheckDB
    Issue --> CheckAPI
    
    CheckQueue -->|High| QueueFull
    CheckQueue -->|Empty| QueueEmpty
    
    CheckWorkers -->|Not Running| WorkersDown
    CheckWorkers -->|High CPU/Memory| WorkersOverloaded
    
    CheckDB -->|Connection Failed| DBConnectionError
    CheckDB -->|Slow Queries| DBPerformanceIssue
    
    CheckAPI -->|429 Error| APIRateLimit
    CheckAPI -->|5xx Error| APIDown
    
    classDef issue fill:#ffebee,stroke:#c62828,stroke-width:2px
    classDef check fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef solution fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    
    class Issue issue
    class CheckQueue,CheckWorkers,CheckDB,CheckAPI check
    class QueueFull,QueueEmpty,WorkersDown,WorkersOverloaded,DBConnectionError,DBPerformanceIssue,APIRateLimit,APIDown solution
```

#### 12.2.2 Emergency Response Procedures
```python
EMERGENCY_PROCEDURES = {
    "system_overload": {
        "detection": "CPU > 90% for 5+ minutes",
        "immediate_action": [
            "Scale down batch sizes",
            "Pause new document ingestion",
            "Clear non-critical queues"
        ],
        "escalation_time": "15 minutes",
        "escalation_action": "Add worker nodes"
    },
    
    "data_corruption_detected": {
        "detection": "Integrity check failures",
        "immediate_action": [
            "Stop all processing",
            "Isolate affected data",
            "Begin rollback procedure"
        ],
        "escalation_time": "5 minutes",
        "escalation_action": "Activate disaster recovery"
    },
    
    "external_api_outage": {
        "detection": "API error rate > 50%",
        "immediate_action": [
            "Switch to fallback provider",
            "Queue requests for retry",
            "Notify stakeholders"
        ],
        "escalation_time": "30 minutes",
        "escalation_action": "Activate local processing"
    }
}
```

### 12.3 Performance Optimization

#### 12.3.1 Continuous Improvement Process
```python
OPTIMIZATION_CYCLE = {
    "monitoring_phase": {
        "duration": "1 week",
        "metrics_collection": [
            "processing_throughput",
            "resource_utilization", 
            "error_rates",
            "user_satisfaction"
        ]
    },
    
    "analysis_phase": {
        "duration": "2 days",
        "activities": [
            "bottleneck_identification",
            "performance_trending",
            "cost_analysis",
            "improvement_opportunities"
        ]
    },
    
    "optimization_phase": {
        "duration": "3 days",
        "activities": [
            "parameter_tuning",
            "algorithm_optimization",
            "resource_reallocation",
            "configuration_updates"
        ]
    },
    
    "validation_phase": {
        "duration": "2 days",
        "activities": [
            "a_b_testing",
            "performance_validation",
            "regression_testing",
            "rollback_if_needed"
        ]
    }
}
```

#### 12.3.2 Performance Tuning Parameters
```python
TUNING_PARAMETERS = {
    "batch_processing": {
        "optimal_batch_size": "dynamic based on document size",
        "concurrent_batches": "CPU cores * 0.8",
        "queue_prefetch": "batch_size * 2",
        "processing_timeout": "adaptive based on content"
    },
    
    "memory_optimization": {
        "chunk_buffer_size": "configurable (default: 1000 chunks)",
        "embedding_cache_size": "10% of available RAM",
        "garbage_collection": "generational with tuned thresholds",
        "memory_mapped_files": "for large document processing"
    },
    
    "database_optimization": {
        "connection_pool_size": "worker_count * 2",
        "statement_timeout": "30 seconds",
        "batch_insert_size": "1000 records",
        "index_maintenance": "automated during low-traffic hours"
    }
}
```

---

## 13. DOCUMENTATION & KNOWLEDGE TRANSFER

### 13.1 Documentation Deliverables

#### 13.1.1 Technical Documentation
```
📁 Documentation Structure:
├── 📄 API_Documentation.md
├── 📄 Database_Schema.md  
├── 📄 Deployment_Guide.md
├── 📄 Configuration_Reference.md
├── 📁 architecture/
│   ├── 📄 system_design.md
│   ├── 📄 data_flow.md
│   └── 📊 diagrams/
├── 📁 operations/
│   ├── 📄 monitoring_guide.md
│   ├── 📄 troubleshooting_guide.md
│   └── 📄 maintenance_procedures.md
└── 📁 development/
    ├── 📄 coding_standards.md
    ├── 📄 testing_guide.md
    └── 📄 contribution_guide.md
```

#### 13.1.2 User Documentation
```python
USER_GUIDES = {
    "administrator_guide": {
        "audience": "System Administrators",
        "content": [
            "System installation",
            "Configuration management", 
            "User management",
            "Monitoring setup"
        ]
    },
    
    "developer_guide": {
        "audience": "Development Team",
        "content": [
            "API integration",
            "Custom extensions",
            "Testing procedures",
            "Deployment processes"
        ]
    },
    
    "operations_guide": {
        "audience": "Operations Team", 
        "content": [
            "Daily operations",
            "Incident response",
            "Performance tuning",
            "Backup procedures"
        ]
    }
}
```

### 13.2 Knowledge Transfer Plan

#### 13.2.1 Training Schedule
```mermaid
gantt
    title Knowledge Transfer Schedule
    dateFormat  YYYY-MM-DD
    section Phase 1: Technical Overview
    System Architecture    :2025-09-01, 2d
    Database Design       :2025-09-03, 1d
    API Documentation     :2025-09-04, 1d
    
    section Phase 2: Implementation Details  
    Core Processing Logic :2025-09-05, 2d
    Error Handling       :2025-09-07, 1d
    Performance Optimization :2025-09-08, 1d
    
    section Phase 3: Operations
    Deployment Procedures :2025-09-09, 1d  
    Monitoring & Alerting :2025-09-10, 1d
    Troubleshooting      :2025-09-11, 1d
    
    section Phase 4: Hands-on Training
    Live System Demo     :2025-09-12, 1d
    Practice Scenarios   :2025-09-13, 1d
    Q&A Sessions        :2025-09-14, 1d
```

#### 13.2.2 Competency Validation
```python
COMPETENCY_CHECKLIST = {
    "technical_team": {
        "system_understanding": [
            "Can explain overall architecture",
            "Understands data flow", 
            "Knows integration points"
        ],
        "implementation_skills": [
            "Can modify processing parameters",
            "Can add new embedding providers",
            "Can troubleshoot common issues"
        ],
        "operational_skills": [
            "Can deploy system updates",
            "Can monitor system health",
            "Can respond to incidents"
        ]
    },
    
    "operations_team": {
        "monitoring_skills": [
            "Can read system dashboards",
            "Can identify performance issues",
            "Can escalate appropriately"
        ],
        "maintenance_skills": [
            "Can perform routine maintenance",
            "Can backup/restore data", 
            "Can update configurations"
        ]
    }
}
```

---

## 14. FINAL VALIDATION & GO-LIVE

### 14.1 Pre-Production Checklist

#### 14.1.1 System Readiness
```bash
# System Readiness Validation Script
#!/bin/bash

echo "=== Data Ingestion Pipeline - Go-Live Readiness Check ==="

# Check 1: All dependencies installed
echo "1. Checking dependencies..."
python -c "import src.core.pipeline; print('✅ Core pipeline imported')"
python -c "import chromadb; print('✅ Vector DB available')" 
python -c "import psycopg2; print('✅ PostgreSQL client ready')"

# Check 2: Database connectivity
echo "2. Checking database connections..."
psql $DATABASE_URL -c "SELECT 1;" && echo "✅ PostgreSQL connected"
redis-cli ping && echo "✅ Redis connected"

# Check 3: External API connectivity  
echo "3. Checking external APIs..."
python scripts/test_embedding_apis.py && echo "✅ Embedding APIs working"

# Check 4: Processing pipeline test
echo "4. Testing end-to-end pipeline..."
python scripts/integration_test.py --quick && echo "✅ Pipeline functional"

# Check 5: Performance benchmarks
echo "5. Running performance tests..."
python scripts/performance_test.py --duration 300 && echo "✅ Performance acceptable"

# Check 6: Security validation
echo "6. Security validation..."
python scripts/security_test.py && echo "✅ Security checks passed"

echo "=== Pre-Production Checklist Complete ==="
```

#### 14.1.2 Data Quality Validation
```python
DATA_QUALITY_GATES = {
    "processing_accuracy": {
        "metric": "successful_processing_rate",
        "threshold": ">= 99%",
        "test_dataset": "100 diverse documents"
    },
    
    "chunking_quality": {
        "metric": "chunks_in_token_range",
        "threshold": ">= 95%",
        "range": "500-1000 tokens"
    },
    
    "embedding_consistency": {
        "metric": "same_input_same_output",
        "threshold": "100%",
        "test_cases": "50 duplicate documents"
    },
    
    "storage_integrity": {
        "metric": "vector_metadata_consistency", 
        "threshold": "100%",
        "validation": "cross_reference_check"
    }
}
```

### 14.2 Production Deployment

#### 14.2.1 Deployment Strategy
```yaml
# Blue-Green Deployment Configuration
deployment_strategy:
  type: blue_green
  
  blue_environment:
    namespace: ingestion-prod
    replicas: 4
    resources:
      cpu: "2000m"
      memory: "4Gi"
  
  green_environment:
    namespace: ingestion-staging
    replicas: 2
    resources:
      cpu: "1000m"
      memory: "2Gi"
      
  deployment_steps:
    1. Deploy to green environment
    2. Run smoke tests
    3. Route 10% traffic to green
    4. Monitor for 30 minutes
    5. Route 100% traffic to green
    6. Decommission blue environment
    
  rollback_criteria:
    - error_rate > 1%
    - response_time > 120s
    - manual_trigger
```

#### 14.2.2 Go-Live Monitoring
```python
GO_LIVE_MONITORING = {
    "critical_metrics": {
        "processing_success_rate": {
            "threshold": "> 99%",
            "alert_delay": "5 minutes"
        },
        "response_time_p95": {
            "threshold": "< 60 seconds", 
            "alert_delay": "2 minutes"
        },
        "queue_depth": {
            "threshold": "< 1000 items",
            "alert_delay": "immediate"
        },
        "error_rate": {
            "threshold": "< 0.5%",
            "alert_delay": "immediate"
        }
    },
    
    "monitoring_duration": "72 hours",
    "escalation_path": [
        "Development Team (0-2 hours)",
        "Technical Lead (2-8 hours)", 
        "Engineering Manager (8-24 hours)",
        "CTO (24+ hours)"
    ]
}
```

### 14.3 Success Criteria & Sign-off

#### 14.3.1 Business Acceptance Criteria
```python
BUSINESS_ACCEPTANCE = {
    "functional_requirements": {
        "document_processing": "✅ All supported formats processed",
        "chunking_accuracy": "✅ 95%+ chunks in target range",
        "embedding_generation": "✅ All chunks have valid embeddings",
        "storage_reliability": "✅ 100% data consistency",
        "batch_processing": "✅ Handles 100+ concurrent documents",
        "real_time_processing": "✅ <60s processing for <5MB files"
    },
    
    "performance_requirements": {
        "throughput": "✅ >10 documents/minute",
        "availability": "✅ >99.5% uptime",
        "scalability": "✅ Scales to 4x current load", 
        "recovery_time": "✅ <5 minutes from failure"
    },
    
    "operational_requirements": {
        "monitoring": "✅ Comprehensive monitoring deployed",
        "alerting": "✅ All critical alerts configured",
        "documentation": "✅ Complete documentation delivered",
        "training": "✅ Operations team trained"
    }
}
```

#### 14.3.2 Final Sign-off
```
📋 SIGN-OFF CHECKLIST:

□ Technical Lead Approval
  - Code review completed
  - Architecture validated  
  - Performance benchmarks met
  - Security review passed

□ Operations Team Approval
  - Monitoring configured
  - Runbooks validated
  - Training completed
  - Support procedures in place

□ Business Stakeholder Approval
  - Functional requirements met
  - User acceptance testing passed
  - Documentation approved
  - Go-live plan agreed

□ Project Manager Approval
  - All deliverables completed
  - Risk mitigation in place
  - Support transition plan executed
  - Post-go-live monitoring plan active

Signatures:
_________________  Date: _______  Technical Lead
_________________  Date: _______  Operations Manager  
_________________  Date: _______  Business Owner
_________________  Date: _______  Project Manager
```

---

**🎉 MODULE FR-03.3 DATA INGESTION PIPELINE - IMPLEMENTATION GUIDE COMPLETE**

Tài liệu này cung cấp roadmap đầy đủ để team kỹ thuật implement Data Ingestion Pipeline theo đúng yêu cầu, từ setup môi trường đến deployment production. Mọi bước đều có validation criteria rõ ràng và fallback procedures khi gặp vấn đề.

**Next Actions:**
1. Review và approve tài liệu này với technical team
2. Setup development environment theo Section 2
3. Begin implementation theo Phase 1 trong Section 4
4. Schedule weekly progress reviews
5. Plan integration testing với upstream/downstream modules
