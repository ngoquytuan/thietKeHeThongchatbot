FR-01.1: ĐÁNH GIÁ VÀ LỰA CHỌN EMBEDDING MODELS TIẾNG VIỆT
Dựa trên tài liệu hướng dẫn, tôi sẽ triển khai full source code cho hệ thống đánh giá và lựa chọn embedding models tối ưu cho tiếng Việt. Dưới đây là tất cả các file cần thiết:

---

### 1. Configuration Files

#### `configs/models.json`
```json
{
  "models": [
    {
      "name": "vietnamese_embedding_v1",
      "model_id": "AITeamVN/Vietnamese_Embedding",
      "provider": "huggingface",
      "max_seq_length": 512,
      "batch_size": 32,
      "normalize_embeddings": true,
      "priority": 1
    },
    {
      "name": "qwen3_embedding",
      "model_id": "Qwen/Qwen2.5-72B-Instruct", 
      "provider": "huggingface",
      "max_seq_length": 512,
      "batch_size": 16,
      "normalize_embeddings": true,
      "priority": 2
    },
    {
      "name": "paraphrase_multilingual",
      "model_id": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
      "provider": "huggingface",
      "max_seq_length": 512,
      "batch_size": 32,
      "normalize_embeddings": true,
      "priority": 3
    },
    {
      "name": "multilingual_e5_base",
      "model_id": "intfloat/multilingual-e5-base",
      "provider": "huggingface",
      "max_seq_length": 512,
      "batch_size": 16,
      "normalize_embeddings": true,
      "priority": 4
    },
    {
      "name": "bge_m3",
      "model_id": "BAAI/bge-m3",
      "provider": "huggingface",
      "max_seq_length": 512,
      "batch_size": 16,
      "normalize_embeddings": true,
      "priority": 5
    },
    {
      "name": "vietnamese_sbert",
      "model_id": "keepitreal/vietnamese-sbert",
      "provider": "huggingface",
      "max_seq_length": 512,
      "batch_size": 16,
      "normalize_embeddings": true,
      "priority": 6
    }
  ],
  "evaluation_settings": {
    "top_k": [1, 3, 5, 10],
    "similarity_threshold": 0.7,
    "batch_processing": true
  }
}
```

#### `configs/evaluation_settings.json`
```json
{
  "evaluation_parameters": {
    "k_values": [1, 3, 5, 10],
    "similarity_metrics": ["cosine", "euclidean"],
    "normalization": true,
    "cross_validation_folds": 5,
    "test_split": 0.2,
    "random_seed": 42
  },
  "benchmarking": {
    "warmup_runs": 3,
    "measurement_runs": 10,
    "generate_embeddings": true,
    "measure_search_latency": true
  },
  "output_settings": {
    "save_embeddings_cache": true,
    "save_detailed_results": true,
    "generate_visualization": true,
    "output_formats": ["json", "html", "markdown"]
  }
}
```

#### `configs/gpu_settings.json`
```json
{
  "gpu_settings": {
    "enabled": true,
    "device_id": "auto",
    "memory_limit_gb": 8,
    "mixed_precision": true,
    "optimize_memory": true,
    "enable_cuda_kernels": true,
    "tensor_parallel_size": 1
  },
  "batch_settings": {
    "auto_adjust_batch_size": true,
    "max_batch_size": 128,
    "min_batch_size": 4,
    "memory_utilization_threshold": 0.8,
    "out_of_memory_retries": 3
  },
  "monitoring": {
    "trace_gpu_memory": true,
    "track_utilization": true,
    "log_performance": true,
    "alert_threshold_memory": 0.9,
    "alert_threshold_temperature": 85
  }
}
```

---

### 2. Core Implementation

#### `src/__init__.py`
```python
"""
Vietnamese Embedding Model Evaluation Framework
"""

__version__ = "1.0.0"
__author__ = "AI Team VN"
__email__ = "ai@teamvn.com"
```

#### `src/data_processor.py`
```python
"""
Vietnamese Text Processing Module
Handles text preprocessing, cleaning, and chunking for Vietnamese documents
"""

import os
import json
import re
import unicodedata2
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
from tqdm import tqdm
import pandas as pd
import regex  # Advanced regex patterns
from pyvi import ViTokenizer, ViPosTagger  # Vietnamese-specific tokenization


class VietnameseTextProcessor:
    """
    A specialized processor for Vietnamese text documents, including:
    - Text cleaning and normalization
    - Diacritics handling
    - Tokenization using PyVi
    - Document chunking
    """

    def __init__(self, config: Optional[Dict] = None):
        """
        Initialize the Vietnamese text processor
        
        Args:
            config: Configuration dictionary with processing parameters
        """
        self.config = config or {}
        self.tokenizer = ViTokenizer()
        self.pos_tagger = ViPosTagger()
        
        # Default chunk settings
        self.chunk_size = self.config.get("chunk_size", 512)
        self.chunk_overlap = self.config.get("chunk_overlap", 50)
        
        # Define Vietnamese-specific patterns
        self.vietnamese_chars = [
            'a', 'á', 'à', 'ả', 'ã', 'ạ', 'ă', 'ắ', 'ằ', 'ẳ', 'ẵ', 'ặ', 'â', 'ấ', 'ầ', 'ẩ', 'ẫ', 'ậ',
            'e', 'é', 'è', 'ẻ', 'ẽ', 'ẹ', 'ê', 'ế', 'ề', 'ể', 'ễ', 'ệ',
            'i', 'í', 'ì', 'ỉ', 'ĩ', 'ị',
            'o', 'ó', 'ò', 'ỏ', 'õ', 'ọ', 'ô', 'ố', 'ồ', 'ổ', 'ỗ', 'ộ', 'ơ', 'ớ', 'ờ', 'ở', 'ỡ', 'ợ',
            'u', 'ú', 'ù', 'ủ', 'ũ', 'ụ', 'ư', 'ứ', 'ừ', 'ử', 'ữ', 'ự',
            'y', 'ý', 'ỳ', 'ỷ', 'ỹ', 'ỵ',
            'đ', 'Đ',
            'A', 'Á', 'À', 'Ả', 'Ã', 'Ạ', 'Ă', 'Ắ', 'Ằ', 'Ẳ', 'Ẵ', 'Ặ', 'Â', 'Ấ', 'Ầ', 'Ẩ', 'Ẫ', 'Ậ',
            'E', 'É', 'È', 'Ẻ', 'Ẽ', 'Ẹ', 'Ê', 'Ế', 'Ề', 'Ể', 'Ễ', 'Ệ',
            'I', 'Í', 'Ì', 'Ỉ', 'Ĩ', 'Ị',
            'O', 'Ó', 'Ò', 'Ỏ', 'Õ', 'Ọ', 'Ô', 'Ố', 'Ồ', 'Ổ', 'Ỗ', 'Ộ', 'Ơ', 'Ớ', 'Ờ', 'Ở', 'Ỡ', 'Ƣ',
            'U', 'Ú', 'Ù', 'Ủ', 'Ũ', 'Ụ', 'Ư', 'Ứ', 'Ừ', 'Ử', 'Ữ', 'Ự',
            'Y', 'Ý', 'Ỳ', 'Ỷ', 'Ỹ', 'Ỵ',
            'Đ'
        ]
        
        # Initialize regex patterns
        self._compile_patterns()

    def _compile_patterns(self):
        """Compile frequently used regex patterns for Vietnamese text processing"""
        # Vietnamese-specific punctuation and special characters
        self.vietnamese_punctuation_pattern = r'[!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~]'
        
        # Vietnamese-specific phone number pattern
        self.vietnamese_phone_pattern = r'(\+84|0)[1-9][0-9]{8,9}'
        
        # Vietnamese-specific date patterns
        self.vietnamese_date_patterns = [
            r'\d{1,2}/\d{1,2}/\d{4}',  # dd/mm/yyyy
            r'\d{1,2}-\d{1,2}-\d{4}',  # dd-mm-yyyy
            r'\d{4}/\d{1,2}/\d{1,2}',  # yyyy/mm/dd
            r'\d{4}-\d{1,2}-\d{1,2}'   # yyyy-mm-dd
        ]
        
        # Vietnamese-specific patterns to normalize
        self.whitespace_pattern = r'\s+'
        self.newline_pattern = r'[\r\n]+'
        self.url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
        self.email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}'
        
        # Vietnamese-specific text patterns
        self.english_in_vietnamese = r'[a-zA-Z]{3,}'  # Matches English words longer than 3 chars
        self.sentence_endings = r'[.!?]'  # Standard sentence endings
        self.vietnamese_sentence_endings = r'[.!?।]'  # Includes additional Vietnamese-specific ending

    def normalize_unicode(self, text: str) -> str:
        """
        Normalize Vietnamese Unicode text using NFC normalization
        
        Args:
            text: Input text to normalize
            
        Returns:
            Normalized text
        """
        return unicodedata2.normalize('NFC', text)

    def clean_text(self, text: str) -> str:
        """
        Clean and preprocess Vietnamese text
        
        Args:
            text: Raw Vietnamese text
            
        Returns:
            Cleaned text
        """
        if not text:
            return ""
        
        # Normalize Unicode
        text = self.normalize_unicode(text)
        
        # Remove URLs
        text = re.sub(self.url_pattern, ' ', text)
        
        # Remove email addresses
        text = re.sub(self.email_pattern, ' ', text)
        
        # Remove phone numbers
        text = re.sub(self.vietnamese_phone_pattern, ' ', text)
        
        # Remove dates
        for pattern in self.vietnamese_date_patterns:
            text = re.sub(pattern, ' ', text)
        
        # Handle Vietnamese-specific punctuation
        text = regex.sub(self.vietnamese_punctuation_pattern, ' ', text)
        
        # Replace multiple whitespace with a single space
        text = re.sub(self.whitespace_pattern, ' ', text)
        
        # Replace newlines with spaces
        text = re.sub(self.newline_pattern, ' ', text)
        
        # Strip leading/trailing whitespace
        text = text.strip()
        
        return text

    def tokenize(self, text: str) -> List[str]:
        """
        Tokenize Vietnamese text using PyVi
        
        Args:
            text: Vietnamese text to tokenize
            
        Returns:
            List of tokens
        """
        if not text:
            return []
        
        # Use PyVi for Vietnamese-specific tokenization
        tokens = self.tokenizer.tokenize(text)
        
        # Split into individual words
        word_tokens = tokens.split()
        
        return word_tokens

    def create_chunks(self, text: str, chunk_size: int = None, chunk_overlap: int = None) -> List[str]:
        """
        Create chunks of text with intelligent boundary detection for Vietnamese
        
        Args:
            text: Document text to split
            chunk_size: Maximum size of each chunk in characters
            chunk_overlap: Number of overlapping characters between chunks
            
        Returns:
            List of text chunks
        """
        if not text:
            return []
        
        if chunk_size is None:
            chunk_size = self.chunk_size
        if chunk_overlap is None:
            chunk_overlap = self.chunk_overlap
        
        # Clean the text first
        text = self.clean_text(text)
        
        # Initialize chunks list
        chunks = []
        
        # Split into sentences using Vietnamese punctuation
        sentences = re.split(self.vietnamese_sentence_endings, text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        current_chunk = ""
        current_chunk_size = 0
        
        for sentence in sentences:
            # If adding the sentence exceeds the chunk size
            if current_chunk_size + len(sentence) > chunk_size and current_chunk:
                # Add current chunk to list
                chunks.append(current_chunk)
                
                # Start new chunk with overlap
                if chunk_overlap > 0:
                    # Split the current chunk into words for overlap handling
                    words = current_chunk.split()
                    overlap_size = 0
                    overlap_words = []
                    
                    # Add words from the end until we reach the desired overlap
                    for word in reversed(words):
                        if overlap_size + len(word) + 1 <= chunk_overlap:
                            overlap_words.insert(0, word)
                            overlap_size += len(word) + 1
                        else:
                            break
                    
                    current_chunk = " ".join(overlap_words)
                    current_chunk_size = len(current_chunk)
                else:
                    current_chunk = ""
                    current_chunk_size = 0
            
            # Add sentence to current chunk
            if current_chunk:
                current_chunk += ". " + sentence
            else:
                current_chunk = sentence
            current_chunk_size += len(sentence) + 2  # +2 for ". "
        
        # Add the last chunk if it's not empty
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks

    def process_documents(self, documents: List[Dict]) -> List[Dict]:
        """
        Process a list of Vietnamese documents
        
        Args:
            documents: List of dictionaries containing document data
            
        Returns:
            List of processed documents with chunks
        """
        processed_docs = []
        
        for doc in tqdm(documents, desc="Processing documents"):
            # Extract text content
            content = doc.get("content", "")
            metadata = doc.get("metadata", {})
            
            # Clean and tokenize content
            cleaned_content = self.clean_text(content)
            tokens = self.tokenize(cleaned_content)
            
            # Create chunks
            chunks = self.create_chunks(cleaned_content)
            
            # Create processed document
            processed_doc = {
                "id": doc.get("id"),
                "title": doc.get("title", ""),
                "metadata": metadata,
                "content": cleaned_content,
                "tokens": tokens,
                "chunks": [
                    {
                        "id": f"{doc.get('id', '')}_chunk_{i}",
                        "text": chunk,
                        "tokens": self.tokenize(chunk)
                    }
                    for i, chunk in enumerate(chunks)
                ]
            }
            
            processed_docs.append(processed_doc)
        
        return processed_docs

    def prepare_evaluation_dataset(self, raw_data_path: str, output_path: str) -> None:
        """
        Prepare evaluation dataset from raw documents
        
        Args:
            raw_data_path: Path to directory containing raw documents
            output_path: Path to save processed dataset
        """
        # Create output directory if it doesn't exist
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # Load raw documents
        documents = []
        raw_data_dir = Path(raw_data_path)
        
        # Process all JSON files in the directory
        for json_file in raw_data_dir.glob("*.json"):
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if isinstance(data, list):
                    documents.extend(data)
                else:
                    documents.append(data)
        
        # Process documents
        processed_docs = self.process_documents(documents)
        
        # Save processed documents
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(processed_docs, f, ensure_ascii=False, indent=2)
        
        print(f"Processed {len(processed_docs)} documents and saved to {output_path}")

    def load_processed_documents(self, processed_data_path: str) -> List[Dict]:
        """
        Load previously processed documents
        
        Args:
            processed_data_path: Path to processed documents JSON file
            
        Returns:
            List of processed documents
        """
        with open(processed_data_path, 'r', encoding='utf-8') as f:
            return json.load(f)
```

#### `src/embedding_manager.py`
```python
"""
Embedding Manager Module
Handles loading, managing, and evaluating different embedding models for Vietnamese
"""

import os
import json
import time
import traceback
import logging
from typing import List, Dict, Any, Optional, Union, Tuple
from pathlib import Path
from tqdm import tqdm
import numpy as np
import torch
import psutil
from sentence_transformers import SentenceTransformer
from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification
from src.gpu_optimizer import GPUOptimizer

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
)
logger = logging.getLogger(__name__)


class EmbeddingManager:
    """
    Manages loading and running embedding models for Vietnamese text evaluation
    """

    def __init__(self, config_path: str, gpu_config_path: str):
        """
        Initialize the embedding manager
        
        Args:
            config_path: Path to models configuration file
            gpu_config_path: Path to GPU configuration file
        """
        # Load configurations
        self.model_configs = self._load_config(config_path)
        self.gpu_configs = self._load_config(gpu_config_path)
        
        # Initialize GPU optimizer
        self.gpu_optimizer = GPUOptimizer(self.gpu_configs)
        
        # Initialize models dictionary
        self.models = {}
        self.tokenizers = {}
        self.model_info = {}
        
        # Setup device
        self.device = self.gpu_optimizer.device
        
        # Initialize cache directory
        self.cache_dir = "cache/embeddings"
        os.makedirs(self.cache_dir, exist_ok=True)
        
        logger.info(f"Initialized EmbeddingManager with device: {self.device}")

    def _load_config(self, config_path: str) -> Dict:
        """
        Load configuration from file
        
        Args:
            config_path: Path to configuration file
            
        Returns:
            Configuration dictionary
        """
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"Error loading config from {config_path}: {str(e)}")
            raise

    def _load_huggingface_model(self, model_config: Dict) -> Tuple[Any, Dict]:
        """
        Load a HuggingFace model
        
        Args:
            model_config: Model configuration dictionary
            
        Returns:
            Tuple of (model, model_info)
        """
        model_id = model_config["model_id"]
        model_name = model_config["name"]
        
        logger.info(f"Loading model {model_name} from HuggingFace: {model_id}")
        
        try:
            # Record start time for performance measurement
            start_time = time.time()
            
            # Setup model loading kwargs
            model_kwargs = {
                "device": self.device,
            }
            
            if self.gpu_configs["gpu_settings"]["mixed_precision"]:
                model_kwargs["torch_dtype"] = torch.float16
            
            # Load model and tokenizer
            if sentence_transformers.SentenceTransformer._is_sentence_transformer_model(model_id):
                # Load as SentenceTransformer
                model = SentenceTransformer(
                    model_id,
                    device=model_kwargs["device"],
                    use_auth_token=None,
                    cache_folder="models",
                    **model_kwargs
                )
                
                # Get tokenizer from the model
                tokenizer = model.tokenizer
                
                # Get sequence length from config
                max_seq_length = model_config.get("max_seq_length", 512)
                if hasattr(model, 'max_seq_length'):
                    model.max_seq_length = max_seq_length
                else:
                    model._first_module().max_seq_length = max_seq_length
            else:
                # Load as standard HuggingFace model
                tokenizer = AutoTokenizer.from_pretrained(
                    model_id,
                    cache_dir="models",
                    use_auth_token=None
                )
                
                model = AutoModel.from_pretrained(
                    model_id,
                    cache_dir="models",
                    use_auth_token=None,
                    **model_kwargs
                )
                
                # Set model-specific configurations
                max_seq_length = model_config.get("max_seq_length", 512)
                if hasattr(tokenizer, 'model_max_length'):
                    tokenizer.model_max_length = max_seq_length
            
            # Calculate loading time
            loading_time = time.time() - start_time
            
            # Collect model information
            model_info = {
                "name": model_name,
                "model_id": model_id,
                "provider": "huggingface",
                "max_seq_length": max_seq_length,
                "loading_time": loading_time,
                "config": model_config,
                "loaded_at": time.time(),
                "memory_usage": self.gpu_optimizer.get_model_memory_usage(model) if torch.cuda.is_available() else 0,
                "is_sentence_transformer": isinstance(model, SentenceTransformer)
            }
            
            logger.info(f"Successfully loaded model {model_name} in {loading_time:.2f}s")
            
            if torch.cuda.is_available():
                logger.info(f"Model memory usage: {model_info['memory_usage']:.2f} GB")
            
            return model, model_info
        
        except Exception as e:
            logger.error(f"Error loading model {model_name}: {str(e)}")
            logger.error(traceback.format_exc())
            raise

    def load_all_models(self) -> Dict:
        """
        Load all embedding models from configuration
        
        Returns:
            Dictionary of loaded model information
        """
        results = {}
        
        # Sort models by priority
        sorted_models = sorted(self.model_configs["models"], key=lambda x: x["priority"])
        
        for model_config in sorted_models:
            model_name = model_config["name"]
            
            try:
                # Pre-GPU optimization if enabled
                if self.gpu_configs["gpu_settings"]["enabled"]:
                    self.gpu_optimizer.pre_model_loading_hook(model_name)
                
                # Load the model
                model, model_info = self._load_huggingface_model(model_config)
                
                # Store model and info
                self.models[model_name] = model
                self.model_info[model_name] = model_info
                
                # Post-GPU optimization if enabled
                if self.gpu_configs["gpu_settings"]["enabled"]:
                    self.gpu_optimizer.post_model_loading_hook(model_name, model)
                
                # Add to results
                results[model_name] = model_info
                
                logger.info(f"Model {model_name} loaded successfully")
            
            except Exception as e:
                logger.error(f"Failed to load model {model_name}: {str(e)}")
                results[model_name] = {
                    "name": model_name,
                    "error": str(e),
                    "success": False
                }
        
        return results

    def generate_embeddings_batch(self, texts: List[str], model_name: str, use_cache: bool = True) -> np.ndarray:
        """
        Generate embeddings for a batch of texts using specified model
        
        Args:
            texts: List of texts to embed
            model_name: Name of the model to use
            use_cache: Whether to use caching for embeddings
            
        Returns:
            Numpy array of embeddings
        """
        # Check if model exists
        if model_name not in self.models:
            raise ValueError(f"Model {model_name} not loaded")
        
        model = self.models[model_name]
        model_info = self.model_info[model_name]
        batch_size = model_info["config"].get("batch_size", 32)
        normalize_embeddings = model_info["config"].get("normalize_embeddings", True)
        
        logger.info(f"Generating embeddings using {model_name} for {len(texts)} texts with batch size {batch_size}")
        
        # Check cache if enabled
        cache_key = f"{model_name}_{'_'.join([text.strip()[:20] for text in texts[:3]])}_{len(texts)}"
        cache_path = os.path.join(self.cache_dir, f"{hash(cache_key)}.npy")
        
        if use_cache and os.path.exists(cache_path):
            logger.info(f"Loading embeddings from cache: {cache_path}")
            return np.load(cache_path)
        
        # Record start time
        start_time = time.time()
        
        # Process texts in batches
        all_embeddings = []
        
        try:
            if model_info.get("is_sentence_transformer", False):
                # Using SentenceTransformer
                for i in tqdm(range(0, len(texts), batch_size), desc="Creating embeddings"):
                    batch_texts = texts[i:i + batch_size]
                    
                    # Generate embeddings
                    batch_embeddings = model.encode(
                        batch_texts,
                        batch_size=batch_size,
                        show_progress_bar=False,
                        convert_to_numpy=True,
                        normalize_embeddings=normalize_embeddings
                    )
                    
                    all_embeddings.append(batch_embeddings)
            else:
                # Using standard HuggingFace model
                tokenizer = self.tokenizers.get(model_name)
                if tokenizer is None:
                    # Need to get tokenizer from model
                    if hasattr(model, 'tokenizer'):
                        tokenizer = model.tokenizer
                    elif hasattr(model, 'get_tokenizer'):
                        tokenizer = model.get_tokenizer()
                    else:
                        # Fallback for custom models
                        from transformers import AutoTokenizer
                        tokenizer = AutoTokenizer.from_pretrained(model_info["model_id"])
                    
                    self.tokenizers[model_name] = tokenizer
                
                # Process batches
                for i in tqdm(range(0, len(texts), batch_size), desc="Creating embeddings"):
                    batch_texts = texts[i:i + batch_size]
                    
                    # Tokenize
                    encoded_inputs = tokenizer(
                        batch_texts,
                        padding=True,
                        truncation=True,
                        max_length=model_info["max_seq_length"],
                        return_tensors="pt"
                    ).to(self.device)
                    
                    # Generate embeddings
                    with torch.no_grad():
                        outputs = model(**encoded_inputs)
                    
                    # Extract embeddings (use average of last hidden state)
                    batch_embeddings = self._extract_embeddings(outputs, encoded_inputs)
                    
                    # Normalize if needed
                    if normalize_embeddings:
                        batch_embeddings = torch.nn.functional.normalize(batch_embeddings, p=2, dim=1)
                    
                    all_embeddings.append(batch_embeddings.cpu().numpy())
            
            # Combine all embeddings
            embeddings = np.vstack(all_embeddings)
            
            # Calculate processing time
            processing_time = time.time() - start_time
            tokens_per_second = len(texts) * 50 / processing_time  # Rough token estimation
            
            logger.info(f"Generated {len(embeddings)} embeddings in {processing_time:.2f}s ({tokens_per_second:.2f} tokens/sec)")
            
            # Save to cache if enabled
            if use_cache:
                np.save(cache_path, embeddings)
                logger.info(f"Saved embeddings to cache: {cache_path}")
            
            # Update model info
            self.model_info[model_name]["last_used"] = time.time()
            self.model_info[model_name]["total_embeddings"] = self.model_info[model_name].get("total_embeddings", 0) + len(texts)
            self.model_info[model_name]["total_time"] = self.model_info[model_name].get("total_time", 0) + processing_time
            
            return embeddings
        
        except Exception as e:
            logger.error(f"Error generating embeddings: {str(e)}")
            raise

    def _extract_embeddings(self, outputs, encoded_inputs):
        """
        Extract embeddings from model outputs
        
        Args:
            outputs: Model outputs
            encoded_inputs: Tokenized inputs
            
        Returns:
            Extracted embeddings
        """
        # Common embedding extraction strategies
        
        # Option 1: Use the [CLS] token representation
        if hasattr(outputs, "pooler_output"):
            return outputs.pooler_output
        
        # Option 2: Use the average of all token representations
        if hasattr(outputs, "last_hidden_state"):
            # Get attention mask to ignore padding tokens
            attention_mask = encoded_inputs.get("attention_mask")
            
            if attention_mask is not None:
                # Mask out padding tokens
                input_mask_expanded = attention_mask.unsqueeze(-1).expand(outputs.last_hidden_state.size()).float()
                sum_embeddings = torch.sum(outputs.last_hidden_state * input_mask_expanded, 1)
                sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)
                
                return sum_embeddings / sum_mask
            
        # Fallback: use mean of all token embeddings
        return torch.mean(outputs.last_hidden_state, dim=1)

    def compare_models_parallel(self, test_queries: List[str], corpus: List[str]) -> Dict[str, Dict]:
        """
        Compare multiple models on test queries and corpus
        
        Args:
            test_queries: List of test queries
            corpus: List of corpus documents
            
        Returns:
            Dictionary with results for each model
        """
        if not self.models:
            logger.error("No models loaded. Please load models first.")
            return {}
        
        results = {}
        
        for model_name, model in self.models.items():
            logger.info(f"Comparing model: {model_name}")
            
            try:
                start_time = time.time()
                
                # Generate embeddings for corpus
                corpus_embeddings = self.generate_embeddings_batch(corpus, model_name)
                
                # Generate embeddings for queries
                query_embeddings = self.generate_embeddings_batch(test_queries, model_name)
                
                # Record timing
                embedding_time = time.time() - start_time
                
                # Calculate similarity scores
                search_start = time.time()
                similarity_matrix = self._calculate_similarity_matrix(query_embeddings, corpus_embeddings)
                search_time = time.time() - search_start
                
                # Store results
                results[model_name] = {
                    "corpus_embeddings": corpus_embeddings,
                    "query_embeddings": query_embeddings,
                    "similarity_matrix": similarity_matrix,
                    "embedding_time": embedding_time,
                    "search_time": search_time,
                    "model_info": self.model_info[model_name]
                }
                
                logger.info(f"Model {model_name} comparison completed in {embedding_time + search_time:.2f}s")
                
            except Exception as e:
                logger.error(f"Error comparing model {model_name}: {str(e)}")
                results[model_name] = {
                    "error": str(e),
                    "success": False
                }
        
        return results

    def _calculate_similarity_matrix(self, query_embeddings: np.ndarray, corpus_embeddings: np.ndarray) -> np.ndarray:
        """
        Calculate cosine similarity between query and corpus embeddings
        
        Args:
            query_embeddings: Query embeddings (n_queries, embedding_dim)
            corpus_embeddings: Corpus embeddings (n_corpus, embedding_dim)
            
        Returns:
            Similarity matrix (n_queries, n_corpus)
        """
        # Normalize embeddings for cosine similarity
        query_norms = np.linalg.norm(query_embeddings, axis=1, keepdims=True)
        corpus_norms = np.linalg.norm(corpus_embeddings, axis=1, keepdims=True)
        
        # Avoid division by zero
        query_norms[query_norms == 0] = 1e-9
        corpus_norms[corpus_norms == 0] = 1e-9
        
        # Calculate normalized embeddings
        normalized_query = query_embeddings / query_norms
        normalized_corpus = corpus_embeddings / corpus_norms
        
        # Compute similarity matrix
        similarity_matrix = np.matmul(normalized_query, normalized_corpus.T)
        
        return similarity_matrix

    def benchmark_models(self, test_texts: List[str]) -> Dict:
        """
        Benchmark multiple models for speed and memory usage
        
        Args:
            test_texts: List of test texts
            
        Returns:
            Dictionary with benchmark results
        """
        results = {}
        
        # Prepare different batch sizes for testing
        batch_sizes = [1, 8, 16, 32]
        
        for model_name, model in self.models.items():
            logger.info(f"Benchmarking model: {model_name}")
            
            model_results = {
                "batch_results": {},
                "memory_usage": {},
                "model_info": self.model_info[model_name]
            }
            
            try:
                # Get initial memory usage
                initial_memory = self.gpu_optimizer.get_current_memory_usage()
                
                for batch_size in batch_sizes:
                    # Create batches of texts
                    batches = [test_texts[i:i + batch_size] for i in range(0, len(test_texts), batch_size)]
                    
                    if not batches:
                        continue
                    
                    # Benchmark each batch
                    batch_times = []
                    memory_usages = []
                    
                    for _ in range(3):  # Run multiple times for stability
                        for batch in batches:
                            start_time = time.time()
                            
                            # Generate embeddings
                            _ = self.generate_embeddings_batch(batch, model_name)
                            
                            # Record time and memory
                            end_time = time.time()
                            batch_times.append(end_time - start_time)
                            memory_usages.append(self.gpu_optimizer.get_current_memory_usage())
                    
                    # Calculate average metrics
                    avg_time = sum(batch_times) / len(batch_times)
                    avg_memory = sum(memory_usages) / len(memory_usages)
                    
                    model_results["batch_results"][batch_size] = {
                        "avg_time_seconds": avg_time,
                        "tokens_per_second": len(batch) * 50 / avg_time,
                        "peak_memory_gb": max(memory_usages)
                    }
                
                # Calculate final memory usage
                final_memory = self.gpu_optimizer.get_current_memory_usage()
                model_results["memory_usage"] = {
                    "initial_memory_gb": initial_memory,
                    "peak_memory_gb": model_results["batch_results"][batch_sizes[-1]]["peak_memory_gb"],
                    "final_memory_gb": final_memory,
                    "memory_increase_gb": final_memory - initial_memory
                }
                
                results[model_name] = model_results
                
            except Exception as e:
                logger.error(f"Error benchmarking model {model_name}: {str(e)}")
                results[model_name] = {
                    "error": str(e),
                    "success": False
                }
        
        return results

    def get_model_info(self, model_name: str) -> Dict:
        """
        Get information about a loaded model
        
        Args:
            model_name: Name of the model
            
        Returns:
            Dictionary with model information
        """
        if model_name not in self.model_info:
            raise ValueError(f"Model {model_name} not loaded")
        
        return self.model_info[model_name]

    def unload_model(self, model_name: str) -> bool:
        """
        Unload a model to free memory
        
        Args:
            model_name: Name of the model to unload
            
        Returns:
            True if successful, False otherwise
        """
        if model_name not in self.models:
            logger.warning(f"Model {model_name} not loaded")
            return False
        
        try:
            # Delete model and info
            if model_name in self.models:
                del self.models[model_name]
            
            if model_name in self.model_info:
                del self.model_info[model_name]
            
            if model_name in self.tokenizers:
                del self.tokenizers[model_name]
            
            # Clear GPU cache if available
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                
                # Record memory usage after unloading
                memory_after = self.gpu_optimizer.get_current_memory_usage()
                logger.info(f"Unloaded model {model_name}, GPU memory now: {memory_after:.2f} GB")
            
            return True
            
        except Exception as e:
            logger.error(f"Error unloading model {model_name}: {str(e)}")
            return False

    def unload_all_models(self) -> None:
        """
        Unload all models to free memory
        """
        model_names = list(self.models.keys())
        
        for model_name in model_names:
            self.unload_model(model_name)
        
        logger.info("All models unloaded")
```

#### `src/evaluator.py`
```python
"""
Evaluator Module
Handles model evaluation logic for embedding models on Vietnamese text
"""

import os
import json
import time
import logging
from typing import List, Dict, Any, Optional, Tuple, Union
from tqdm import tqdm
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from src.metrics import calculate_hit_rate, calculate_mrr, calculate_ndcg, calculate_precision_recall

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
)
logger = logging.getLogger(__name__)


class ModelEvaluator:
    """
    Handles the evaluation of embedding models on Vietnamese text using various metrics
    """

    def __init__(self, embedding_manager, evaluation_config=None):
        """
        Initialize the model evaluator
        
        Args:
            embedding_manager: An instance of EmbeddingManager
            evaluation_config: Evaluation configuration dictionary
        """
        self.embedding_manager = embedding_manager
        self.evaluation_config = evaluation_config or {}
        
        # Set default evaluation parameters
        self.k_values = self.evaluation_config.get("k_values", [1, 3, 5, 10])
        self.similarity_metrics = self.evaluation_config.get("similarity_metrics", ["cosine"])
        self.cross_validation_folds = self.evaluation_config.get("cross_validation_folds", 5)
        self.test_split = self.evaluation_config.get("test_split", 0.2)
        self.random_seed = self.evaluation_config.get("random_seed", 42)
        self.benchmarking = self.evaluation_config.get("benchmarking", {})
        
        # Results storage
        self.results = {}
        self.comparison_results = {}
        self.benchmark_results = {}
        
        logger.info(f"Initialized ModelEvaluator with parameters: {self.evaluation_config}")

    def load_test_data(self, test_data_path: str) -> Dict:
        """
        Load test data from file
        
        Args:
            test_data_path: Path to test data file
            
        Returns:
            Dictionary with test data
        """
        try:
            with open(test_data_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            logger.info(f"Loaded test data with {len(data)} queries")
            return data
            
        except Exception as e:
            logger.error(f"Error loading test data: {str(e)}")
            raise

    def prepare_evaluation_data(self, documents: List[Dict], test_queries: List[Dict]) -> Tuple[List[str], List[str], List[List[str]]]:
        """
        Prepare data for evaluation
        
        Args:
            documents: List of document dictionaries
            test_queries: List of query dictionaries
            
        Returns:
            Tuple of (corpus_texts, query_texts, relevant_docs)
        """
        # Extract corpus texts from documents
        corpus_texts = []
        doc_id_to_index = {}
        
        for i, doc in enumerate(documents):
            if "chunks" in doc:
                # Use document chunks
                for chunk in doc["chunks"]:
                    corpus_texts.append(chunk["text"])
                    doc_id_to_index[chunk["id"]] = len(corpus_texts) - 1
            else:
                # Use full document text
                corpus_texts.append(doc.get("content", ""))
                doc_id_to_index[doc.get("id", f"doc_{i}")] = len(corpus_texts) - 1
        
        # Extract query texts and relevant document IDs
        query_texts = []
        relevant_docs = []
        
        for query in test_queries:
            query_texts.append(query.get("query", ""))
            
            # Get relevant document IDs and convert to indices
            doc_ids = query.get("relevant_documents", [])
            doc_indices = []
            
            for doc_id in doc_ids:
                if doc_id in doc_id_to_index:
                    doc_indices.append(doc_id_to_index[doc_id])
            
            relevant_docs.append(doc_indices)
        
        logger.info(f"Prepared {len(corpus_texts)} document texts and {len(query_texts)} queries")
        return corpus_texts, query_texts, relevant_docs

    def run_full_evaluation(self, documents: List[Dict], test_queries: List[Dict]) -> Dict:
        """
        Run complete evaluation for all loaded models
        
        Args:
            documents: List of document dictionaries
            test_queries: List of query dictionaries
            
        Returns:
            Dictionary with evaluation results for all models
        """
        if not self.embedding_manager.models:
            logger.error("No models loaded. Please load models first.")
            return {}
        
        # Prepare evaluation data
        corpus_texts, query_texts, relevant_docs = self.prepare_evaluation_data(documents, test_queries)
        
        # Store evaluation data for later use
        self.evaluation_data = {
            "corpus_texts": corpus_texts,
            "query_texts": query_texts,
            "relevant_docs": relevant_docs
        }
        
        # Get list of model names
        model_names = list(self.embedding_manager.models.keys())
        
        # Run evaluation for each model
        for model_name in model_names:
            logger.info(f"Running evaluation for model: {model_name}")
            
            try:
                # Evaluate model
                model_results = self.evaluate_model(
                    model_name=model_name,
                    corpus_texts=corpus_texts,
                    query_texts=query_texts,
                    relevant_docs=relevant_docs
                )
                
                # Store results
                self.results[model_name] = model_results
                
                logger.info(f"Evaluation for model {model_name} completed successfully")
                
            except Exception as e:
                logger.error(f"Error evaluating model {model_name}: {str(e)}")
                self.results[model_name] = {
                    "error": str(e),
                    "success": False
                }
        
        # Compare model results
        self.compare_models()
        
        # Return results
        return self.results

    def evaluate_model(self, model_name: str, corpus_texts: List[str], query_texts: List[str], 
                       relevant_docs: List[List[str]]) -> Dict:
        """
        Evaluate a single model
        
        Args:
            model_name: Name of the model to evaluate
            corpus_texts: List of corpus document texts
            query_texts: List of query texts
            relevant_docs: List of relevant document indices for each query
            
        Returns:
            Dictionary with evaluation results
        """
        logger.info(f"Evaluating model {model_name}")
        
        # Record start time
        start_time = time.time()
        
        # Generate embeddings
        logger.info("Generating embeddings for corpus documents")
        corpus_embeddings = self.embedding_manager.generate_embeddings_batch(
            corpus_texts, model_name
        )
        
        logger.info("Generating embeddings for queries")
        query_embeddings = self.embedding_manager.generate_embeddings_batch(
            query_texts, model_name
        )
        
        # Record embedding time
        embedding_time = time.time() - start_time
        logger.info(f"Embeddings generated in {embedding_time:.2f} seconds")
        
        # Calculate similarity between queries and corpus
        similarity_matrix = self.embedding_manager._calculate_similarity_matrix(
            query_embeddings, corpus_embeddings
        )
        
        # Record search time
        search_time = time.time() - embedding_time - start_time
        
        # Get the top K most similar documents for each query
        all_results = []
        for query_idx in range(len(query_texts)):
            query_similarities = similarity_matrix[query_idx]
            
            # Get document indices in descending order of similarity
            sorted_indices = np.argsort(query_similarities)[::-1]
            
            # Store results for all K values
            query_results = {}
            for k in self.k_values:
                top_k_indices = sorted_indices[:k]
                query_results[f"top_{k}"] = top_k_indices.tolist()
            
            all_results.append(query_results)
        
        # Calculate metrics for each K value
        metrics = {}
        for k in self.k_values:
            # Get top K results for each query
            top_k_results = [res[f"top_{k}"] for res in all_results]
            
            # Calculate metrics
            metrics[f"hit_rate@{k}"] = calculate_hit_rate(top_k_results, relevant_docs, k)
            metrics[f"mrr@{k}"] = calculate_mrr(top_k_results, relevant_docs)
            metrics[f"ndcg@{k}"] = calculate_ndcg(top_k_results, relevant_docs, k)
            
            # Calculate precision and recall
            precision, recall = calculate_precision_recall(top_k_results, relevant_docs)
            metrics[f"precision@{k}"] = precision
            metrics[f"recall@{k}"] = recall
        
        # Calculate average metrics across all K values
        metrics["avg_hit_rate"] = sum([metrics[f"hit_rate@{k}"] for k in self.k_values]) / len(self.k_values)
        metrics["avg_mrr"] = sum([metrics[f"mrr@{k}"] for k in self.k_values]) / len(self.k_values)
        metrics["avg_ndcg"] = sum([metrics[f"ndcg@{k}"] for k in self.k_values]) / len(self.k_values)
        metrics["avg_precision"] = sum([metrics[f"precision@{k}"] for k in self.k_values]) / len(self.k_values)
        metrics["avg_recall"] = sum([metrics[f"recall@{k}"] for k in self.k_values]) / len(self.k_values)
        
        # Get model info
        model_info = self.embedding_manager.get_model_info(model_name)
        
        # Return results
        return {
            "model_name": model_name,
            "model_info": model_info,
            "metrics": metrics,
            "embedding_time": embedding_time,
            "search_time": search_time,
            "total_queries": len(query_texts),
            "total_documents": len(corpus_texts),
            "success": True,
            "similarity_matrix": similarity_matrix.tolist() if self.evaluation_config.get("save_detailed_results", False) else None,
            "query_results": all_results if self.evaluation_config.get("save_detailed_results", False) else None
        }

    def compare_models(self) -> Dict:
        """
        Compare results across all models
        
        Returns:
            Dictionary with comparison results
        """
        if not self.results:
            logger.warning("No evaluation results available for comparison")
            return {}
        
        # Initialize comparison results
        self.comparison_results = {
            "models": list(self.results.keys()),
            "metrics": {}
        }
        
        # Get all metric keys from the first model
        first_model = next(iter(self.results.values()))
        if "metrics" not in first_model:
            logger.warning("No metrics found in results")
            return self.comparison_results
        
        # Collect metric values for each model
        for metric_key in first_model["metrics"].keys():
            self.comparison_results["metrics"][metric_key] = {}
            
            for model_name, model_results in self.results.items():
                if "metrics" in model_results and metric_key in model_results["metrics"]:
                    self.comparison_results["metrics"][metric_key][model_name] = model_results["metrics"][metric_key]
        
        # Calculate ranking for each metric
        rankings = {}
        for metric_key, metric_values in self.comparison_results["metrics"].items():
            # Sort models by metric value (descending for metrics where higher is better)
            is_higher_better = metric_key in ['hit_rate@1', 'hit_rate@3', 'hit_rate@5', 'hit_rate@10',
                                             'mrr@1', 'mrr@3', 'mrr@5', 'mrr@10',
                                             'ndcg@1', 'ndcg@3', 'ndcg@5', 'ndcg@10',
                                             'precision@1', 'precision@3', 'precision@5', 'precision@10',
                                             'recall@1', 'recall@3', 'recall@5', 'recall@10',
                                             'avg_hit_rate', 'avg_mrr', 'avg_ndcg', 'avg_precision', 'avg_recall']
            
            sorted_models = sorted(metric_values.items(), key=lambda x: x[1], reverse=is_higher_better)
            rankings[metric_key] = [model_name for model_name, _ in sorted_models]
        
        self.comparison_results["rankings"] = rankings
        
        # Calculate overall scores
        overall_scores = {}
        for model_name in self.results.keys():
            # Calculate weighted average of key metrics
            if "metrics" in self.results[model_name]:
                metrics = self.results[model_name]["metrics"]
                
                # Define weights for different metrics
                hit_rate_weight = 0.3
                mrr_weight = 0.3
                ndcg_weight = 0.2
                precision_weight = 0.1
                recall_weight = 0.1
                
                # Calculate weighted average
                overall_score = (
                    metrics["avg_hit_rate"] * hit_rate_weight +
                    metrics["avg_mrr"] * mrr_weight +
                    metrics["avg_ndcg"] * ndcg_weight +
                    metrics["avg_precision"] * precision_weight +
                    metrics["avg_recall"] * recall_weight
                )
                
                # Incorporate performance metrics (lower is better)
                embedding_time = self.results[model_name].get("embedding_time", 0)
                search_time = self.results[model_name].get("search_time", 0)
                
                # Normalize performance metrics (here we use inverse to make higher better)
                perf_factor = 1 / (1 + (embedding_time + search_time) / len(self.evaluation_data["query_texts"]))
                
                # Combine accuracy and performance (adjust weights as needed)
                accuracy_weight = 0.8
                performance_weight = 0.2
                
                final_score = overall_score * accuracy_weight + perf_factor * performance_weight
                overall_scores[model_name] = final_score
        
        # Sort models by overall score
        ranked_overall = sorted(overall_scores.items(), key=lambda x: x[1], reverse=True)
        self.comparison_results["overall_scores"] = overall_scores
        self.comparison_results["overall_ranking"] = [model_name for model_name, _ in ranked_overall]
        self.comparison_results["overall_scores_normalized"] = {model_name: score for model_name, score in ranked_overall}
        
        logger.info("Model comparison completed")
        return self.comparison_results

    def benchmark_performance(self, model_name: str, test_texts: List[str]) -> Dict:
        """
        Benchmark model performance
        
        Args:
            model_name: Name of the model to benchmark
            test_texts: List of texts to use for benchmarking
            
        Returns:
            Dictionary with benchmark results
        """
        if model_name not in self.embedding_manager.models:
            logger.warning(f"Model {model_name} not loaded for benchmarking")
            return {}
        
        logger.info(f"Benchmarking model: {model_name}")
        
        # Get benchmarking configuration
        warmup_runs = self.benchmarking.get("warmup_runs", 3)
        measurement_runs = self.benchmarking.get("measurement_runs", 10)
        
        # Initialize results
        benchmark_results = {
            "model_name": model_name,
            "warmup_runs": warmup_runs,
            "measurement_runs": measurement_runs,
            "measurements": []
        }
        
        # Warmup runs
        logger.info(f"Running {warmup_runs} warmup runs")
        for i in range(warmup_runs):
            start_time = time.time()
            _ = self.embedding_manager.generate_embeddings_batch(test_texts, model_name)
            elapsed_time = time.time() - start_time
            
            logger.info(f"Warmup run {i+1}/{warmup_runs} completed in {elapsed_time:.2f}s")
        
        # Measurement runs
        logger.info(f"Running {measurement_runs} measurement runs")
        for i in range(measurement_runs):
            start_time = time.time()
            
            # Generate embeddings
            embeddings = self.embedding_manager.generate_embeddings_batch(test_texts, model_name)
            
            elapsed_time = time.time() - start_time
            embeddings_per_second = len(test_texts) / elapsed_time
            tokens_per_second = len(test_texts) * 50 / elapsed_time  # Estimate 50 tokens per text
            
            measurement = {
                "run": i + 1,
                "time_seconds": elapsed_time,
                "embeddings_per_second": embeddings_per_second,
                "tokens_per_second": tokens_per_second,
                "embedding_dimensions": embeddings.shape[1],
                "total_texts": len(test_texts)
            }
            
            benchmark_results["measurements"].append(measurement)
            
            logger.info(f"Measurement run {i+1}/{measurement_runs}, time: {elapsed_time:.2f}s, "
                        f"embeddings/sec: {embeddings_per_second:.2f}, tokens/sec: {tokens_per_second:.2f}")
        
        # Calculate statistics
        times = [m["time_seconds"] for m in benchmark_results["measurements"]]
        eps_values = [m["embeddings_per_second"] for m in benchmark_results["measurements"]]
        tps_values = [m["tokens_per_second"] for m in benchmark_results["measurements"]]
        
        benchmark_results["statistics"] = {
            "avg_time_seconds": sum(times) / len(times),
            "min_time_seconds": min(times),
            "max_time_seconds": max(times),
            "std_time_seconds": np.std(times),
            "avg_embeddings_per_second": sum(eps_values) / len(eps_values),
            "avg_tokens_per_second": sum(tps_values) / len(tps_values),
            "embedding_dimensions": benchmark_results["measurements"][0]["embedding_dimensions"]
        }
        
        # Store benchmark result
        if model_name not in self.benchmark_results:
            self.benchmark_results[model_name] = benchmark_results
        else:
            self.benchmark_results[model_name] = benchmark_results
        
        logger.info(f"Benchmarking for model {model_name} completed")
        
        return benchmark_results

    def run_cross_validation(self, documents: List[Dict], test_queries: List[Dict], 
                            model_name: str) -> Dict:
        """
        Run cross-validation evaluation for a model
        
        Args:
            documents: List of document dictionaries
            test_queries: List of query dictionaries
            model_name: Name of the model to evaluate
            
        Returns:
            Dictionary with cross-validation results
        """
        if model_name not in self.embedding_manager.models:
            logger.warning(f"Model {model_name} not loaded for cross-validation")
            return {}
        
        logger.info(f"Running cross-validation for model: {model_name}")
        
        # Prepare evaluation data
        corpus_texts, query_texts, relevant_docs = self.prepare_evaluation_data(documents, test_queries)
        
        # Initialize cross-validation results
        cv_results = {
            "model_name": model_name,
            "folds": self.cross_validation_folds,
            "fold_results": [],
            "average_metrics": {}
        }
        
        # Get all metric keys to initialize average metrics
        first_fold_cv = False
        
        # Store original test queries
        all_queries = list(zip(query_texts, relevant_docs))
        total_queries = len(all_queries)
        fold_size = int(total_queries / self.cross_validation_folds)
        
        # Run cross-validation
        for fold in range(self.cross_validation_folds):
            logger.info(f"Running fold {fold+1}/{self.cross_validation_folds}")
            
            # Calculate test start and end indices
            test_start = fold * fold_size
            test_end = (fold + 1) * fold_size if fold < self.cross_validation_folds - 1 else total_queries
            
            # Split data
            test_queries_fold = all_queries[test_start:test_end]
            train_queries_fold = all_queries[:test_start] + all_queries[test_end:]
            
            # Prepare data for this fold
            test_query_texts, test_relevant_docs = zip(*test_queries_fold)
            train_query_texts, train_relevant_docs = zip(*train_queries_fold) if train_queries_fold else ([], [])
            
            # Use full corpus for all folds
            current_corpus = corpus_texts
            
            # Run evaluation for this fold
            fold_result = self.evaluate_model(
                model_name=model_name,
                corpus_texts=current_corpus,
                query_texts=list(test_query_texts),
                relevant_docs=list(test_relevant_docs)
            )
            
            # Store fold results
            cv_results["fold_results"].append(fold_result)
            
            # Initialize average metrics if not done yet
            if not first_fold_cv:
                cv_results["average_metrics"] = {
                    metric: 0.0 for metric in fold_result["metrics"].keys()
                }
                first_fold_cv = True
            
            # Accumulate metrics
            for metric_key, metric_value in fold_result["metrics"].items():
                cv_results["average_metrics"][metric_key] += metric_value
        
        # Calculate average metrics
        for metric_key in cv_results["average_metrics"]:
            cv_results["average_metrics"][metric_key] /= self.cross_validation_folds
        
        # Calculate standard deviation for metrics
        cv_results["std_metrics"] = {
            metric: 0.0 for metric in cv_results["average_metrics"].keys()
        }
        
        for metric_key in cv_results["std_metrics"]:
            # Collect values from all folds
            fold_values = [
                fold["metrics"][metric_key]
                for fold in cv_results["fold_results"]
            ]
            
            # Calculate standard deviation
            cv_results["std_metrics"][metric_key] = np.std(fold_values)
        
        logger.info(f"Cross-validation for model {model_name} completed")
        
        return cv_results

    def get_top_models(self, n: int = 2) -> List[Dict]:
        """
        Get top N models based on overall ranking
        
        Args:
            n: Number of top models to return
            
        Returns:
            List of top models with their information
        """
        if not self.comparison_results or "overall_ranking" not in self.comparison_results:
            logger.warning("No comparison results available")
            return []
        
        # Get top N model names
        top_model_names = self.comparison_results["overall_ranking"][:n]
        
        # Get model information
        top_models = []
        for model_name in top_model_names:
            if model_name in self.results:
                model_info = self.results[model_name]["model_info"]
                model_metrics = self.results[model_name]["metrics"]
                model_score = self.comparison_results["overall_scores_normalized"].get(model_name, 0)
                
                top_models.append({
                    "name": model_name,
                    "model_info": model_info,
                    "metrics": model_metrics,
                    "score": model_score
                })
        
        return top_models

    def save_results(self, output_path: str) -> None:
        """
        Save evaluation results to file
        
        Args:
            output_path: Path to save results
        """
        logger.info(f"Saving evaluation results to {output_path}")
        
        # Create output directory if it doesn't exist
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # Prepare results data
        results_data = {
            "evaluation_config": self.evaluation_config,
            "results": self.results,
            "comparison_results": self.comparison_results,
            "benchmark_results": self.benchmark_results,
            "timestamp": time.time()
        }
        
        # Save results
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"Evaluation results saved to {output_path}")
```

#### `src/metrics.py`
```python
"""
Metrics Module
Provides functions to calculate various evaluation metrics for embedding models
"""

import numpy as np
from typing import List, Dict, Any, Tuple, Union


def calculate_hit_rate(query_results: List[List[str]], 
                      ground_truth: List[List[str]], 
                      k: int = 5) -> float:
    """
    Calculate Hit Rate@K
    
    Hit Rate@K is the proportion of queries for which at least one relevant document
    is found in the top K results.
    
    Args:
        query_results: List of top-K document indices for each query
        ground_truth: List of relevant document indices for each query
        k: Number of top results to consider
        
    Returns:
        Hit rate score (0.0 to 1.0)
    """
    if len(query_results) != len(ground_truth):
        raise ValueError("Number of queries in query_results and ground_truth must match")
    
    hits = 0
    
    for i, (result, relevant) in enumerate(zip(query_results, ground_truth)):
        # If there are no relevant documents, this query can't have a hit
        if not relevant:
            continue
        
        # Consider only the top K results
        top_k_results = set(result[:k])
        
        # Check if at least one relevant document is in the top K
        if any(doc_idx in top_k_results for doc_idx in relevant):
            hits += 1
    
    # Calculate hit rate
    hit_rate = hits / len(query_results) if query_results else 0.0
    
    return hit_rate


def calculate_mrr(query_results: List[List[str]], 
                  ground_truth: List[List[str]]) -> float:
    """
    Calculate Mean Reciprocal Rank (MRR)
    
    MRR is the average of the reciprocal ranks of the first relevant document
    for each query. The reciprocal rank is 1/rank where rank is the position
    of the first relevant document in the results.
    
    Args:
        query_results: List of document indices sorted by relevance for each query
        ground_truth: List of relevant document indices for each query
        
    Returns:
        Mean reciprocal rank score (0.0 to 1.0)
    """
    if len(query_results) != len(ground_truth):
        raise ValueError("Number of queries in query_results and ground_truth must match")
    
    reciprocal_ranks = []
    
    for i, (result, relevant) in enumerate(zip(query_results, ground_truth)):
        # If there are no relevant documents, skip this query
        if not relevant:
            continue
        
        # Find the rank of the first relevant document
        rank = None
        for position, doc_idx in enumerate(result, 1):  # positions start at 1
            if doc_idx in relevant:
                rank = position
                break
        
        # If a relevant document was found, add reciprocal rank
        if rank is not None:
            reciprocal_ranks.append(1.0 / rank)
    
    # Calculate MRR
    mrr = sum(reciprocal_ranks) / len(reciprocal_ranks) if reciprocal_ranks else 0.0
    
    return mrr


def calculate_ndcg(query_results: List[List[str]], 
                   ground_truth: List[List[str]], 
                   k: int = 10) -> float:
    """
    Calculate Normalized Discounted Cumulative Gain (NDCG@K)
    
    NDCG measures the quality of ranking by considering both the position of
    relevant documents and their relevance levels. The NDCG score is normalized
    by the ideal ranking (the DCG of a perfect ranking).
    
    Note: In this implementation, all relevant documents are assumed to have
    equal relevance (1.0), and non-relevant documents have relevance 0.0.
    
    Args:
        query_results: List of document indices sorted by relevance for each query
        ground_truth: List of relevant document indices for each query
        k: Number of top results to consider
        
    Returns:
        Normalized Discounted Cumulative Gain score (0.0 to 1.0)
    """
    if len(query_results) != len(ground_truth):
        raise ValueError("Number of queries in query_results and ground_truth must match")
    
    # Ensure k is not larger than the number of results
    k = min(k, min(len(result) for result in query_results) if query_results else 0)
    
    ndcg_scores = []
    
    for result, relevant in zip(query_results, ground_truth):
        # If there are no relevant documents, skip this query
        if not relevant:
            continue
        
        # Create relevance dictionary for this query
        relevance = {doc_idx: 1.0 for doc_idx in relevant}
        
        # Calculate DCG for the actual ranking
        dcg = 0.0
        for i, doc_idx in enumerate(result[:k]):
            # Gain is 1.0 if the document is relevant, 0.0 otherwise
            gain = relevance.get(doc_idx, 0.0)
            
            # Discount is log2(i+2) since positions start at 0 (i+1) and log2(2) = 1
            discount = np.log2(i + 2)
            
            dcg += gain / discount
        
        # Calculate IDCG (ideal DCG) - all relevant documents at the top
        idcg = 0.0
        # Sort relevant documents by relevance (all 1.0 in this case)
        for i in range(min(k, len(relevant))):
            discount = np.log2(i + 2)
            idcg += 1.0 / discount
        
        # Calculate NDCG
        ndcg = dcg / idcg if idcg > 0 else 0.0
        ndcg_scores.append(ndcg)
    
    # Calculate average NDCG
    avg_ndcg = sum(ndcg_scores) / len(ndcg_scores) if ndcg_scores else 0.0
    
    return avg_ndcg


def calculate_precision_recall(query_results: List[List[str]], 
                              ground_truth: List[List[str]],
                              k: int = None) -> Tuple[float, float]:
    """
    Calculate Precision and Recall at the specified K
    
    Precision is the proportion of retrieved documents that are relevant.
    Recall is the proportion of relevant documents that are retrieved.
    
    Args:
        query_results: List of document indices sorted by relevance for each query
        ground_truth: List of relevant document indices for each query
        k: Number of top results to consider. If None, use all results.
        
    Returns:
        Tuple of (precision, recall)
    """
    if len(query_results) != len(ground_truth):
        raise ValueError("Number of queries in query_results and ground_truth must match")
    
    precision_scores = []
    recall_scores = []
    
    for result, relevant in zip(query_results, ground_truth):
        # If there are no relevant documents, skip this query
        if not relevant:
            continue
        
        # If k is specified, consider only the top K results
        if k is not None:
            retrieved = result[:k]
        else:
            retrieved = result
        
        # Convert to sets for easier comparison
        retrieved_set = set(retrieved)
        relevant_set = set(relevant)
        
        # Calculate the number of relevant retrieved documents
        relevant_retrieved = len(retrieved_set.intersection(relevant_set))
        
        # Calculate precision and recall
        precision = relevant_retrieved / len(retrieved) if retrieved else 0.0
        recall = relevant_retrieved / len(relevant) if relevant else 0.0
        
        precision_scores.append(precision)
        recall_scores.append(recall)
    
    # Calculate average precision and recall
    avg_precision = sum(precision_scores) / len(precision_scores) if precision_scores else 0.0
    avg_recall = sum(recall_scores) / len(recall_scores) if recall_scores else 0.0
    
    return avg_precision, avg_recall


def calculate_f1_score(query_results: List[List[str]], 
                       ground_truth: List[List[str]],
                       k: int = None) -> float:
    """
    Calculate F1 Score at the specified K
    
    F1 Score is the harmonic mean of precision and recall.
    
    Args:
        query_results: List of document indices sorted by relevance for each query
        ground_truth: List of relevant document indices for each query
        k: Number of top results to consider. If None, use all results.
        
    Returns:
        F1 score (0.0 to 1.0)
    """
    precision, recall = calculate_precision_recall(query_results, ground_truth, k)
    
    # Calculate F1 score
    if precision + recall > 0:
        f1 = 2 * (precision * recall) / (precision + recall)
    else:
        f1 = 0.0
    
    return f1


def calculate_map(query_results: List[List[str]], 
                 ground_truth: List[List[str]],
                 k: int = None) -> float:
    """
    Calculate Mean Average Precision (MAP)
    
    MAP is the mean of the average precision scores for each query.
    Average precision is the average of precision values at each position
    where a relevant document is found.
    
    Args:
        query_results: List of document indices sorted by relevance for each query
        ground_truth: List of relevant document indices for each query
        k: Number of top results to consider. If None, use all results.
        
    Returns:
        Mean Average Precision score (0.0 to 1.0)
    """
    if len(query_results) != len(ground_truth):
        raise ValueError("Number of queries in query_results and ground_truth must match")
    
    all_avg_precisions = []
    
    for result, relevant in zip(query_results, ground_truth):
        # If there are no relevant documents, skip this query
        if not relevant:
            continue
        
        # If k is specified, consider only the top K results
        if k is not None:
            retrieved = result[:k]
        else:
            retrieved = result
        
        # Convert to set for easier comparison
        relevant_set = set(relevant)
        
        # Track precision at each position
        precisions = []
        relevant_count = 0
        
        for i, doc_idx in enumerate(retrieved):
            if doc_idx in relevant_set:
                relevant_count += 1
                precision_at_i = relevant_count / (i + 1)
                precisions.append(precision_at_i)
        
        # Calculate average precision for this query
        if precisions:
            avg_precision = sum(precisions) / len(precisions)
        else:
            avg_precision = 0.0
        
        all_avg_precisions.append(avg_precision)
    
    # Calculate mean average precision
    map_score = sum(all_avg_precisions) / len(all_avg_precisions) if all_avg_precisions else 0.0
    
    return map_score


def evaluate_all_metrics(query_results: List[List[str]], 
                        ground_truth: List[List[str]],
                        k_values: List[int] = None) -> Dict[str, float]:
    """
    Calculate all evaluation metrics for the given results
    
    Args:
        query_results: List of document indices sorted by relevance for each query
        ground_truth: List of relevant document indices for each query
        k_values: List of K values to evaluate. If None, uses [1, 3, 5, 10]
        
    Returns:
        Dictionary with all metric names and values
    """
    if k_values is None:
        k_values = [1, 3, 5, 10]
    
    metrics = {}
    
    # Calculate Hit Rate@K for each K
    for k in k_values:
        metrics[f"hit_rate@{k}"] = calculate_hit_rate(query_results, ground_truth, k)
    
    # Calculate MRR
    metrics["mrr"] = calculate_mrr(query_results, ground_truth)
    
    # Calculate NDCG@K for each K
    for k in k_values:
        metrics[f"ndcg@{k}"] = calculate_ndcg(query_results, ground_truth, k)
    
    # Calculate Precision@K and Recall@K for each K
    for k in k_values:
        precision, recall = calculate_precision_recall(query_results, ground_truth, k)
        metrics[f"precision@{k}"] = precision
        metrics[f"recall@{k}"] = recall
        
        # Calculate F1@K
        metrics[f"f1@{k}"] = calculate_f1_score(query_results, ground_truth, k)
    
    # Calculate MAP for each K
    for k in k_values:
        metrics[f"map@{k}"] = calculate_map(query_results, ground_truth, k)
    
    # Calculate overall metrics (using the largest K)
    metrics["avg_hit_rate"] = sum(metrics[f"hit_rate@{k}"] for k in k_values) / len(k_values)
    metrics["avg_precision"] = sum(metrics[f"precision@{k}"] for k in k_values) / len(k_values)
    metrics["avg_recall"] = sum(metrics[f"recall@{k}"] for k in k_values) / len(k_values)
    metrics["avg_f1"] = sum(metrics[f"f1@{k}"] for k in k_values) / len(k_values)
    metrics["avg_ndcg"] = sum(metrics[f"ndcg@{k}"] for k in k_values) / len(k_values)
    metrics["avg_map"] = sum(metrics[f"map@{k}"] for k in k_values) / len(k_values)
    
    return metrics
```

#### `src/gpu_optimizer.py`
```python
"""
GPU Optimizer Module
Handles GPU optimization for embedding model loading and execution
"""

import os
import logging
import psutil
import pynvml
import numpy as np
from typing import Dict, Any, Optional, List
# Import torch only if CUDA is available
try:
    import torch
    CUDA_AVAILABLE = torch.cuda.is_available()
except ImportError:
    CUDA_AVAILABLE = False

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
)
logger = logging.getLogger(__name__)


class GPUOptimizer:
    """
    Handles GPU optimization strategies for efficient embedding model execution
    """

    def __init__(self, config: Dict):
        """
        Initialize the GPU optimizer
        
        Args:
            config: GPU configuration dictionary
        """
        self.config = config
        self.gpu_settings = config.get("gpu_settings", {})
        self.batch_settings = config.get("batch_settings", {})
        self.monitoring = config.get("monitoring", {})
        
        # Initialize GPU monitoring if applicable
        self.gpu_monitoring_enabled = False
        self.gpu_handle = None
        self.gpu_count = 0
        
        if CUDA_AVAILABLE and self.gpu_settings.get("enabled", True):
            try:
                self._init_gpu_monitoring()
            except Exception as e:
                logger.warning(f"Failed to initialize GPU monitoring: {str(e)}")
                
        # Set device
        self.device = self._determine_device()
        
        # Initialize memory tracking variables
        self.peak_memory_usage = 0
        self.initial_memory_usage = 0
        
        logger.info(f"Initialized GPUOptimizer with device: {self.device}")

    def _init_gpu_monitoring(self):
        """Initialize GPU monitoring capabilities"""
        try:
            pynvml.nvmlInit()
            self.gpu_count = pynvml.nvmlDeviceGetCount()
            if self.gpu_count > 0:
                self.gpu_monitoring_enabled = True
                logger.info(f"Initialized GPU monitoring for {self.gpu_count} GPUs")
                
                # Get GPU handle for primary device (or specified ID)
                device_id = self.gpu_settings.get("device_id", "auto")
                
                if device_id == "auto":
                    # Find GPU with most free memory
                    device_id = self._find_best_gpu()
                
                device_id = int(device_id) if device_id != "auto" else 0
                if device_id >= self.gpu_count:
                    device_id = 0
                    
                self.gpu_handle = pynvml.nvmlDeviceGetHandleByIndex(device_id)
                
                # Store initial memory usage
                meminfo = pynvml.nvmlDeviceGetMemoryInfo(self.gpu_handle)
                self.initial_memory_usage = meminfo.used / (1024 ** 3)  # Convert to GB
        except Exception as e:
            logger.error(f"Error initializing GPU monitoring: {str(e)}")
            self.gpu_monitoring_enabled = False

    def _find_best_gpu(self) -> int:
        """
        Find the GPU with the most free memory
        
        Returns:
            Index of the GPU with most free memory
        """
        best_gpu = 0
        max_free_memory = 0
        
        for i in range(self.gpu_count):
            try:
                handle = pynvml.nvmlDeviceGetHandleByIndex(i)
                meminfo = pynvml.nvmlDeviceGetMemoryInfo(handle)
                free_memory = meminfo.free
                
                if free_memory > max_free_memory:
                    max_free_memory = free_memory
                    best_gpu = i
                    
            except Exception as e:
                logger.warning(f"Error checking GPU {i}: {str(e)}")
                
        logger.info(f"Selected GPU {best_gpu} with most free memory")
        return best_gpu

    def _determine_device(self):
        """
        Determine the appropriate device for model execution
        
        Returns:
            PyTorch device object
        """
        if not CUDA_AVAILABLE or not self.gpu_settings.get("enabled", True):
            logger.info("Using CPU device")
            return torch.device("cpu")
        
        # Check if GPU should be used
        use_gpu = self.gpu_settings.get("enabled", True)
        
        if use_gpu:
            device_id = self.gpu_settings.get("device_id", "auto")
            if device_id == "auto":
                device_id = self._find_best_gpu()
            else:
                device_id = int(device_id)
                
            logger.info(f"Using GPU device: cuda:{device_id}")
            return torch.device(f"cuda:{device_id}")
        else:
            logger.info("Using CPU device (GPU disabled)")
            return torch.device("cpu")

    def pre_model_loading_hook(self, model_name: str):
        """
        Hook called before loading a model
        
        Args:
            model_name: Name of the model being loaded
        """
        logger.info(f"Pre-loading hook for model: {model_name}")
        
        if not self.gpu_monitoring_enabled:
            return
            
        try:
            # Get current GPU memory usage
            meminfo = pynvml.nvmlDeviceGetMemoryInfo(self.gpu_handle)
            current_memory = meminfo.used / (1024 ** 3)  # Convert to GB
            
            logger.info(f"Current GPU memory usage: {current_memory:.2f} GB")
            
            # Check if we need to clear memory
            memory_limit = self.gpu_settings.get("memory_limit_gb", 8)
            if current_memory > memory_limit * 0.9:  # 90% of limit
                logger.warning(f"GPU memory ({current_memory:.2f} GB) approaching limit ({memory_limit} GB)")
                self.clear_gpu_memory()
                
        except Exception as e:
            logger.error(f"Error in pre-loading hook: {str(e)}")

    def post_model_loading_hook(self, model_name: str, model):
        """
        Hook called after loading a model
        
        Args:
            model_name: Name of the model being loaded
            model: The loaded model
        """
        logger.info(f"Post-loading hook for model: {model_name}")
        
        if not self.gpu_monitoring_enabled:
            return
            
        try:
            # Get current GPU memory usage
            meminfo = pynvml.nvmlDeviceGetMemoryInfo(self.gpu_handle)
            current_memory = meminfo.used / (1024 ** 3)  # Convert to GB
            
            logger.info(f"Post-loading GPU memory usage: {current_memory:.2f} GB")
            
            # Update peak memory usage
            if current_memory > self.peak_memory_usage:
                self.peak_memory_usage = current_memory
                
            # Enable optimizations for the model
            if self.gpu_settings.get("optimize_memory", True):
                self._optimize_model_memory(model)
                
        except Exception as e:
            logger.error(f"Error in post-loading hook: {str(e)}")

    def _optimize_model_memory(self, model):
        """
        Apply memory optimizations to the model
        
        Args:
            model: PyTorch model to optimize
        """
        try:
            # Enable gradient checkpointing if available
            if hasattr(model, "gradient_checkpointing_enable"):
                model.gradient_checkpointing_enable()
                logger.info("Enabled gradient checkpointing")
                
            # Use mixed precision if enabled
            if self.gpu_settings.get("mixed_precision", True):
                # Model will be converted to float16 when possible
                logger.info("Mixed precision will be used (model will be converted to float16 when possible)")
                
        except Exception as e:
            logger.error(f"Error optimizing model memory: {str(e)}")

    def get_current_memory_usage(self) -> float:
        """
        Get current GPU memory usage in GB
        
        Returns:
            Current GPU memory usage in GB, or 0 if GPU monitoring is disabled
        """
        if not self.gpu_monitoring_enabled:
            return 0.0
            
        try:
            meminfo = pynvml.nvmlDeviceGetMemoryInfo(self.gpu_handle)
            return meminfo.used / (1024 ** 3)  # Convert to GB
        except Exception as e:
            logger.error(f"Error getting memory usage: {str(e)}")
            return 0.0

    def get_model_memory_usage(self, model) -> float:
        """
        Estimate memory usage of a model in GB
        
        Args:
            model: PyTorch model
            
        Returns:
            Estimated memory usage in GB
        """
        if not CUDA_AVAILABLE:
            return 0.0
            
        try:
            # Get model parameters
            param_size = 0
            param_count = 0
            
            for param in model.parameters():
                param_size += param.nelement() * param.element_size()
                param_count += param.nelement()
                
            # Get buffer size (e.g., for batchnorm)
            buffer_size = 0
            for buffer in model.buffers():
                buffer_size += buffer.nelement() * buffer.element_size()
                
            # Calculate total size in GB
            model_size = (param_size + buffer_size) / (1024 ** 3)
            
            # Add additional memory for activations (simple estimation)
            activation_overhead = model_size * 0.5  # Assume 50% overhead
            
            total_size = model_size + activation_overhead
            
            logger.info(f"Model memory estimation: {total_size:.2f} GB "
                       f"(parameters: {model_size:.2f} GB, activations: {activation_overhead:.2f} GB, "
                       f"{param_count} parameters)")
            
            return total_size
            
        except Exception as e:
            logger.error(f"Error estimating model memory usage: {str(e)}")
            return 0.0

    def optimize_batch_size(self, min_batch_size: int = 1, max_batch_size: int = 128) -> int:
        """
        Find the optimal batch size based on available memory
        
        Args:
            min_batch_size: Minimum batch size to try
            max_batch_size: Maximum batch size to try
            
        Returns:
            Optimal batch size
        """
        if not self.gpu_monitoring_enabled:
            logger.info("GPU monitoring disabled, using batch size from config")
            return self.batch_settings.get("max_batch_size", 32)
            
        # Get memory limits and current usage
        memory_limit = self.gpu_settings.get("memory_limit_gb", 8)
        current_memory = self.get_current_memory_usage()
        available_memory = memory_limit - current_memory
        
        # Get utilization threshold
        util_threshold = self.batch_settings.get("memory_utilization_threshold", 0.8)
        
        # Binary search for optimal batch size
        low, high = min_batch_size, max_batch_size
        best_batch_size = min_batch_size
        
        logger.info(f"Finding optimal batch size. Available memory: {available_memory:.2f} GB")
        
        # Safe batch size (conservative estimate)
        safe_batch_size = 1
        
        while low <= high:
            mid = (low + high) // 2
            
            # Estimate memory for this batch size
            # This is a simplified estimation - in practice you might want to run a small test
            estimated_memory = self.estimate_batch_memory_usage(mid)
            
            if estimated_memory < available_memory * util_threshold:
                safe_batch_size = mid
                low = mid + 1
            else:
                high = mid - 1
        
        logger.info(f"Determined optimal batch size: {safe_batch_size}")
        return safe_batch_size

    def estimate_batch_memory_usage(self, batch_size: int) -> float:
        """
        Estimate memory usage for a given batch size
        
        Args:
            batch_size: Batch size to estimate memory for
            
        Returns:
            Estimated memory usage in GB
        """
        # This is a simplified estimation
        # A more accurate estimation would involve running a small test
        
        # Base estimation parameters
        base_memory = 0.5  # Base model memory (GB)
        per_sample_memory = 0.001  # Memory per sample (GB)
        
        # Calculate estimated memory
        estimated_memory = base_memory + per_sample_memory * batch_size
        
        # Add some margin of error
        estimated_memory *= 1.2
        
        return estimated_memory

    def clear_gpu_memory(self):
        """Clear GPU memory cache"""
        if not CUDA_AVAILABLE:
            return
            
        try:
            # Clear PyTorch cache
            torch.cuda.empty_cache()
            
            # Reset peak memory stats
            torch.cuda.reset_peak_memory_stats()
            
            # Get memory after clearing
            memory_after = self.get_current_memory_usage()
            logger.info(f"GPU memory cleared. Current usage: {memory_after:.2f} GB")
            
        except Exception as e:
            logger.error(f"Error clearing GPU memory: {str(e)}")

    def monitor_gpu_usage(self):
        """
        Monitor GPU usage and log statistics
        
        Returns:
            Dictionary with GPU statistics
        """
        if not self.gpu_monitoring_enabled:
            return {
                "gpu_enabled": False,
                "error": "GPU monitoring not initialized"
            }
            
        stats = {
            "gpu_enabled": True,
            "gpu_count": self.gpu_count
        }
        
        try:
            # Get memory info
            meminfo = pynvml.nvmlDeviceGetMemoryInfo(self.gpu_handle)
            
            stats.update({
                "memory_used_gb": meminfo.used / (1024 ** 3),
                "memory_total_gb": meminfo.total / (1024 ** 3),
                "memory_free_gb": meminfo.free / (1024 ** 3),
                "memory_usage_percent": (meminfo.used / meminfo.total) * 100,
                "peak_memory_gb": self.peak_memory_usage,
                "memory_increase_gb": self.peak_memory_usage - self.initial_memory_usage
            })
            
            # Get GPU utilization
            try:
                utilization = pynvml.nvmlDeviceGetUtilizationRates(self.gpu_handle)
                stats.update({
                    "gpu_utilization_percent": utilization.gpu,
                    "memory_utilization_percent": utilization.memory
                })
            except Exception:
                stats.update({
                    "gpu_utilization_percent": -1,
                    "memory_utilization_percent": -1,
                    "error": "Could not retrieve utilization rates"
                })
            
            # Get temperature
            try:
                temp = pynvml.nvmlDeviceGetTemperature(self.gpu_handle, pynvml.NVML_TEMPERATURE_GPU)
                stats["temperature_celsius"] = temp
                
                # Check against threshold
                temp_threshold = self.monitoring.get("alert_threshold_temperature", 85)
                if temp > temp_threshold:
                    logger.warning(f"GPU temperature ({temp}°C) exceeds threshold ({temp_threshold}°C)")
            except Exception:
                stats["temperature_celsius"] = -1
                stats["temperature_error"] = "Could not retrieve temperature"
            
            # Log memory usage
            if self.monitoring.get("trace_gpu_memory", True):
                logger.info(f"GPU Memory: {stats['memory_used_gb']:.2f}/{stats['memory_total_gb']:.2f} GB "
                           f"({stats['memory_usage_percent']:.1f}%)")
            
            # Check memory threshold
            mem_threshold = self.monitoring.get("alert_threshold_memory", 0.9)
            if stats["memory_usage_percent"] / 100 > mem_threshold:
                logger.warning(f"GPU memory usage ({stats['memory_usage_percent']:.1f}%) exceeds threshold "
                              f"({mem_threshold * 100:.1f}%)")
            
            return stats
            
        except Exception as e:
            logger.error(f"Error monitoring GPU usage: {str(e)}")
            stats.update({
                "error": str(e),
                "status": "error"
            })
            
            return stats

    def get_optimal_batch_processing_parameters(self, num_samples: int) -> Dict:
        """
        Get optimal batch processing parameters for the given number of samples
        
        Args:
            num_samples: Total number of samples to process
            
        Returns:
            Dictionary with optimal parameters
        """
        # Get optimal batch size
        batch_size = self.optimize_batch_size()
        
        # Calculate number of batches
        num_batches = (num_samples + batch_size - 1) // batch_size
        
        # Determine if we should use overlapping batches for better memory usage
        use_overlapping = num_samples > 1000  # Use overlapping for large datasets
        
        # Calculate overlaps (10% of batch size)
        overlap_size = int(batch_size * 0.1) if use_overlapping else 0
        
        return {
            "batch_size": batch_size,
            "num_batches": num_batches,
            "use_overlapping": use_overlapping,
            "overlap_size": overlap_size
        }
```

#### `src/visualizer.py`
```python
"""
Report Visualization Module
Handles the creation of charts, tables, and reports for model comparison
"""

import os
import json
import logging
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import plotly.offline as pyo
import jinja2
from IPython.display import HTML, display

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
)
logger = logging.getLogger(__name__)


class ReportGenerator:
    """
    Generates visualization and reports for model evaluation results
    """

    def __init__(self, output_path: str = "reports/"):
        """
        Initialize the report generator
        
        Args:
            output_path: Path where reports will be saved
        """
        self.output_path = output_path
        self.performance_charts_path = os.path.join(output_path, "performance_charts")
        
        # Create directories if they don't exist
        os.makedirs(output_path, exist_ok=True)
        os.makedirs(self.performance_charts_path, exist_ok=True)
        
        # Set up Jinja2 environment for templates
        self.template_env = jinja2.Environment(
            loader=jinja2.FileSystemLoader('templates'),
            autoescape=True
        )
        
        # Set up plotting styles
        self._setup_plotting_styles()
        
        logger.info(f"Initialized ReportGenerator with output path: {output_path}")

    def _setup_plotting_styles(self):
        """Set up consistent plotting styles"""
        # Set matplotlib style
        plt.style.use('ggplot')
        sns.set_palette(sns.color_palette("viridis", 10))
        
        # Set plotly template
        pio_templates = pio.templates
        pio_templates.default = "plotly_white"

    def generate_comparison_charts(self, results: Dict) -> None:
        """
        Generate visualization charts for model comparison
        
        Args:
            results: Evaluation results dictionary
        """
        if "comparison_results" not in results:
            logger.warning("No comparison results found for chart generation")
            return
            
        metric_data = results["comparison_results"].get("metrics", {})
        overall_ranking = results["comparison_results"].get("overall_ranking", [])
        
        if not metric_data:
            logger.warning("No metric data found for chart generation")
            return
            
        # Generate Hit Rate comparison chart
        self._generate_hit_rate_chart(metric_data)
        
        # Generate MRR comparison chart
        self._generate_mrr_chart(metric_data)
        
        # Generate Speed vs Accuracy chart
        self._generate_speed_accuracy_chart(results)
        
        # Generate Memory usage chart
        self._generate_memory_usage_chart(results)
        
        # Generate detailed performance heatmap
        self._generate_performance_heatmap(metric_data)
        
        # Generate overall ranking chart
        self._generate_ranking_chart(metric_data, overall_ranking)
        
        logger.info("All comparison charts generated successfully")

    def _generate_hit_rate_chart(self, metric_data: Dict) -> None:
        """
        Generate Hit Rate@K comparison chart
        
        Args:
            metric_data: Dictionary with metrics for all models
        """
        # Extract Hit Rate metrics
        hit_rates = {}
        for metric_name, values in metric_data.items():
            if metric_name.startswith('hit_rate@'):
                k = metric_name.split('@')[1]
                hit_rates[k] = values
        
        if not hit_rates:
            logger.warning("No Hit Rate metrics found")
            return
            
        # Convert to DataFrame
        df = pd.DataFrame(hit_rates)
        df.index.name = 'Model'
        df = df.reset_index().melt(id_vars='Model', var_name='K', value_name='Hit Rate')
        
        # Create plot
        plt.figure(figsize=(12, 7))
        ax = sns.barplot(x='K', y='Hit Rate', hue='Model', data=df)
        
        # Add value labels on bars
        for p in ax.patches:
            height = p.get_height()
            if height > 0:  # Only add label if height > 0
                ax.text(p.get_x() + p.get_width() / 2., height + 0.001,
                        f'{height:.3f}', ha="center", fontsize=8)
        
        plt.title('Hit Rate@K Comparison Across Models', fontsize=14, fontweight='bold')
        plt.ylim(0, 1.0)
        plt.legend(loc='upper right')
        plt.tight_layout()
        
        # Save chart
        chart_path = os.path.join(self.performance_charts_path, "hit_rate_comparison.png")
        plt.savefig(chart_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        # Also create interactive Plotly chart
        fig = px.bar(
            df,
            x='K',
            y='Hit Rate',
            color='Model',
            barmode='group',
            title='Hit Rate@K Comparison Across Models',
            text_auto='.3f'
        )
        
        fig.update_layout(
            yaxis=dict(range=[0, 1], title='Hit Rate'),
            xaxis_title='K Value',
            height=500
        )
        
        chart_path = os.path.join(self.performance_charts_path, "hit_rate_comparison_interactive.html")
        fig.write_html(chart_path)

    def _generate_mrr_chart(self, metric_data: Dict) -> None:
        """
        Generate Mean Reciprocal Rank (MRR) comparison chart
        
        Args:
            metric_data: Dictionary with metrics for all models
        """
        # Extract MRR metrics
        mrr_values = {
            model: values.get('mrr', 0) 
            for model, values in metric_data.items()
        }
        
        if not any(mrr_values.values()):
            logger.warning("No MRR metrics found")
            return
            
        # Convert to DataFrame
        df = pd.DataFrame(list(mrr_values.items()), columns=['Model', 'MRR'])
        df = df.sort_values('MRR', ascending=False)
        
        # Create plot
        plt.figure(figsize=(10, 6))
        ax = sns.barplot(x='MRR', y='Model', data=df)
        
        # Add value labels on bars
        for i, (_, row) in enumerate(df.iterrows()):
            ax.text(row['MRR'] + 0.01, i, f"{row['MRR']:.3f}", va='center')
        
        plt.title('Mean Reciprocal Rank (MRR) Comparison', fontsize=14, fontweight='bold')
        plt.xlim(0, 1.0)
        plt.tight_layout()
        
        # Save chart
        chart_path = os.path.join(self.performance_charts_path, "mrr_comparison.png")
        plt.savefig(chart_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        # Also create interactive Plotly chart
        fig = px.bar(
            df,
            x='MRR',
            y='Model',
            orientation='h',
            title='Mean Reciprocal Rank (MRR) Comparison',
            text='MRR'
        )
        
        fig.update_traces(texttemplate='%{text:.3f}', textposition='outside')
        fig.update_layout(
            xaxis=dict(range=[0, 1], title='MRR'),
            yaxis=dict(categoryorder='total ascending'),
            height=500
        )
        
        chart_path = os.path.join(self.performance_charts_path, "mrr_comparison_interactive.html")
        fig.write_html(chart_path)

    def _generate_speed_accuracy_chart(self, results: Dict) -> None:
        """
        Generate Speed vs Accuracy scatter plot
        
        Args:
            results: Evaluation results dictionary
        """
        # Extract data
        models_data = []
        
        for model_name, model_results in results.get("results", {}).items():
            if "metrics" in model_results and "model_info" in model_results:
                # Get accuracy metric (average of all hit rates and MRR)
                metrics = model_results["metrics"]
                accuracy = (metrics.get("avg_hit_rate", 0) + metrics.get("avg_mrr", 0)) / 2
                
                # Get speed metric (inverse of total time)
                total_time = model_results.get("embedding_time", 0) + model_results.get("search_time", 0)
                queries = model_results.get("total_queries", 1)
                speed = queries / total_time if total_time > 0 else 0
                
                models_data.append({
                    'Model': model_name,
                    'Accuracy': accuracy,
                    'Speed (queries/sec)': speed,
                    'Memory (GB)': model_results.get("model_info", {}).get("memory_usage", 0),
                    'Size': abs(model_results.get("model_info", {}).get("memory_usage", 0) * 50)  # Bubble size
                })
        
        if not models_data:
            logger.warning("No data available for speed-accuracy chart")
            return
            
        # Convert to DataFrame
        df = pd.DataFrame(models_data)
        
        # Create plot
        plt.figure(figsize=(12, 8))
        ax = sns.scatterplot(x='Speed (queries/sec)', y='Accuracy', size='Memory (GB)', 
                            hue='Model', sizes=(100, 500), data=df, alpha=0.7)
        
        plt.title('Speed vs Accuracy Comparison', fontsize=14, fontweight='bold')
        plt.grid(True, linestyle='--', alpha=0.6)
        plt.tight_layout()
        
        # Save chart
        chart_path = os.path.join(self.performance_charts_path, "speed_accuracy_comparison.png")
        plt.savefig(chart_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        # Create interactive Plotly chart with bubble chart
        fig = px.scatter(
            df,
            x='Speed (queries/sec)',
            y='Accuracy',
            size='Size',
            color='Model',
            hover_name='Model',
            hover_data=['Memory (GB)'],
            title='Speed vs Accuracy Comparison',
            size_max=60
        )
        
        fig.update_layout(
            xaxis_title='Speed (queries/sec)',
            yaxis_title='Accuracy (combined metric)',
            height=600
        )
        
        chart_path = os.path.join(self.performance_charts_path, "speed_accuracy_comparison_interactive.html")
        fig.write_html(chart_path)

    def _generate_memory_usage_chart(self, results: Dict) -> None:
        """
        Generate Memory usage comparison chart
        
        Args:
            results: Evaluation results dictionary
        """
        # Extract memory usage data
        memory_data = []
        
        for model_name, model_results in results.get("results", {}).items():
            if "model_info" in model_results:
                memory_usage = model_results["model_info"].get("memory_usage", 0)
                model_size = model_results["model_info"].get("model_size", 0)
                
                memory_data.append({
                    'Model': model_name,
                    'Memory Usage (GB)': memory_usage,
                    'Model Size': model_size
                })
        
        if not memory_data:
            logger.warning("No memory usage data found")
            return
            
        # Convert to DataFrame
        df = pd.DataFrame(memory_data)
        
        # Create plot
        plt.figure(figsize=(10, 6))
        ax = sns.barplot(x='Memory Usage (GB)', y='Model', data=df)
        
        # Add value labels on bars
        for i, (_, row) in enumerate(df.iterrows()):
            ax.text(row['Memory Usage (GB)'] + 0.05, i, 
                   f"{row['Memory Usage (GB)']:.2f} GB", va='center')
        
        plt.title('GPU Memory Usage Comparison', fontsize=14, fontweight='bold')
        plt.tight_layout()
        
        # Save chart
        chart_path = os.path.join(self.performance_charts_path, "memory_usage_comparison.png")
        plt.savefig(chart_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        # Also create interactive Plotly chart
        fig = px.bar(
            df,
            x='Memory Usage (GB)',
            y='Model',
            orientation='h',
            title='GPU Memory Usage Comparison',
            text='Memory Usage (GB)'
        )
        
        fig.update_traces(texttemplate='%{text} GB', textposition='outside')
        fig.update_layout(
            yaxis=dict(categoryorder='total ascending'),
            height=500
        )
        
        chart_path = os.path.join(self.performance_charts_path, "memory_usage_comparison_interactive.html")
        fig.write_html(chart_path)

    def _generate_performance_heatmap(self, metric_data: Dict) -> None:
        """
        Generate performance heatmap for all metrics
        
        Args:
            metric_data: Dictionary with metrics for all models
        """
        # Filter relevant metrics
        relevant_metrics = ['hit_rate@1', 'hit_rate@3', 'hit_rate@5', 'hit_rate@10',
                          'mrr', 'precision@5', 'recall@5', 'ndcg@10']
        
        # Extract relevant metrics
        heatmap_data = []
        for metric in relevant_metrics:
            if metric in metric_data:
                for model, value in metric_data[metric].items():
                    heatmap_data.append({
                        'Model': model,
                        'Metric': metric,
                        'Value': value
                    })
        
        if not heatmap_data:
            logger.warning("No data available for heatmap")
            return
            
        # Convert to DataFrame
        df = pd.DataFrame(heatmap_data)
        
        # Pivot for heatmap
        heatmap_df = df.pivot(index='Model', columns='Metric', values='Value')
        
        # Create plot
        plt.figure(figsize=(12, 8))
        sns.heatmap(heatmap_df, annot=True, cmap='YlGnBu', fmt='.3f', linewidths=0.5)
        
        plt.title('Model Performance Heatmap', fontsize=14, fontweight='bold')
        plt.tight_layout()
        
        # Save chart
        chart_path = os.path.join(self.performance_charts_path, "performance_heatmap.png")
        plt.savefig(chart_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        # Create interactive Plotly heatmap
        fig = px.imshow(
            heatmap_df,
            labels=dict(x="Metric", y="Model", color="Value"),
            color_continuous_scale='YlGnBu',
            title='Model Performance Heatmap',
            aspect="auto"
        )
        
        fig.update_layout(height=600)
        
        chart_path = os.path.join(self.performance_charts_path, "performance_heatmap_interactive.html")
        fig.write_html(chart_path)

    def _generate_ranking_chart(self, metric_data: Dict, overall_ranking: List) -> None:
        """
        Generate overall ranking chart based on weighted scores
        
        Args:
            metric_data: Dictionary with metrics for all models
            overall_ranking: List of model names in ranking order
        """
        # Get overall scores (from metric data)
        overall_scores = {}
        
        # Extract relevant metrics for overall score
        accuracy_metrics = ['avg_hit_rate', 'avg_mrr', 'avg_ndcg', 'avg_precision']
        
        for model in metric_data[list(metric_data.keys())[0]].keys():
            if model in metric_data:
                relevant_values = []
                for metric in accuracy_metrics:
                    if metric in metric_data and model in metric_data[metric]:
                        relevant_values.append(metric_data[metric][model])
                
                if relevant_values:
                    overall_scores[model] = sum(relevant_values) / len(relevant_values)
        
        if not overall_scores:
            logger.warning("No data available for ranking chart")
            return
            
        # Create DataFrame with ranking
        ranking_df = pd.DataFrame({
            'Model': overall_ranking,
            'Rank': range(1, len(overall_ranking) + 1),
            'Score': [overall_scores.get(model, 0) for model in overall_ranking]
        })
        
        # Create plot
        plt.figure(figsize=(10, 6))
        ax = sns.barplot(x='Rank', y='Score', hue='Model', data=ranking_df, dodge=False)
        
        # Add value labels on bars
        for i, (_, row) in enumerate(ranking_df.iterrows()):
            ax.text(i, row['Score'] + 0.01, f"{row['Score']:.3f}", ha="center")
        
        plt.title('Overall Model Ranking', fontsize=14, fontweight='bold')
        plt.ylim(0, 1.0)
        plt.legend(loc='upper right')
        plt.tight_layout()
        
        # Save chart
        chart_path = os.path.join(self.performance_charts_path, "overall_ranking.png")
        plt.savefig(chart_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        # Create interactive Plotly chart
        fig = px.bar(
            ranking_df,
            x='Rank',
            y='Score',
            color='Model',
            title='Overall Model Ranking',
            text='Score'
        )
        
        fig.update_traces(texttemplate='%{text:.3f}', textposition='outside')
        fig.update_layout(
            yaxis=dict(range=[0, 1], title='Combined Score'),
            xaxis_title='Ranking',
            height=500
        )
        
        chart_path = os.path.join(self.performance_charts_path, "overall_ranking_interactive.html")
        fig.write_html(chart_path)

    def create_model_ranking_table(self, results: Dict) -> pd.DataFrame:
        """
        Create model ranking table with weighted scoring
        
        Args:
            results: Evaluation results dictionary
            
        Returns:
            DataFrame with model rankings
        """
        # Define evaluation weights
        EVALUATION_WEIGHTS = {
            'hit_rate_5': 0.20,      # Hit Rate@5
            'mrr': 0.20,             # Mean Reciprocal Rank
            'embedding_speed': 0.15,  # Tokens per second
            'search_speed': 0.15,     # Query response time
            'memory_efficiency': 0.10, # GPU memory usage
            'vietnamese_performance': 0.10, # Vietnamese-specific test
            'model_size': 0.05,       # Storage requirements
            'stability': 0.05         # Error rate & consistency
        }
        
        # Extract results data
        ranking_data = []
        
        for model_name, model_results in results.get("results", {}).items():
            if "metrics" in model_results and "model_info" in model_results:
                metrics = model_results["metrics"]
                model_info = model_results["model_info"]
                
                # Normalize metrics to 0-1 scale
                hit_rate_5 = metrics.get("hit_rate@5", 0)
                mrr = metrics.get("mrr", 0)
                
                # Get speed metrics (normalize to 0-1, higher is better)
                total_queries = model_results.get("total_queries", 1)
                embedding_time = model_results.get("embedding_time", 1)
                search_time = model_results.get("search_time", 1)
                
                # Calculate speeds (queries per second)
                embedding_speed = total_queries / embedding_time if embedding_time > 0 else 0
                search_speed = total_queries / search_time if search_time > 0 else 0
                
                # Normalize speeds (using a reasonable max value)
                embedding_speed_norm = min(embedding_speed / 100, 1.0)  # Assume max 100 qps is best
                search_speed_norm = min(search_speed / 1000, 1.0)  # Assume max 1000 qps is best
                
                # Memory efficiency (inverse of memory usage, higher is better)
                memory_usage = model_info.get("memory_usage", 8)  # Assume 8 GB as reference
                memory_efficiency = max(0, 1 - (memory_usage - 1) / 8)  # Normalize to 0-1
                
                # Vietnamese performance - use a custom metric or assume same as hit rate
                vietnamese_performance = metrics.get("hit_rate@5", 0)
                
                # Model size efficiency (inverse of size, higher is better)
                model_size = model_info.get("model_size", 1)  # Normalize to 0.1-1 range
                model_size_efficiency = max(0.1, 1 / (model_size + 1))
                
                # Stability - assume perfect for now, but could be error rate
                stability = 1.0
                
                # Calculate weighted score
                weighted_score = (
                    hit_rate_5 * EVALUATION_WEIGHTS['hit_rate_5'] +
                    mrr * EVALUATION_WEIGHTS['mrr'] +
                    embedding_speed_norm * EVALUATION_WEIGHTS['embedding_speed'] +
                    search_speed_norm * EVALUATION_WEIGHTS['search_speed'] +
                    memory_efficiency * EVALUATION_WEIGHTS['memory_efficiency'] +
                    vietnamese_performance * EVALUATION_WEIGHTS['vietnamese_performance'] +
                    model_size_efficiency * EVALUATION_WEIGHTS['model_size'] +
                    stability * EVALUATION_WEIGHTS['stability']
                )
                
                ranking_data.append({
                    'Model': model_name,
                    'Hit Rate@5': hit_rate_5,
                    'MRR': mrr,
                    'Embedding Speed (norm)': embedding_speed_norm,
                    'Search Speed (norm)': search_speed_norm,
                    'Memory Efficiency': memory_efficiency,
                    'Vietnamese Performance': vietnamese_performance,
                    'Model Size Efficiency': model_size_efficiency,
                    'Stability': stability,
                    'Weighted Score': weighted_score
                })
        
        if not ranking_data:
            logger.warning("No data available for ranking table")
            return pd.DataFrame()
        
        # Convert to DataFrame and sort by weighted score
        ranking_df = pd.DataFrame(ranking_data)
        ranking_df = ranking_df.sort_values('Weighted Score', ascending=False)
        ranking_df['Rank'] = range(1, len(ranking_df) + 1)
        ranking_df = ranking_df.set_index('Rank')
        
        # Save table to CSV
        csv_path = os.path.join(self.output_path, "model_ranking.csv")
        ranking_df.to_csv(csv_path)
        
        # Create styled HTML table
        html_path = os.path.join(self.output_path, "model_ranking.html")
        
        # Round values for display
        display_df = ranking_df.copy()
        for col in display_df.columns:
            if display_df[col].dtype == 'float64':
                display_df[col] = display_df[col].round(3)
        
        # Create HTML table with styling
        styled_table = display_df.style.background_gradient(cmap='YlGnBu').set_precision(3)
        styled_table.to_html(html_path)
        
        return ranking_df

    def export_final_report(self, results: Dict) -> None:
        """
        Create final report in multiple formats
        
        Args:
            results: Evaluation results dictionary
        """
        if not results:
            logger.warning("No results to export")
            return
            
        # Generate all visualizations
        self.generate_comparison_charts(results)
        
        # Create model ranking table
        ranking_df = self.create_model_ranking_table(results)
        
        # Export as JSON
        json_path = os.path.join(self.output_path, "model_comparison_report.json")
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # Create Markdown report
        self._create_markdown_report(results, ranking_df)
        
        # Create HTML report
        self._create_html_report(results, ranking_df)
        
        logger.info(f"Final report exported to {self.output_path}")

    def _create_markdown_report(self, results: Dict, ranking_df: pd.DataFrame) -> None:
        """
        Create Markdown report
        
        Args:
            results: Evaluation results dictionary
            ranking_df: Model ranking DataFrame
        """
        # Get top models
        top_models = []
        for model_name in ranking_df.index[:3]:
            model_info = ranking_df.loc[model_name]
            top_models.append({
                'name': model_name,
                'score': model_info['Weighted Score']
            })
        
        # Create Markdown content
        md_content = f"""# Vietnamese Embedding Model Evaluation Report

## Executive Summary

This report presents the results of the Vietnamese embedding model evaluation conducted on {len(results['results'])} models. The evaluation focused on measuring retrieval effectiveness using metrics such as Hit Rate and Mean Reciprocal Rank (MRR).

## Recommended Models

Based on the weighted scoring system, the following models are recommended for production use:

1. **{top_models[0]['name']}** - Score: {top_models[0]['score']:.3f}
2. **{top_models[1]['name']}** - Score: {top_models[1]['score']:.3f}
3. **{top_models[2]['name']}** - Score: {top_models[2]['score']:.3f}

## Evaluation Methodology

### Metrics Used

- **Hit Rate@K**: Proportion of queries with at least one relevant document in top K results
- **Mean Reciprocal Rank (MRR)**: Average of reciprocal ranks of first relevant document
- **Normalised Discounted Cumulative Gain (NDCG@K)**: Measures ranking quality considering position
- **Precision@K & Recall@K**: Traditional information retrieval metrics
- **Speed**: Embedding generation and search speed (queries/second)
- **Memory Usage**: GPU memory required for model operation

### Weighted Scoring System

```
"""
        
        # Add scoring weights table
        EVALUATION_WEIGHTS = {
            'hit_rate_5': 0.20,
            'mrr': 0.20,
            'embedding_speed': 0.15,
            'search_speed': 0.15,
            'memory_efficiency': 0.10,
            'vietnamese_performance': 0.10,
            'model_size': 0.05,
            'stability': 0.05
        }
        
        md_content += "| Metric | Weight |\n"
        md_content += "|--------|--------|\n"
        for metric, weight in EVALUATION_WEIGHTS.items():
            md_content += f"| {metric.replace('_', ' ').title()} | {weight*100:.0f}% |\n"
        
        md_content += f"""\n

## Detailed Results

### Overall Model Ranking

{ranking_df.to_markdown()}

### Performance Charts

Charts are available in the `performance_charts/` subdirectory:

- Hit Rate@K Comparison
- MRR Comparison
- Speed vs Accuracy Comparison
- Memory Usage Comparison
- Performance Heatmap
- Overall Ranking

## Model-Specific Details

"""
        
        # Add details for each model
        for model_name, model_results in results.get("results", {}).items():
            if "metrics" not in model_results or "model_info" not in model_results:
                continue
                
            metrics = model_results["metrics"]
            model_info = model_results["model_info"]
            
            md_content += f"""### {model_name}

**Model ID:** {model_info.get('model_id', 'N/A')}

**Provider:** {model_info.get('provider', 'N/A')}

**Model Size:** {model_info.get('memory_usage', 'N/A')} GB

**Key Metrics:**
- Average Hit Rate: {metrics.get('avg_hit_rate', 0):.3f}
- MRR: {metrics.get('mrr', 0):.3f}
- Average NDCG: {metrics.get('avg_ndcg', 0):.3f}
- Embedding Time: {model_results.get('embedding_time', 0):.2f}s
- Search Time: {model_results.get('search_time', 0):.2f}s

"""
        
        # Add conclusion and recommendations
        md_content += """## Conclusion and Recommendations

### Primary Recommendation

Based on the comprehensive evaluation, **[TOP MODEL]** is recommended as the primary embedding model for Vietnamese text processing. This model demonstrated the best balance of accuracy, speed, and resource efficiency.

### Fallback Recommendation

**[SECOND TOP MODEL]** is recommended as a fallback option, particularly in scenarios where computational resources are limited.

### Implementation Considerations

1. **Hardware Requirements**: Ensure sufficient GPU memory for the selected models
2. **Performance Monitoring**: Monitor query latency and memory usage in production
3. **Model Updates**: Schedule regular evaluations to compare with new models
4. **Data Specificity**: Consider fine-tuning the selected model for specific domains if needed

### Next Steps

1. Deploy the recommended models in a staging environment
2. Conduct user acceptance testing with representative queries
3. Implement A/B testing to validate performance improvements
4. Establish monitoring and alerting for performance metrics

---

*Report generated on: [DATE]*  
*Evaluation version: 1.0.0*
"""
        
        # Save Markdown report
        md_path = os.path.join(self.output_path, "final_recommendation.md")
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(md_content)

    def _create_html_report(self, results: Dict, ranking_df: pd.DataFrame) -> None:
        """
        Create HTML report with embedded visualizations
        
        Args:
            results: Evaluation results dictionary
            ranking_df: Model ranking DataFrame
        """
        # This is a simplified version - in practice, you would use Jinja2 templates
        
        # Read in markdown report
        md_path = os.path.join(self.output_path, "final_recommendation.md")
        
        try:
            with open(md_path, 'r', encoding='utf-8') as f:
                md_content = f.read()
        except FileNotFoundError:
            # If markdown report doesn't exist, create a basic one
            self._create_markdown_report(results, ranking_df)
            with open(md_path, 'r', encoding='utf-8') as f:
                md_content = f.read()
        
        # Convert markdown to HTML (simplified approach)
        html_content = f"""<!DOCTYPE html>
<html>
<head>
    <title>Vietnamese Embedding Model Evaluation Report</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/5.1.0/github-markdown.min.css">
    <style>
        body {{
            box-sizing: border-box;
            max-width: 980px;
            margin: 0 auto;
            padding: 45px;
        }}
        .markdown-body {{
            box-sizing: border-box;
            min-width: 200px;
            max-width: 980px;
            margin: 0 auto;
            padding: 45px;
        }}
        h1 {{
            border-bottom: 1px solid #eaecef;
            padding-bottom: 0.3em;
        }}
        h2 {{
            border-bottom: 1px solid #eaecef;
            padding-bottom: 0.3em;
        }}
        table {{
            border-collapse: collapse;
            border-spacing: 0;
        }}
        table th {{
            background-color: #f6f8fa;
            border: 1px solid #d0d7de;
            padding: 6px 13px;
            text-align: left;
        }}
        table td {{
            border: 1px solid #d0d7de;
            padding: 6px 13px;
        }}
        table tr:nth-child(even) {{
            background-color: #f6f8fa;
        }}
        img {{
            max-width: 100%;
            box-sizing: border-box;
            background-color: #fff;
        }}
        .chart-container {{
            margin: 20px 0;
            text-align: center;
        }}
        iframe {{
            border: none;
            width: 100%;
            height: 500px;
        }}
    </style>
</head>
<body class="markdown-body>
    <div>
"""
        
        # Convert basic markdown to HTML (simplified)
        html_content += md_content.replace('## ', '<h2>')
        html_content = html_content.replace('### ', '<h3>')
        html_content = html_content.replace('**', '<strong>').replace('**', '</strong>')
        html_content = html_content.replace('\n\n', '</p><p>')
        html_content = '<p>' + html_content + '</p>'
        
        # Add performance charts
        html_content += """
        <h2>Performance Charts</h2>
        
        <div class="chart-container">
            <h3>Hit Rate@K Comparison</h3>
            <iframe src="performance_charts/hit_rate_comparison_interactive.html"></iframe>
        </div>
        
        <div class="chart-container">
            <h3>Mean Reciprocal Rank (MRR) Comparison</h3>
            <iframe src="performance_charts/mrr_comparison_interactive.html"></iframe>
        </div>
        
        <div class="chart-container">
            <h3>Speed vs Accuracy Comparison</h3>
            <iframe src="performance_charts/speed_accuracy_comparison_interactive.html"></iframe>
        </div>
        
        <div class="chart-container">
            <h3>GPU Memory Usage Comparison</h3>
            <iframe src="performance_charts/memory_usage_comparison_interactive.html"></iframe>
        </div>
        
        <div class="chart-container">
            <h3>Model Performance Heatmap</h3>
            <iframe src="performance_charts/performance_heatmap_interactive.html"></iframe>
        </div>
        
        <div class="chart-container">
            <h3>Overall Model Ranking</h3>
            <iframe src="performance_charts/overall_ranking_interactive.html"></iframe>
        </div>
    </div>
</body>
</html>
"""
        
        # Save HTML report
        html_path = os.path.join(self.output_path, "final_recommendation.html")
        with open(html_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        return html_path
```

---

### 3. Scripts

#### `scripts/run_evaluation.py`
```python
#!/usr/bin/env python3
"""
Main evaluation runner script
Usage: python scripts/run_evaluation.py --config configs/models.json --gpu-config configs/gpu_settings.json --output reports/
"""

import os
import sys
import json
import time
import logging
import argparse
from pathlib import Path
from datetime import datetime
import typer
from dotenv import load_dotenv

# Add src directory to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

# Import modules
from data_processor import VietnameseTextProcessor
from embedding_manager import EmbeddingManager
from evaluator import ModelEvaluator
from visualizer import ReportGenerator

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('evaluation.log')
    ]
)
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()

app = typer.Typer(help="Run Vietnamese embedding model evaluation pipeline")

def main(
    config_path: Path = typer.Option("configs/models.json", help="Path to models config"),
    eval_config_path: Path = typer.Option("configs/evaluation_settings.json", help="Path to evaluation settings config"),
    gpu_config_path: Path = typer.Option("configs/gpu_settings.json", help="Path to GPU config"),
    data_path: Path = typer.Option("data/", help="Path to data directory"),
    output_path: Path = typer.Option("reports/", help="Output directory"),
    report_name: str = typer.Option(f"evaluation-{datetime.now().strftime('%Y%m%d-%H%M%S')}", help="Report name"),
    gpu_enabled: bool = typer.Option(True, help="Enable GPU acceleration"),
    models_to_evaluate: str = typer.Option("all", help="Comma-separated list of model names to evaluate, or 'all'"),
    skip_benchmarking: bool = typer.Option(False, help="Skip performance benchmarking"),
    load_cache: bool = typer.Option(True, help="Load cached embeddings if available"),
    verbose: bool = typer.Option(False, help="Verbose logging")
):
    """
    Run complete embedding model evaluation pipeline
    
    Steps executed:
    1. Load and validate configuration
    2. Prepare test data
    3. Initialize models
    4. Run evaluation
    5. Generate reports
    """
    
    # Set up logging level
    if verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    logger.info("Starting Vietnamese embedding model evaluation pipeline")
    
    # Create output directory
    output_path = os.path.join(output_path, report_name)
    os.makedirs(output_path, exist_ok=True)
    
    # Start timer for overall execution
    start_time = time.time()
    
    try:
        # Step 1: Load configurations
        logger.info(f"Loading configuration from {config_path}")
        
        # Update GPU settings if GPU is disabled
        if not gpu_enabled:
            with open(gpu_config_path, 'r') as f:
                gpu_config = json.load(f)
            gpu_config["gpu_settings"]["enabled"] = False
            
            # Write modified config
            modified_gpu_path = os.path.join(output_path, "gpu_settings_modified.json")
            with open(modified_gpu_path, 'w') as f:
                json.dump(gpu_config, f, indent=2)
            
            gpu_config_path = modified_gpu_path
        
        # Load evaluation settings
        with open(eval_config_path, 'r') as f:
            evaluation_config = json.load(f)
        
        # Step 2: Initialize data processor
        logger.info("Initializing Vietnamese text processor")
        data_processor = VietnameseTextProcessor()
        
        # Step 3: Initialize embedding manager
        logger.info("Initializing embedding manager")
        
        # Override models to evaluate if specified
        if models_to_evaluate != "all":
            with open(config_path, 'r') as f:
                config = json.load(f)
            
            requested_models = [m.strip() for m in models_to_evaluate.split(',')]
            
            # Filter models
            original_models = config["models"]
            filtered_models = [m for m in original_models if m["name"] in requested_models]
            
            if not filtered_models:
                logger.error(f"None of the requested models {requested_models} found in config")
                raise ValueError(f"Invalid model names: {requested_models}")
            
            config["models"] = filtered_models
            
            # Write modified config
            modified_config_path = os.path.join(output_path, "models_modified.json")
            with open(modified_config_path, 'w') as f:
                json.dump(config, f, indent=2)
            
            config_path = modified_config_path
        
        embedding_manager = EmbeddingManager(
            config_path=config_path,
            gpu_config_path=gpu_config_path
        )
        
        # Step 4: Load all models
        logger.info("Loading models...")
        models_status = embedding_manager.load_all_models()
        
        # Check if models loaded successfully
        loaded_models = [name for name, status in models_status.items() if status.get("success", False)]
        
        if not loaded_models:
            logger.error("No models loaded successfully")
            raise RuntimeError
