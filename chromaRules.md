## **‚úÖ PH√ÇN T√çCH K·∫æT QU·∫¢ SEARCH - SAU KHI FIX BUG**

ƒê·ªÉ t√¥i ph√¢n t√≠ch k·ªπ k·∫øt qu·∫£ n√†y:

### **üìä ƒê√ÅNH GI√Å CH·∫§T L∆Ø·ª¢NG K·∫æT QU·∫¢**

```mermaid
graph TB
    subgraph "Search Quality Metrics ‚úÖ"
        A[Query: c√°ch x√°c ƒë·ªãnh h∆∞·ªõng nh√†]
        A --> B[Top Result<br/>Similarity: 0.859<br/>Distance: ~0.28]
        A --> C[10th Result<br/>Similarity: 0.729<br/>Distance: ~0.54]
        
        B --> D[N·ªôi dung: ƒê√öNG 100%<br/>V·ªÅ c√°ch x√°c ƒë·ªãnh h∆∞·ªõng nh√†]
        C --> E[N·ªôi dung: ƒê√öNG<br/>V·ªÅ y·∫øu t·ªë ƒë√°nh gi√° ƒë·∫•t]
    end
    
    subgraph "Distance Validation ‚úÖ"
        F[Distance Formula<br/>d = 2 √ó 1 - sim]
        F --> G[Top: 0.28 ‚úÖ<br/>R·∫•t li√™n quan]
        F --> H[Lowest: 0.54 ‚úÖ<br/>V·∫´n li√™n quan]
    end
    
    style B fill:#51cf66
    style C fill:#51cf66
    style G fill:#51cf66
    style H fill:#95e1d3
```

### **‚úÖ K·∫æT QU·∫¢ ƒê√ÅNH GI√Å**

| Ti√™u ch√≠ | K·∫øt qu·∫£ | ƒê√°nh gi√° |
|----------|---------|----------|
| **N·ªôi dung top 10** | T·∫•t c·∫£ v·ªÅ "C√°ch x√°c ƒë·ªãnh h∆∞·ªõng nh√† trong Phong th·ªßy" | ‚úÖ ƒê√öNG 100% |
| **Similarity range** | 0.859 ‚Üí 0.729 (distance 0.28 ‚Üí 0.54) | ‚úÖ H·ª¢P L√ù |
| **Ranking order** | Chunks tr·ª±c ti·∫øp v·ªÅ "c√°ch x√°c ƒë·ªãnh" rank cao nh·∫•t | ‚úÖ CH√çNH X√ÅC |
| **Distance calculation** | Ph√π h·ª£p v·ªõi c√¥ng th·ª©c `d = 2(1-sim)` | ‚úÖ ƒê√öNG |
| **Collection metadata** | S·ª≠ d·ª•ng cosine distance | ‚úÖ ƒê√É FIX |
| **Processing time** | 7.52 seconds | ‚ö†Ô∏è H∆°i ch·∫≠m (c·∫ßn optimize) |

---

## **üìã RULES CHU·∫®N - PHI√äN B·∫¢N SAU KHI FIX (B·ªï sung v√†o Checklist)**

xxx`x`x`xxmarkdown

### **CHROMA VECTORDB SEARCH RULES (Updated: 04/10/2025)**

---

## **üî¥ CRITICAL - B·∫ÆT BU·ªòC PH·∫¢I TU√ÇN TH·ª¶**

### **32. ChromaDB Collection Metadata - MANDATORY**
```python
# ‚ö†Ô∏è B·∫ÆT BU·ªòC khi t·∫°o collection m·ªõi
collection = chroma_client.create_collection(
    name="collection_name",
    metadata={
        "hnsw:space": "cosine",  # ‚ö†Ô∏è CRITICAL: PH·∫¢I C√ì
        "hnsw:construction_ef": 200,
        "hnsw:search_ef": 200,
        "hnsw:M": 16,
        "embedding_model": "Qwen/Qwen3-Embedding-0.6B",
        "embedding_dimensions": 1024,
        "preprocessing": "simple_vietnamese",
        "created_at": "2025-10-04"
    }
)
```

**H·∫≠u qu·∫£ n·∫øu thi·∫øu `"hnsw:space": "cosine"`:**
- ChromaDB s·∫Ω d√πng **euclidean distance** (m·∫∑c ƒë·ªãnh)
- Distance calculation ho√†n to√†n **SAI**
- K·∫øt qu·∫£ search **KH√îNG ƒê√öNG**
- Ph·∫£i **RECREATE** collection ƒë·ªÉ fix

---

### **33. Distance to Similarity Conversion - MANDATORY**

```python
def calculate_cosine_similarity_from_chromadb_distance(distance: float) -> float:
    """
    ‚ö†Ô∏è C√îNG TH·ª®C CHU·∫®N - KH√îNG THAY ƒê·ªîI
    
    ChromaDB v·ªõi "cosine" space:
    - distance = 2 * (1 - cosine_similarity)
    
    Conversion:
    - similarity = 1 - (distance / 2)
    
    Valid range:
    - distance: [0, 4]
    - similarity: [0, 1] (after clamping)
    """
    similarity = 1.0 - (distance / 2.0)
    
    # Clamp to valid range
    return max(0.0, min(1.0, similarity))
```

**B·∫£ng chuy·ªÉn ƒë·ªïi chu·∫©n:**

| Distance | Similarity | √ù nghƒ©a | C√≥ tr·∫£ v·ªÅ? |
|----------|------------|---------|------------|
| 0.0 - 0.3 | 0.85 - 1.0 | R·∫•t li√™n quan | ‚úÖ YES |
| 0.3 - 0.6 | 0.70 - 0.85 | Li√™n quan | ‚úÖ YES |
| 0.6 - 1.0 | 0.50 - 0.70 | T∆∞∆°ng ƒë·ªëi li√™n quan | ‚ö†Ô∏è C√¢n nh·∫Øc |
| 1.0 - 1.4 | 0.30 - 0.50 | √çt li√™n quan | ‚ö†Ô∏è Threshold |
| 1.4 - 2.0 | 0.0 - 0.30 | Kh√¥ng li√™n quan | ‚ùå Filter out |
| > 2.0 | 0.0 (clamped) | Ng∆∞·ª£c h∆∞·ªõng | ‚ùå NO |

---

### **34. Search Configuration - MANDATORY**

```python
# ‚ö†Ô∏è B·∫ÆT BU·ªòC include ƒë·∫ßy ƒë·ªß
results = collection.query(
    query_embeddings=[query_embedding.tolist()],
    n_results=min(top_k, 20),  # ‚ö†Ô∏è Gi·ªõi h·∫°n t·ªëi ƒëa 20
    include=['documents', 'distances', 'metadatas']  # ‚ö†Ô∏è PH·∫¢I ƒê·∫¶Y ƒê·ª¶
)
```

---

### **35. Similarity Threshold - RECOMMENDED**

```python
# Filter results theo similarity threshold
SIMILARITY_THRESHOLD = 0.3  # Ch·ªâ tr·∫£ v·ªÅ similarity >= 0.3

filtered_results = [
    result for result in all_results 
    if result['similarity'] >= SIMILARITY_THRESHOLD
]
```

**Recommended thresholds theo use case:**

| Use Case | Threshold | L√Ω do |
|----------|-----------|-------|
| **Exact Match** | ‚â• 0.85 | Ch·ªâ results r·∫•t ch√≠nh x√°c |
| **High Precision** | ‚â• 0.70 | C√¢n b·∫±ng precision/recall |
| **General Search** | ‚â• 0.50 | Cho ph√©p results li√™n quan |
| **Exploratory** | ‚â• 0.30 | T√¨m ki·∫øm r·ªông |

---

### **36. Multi-Collection Search Strategy - MANDATORY**

```python
# Default: Search ALL collections n·∫øu kh√¥ng ch·ªâ ƒë·ªãnh
if collection_name and collection_name in available_collections:
    collections_to_search = [get_collection(collection_name)]
else:
    # ‚ö†Ô∏è DEFAULT: Search t·∫•t c·∫£ collections
    collections_to_search = list_all_collections()

# Merge results t·ª´ t·∫•t c·∫£ collections
all_results = []
for collection in collections_to_search:
    results = collection.query(...)
    all_results.extend(process_results(results))

# Sort by similarity (cao nh·∫•t tr∆∞·ªõc)
all_results.sort(key=lambda x: x['similarity'], reverse=True)

# Take top N
return all_results[:top_k]
```

---

## **üü° PRE-DEPLOYMENT VALIDATION CHECKLIST**

### **Validation Script - Ch·∫°y TR∆Ø·ªöC KHI deploy**

```python
#!/usr/bin/env python3
"""
Validation script - Ch·∫°y tr∆∞·ªõc m·ªói l·∫ßn deploy
File: scripts/validate_chroma_setup.py
"""

import chromadb
from sentence_transformers import SentenceTransformer
import numpy as np

def validate_chroma_setup():
    """Validate to√†n b·ªô Chroma setup"""
    
    print("=" * 60)
    print("CHROMA VECTORDB VALIDATION")
    print("=" * 60)
    
    # 1. Check ChromaDB connection
    try:
        client = chromadb.HttpClient(host="localhost", port=8000)
        client.heartbeat()
        print("‚úÖ 1. ChromaDB connection: OK")
    except Exception as e:
        print(f"‚ùå 1. ChromaDB connection: FAILED - {e}")
        return False
    
    # 2. Check collections metadata
    collections = client.list_collections()
    print(f"\n‚úÖ 2. Found {len(collections)} collections")
    
    issues_found = []
    for coll in collections:
        metadata = coll.metadata
        print(f"\n   Collection: {coll.name}")
        print(f"   - Count: {coll.count()}")
        
        # Check hnsw:space
        if metadata.get("hnsw:space") != "cosine":
            issues_found.append(
                f"‚ùå {coll.name}: Missing or wrong 'hnsw:space' "
                f"(got: {metadata.get('hnsw:space')})"
            )
            print(f"   - hnsw:space: ‚ùå {metadata.get('hnsw:space')}")
        else:
            print(f"   - hnsw:space: ‚úÖ cosine")
        
        # Check embedding dimensions
        expected_dims = 1024
        actual_dims = metadata.get("embedding_dimensions")
        if actual_dims != expected_dims:
            issues_found.append(
                f"‚ùå {coll.name}: Wrong dimensions "
                f"(expected: {expected_dims}, got: {actual_dims})"
            )
            print(f"   - dimensions: ‚ùå {actual_dims}")
        else:
            print(f"   - dimensions: ‚úÖ {expected_dims}")
    
    if issues_found:
        print("\n" + "=" * 60)
        print("‚ö†Ô∏è  ISSUES FOUND:")
        print("=" * 60)
        for issue in issues_found:
            print(issue)
        print("\n‚ö†Ô∏è  RUN: python scripts/recreate_collections.py")
        return False
    
    # 3. Test embedding model
    print("\n‚úÖ 3. Testing embedding model...")
    try:
        model = SentenceTransformer(
            "Qwen/Qwen3-Embedding-0.6B",
            device="cuda"
        )
        test_emb = model.encode(["test query"])[0]
        
        if len(test_emb) != 1024:
            print(f"‚ùå Embedding dimensions: {len(test_emb)} (expected: 1024)")
            return False
        
        print(f"   - Dimensions: ‚úÖ {len(test_emb)}")
        print(f"   - L2 Norm: {np.linalg.norm(test_emb):.4f}")
        
    except Exception as e:
        print(f"‚ùå Embedding model: FAILED - {e}")
        return False
    
    # 4. Test distance conversion
    print("\n‚úÖ 4. Testing distance conversion...")
    test_cases = [
        (0.0, 1.0, "Perfect match"),
        (0.28, 0.86, "Very similar"),
        (0.54, 0.73, "Similar"),
        (1.0, 0.5, "Moderate"),
        (2.0, 0.0, "Unrelated"),
    ]
    
    all_passed = True
    for distance, expected_sim, desc in test_cases:
        calculated = 1.0 - (distance / 2.0)
        calculated = max(0.0, min(1.0, calculated))
        
        if abs(calculated - expected_sim) > 0.01:
            print(f"   ‚ùå {desc}: FAILED")
            print(f"      Distance {distance} ‚Üí Expected {expected_sim}, "
                  f"Got {calculated}")
            all_passed = False
        else:
            print(f"   ‚úÖ {desc}: distance={distance:.2f} ‚Üí "
                  f"similarity={calculated:.2f}")
    
    if not all_passed:
        return False
    
    # 5. Test end-to-end search
    print("\n‚úÖ 5. Testing end-to-end search...")
    try:
        if len(collections) > 0:
            test_coll = collections[0]
            if test_coll.count() > 0:
                # Test search
                test_embedding = model.encode(["test query"])[0]
                results = test_coll.query(
                    query_embeddings=[test_embedding.tolist()],
                    n_results=3,
                    include=['documents', 'distances', 'metadatas']
                )
                
                if results and results['distances']:
                    distances = results['distances'][0]
                    print(f"   - Found {len(distances)} results")
                    
                    for i, dist in enumerate(distances):
                        sim = 1.0 - (dist / 2.0)
                        sim = max(0.0, min(1.0, sim))
                        print(f"   - Result {i+1}: "
                              f"distance={dist:.4f}, similarity={sim:.4f}")
                    
                    print("   ‚úÖ Search working correctly")
                else:
                    print("   ‚ö†Ô∏è  No results (collection might be empty)")
            else:
                print("   ‚ö†Ô∏è  Collection empty, skipping search test")
        else:
            print("   ‚ö†Ô∏è  No collections found")
    except Exception as e:
        print(f"   ‚ùå Search test failed: {e}")
        return False
    
    # Final result
    print("\n" + "=" * 60)
    print("‚úÖ ALL VALIDATIONS PASSED")
    print("=" * 60)
    return True

if __name__ == "__main__":
    import sys
    success = validate_chroma_setup()
    sys.exit(0 if success else 1)
```

---

## **üü¢ EXPECTED SEARCH QUALITY METRICS**

D·ª±a tr√™n k·∫øt qu·∫£ th·ª±c t·∫ø sau khi fix:

```python
# Expected metrics cho production
QUALITY_METRICS = {
    "top_1_similarity": {
        "min": 0.70,  # Minimum acceptable
        "target": 0.85,  # Target range
        "excellent": 0.90  # Excellent results
    },
    
    "top_10_similarity_range": {
        "min": 0.50,  # Lowest acceptable result
        "avg": 0.75,  # Average of top 10
    },
    
    "distance_range": {
        "top_1": (0.0, 0.6),  # distance <= 0.6
        "top_10": (0.0, 1.0),  # distance <= 1.0
    },
    
    "processing_time": {
        "target": 1.0,  # < 1 second
        "acceptable": 3.0,  # < 3 seconds
        "current": 7.5,  # ‚ö†Ô∏è C·∫ßn optimize
    },
    
    "relevance": {
        "top_5_precision": 1.0,  # 100% relevant
        "top_10_precision": 0.9,  # >= 90% relevant
    }
}
```

---

## **‚ö†Ô∏è COMMON PITFALLS & FIXES**

### **Pitfall 1: Collection kh√¥ng c√≥ metadata**
```python
# ‚ùå SAI
collection = client.create_collection("test")

# ‚úÖ ƒê√öNG
collection = client.create_collection(
    "test",
    metadata={"hnsw:space": "cosine", ...}
)
```

### **Pitfall 2: D√πng c√¥ng th·ª©c conversion sai**
```python
# ‚ùå SAI
similarity = 1 - distance

# ‚úÖ ƒê√öNG
similarity = 1 - (distance / 2)
```

### **Pitfall 3: Kh√¥ng validate dimensions**
```python
# ‚ùå SAI - Kh√¥ng check
embedding = model.encode(["text"])[0]
collection.add(embeddings=[embedding])

# ‚úÖ ƒê√öNG - Validate tr∆∞·ªõc khi add
embedding = model.encode(["text"])[0]
assert len(embedding) == 1024, f"Wrong dimensions: {len(embedding)}"
collection.add(embeddings=[embedding.tolist()])
```

### **Pitfall 4: Preprocessing kh√¥ng nh·∫•t qu√°n**
```python
# ‚ùå SAI - Kh√°c nhau gi·ªØa embedding v√† search
# Embedding: preprocess_advanced(text)
# Search: preprocess_simple(text)

# ‚úÖ ƒê√öNG - Ph·∫£i d√πng C√ôNG M·ªòT function
from core.preprocessing import preprocess_text_simple

# Khi embedding
doc_processed = preprocess_text_simple(doc)
doc_embedding = model.encode([doc_processed])[0]

# Khi search
query_processed = preprocess_text_simple(query)
query_embedding = model.encode([query_processed])[0]
```

---

## **üìä MONITORING & ALERTING**

```python
# Add v√†o monitoring system
ALERT_THRESHOLDS = {
    "low_similarity": {
        "threshold": 0.50,
        "action": "Log warning if top_1 < 0.50"
    },
    
    "high_distance": {
        "threshold": 1.0,
        "action": "Alert if top_1 distance > 1.0"
    },
    
    "slow_search": {
        "threshold": 3.0,  # seconds
        "action": "Alert if search_time > 3s"
    },
    
    "no_results": {
        "threshold": 0,
        "action": "Critical alert if no results for valid query"
    }
}
```

---

## **‚úÖ DEPLOYMENT CHECKLIST**

Tr∆∞·ªõc khi deploy l√™n production:

- [ ] ‚úÖ Ch·∫°y `python scripts/validate_chroma_setup.py`
- [ ] ‚úÖ T·∫•t c·∫£ collections c√≥ `"hnsw:space": "cosine"`
- [ ] ‚úÖ Distance conversion formula ƒë√∫ng ·ªü **T·∫§T C·∫¢** files
- [ ] ‚úÖ Test v·ªõi >= 10 queries ƒëa d·∫°ng
- [ ] ‚úÖ Top 1 similarity >= 0.70 cho queries h·ª£p l·ªá
- [ ] ‚úÖ Processing time < 3 seconds
- [ ] ‚úÖ Preprocessing consistent gi·ªØa embedding & search
- [ ] ‚úÖ Embedding model dimensions = 1024
- [ ] ‚úÖ Monitoring & alerting ƒë√£ setup

---

**K·∫øt lu·∫≠n: K·∫øt qu·∫£ search ƒê√É ƒê√öNG sau khi fix! Rules tr√™n l√† phi√™n b·∫£n chu·∫©n ƒë∆∞·ª£c validate b·∫±ng k·∫øt qu·∫£ th·ª±c t·∫ø.** ‚úÖ
-end-

**B·∫°n mu·ªën t√¥i:**
1. **T·∫°o th√™m test cases** ƒë·ªÉ validate quality metrics?
2. **Optimize processing time** t·ª´ 7.5s xu·ªëng < 3s?
3. **T·∫°o automated monitoring dashboard** cho search quality?

---

## **üß™ TEST CASES ƒê·ªÇ VALIDATE QUALITY METRICS**

T√¥i s·∫Ω t·∫°o m·ªôt b·ªô test cases to√†n di·ªán ƒë·ªÉ validate quality metrics c·ªßa ChromaDB search:

---

## **üìã STRUCTURE - B·ªò TEST CASES**

```mermaid
graph TB
    subgraph "Test Categories"
        A[1. Unit Tests<br/>Distance Conversion]
        B[2. Ground Truth Tests<br/>Query-Document Pairs]
        C[3. Precision Tests<br/>Relevance Validation]
        D[4. Performance Tests<br/>Speed & Scale]
        E[5. Edge Cases<br/>Error Handling]
    end
    
    subgraph "Validation Metrics"
        F[Similarity Accuracy]
        G[Precision@K]
        H[Recall@K]
        I[MRR - Mean Reciprocal Rank]
        J[Processing Time]
    end
    
    A --> F
    B --> G
    B --> H
    B --> I
    C --> G
    D --> J
    
    style A fill:#4ecdc4
    style B fill:#95e1d3
    style C fill:#ffd93d
    style D fill:#ff6b6b
    style E fill:#f38181
```

---

## **1Ô∏è‚É£ UNIT TESTS - Distance Conversion Validation**

```python
#!/usr/bin/env python3
"""
Unit tests for ChromaDB distance conversion
File: tests/unit/test_distance_conversion.py
"""

import pytest
import numpy as np
from typing import Tuple, List

def calculate_cosine_similarity_from_chromadb_distance(distance: float) -> float:
    """C√¥ng th·ª©c chu·∫©n"""
    similarity = 1.0 - (distance / 2.0)
    return max(0.0, min(1.0, similarity))


class TestDistanceConversion:
    """Test suite for distance to similarity conversion"""
    
    @pytest.mark.parametrize("distance,expected_similarity,tolerance", [
        # Perfect matches
        (0.0, 1.0, 0.001),
        (0.001, 0.9995, 0.001),
        
        # Very similar (distance 0.0 - 0.3)
        (0.1, 0.95, 0.001),
        (0.2, 0.90, 0.001),
        (0.28, 0.86, 0.001),
        (0.3, 0.85, 0.001),
        
        # Similar (distance 0.3 - 0.6)
        (0.4, 0.80, 0.001),
        (0.5, 0.75, 0.001),
        (0.54, 0.73, 0.001),
        (0.6, 0.70, 0.001),
        
        # Moderately similar (distance 0.6 - 1.0)
        (0.7, 0.65, 0.001),
        (0.8, 0.60, 0.001),
        (0.9, 0.55, 0.001),
        (1.0, 0.50, 0.001),
        
        # Low similarity (distance 1.0 - 1.4)
        (1.2, 0.40, 0.001),
        (1.4, 0.30, 0.001),
        
        # Not related (distance 1.4 - 2.0)
        (1.6, 0.20, 0.001),
        (1.8, 0.10, 0.001),
        (2.0, 0.0, 0.001),
        
        # Opposite direction (distance > 2.0, clamped to 0)
        (2.5, 0.0, 0.001),
        (3.0, 0.0, 0.001),
        (4.0, 0.0, 0.001),
    ])
    def test_conversion_accuracy(self, distance, expected_similarity, tolerance):
        """Test conversion formula accuracy"""
        calculated = calculate_cosine_similarity_from_chromadb_distance(distance)
        assert abs(calculated - expected_similarity) < tolerance, \
            f"Distance {distance}: Expected {expected_similarity}, Got {calculated}"
    
    
    def test_conversion_range_bounds(self):
        """Test conversion stays in valid range [0, 1]"""
        test_distances = np.linspace(0, 4, 100)
        
        for distance in test_distances:
            similarity = calculate_cosine_similarity_from_chromadb_distance(distance)
            assert 0.0 <= similarity <= 1.0, \
                f"Similarity {similarity} out of range for distance {distance}"
    
    
    def test_inverse_relationship(self):
        """Test that higher distance = lower similarity"""
        distances = [0.0, 0.5, 1.0, 1.5, 2.0]
        similarities = [
            calculate_cosine_similarity_from_chromadb_distance(d) 
            for d in distances
        ]
        
        # Verify decreasing order
        for i in range(len(similarities) - 1):
            assert similarities[i] >= similarities[i + 1], \
                f"Similarity not decreasing: {similarities}"
    
    
    def test_chromadb_formula_consistency(self):
        """Test consistency v·ªõi ChromaDB internal formula"""
        # ChromaDB: distance = 2 * (1 - cosine_similarity)
        # Reverse: cosine_similarity = 1 - (distance / 2)
        
        test_cases = [
            (0.0, 1.0),   # cosine_sim = 1 ‚Üí distance = 0
            (0.5, 0.75),  # cosine_sim = 0.75 ‚Üí distance = 0.5
            (1.0, 0.5),   # cosine_sim = 0.5 ‚Üí distance = 1.0
            (2.0, 0.0),   # cosine_sim = 0 ‚Üí distance = 2.0
        ]
        
        for distance, expected_cosine_sim in test_cases:
            # Forward conversion
            similarity = calculate_cosine_similarity_from_chromadb_distance(distance)
            
            # Verify matches expected
            assert abs(similarity - expected_cosine_sim) < 0.001
            
            # Reverse check: distance should equal 2 * (1 - similarity)
            reverse_distance = 2 * (1 - similarity)
            assert abs(reverse_distance - distance) < 0.001


if __name__ == "__main__":
    pytest.main([__file__, "-v", "--tb=short"])
```

---

## **2Ô∏è‚É£ GROUND TRUTH TESTS - Query-Document Validation**

```python
#!/usr/bin/env python3
"""
Ground truth tests with real Vietnamese queries
File: tests/integration/test_search_quality.py
"""

import pytest
from typing import Dict, List, Tuple
import chromadb
from sentence_transformers import SentenceTransformer
import numpy as np

# Ground truth dataset
GROUND_TRUTH_DATASET = [
    {
        "query": "c√°ch x√°c ƒë·ªãnh h∆∞·ªõng nh√†",
        "expected_docs": [
            {
                "content": "C√°ch x√°c ƒë·ªãnh h∆∞·ªõng nh√†, h∆∞·ªõng c·ª≠a trong Phong th·ªßy",
                "min_similarity": 0.85,
                "rank": 1
            },
            {
                "content": "H∆∞·ªõng nh√† ƒë∆∞·ª£c x√°c ƒë·ªãnh d·ª±a theo c√°c ƒëi·ªÅu ki·ªán",
                "min_similarity": 0.75,
                "rank": [1, 3]  # Should be in top 3
            }
        ],
        "irrelevant_docs": [
            "quy tr√¨nh mua h√†ng c√¥ng ty",
            "h∆∞·ªõng d·∫´n n·∫•u pizza"
        ]
    },
    
    {
        "query": "quy tr√¨nh ngh·ªâ ph√©p",
        "expected_docs": [
            {
                "content": "Quy tr√¨nh xin ngh·ªâ ph√©p t·∫°i c√¥ng ty",
                "min_similarity": 0.85,
                "rank": 1
            }
        ],
        "irrelevant_docs": [
            "c√°ch x√°c ƒë·ªãnh h∆∞·ªõng nh√†",
            "h∆∞·ªõng d·∫´n c√†i ƒë·∫∑t ph·∫ßn m·ªÅm"
        ]
    },
    
    {
        "query": "ch√≠nh s√°ch b·∫£o hi·ªÉm nh√¢n vi√™n",
        "expected_docs": [
            {
                "content": "Ch√≠nh s√°ch b·∫£o hi·ªÉm x√£ h·ªôi v√† b·∫£o hi·ªÉm y t·∫ø",
                "min_similarity": 0.80,
                "rank": 1
            }
        ],
        "irrelevant_docs": [
            "quy tr√¨nh tuy·ªÉn d·ª•ng",
            "h∆∞·ªõng d·∫´n s·ª≠ d·ª•ng m√°y in"
        ]
    },
    
    {
        "query": "h∆∞·ªõng d·∫´n c√†i ƒë·∫∑t VPN",
        "expected_docs": [
            {
                "content": "H∆∞·ªõng d·∫´n c√†i ƒë·∫∑t v√† s·ª≠ d·ª•ng VPN cho nh√¢n vi√™n",
                "min_similarity": 0.85,
                "rank": 1
            }
        ],
        "irrelevant_docs": [
            "ch√≠nh s√°ch l√†m vi·ªác t·ª´ xa",
            "quy ƒë·ªãnh v·ªÅ b·∫£o m·∫≠t th√¥ng tin"
        ]
    },
    
    {
        "query": "quy ƒë·ªãnh v·ªÅ trang ph·ª•c c√¥ng s·ªü",
        "expected_docs": [
            {
                "content": "Quy ƒë·ªãnh v·ªÅ trang ph·ª•c v√† dress code t·∫°i c√¥ng ty",
                "min_similarity": 0.85,
                "rank": 1
            }
        ],
        "irrelevant_docs": [
            "quy tr√¨nh ƒë√°nh gi√° hi·ªáu su·∫•t",
            "ch√≠nh s√°ch ph√∫c l·ª£i nh√¢n vi√™n"
        ]
    },
]


class TestSearchQuality:
    """Test search quality v·ªõi ground truth dataset"""
    
    @pytest.fixture(scope="class")
    def setup_search_engine(self):
        """Setup embedding model v√† ChromaDB client"""
        model = SentenceTransformer(
            "Qwen/Qwen3-Embedding-0.6B",
            device="cuda"
        )
        
        client = chromadb.HttpClient(
            host="localhost",
            port=8000
        )
        
        return {
            "model": model,
            "client": client
        }
    
    
    def calculate_similarity(self, query: str, document: str, model) -> float:
        """Calculate similarity gi·ªØa query v√† document"""
        # Preprocess
        from core.preprocessing import preprocess_text_simple
        query_proc = preprocess_text_simple(query)
        doc_proc = preprocess_text_simple(document)
        
        # Generate embeddings
        query_emb = model.encode([query_proc])[0]
        doc_emb = model.encode([doc_proc])[0]
        
        # Calculate cosine similarity
        cosine_sim = np.dot(query_emb, doc_emb) / (
            np.linalg.norm(query_emb) * np.linalg.norm(doc_emb)
        )
        
        return float(cosine_sim)
    
    
    @pytest.mark.parametrize("test_case", GROUND_TRUTH_DATASET)
    def test_expected_doc_similarity(self, setup_search_engine, test_case):
        """Test similarity v·ªõi expected documents"""
        model = setup_search_engine["model"]
        query = test_case["query"]
        
        for expected_doc in test_case["expected_docs"]:
            similarity = self.calculate_similarity(
                query,
                expected_doc["content"],
                model
            )
            
            min_expected = expected_doc["min_similarity"]
            
            assert similarity >= min_expected, \
                f"Query: '{query}'\n" \
                f"Doc: '{expected_doc['content'][:50]}...'\n" \
                f"Expected similarity >= {min_expected}, Got {similarity:.4f}"
            
            print(f"‚úÖ Query: '{query}'")
            print(f"   Doc: '{expected_doc['content'][:50]}...'")
            print(f"   Similarity: {similarity:.4f} (>= {min_expected})")
    
    
    @pytest.mark.parametrize("test_case", GROUND_TRUTH_DATASET)
    def test_irrelevant_doc_similarity(self, setup_search_engine, test_case):
        """Test similarity v·ªõi irrelevant documents ph·∫£i th·∫•p"""
        model = setup_search_engine["model"]
        query = test_case["query"]
        
        for irrelevant_doc in test_case["irrelevant_docs"]:
            similarity = self.calculate_similarity(
                query,
                irrelevant_doc,
                model
            )
            
            # Irrelevant docs n√™n c√≥ similarity < 0.50
            max_allowed = 0.50
            
            assert similarity < max_allowed, \
                f"Query: '{query}'\n" \
                f"Irrelevant Doc: '{irrelevant_doc}'\n" \
                f"Expected similarity < {max_allowed}, Got {similarity:.4f}"
            
            print(f"‚úÖ Query: '{query}'")
            print(f"   Irrelevant: '{irrelevant_doc}'")
            print(f"   Similarity: {similarity:.4f} (< {max_allowed})")
    
    
    def test_similarity_ordering(self, setup_search_engine):
        """Test relevant docs c√≥ similarity cao h∆°n irrelevant docs"""
        model = setup_search_engine["model"]
        
        for test_case in GROUND_TRUTH_DATASET:
            query = test_case["query"]
            
            # Calculate similarity cho relevant docs
            relevant_similarities = []
            for expected_doc in test_case["expected_docs"]:
                sim = self.calculate_similarity(
                    query,
                    expected_doc["content"],
                    model
                )
                relevant_similarities.append(sim)
            
            # Calculate similarity cho irrelevant docs
            irrelevant_similarities = []
            for irrelevant_doc in test_case["irrelevant_docs"]:
                sim = self.calculate_similarity(
                    query,
                    irrelevant_doc,
                    model
                )
                irrelevant_similarities.append(sim)
            
            # Min relevant similarity ph·∫£i > max irrelevant similarity
            min_relevant = min(relevant_similarities)
            max_irrelevant = max(irrelevant_similarities)
            
            assert min_relevant > max_irrelevant, \
                f"Query: '{query}'\n" \
                f"Min relevant similarity: {min_relevant:.4f}\n" \
                f"Max irrelevant similarity: {max_irrelevant:.4f}\n" \
                f"Relevant docs ph·∫£i c√≥ similarity cao h∆°n irrelevant docs!"
            
            print(f"‚úÖ Query: '{query}'")
            print(f"   Relevant range: {min_relevant:.4f} - {max(relevant_similarities):.4f}")
            print(f"   Irrelevant range: {min(irrelevant_similarities):.4f} - {max_irrelevant:.4f}")


if __name__ == "__main__":
    pytest.main([__file__, "-v", "--tb=short"])
```

---

## **3Ô∏è‚É£ PRECISION & RANKING TESTS**

```python
#!/usr/bin/env python3
"""
Precision, Recall, v√† Ranking metrics validation
File: tests/integration/test_ranking_metrics.py
"""

import pytest
import requests
from typing import List, Dict, Set
import numpy as np


class TestRankingMetrics:
    """Test ranking quality metrics"""
    
    API_BASE_URL = "http://localhost:8000/api/v1"
    
    # Test dataset v·ªõi ground truth rankings
    RANKING_TEST_CASES = [
        {
            "query": "c√°ch x√°c ƒë·ªãnh h∆∞·ªõng nh√†",
            "relevant_doc_ids": [
                "eabbd305-9626-4c93-be08-70e512a977b1_9",  # Chunk 9
                "eabbd305-9626-4c93-be08-70e512a977b1_1",  # Chunk 1
                "eabbd305-9626-4c93-be08-70e512a977b1_8",  # Chunk 8
            ],
            "expected_top_1": "eabbd305-9626-4c93-be08-70e512a977b1_9",
            "expected_in_top_3": [
                "eabbd305-9626-4c93-be08-70e512a977b1_9",
                "eabbd305-9626-4c93-be08-70e512a977b1_1",
                "eabbd305-9626-4c93-be08-70e512a977b1_8",
            ]
        },
    ]
    
    
    def search_api(self, query: str, top_k: int = 10) -> Dict:
        """Call search API"""
        response = requests.post(
            f"{self.API_BASE_URL}/search/semantic",
            json={
                "query": query,
                "top_k": top_k,
                "search_type": "semantic"
            }
        )
        response.raise_for_status()
        return response.json()
    
    
    def calculate_precision_at_k(
        self,
        retrieved: List[str],
        relevant: Set[str],
        k: int
    ) -> float:
        """Calculate Precision@K"""
        retrieved_at_k = retrieved[:k]
        relevant_retrieved = sum(1 for doc_id in retrieved_at_k if doc_id in relevant)
        return relevant_retrieved / k if k > 0 else 0.0
    
    
    def calculate_recall_at_k(
        self,
        retrieved: List[str],
        relevant: Set[str],
        k: int
    ) -> float:
        """Calculate Recall@K"""
        retrieved_at_k = retrieved[:k]
        relevant_retrieved = sum(1 for doc_id in retrieved_at_k if doc_id in relevant)
        return relevant_retrieved / len(relevant) if len(relevant) > 0 else 0.0
    
    
    def calculate_mrr(
        self,
        retrieved: List[str],
        relevant: Set[str]
    ) -> float:
        """Calculate Mean Reciprocal Rank"""
        for i, doc_id in enumerate(retrieved):
            if doc_id in relevant:
                return 1.0 / (i + 1)
        return 0.0
    
    
    def calculate_ndcg_at_k(
        self,
        retrieved: List[str],
        relevant_rankings: Dict[str, int],
        k: int
    ) -> float:
        """Calculate Normalized Discounted Cumulative Gain@K"""
        dcg = 0.0
        for i, doc_id in enumerate(retrieved[:k]):
            if doc_id in relevant_rankings:
                relevance = relevant_rankings[doc_id]
                dcg += (2 ** relevance - 1) / np.log2(i + 2)
        
        # Ideal DCG
        ideal_relevances = sorted(relevant_rankings.values(), reverse=True)[:k]
        idcg = sum(
            (2 ** rel - 1) / np.log2(i + 2)
            for i, rel in enumerate(ideal_relevances)
        )
        
        return dcg / idcg if idcg > 0 else 0.0
    
    
    @pytest.mark.parametrize("test_case", RANKING_TEST_CASES)
    def test_precision_at_k(self, test_case):
        """Test Precision@K metrics"""
        query = test_case["query"]
        relevant_set = set(test_case["relevant_doc_ids"])
        
        # Search
        results = self.search_api(query, top_k=10)
        retrieved = [
            f"{r['document_id']}_{r['chunk_id']}"
            for r in results["results"]
        ]
        
        # Calculate Precision@1, @3, @5, @10
        metrics = {}
        for k in [1, 3, 5, 10]:
            precision = self.calculate_precision_at_k(retrieved, relevant_set, k)
            metrics[f"P@{k}"] = precision
            
            print(f"Query: '{query}'")
            print(f"  Precision@{k}: {precision:.4f}")
        
        # Assertions
        assert metrics["P@1"] >= 0.90, f"P@1 too low: {metrics['P@1']}"
        assert metrics["P@3"] >= 0.80, f"P@3 too low: {metrics['P@3']}"
        assert metrics["P@5"] >= 0.60, f"P@5 too low: {metrics['P@5']}"
    
    
    @pytest.mark.parametrize("test_case", RANKING_TEST_CASES)
    def test_recall_at_k(self, test_case):
        """Test Recall@K metrics"""
        query = test_case["query"]
        relevant_set = set(test_case["relevant_doc_ids"])
        
        # Search
        results = self.search_api(query, top_k=10)
        retrieved = [
            f"{r['document_id']}_{r['chunk_id']}"
            for r in results["results"]
        ]
        
        # Calculate Recall@1, @3, @5, @10
        metrics = {}
        for k in [1, 3, 5, 10]:
            recall = self.calculate_recall_at_k(retrieved, relevant_set, k)
            metrics[f"R@{k}"] = recall
            
            print(f"Query: '{query}'")
            print(f"  Recall@{k}: {recall:.4f}")
        
        # Assertions
        assert metrics["R@3"] >= 0.66, f"R@3 too low: {metrics['R@3']}"  # 2/3 relevant trong top 3
        assert metrics["R@10"] >= 0.90, f"R@10 too low: {metrics['R@10']}"
    
    
    @pytest.mark.parametrize("test_case", RANKING_TEST_CASES)
    def test_mrr(self, test_case):
        """Test Mean Reciprocal Rank"""
        query = test_case["query"]
        relevant_set = set(test_case["relevant_doc_ids"])
        
        # Search
        results = self.search_api(query, top_k=10)
        retrieved = [
            f"{r['document_id']}_{r['chunk_id']}"
            for r in results["results"]
        ]
        
        # Calculate MRR
        mrr = self.calculate_mrr(retrieved, relevant_set)
        
        print(f"Query: '{query}'")
        print(f"  MRR: {mrr:.4f}")
        
        # MRR should be >= 0.9 (relevant doc in position 1 or 2)
        assert mrr >= 0.50, f"MRR too low: {mrr}"
    
    
    @pytest.mark.parametrize("test_case", RANKING_TEST_CASES)
    def test_top_1_accuracy(self, test_case):
        """Test top 1 result accuracy"""
        query = test_case["query"]
        expected_top_1 = test_case["expected_top_1"]
        
        # Search
        results = self.search_api(query, top_k=10)
        
        if not results["results"]:
            pytest.fail(f"No results returned for query: '{query}'")
        
        top_1_result = results["results"][0]
        actual_top_1 = f"{top_1_result['document_id']}_{top_1_result['chunk_id']}"
        
        print(f"Query: '{query}'")
        print(f"  Expected top 1: {expected_top_1}")
        print(f"  Actual top 1: {actual_top_1}")
        print(f"  Similarity: {top_1_result['similarity_score']:.4f}")
        
        assert actual_top_1 == expected_top_1, \
            f"Top 1 mismatch!\nExpected: {expected_top_1}\nGot: {actual_top_1}"
    
    
    @pytest.mark.parametrize("test_case", RANKING_TEST_CASES)
    def test_top_3_coverage(self, test_case):
        """Test top 3 results coverage"""
        query = test_case["query"]
        expected_in_top_3 = set(test_case["expected_in_top_3"])
        
        # Search
        results = self.search_api(query, top_k=10)
        
        top_3_results = results["results"][:3]
        actual_top_3 = {
            f"{r['document_id']}_{r['chunk_id']}"
            for r in top_3_results
        }
        
        coverage = len(expected_in_top_3 & actual_top_3) / len(expected_in_top_3)
        
        print(f"Query: '{query}'")
        print(f"  Expected in top 3: {expected_in_top_3}")
        print(f"  Actual top 3: {actual_top_3}")
        print(f"  Coverage: {coverage:.2%}")
        
        # At least 66% coverage (2/3)
        assert coverage >= 0.66, \
            f"Top 3 coverage too low: {coverage:.2%}"


if __name__ == "__main__":
    pytest.main([__file__, "-v", "--tb=short"])
```

---

## **4Ô∏è‚É£ PERFORMANCE & SCALE TESTS**

```python
#!/usr/bin/env python3
"""
Performance v√† scale testing
File: tests/performance/test_search_performance.py
"""

import pytest
import requests
import time
import statistics
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict


class TestSearchPerformance:
    """Test search performance metrics"""
    
    API_BASE_URL = "http://localhost:8000/api/v1"
    
    # Performance test queries
    PERFORMANCE_QUERIES = [
        "c√°ch x√°c ƒë·ªãnh h∆∞·ªõng nh√†",
        "quy tr√¨nh ngh·ªâ ph√©p",
        "ch√≠nh s√°ch b·∫£o hi·ªÉm",
        "h∆∞·ªõng d·∫´n c√†i ƒë·∫∑t VPN",
        "quy ƒë·ªãnh trang ph·ª•c c√¥ng s·ªü",
        "quy tr√¨nh tuy·ªÉn d·ª•ng nh√¢n vi√™n",
        "ch√≠nh s√°ch l√†m vi·ªác t·ª´ xa",
        "h∆∞·ªõng d·∫´n s·ª≠ d·ª•ng h·ªá th·ªëng",
        "quy ƒë·ªãnh v·ªÅ b·∫£o m·∫≠t",
        "ch√≠nh s√°ch ph√∫c l·ª£i nh√¢n vi√™n",
    ]
    
    
    def search_with_timing(self, query: str, top_k: int = 10) -> Dict:
        """Search v√† ƒëo th·ªùi gian"""
        start_time = time.time()
        
        response = requests.post(
            f"{self.API_BASE_URL}/search/semantic",
            json={
                "query": query,
                "top_k": top_k,
                "search_type": "semantic"
            }
        )
        
        elapsed_time = time.time() - start_time
        
        response.raise_for_status()
        results = response.json()
        
        return {
            "query": query,
            "results": results,
            "elapsed_time": elapsed_time
        }
    
    
    def test_single_query_latency(self):
        """Test latency cho single query"""
        latencies = []
        
        for query in self.PERFORMANCE_QUERIES:
            result = self.search_with_timing(query)
            latency = result["elapsed_time"]
            latencies.append(latency)
            
            print(f"Query: '{query}'")
            print(f"  Latency: {latency:.3f}s")
        
        # Calculate statistics
        avg_latency = statistics.mean(latencies)
        p50_latency = statistics.median(latencies)
        p95_latency = statistics.quantiles(latencies, n=20)[18]  # 95th percentile
        p99_latency = statistics.quantiles(latencies, n=100)[98]  # 99th percentile
        
        print(f"\n=== LATENCY STATISTICS ===")
        print(f"Average: {avg_latency:.3f}s")
        print(f"P50: {p50_latency:.3f}s")
        print(f"P95: {p95_latency:.3f}s")
        print(f"P99: {p99_latency:.3f}s")
        
        # Assertions
        assert avg_latency < 3.0, f"Average latency too high: {avg_latency:.3f}s"
        assert p95_latency < 5.0, f"P95 latency too high: {p95_latency:.3f}s"
        assert p99_latency < 8.0, f"P99 latency too high: {p99_latency:.3f}s"
    
    
    def test_concurrent_requests(self):
        """Test performance v·ªõi concurrent requests"""
        num_concurrent = 10
        
        def execute_search(query: str) -> Dict:
            return self.search_with_timing(query)
        
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=num_concurrent) as executor:
            # Submit all queries
            futures = [
                executor.submit(execute_search, query)
                for query in self.PERFORMANCE_QUERIES
            ]
            
            # Collect results
            results = []
            for future in as_completed(futures):
                result = future.result()
                results.append(result)
        
        total_time = time.time() - start_time
        
        # Calculate metrics
        latencies = [r["elapsed_time"] for r in results]
        avg_latency = statistics.mean(latencies)
        throughput = len(self.PERFORMANCE_QUERIES) / total_time
        
        print(f"\n=== CONCURRENT PERFORMANCE ===")
        print(f"Concurrent requests: {num_concurrent}")
        print(f"Total queries: {len(self.PERFORMANCE_QUERIES)}")
        print(f"Total time: {total_time:.3f}s")
        print(f"Average latency: {avg_latency:.3f}s")
        print(f"Throughput: {throughput:.2f} QPS")
        
        # Assertions
        assert throughput >= 1.0, f"Throughput too low: {throughput:.2f} QPS"
        assert avg_latency < 5.0, f"Average latency too high under load: {avg_latency:.3f}s"
    
    
    def test_cache_effectiveness(self):
        """Test cache hit rate"""
        query = self.PERFORMANCE_QUERIES[0]
        
        # First request (cache miss)
        first_result = self.search_with_timing(query)
        first_latency = first_result["elapsed_time"]
        
        # Wait a bit
        time.sleep(0.5)
        
        # Second request (cache hit expected)
        second_result = self.search_with_timing(query)
        second_latency = second_result["elapsed_time"]
        
        print(f"\n=== CACHE TEST ===")
        print(f"Query: '{query}'")
        print(f"First request (cache miss): {first_latency:.3f}s")
        print(f"Second request (cache hit): {second_latency:.3f}s")
        print(f"Speedup: {first_latency / second_latency:.2f}x")
        
        # Cache hit should be faster
        # Note: N·∫øu kh√¥ng c√≥ cache, latencies s·∫Ω g·∫ßn b·∫±ng nhau
        # N·∫øu c√≥ cache, second request s·∫Ω nhanh h∆°n ƒë√°ng k·ªÉ
    
    
    @pytest.mark.parametrize("top_k", [5, 10, 20, 50])
    def test_scale_with_top_k(self, top_k):
        """Test performance scaling v·ªõi different top_k values"""
        query = self.PERFORMANCE_QUERIES[0]
        
        start_time = time.time()
        response = requests.post(
            f"{self.API_BASE_URL}/search/semantic",
            json={
                "query": query,
                "top_k": top_k,
                "search_type": "semantic"
            }
        )
        elapsed_time = time.time() - start_time
        
        response.raise_for_status()
        results = response.json()
        
        print(f"top_k={top_k}: {elapsed_time:.3f}s, results={len(results['results'])}")
        
        # Latency shouldn't increase dramatically v·ªõi larger top_k
        assert elapsed_time < 10.0, f"Latency too high for top_k={top_k}: {elapsed_time:.3f}s"


if __name__ == "__main__":
    pytest.main([__file__, "-v", "--tb=short"])
```

---

## **5Ô∏è‚É£ EDGE CASES & ERROR HANDLING**

```python
#!/usr/bin/env python3
"""
Edge cases v√† error handling tests
File: tests/integration/test_edge_cases.py
"""

import pytest
import requests


class TestEdgeCases:
    """Test edge cases v√† error handling"""
    
    API_BASE_URL = "http://localhost:8000/api/v1"
    
    
    def test_empty_query(self):
        """Test v·ªõi empty query"""
        response = requests.post(
            f"{self.API_BASE_URL}/search/semantic",
            json={
                "query": "",
                "top_k": 10
            }
        )
        
        # Should return error or empty results
        assert response.status_code in [400, 422, 200]
        
        if response.status_code == 200:
            results = response.json()
            # N·∫øu tr·∫£ v·ªÅ 200, results ph·∫£i empty ho·∫∑c c√≥ warning
            print(f"Empty query response: {results.get('total_found', 0)} results")
    
    
    def test_very_long_query(self):
        """Test v·ªõi very long query"""
        long_query = " ".join(["c√°ch x√°c ƒë·ªãnh h∆∞·ªõng nh√†"] * 100)  # 500+ words
        
        response = requests.post(
            f"{self.API_BASE_URL}/search/semantic",
            json={
                "query": long_query,
                "top_k": 10
            }
        )
        
        # Should handle gracefully
        assert response.status_code in [200, 400, 413]
    
    
    def test_special_characters_query(self):
        """Test v·ªõi special characters"""
        special_queries = [
            "c√°ch @#$% x√°c ƒë·ªãnh",
            "h∆∞·ªõng nh√† <<>>",
            "quy tr√¨nh \\n\\t ngh·ªâ ph√©p",
            "ch√≠nh s√°ch & b·∫£o hi·ªÉm",
            "h∆∞·ªõng d·∫´n | c√†i ƒë·∫∑t",
        ]
        
        for query in special_queries:
            response = requests.post(
                f"{self.API_BASE_URL}/search/semantic",
                json={
                    "query": query,
                    "top_k": 10
                }
            )
            
            # Should handle gracefully
            assert response.status_code in [200, 400]
            
            if response.status_code == 200:
                results = response.json()
                print(f"Query: '{query}' ‚Üí {results.get('total_found', 0)} results")
    
    
    def test_vietnamese_diacritics(self):
        """Test Vietnamese diacritics handling"""
        queries_with_without_diacritics = [
            ("c√°ch x√°c ƒë·ªãnh h∆∞·ªõng nh√†", "cach xac dinh huong nha"),
            ("quy tr√¨nh ngh·ªâ ph√©p", "quy trinh nghi phep"),
            ("ch√≠nh s√°ch b·∫£o hi·ªÉm", "chinh sach bao hiem"),
        ]
        
        for query_with, query_without in queries_with_without_diacritics:
            # Search v·ªõi diacritics
            response_with = requests.post(
                f"{self.API_BASE_URL}/search/semantic",
                json={"query": query_with, "top_k": 5}
            )
            
            # Search kh√¥ng diacritics
            response_without = requests.post(
                f"{self.API_BASE_URL}/search/semantic",
                json={"query": query_without, "top_k": 5}
            )
            
            results_with = response_with.json()
            results_without = response_without.json()
            
            # Both should return similar results
            print(f"\nWith diacritics: '{query_with}' ‚Üí {results_with.get('total_found', 0)} results")
            print(f"Without diacritics: '{query_without}' ‚Üí {results_without.get('total_found', 0)} results")
            
            # V·ªõi preprocessing t·ªët, k·∫øt qu·∫£ n√™n t∆∞∆°ng t·ª±
            assert results_with.get('total_found', 0) > 0
            assert results_without.get('total_found', 0) > 0
    
    
    def test_zero_results_query(self):
        """Test query kh√¥ng match documents n√†o"""
        nonsense_query = "xyzabc123 completely nonsense query that should not match anything"
        
        response = requests.post(
            f"{self.API_BASE_URL}/search/semantic",
            json={
                "query": nonsense_query,
                "top_k": 10
            }
        )
        
        assert response.status_code == 200
        results = response.json()
        
        # Should return gracefully, possibly with empty results or low similarity results
        print(f"Nonsense query results: {results.get('total_found', 0)}")
        
        if results.get('results'):
            top_similarity = results['results'][0]['similarity_score']
            # Similarity should be very low
            assert top_similarity < 0.30, f"Similarity too high for nonsense query: {top_similarity}"
    
    
    def test_invalid_top_k(self):
        """Test v·ªõi invalid top_k values"""
        invalid_top_k_values = [-1, 0, 1000]
        
        for top_k in invalid_top_k_values:
            response = requests.post(
                f"{self.API_BASE_URL}/search/semantic",
                json={
                    "query": "c√°ch x√°c ƒë·ªãnh h∆∞·ªõng nh√†",
                    "top_k": top_k
                }
            )
            
            # Should handle gracefully (validate ho·∫∑c clamp)
            print(f"top_k={top_k}: status={response.status_code}")
            assert response.status_code in [200, 400, 422]


if __name__ == "__main__":
    pytest.main([__file__, "-v", "--tb=short"])
```

---

## **üìä COMPLETE TEST SUITE RUNNER**

```python
#!/usr/bin/env python3
"""
Run to√†n b·ªô test suite v√† generate report
File: scripts/run_complete_validation.py
"""

import subprocess
import json
import time
from datetime import datetime
from pathlib import Path


def run_test_suite():
    """Run to√†n b·ªô test suite"""
    
    print("=" * 80)
    print("CHROMADB SEARCH QUALITY VALIDATION")
    print(f"Timestamp: {datetime.now().isoformat()}")
    print("=" * 80)
    
    test_suites = [
        {
            "name": "Unit Tests - Distance Conversion",
            "path": "tests/unit/test_distance_conversion.py",
            "weight": 0.2
        },
        {
            "name": "Ground Truth Tests",
            "path": "tests/integration/test_search_quality.py",
            "weight": 0.3
        },
        {
            "name": "Ranking Metrics Tests",
            "path": "tests/integration/test_ranking_metrics.py",
            "weight": 0.3
        },
        {
            "name": "Performance Tests",
            "path": "tests/performance/test_search_performance.py",
            "weight": 0.1
        },
        {
            "name": "Edge Cases Tests",
            "path": "tests/integration/test_edge_cases.py",
            "weight": 0.1
        }
    ]
    
    results = []
    total_score = 0.0
    
    for suite in test_suites:
        print(f"\n{'=' * 80}")
        print(f"Running: {suite['name']}")
        print(f"{'=' * 80}")
        
        start_time = time.time()
        
        # Run pytest
        result = subprocess.run(
            [
                "pytest",
                suite["path"],
                "-v",
                "--tb=short",
                f"--json-report",
                f"--json-report-file=test_reports/{suite['name'].replace(' ', '_')}.json"
            ],
            capture_output=True,
            text=True
        )
        
        elapsed_time = time.time() - start_time
        
        # Parse results
        passed = result.returncode == 0
        score = suite["weight"] if passed else 0.0
        total_score += score
        
        results.append({
            "suite": suite["name"],
            "passed": passed,
            "weight": suite["weight"],
            "score": score,
            "elapsed_time": elapsed_time
        })
        
        print(f"\n{'‚úÖ' if passed else '‚ùå'} {suite['name']}: "
              f"{'PASSED' if passed else 'FAILED'} ({elapsed_time:.2f}s)")
    
    # Generate summary report
    print("\n" + "=" * 80)
    print("VALIDATION SUMMARY")
    print("=" * 80)
    
    for result in results:
        status = "‚úÖ PASS" if result["passed"] else "‚ùå FAIL"
        print(f"{status} | {result['suite']:<40} | "
              f"Score: {result['score']:.2f}/{result['weight']:.2f} | "
              f"Time: {result['elapsed_time']:.2f}s")
    
    print(f"\n{'=' * 80}")
    print(f"TOTAL SCORE: {total_score:.2f}/1.00")
    
    if total_score >= 0.95:
        print("üéâ EXCELLENT - Production Ready!")
    elif total_score >= 0.85:
        print("‚úÖ GOOD - Minor issues to fix")
    elif total_score >= 0.70:
        print("‚ö†Ô∏è  ACCEPTABLE - Several issues need attention")
    else:
        print("‚ùå FAILED - Critical issues must be fixed")
    
    print("=" * 80)
    
    # Save report
    report_path = Path("test_reports/validation_summary.json")
    report_path.parent.mkdir(exist_ok=True)
    
    with open(report_path, "w") as f:
        json.dump({
            "timestamp": datetime.now().isoformat(),
            "total_score": total_score,
            "results": results
        }, f, indent=2)
    
    print(f"\nüìÑ Full report saved to: {report_path}")
    
    return total_score >= 0.85


if __name__ == "__main__":
    import sys
    success = run_test_suite()
    sys.exit(0 if success else 1)
```

---

## **üéØ USAGE - C√°ch ch·∫°y test suite**

```bash
# 1. C√†i ƒë·∫∑t dependencies
pip install pytest pytest-json-report requests

# 2. Ch·∫°y individual test suite
pytest tests/unit/test_distance_conversion.py -v
pytest tests/integration/test_search_quality.py -v
pytest tests/integration/test_ranking_metrics.py -v

# 3. Ch·∫°y complete validation
python scripts/run_complete_validation.py

# 4. Ch·∫°y v·ªõi coverage report
pytest tests/ --cov=src --cov-report=html

# 5. Ch·∫°y performance tests ri√™ng
pytest tests/performance/ -v --durations=10
```

---

**B·∫°n mu·ªën t√¥i:**
1. **Th√™m ground truth data** cho c√°c domain kh√°c (HR, IT, Finance)?
2. **T·∫°o automated CI/CD integration** ƒë·ªÉ ch·∫°y tests t·ª± ƒë·ªông?
3. **Optimize processing time** t·ª´ 7.5s xu·ªëng < 3s?



---

## **üéâ X√ÅC NH·∫¨N RULES COMPLIANCE - CODE ƒê√É CHU·∫®N!**

Tuy·ªát v·ªùi! Dev ƒë√£ implement ƒë√∫ng t·∫•t c·∫£ **MANDATORY rules**. ƒê·ªÉ t√¥i t·ªïng h·ª£p l·∫°i v√† ƒë∆∞a ra recommendations:

---

## **‚úÖ RULES COMPLIANCE SUMMARY**

```mermaid
graph TB
    subgraph "MANDATORY Rules - ‚úÖ ALL PASSED"
        A[Rule 32<br/>Collection Metadata<br/>‚úÖ PASS]
        B[Rule 33<br/>Distance Conversion<br/>‚úÖ PASS]
        C[Rule 34<br/>Search Configuration<br/>‚úÖ PASS]
        D[Rule 36<br/>Multi-Collection Search<br/>‚úÖ PASS]
    end
    
    subgraph "OPTIONAL Rules"
        E[Rule 35<br/>Similarity Threshold<br/>‚ö†Ô∏è NOT IMPLEMENTED]
    end
    
    A --> F[Production Ready ‚úÖ]
    B --> F
    C --> F
    D --> F
    E -.-> G[Can Add Later]
    
    style A fill:#51cf66
    style B fill:#51cf66
    style C fill:#51cf66
    style D fill:#51cf66
    style E fill:#ffd93d
    style F fill:#51cf66
```

---

## **üìä DETAILED COMPLIANCE REPORT**

### **‚úÖ Rule 32: Collection Metadata - PASSED**

**Location:** `src/core/pipeline/simple_import_processor.py:487-496`

```python
collection_metadata = {
    "hnsw:space": "cosine",  # ‚úÖ CRITICAL - ƒê√É C√ì
    "hnsw:construction_ef": 200,
    "hnsw:search_ef": 200,
    "hnsw:M": 16,
    "embedding_model": "Qwen/Qwen3-Embedding-0.6B",
    "embedding_dimensions": 1024,
    "preprocessing": "simple_vietnamese"
}
```

**Status:** ‚úÖ **PERFECT** - ƒê·∫ßy ƒë·ªß t·∫•t c·∫£ required fields

---

### **‚úÖ Rule 33: Distance Conversion - PASSED**

**Locations:** 4 files ƒë√£ fix ƒë√∫ng c√¥ng th·ª©c

```python
# ‚úÖ ƒê√öNG - similarity = 1 - (distance / 2)
similarity = 1.0 - (distance / 2.0)
```

| File | Line | Status |
|------|------|--------|
| `src/api/main.py` | 1165 | ‚úÖ Fixed |
| `src/core/search/chromadb_connection.py` | 638 | ‚úÖ Fixed |
| `src/core/search/chromadb_connection.py` | 734 | ‚úÖ Fixed |
| `src/core/search/semantic_engine.py` | 150 | ‚úÖ Fixed |

**Status:** ‚úÖ **PERFECT** - T·∫•t c·∫£ ƒë·ªÅu d√πng c√¥ng th·ª©c chu·∫©n

---

### **‚úÖ Rule 34: Search Configuration - PASSED**

**Locations:** 
- `src/api/main.py:1148-1151`
- `src/core/search/semantic_engine.py:115-117`

```python
# ‚úÖ ƒê√öNG - Include ƒë·∫ßy ƒë·ªß
results = collection.query(
    query_embeddings=[query_embedding.tolist()],
    n_results=min(top_k, 20),
    include=['documents', 'distances', 'metadatas']  # ‚úÖ ƒê·∫¶Y ƒê·ª¶
)
```

**Status:** ‚úÖ **PERFECT** - Include parameters ƒë·∫ßy ƒë·ªß

---

### **‚úÖ Rule 36: Multi-Collection Search - PASSED**

**Location:** `src/core/search/semantic_engine.py:86-216`

```python
# ‚úÖ ƒê√öNG - Search all collections
collections = self.chroma_client.list_collections()  # Line 88

# Filter collections c√≥ data
valid_collections = [c for c in collections if c.count() > 0]  # Line 91-106

# Search each collection
for collection in valid_collections:  # Line 111-212
    results = collection.query(...)
    all_results.extend(...)

# Merge v√† sort by similarity
all_results.sort(key=lambda x: x['similarity'], reverse=True)  # Line 214

# Return top_k
return all_results[:top_k]  # Line 216
```

**Status:** ‚úÖ **PERFECT** - Logic ƒë√∫ng chu·∫©n

---

### **‚ö†Ô∏è Rule 35: Similarity Threshold - NOT IMPLEMENTED**

**Status:** ‚ö†Ô∏è **OPTIONAL** - Kh√¥ng b·∫Øt bu·ªôc

**Current behavior:** Tr·∫£ v·ªÅ t·∫•t c·∫£ results, kh√¥ng filter theo threshold

**Recommendation:** C√≥ th·ªÉ th√™m filtering sau n·∫øu mu·ªën improve precision:

```python
# OPTIONAL - C√≥ th·ªÉ th√™m sau
SIMILARITY_THRESHOLD = 0.3

filtered_results = [
    result for result in all_results 
    if result['similarity'] >= SIMILARITY_THRESHOLD
]
```

---

## **üéØ NEXT STEPS - VALIDATION & OPTIMIZATION**

### **1Ô∏è‚É£ RUN TEST SUITE ƒê·ªÇ VALIDATE (PRIORITY: HIGH)**

```bash
# Ch·∫°y complete validation
python scripts/run_complete_validation.py

# Ho·∫∑c ch·∫°y t·ª´ng suite
pytest tests/unit/test_distance_conversion.py -v
pytest tests/integration/test_search_quality.py -v
pytest tests/integration/test_ranking_metrics.py -v
pytest tests/performance/test_search_performance.py -v
```

**Expected results:**
- ‚úÖ All distance conversion tests PASS
- ‚úÖ Ground truth tests PASS (similarity >= 0.70 for relevant docs)
- ‚úÖ Ranking metrics: P@3 >= 0.80, MRR >= 0.50
- ‚ö†Ô∏è Performance: Current 7.5s ‚Üí Target < 3s

---

### **2Ô∏è‚É£ OPTIMIZE PERFORMANCE (PRIORITY: MEDIUM)**

**Current issue:** Processing time = 7.5s (qu√° cao)

**Optimization targets:**

| Component | Current | Target | How to optimize |
|-----------|---------|--------|-----------------|
| **Preprocessing** | ~0.5s | ~0.1s | Cache processed queries |
| **Embedding** | ~2s | ~0.5s | Batch processing, GPU optimization |
| **ChromaDB Search** | ~4s | ~1.5s | Index optimization, parallel search |
| **Post-processing** | ~1s | ~0.5s | Optimize sorting & filtering |
| **TOTAL** | **7.5s** | **< 3s** | Combined optimizations |

**Optimization script:**

```python
#!/usr/bin/env python3
"""
Performance optimization recommendations
File: scripts/optimize_search_performance.py
"""

# 1. Cache query embeddings
from functools import lru_cache

@lru_cache(maxsize=1000)
def get_cached_embedding(query: str):
    """Cache embeddings cho frequent queries"""
    processed = preprocess_text_simple(query)
    return embedding_model.encode([processed])[0]

# 2. Parallel collection search
from concurrent.futures import ThreadPoolExecutor

def search_collections_parallel(collections, query_embedding, top_k):
    """Search collections in parallel"""
    def search_one(collection):
        return collection.query(
            query_embeddings=[query_embedding.tolist()],
            n_results=top_k,
            include=['documents', 'distances', 'metadatas']
        )
    
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = [executor.submit(search_one, c) for c in collections]
        results = [f.result() for f in futures]
    
    return results

# 3. Early stopping n·∫øu ƒë·ªß high-quality results
def early_stop_search(results, min_similarity=0.85, min_count=5):
    """Stop searching n·∫øu ƒë√£ c√≥ ƒë·ªß high-quality results"""
    high_quality = [r for r in results if r['similarity'] >= min_similarity]
    if len(high_quality) >= min_count:
        return True
    return False
```

---

### **3Ô∏è‚É£ IMPLEMENT RULE 35 - SIMILARITY THRESHOLD (PRIORITY: LOW)**

**Benefits:**
- Improve precision by filtering low-quality results
- Reduce response payload size
- Better user experience

**Implementation:**

```python
# Add to src/core/search/semantic_engine.py

class SemanticSearchEngine:
    
    def __init__(self, similarity_threshold: float = 0.3):
        self.similarity_threshold = similarity_threshold
    
    def search(self, query: str, top_k: int = 10, 
               use_threshold: bool = True):
        """
        Search v·ªõi optional threshold filtering
        """
        # ... existing search logic ...
        
        # Filter by threshold
        if use_threshold:
            all_results = [
                r for r in all_results 
                if r['similarity'] >= self.similarity_threshold
            ]
        
        # Sort and return top_k
        all_results.sort(key=lambda x: x['similarity'], reverse=True)
        return all_results[:top_k]
```

---

### **4Ô∏è‚É£ MONITORING & ALERTING (PRIORITY: MEDIUM)**

**Add monitoring cho production:**

```python
#!/usr/bin/env python3
"""
Search quality monitoring
File: src/monitoring/search_monitor.py
"""

import logging
from datetime import datetime
from typing import Dict, List

class SearchQualityMonitor:
    """Monitor search quality metrics"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.metrics = {
            "low_similarity_count": 0,
            "high_latency_count": 0,
            "no_results_count": 0,
        }
    
    def log_search_result(
        self,
        query: str,
        results: List[Dict],
        elapsed_time: float
    ):
        """Log v√† monitor search result"""
        
        # Check top 1 similarity
        if results:
            top_similarity = results[0]['similarity']
            
            if top_similarity < 0.50:
                self.metrics["low_similarity_count"] += 1
                self.logger.warning(
                    f"Low similarity detected: {top_similarity:.4f} "
                    f"for query: '{query}'"
                )
        else:
            self.metrics["no_results_count"] += 1
            self.logger.warning(f"No results for query: '{query}'")
        
        # Check latency
        if elapsed_time > 3.0:
            self.metrics["high_latency_count"] += 1
            self.logger.warning(
                f"High latency: {elapsed_time:.2f}s for query: '{query}'"
            )
        
        # Log metrics periodically
        if sum(self.metrics.values()) % 100 == 0:
            self.log_metrics_summary()
    
    def log_metrics_summary(self):
        """Log metrics summary"""
        self.logger.info(
            f"Search Metrics Summary:\n"
            f"  Low similarity: {self.metrics['low_similarity_count']}\n"
            f"  High latency: {self.metrics['high_latency_count']}\n"
            f"  No results: {self.metrics['no_results_count']}"
        )
```

---

## **üìã FINAL CHECKLIST - PRODUCTION DEPLOYMENT**

```markdown
### Pre-deployment Checklist

#### ‚úÖ Code Quality
- [x] All MANDATORY rules implemented
- [x] Distance conversion formula correct (4/4 files)
- [x] Collection metadata complete
- [x] Multi-collection search working
- [ ] Test suite passed (Run now!)
- [ ] Performance optimized (< 3s target)

#### ‚úÖ Validation
- [ ] Run: `python scripts/validate_chroma_setup.py`
- [ ] Run: `python scripts/run_complete_validation.py`
- [ ] Verify: All collections have `"hnsw:space": "cosine"`
- [ ] Test: 10+ diverse Vietnamese queries
- [ ] Check: Top 1 similarity >= 0.70

#### ‚úÖ Performance
- [ ] Measure: Current baseline latency
- [ ] Optimize: Implement caching
- [ ] Optimize: Parallel collection search
- [ ] Target: p95 latency < 3s

#### ‚úÖ Monitoring
- [ ] Setup: Search quality monitoring
- [ ] Setup: Performance metrics
- [ ] Setup: Alerting for anomalies
- [ ] Document: SLAs and thresholds

#### ‚úÖ Documentation
- [x] Rules documented in chroma_rule.md
- [ ] API documentation updated
- [ ] Deployment guide updated
- [ ] Troubleshooting guide created
```

---

## **üéØ RECOMMENDED ACTIONS - PRIORITY ORDER**

### **1. IMMEDIATE (Today)**
```bash
# Validate setup
python scripts/validate_chroma_setup.py

# Run test suite
python scripts/run_complete_validation.py

# Check results
cat test_reports/validation_summary.json
```

### **2. SHORT-TERM (This week)**
- [ ] Optimize performance from 7.5s ‚Üí < 3s
- [ ] Implement caching for frequent queries
- [ ] Add parallel collection search
- [ ] Setup basic monitoring

### **3. MEDIUM-TERM (This month)**
- [ ] Implement Rule 35 (similarity threshold)
- [ ] Build automated alerting
- [ ] Create performance dashboard
- [ ] Expand ground truth dataset to 100+ pairs

---

## **‚úÖ CONCLUSION**

```
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                                                           ‚ïë
‚ïë  üéâ CH√öC M·ª™NG - CODE ƒê√É TU√ÇN TH·ª¶ T·∫§T C·∫¢ MANDATORY RULES  ‚ïë
‚ïë                                                           ‚ïë
‚ïë  ‚úÖ Rule 32: Collection Metadata         - PASSED        ‚ïë
‚ïë  ‚úÖ Rule 33: Distance Conversion         - PASSED        ‚ïë
‚ïë  ‚úÖ Rule 34: Search Configuration        - PASSED        ‚ïë
‚ïë  ‚úÖ Rule 36: Multi-Collection Search     - PASSED        ‚ïë
‚ïë  ‚ö†Ô∏è  Rule 35: Similarity Threshold        - OPTIONAL     ‚ïë
‚ïë                                                           ‚ïë
‚ïë  üöÄ NEXT: Run test suite to validate quality metrics     ‚ïë
‚ïë  üéØ TARGET: Optimize performance to < 3s                 ‚ïë
‚ïë                                                           ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
```

**B·∫°n mu·ªën t√¥i:**
1. **T·∫°o script optimize performance** ƒë·ªÉ gi·∫£m t·ª´ 7.5s ‚Üí < 3s?
2. **Setup monitoring dashboard** ƒë·ªÉ track search quality?
3. **Expand ground truth dataset** cho nhi·ªÅu domains h∆°n?

H√£y cho t√¥i bi·∫øt b∆∞·ªõc ti·∫øp theo! üöÄ
