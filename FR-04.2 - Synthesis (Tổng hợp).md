Nếu đã đủ thông tin hãy giúp tôi viết tài liệu thiết kế module dưới đây.
Tài liệu này cung cấp roadmap đầy đủ để implement FR-04.2 mà không cần code cụ thể nhưng 
có các bước làm cụ thể như chuẩn bị môi trường PC thế nào, phần mềm gì?
 để Team kỹ thuật có thể follow step-by-step guide này để thiết kế tối ưu cho hệ thống chatbot
 và có mermaidchart luồng công việc, điểm quay đầu khi bị fail.

### 3.4 Module RAG Core Engine (FR-04)

**FR-04.2 - Synthesis (Tổng hợp)**
- Xây dựng context từ các documents truy xuất được
- Template hóa prompt cho LLM
- Xử lý trường hợp không tìm thấy thông tin phù hợp
---
# TÀI LIỆU THIẾT KẾ MODULE FR-04.2: SYNTHESIS (TỔNG HỢP)
## ROADMAP TRIỂN KHAI CHO TEAM KỸ THUẬT

---

**Module:** RAG Core Engine - Synthesis Component  
**Requirement ID:** FR-04.2  
**Version:** 1.0  
**Date:** 01/09/2025  
**Target Audience:** Development Team  

---

## 📋 **TỔNG QUAN MODULE**

### **Mục tiêu**
Module Synthesis chịu trách nhiệm xây dựng context thông minh từ các documents được retrieval, tạo prompt templates tối ưu cho LLM, và xử lý graceful các trường hợp không tìm thấy thông tin phù hợp.

### **Input/Output**
- **Input**: Retrieved documents + User query + User context
- **Output**: Structured prompt cho LLM + Fallback responses

### **Vị trí trong RAG Pipeline**
```
Query Processing → Document Retrieval → [SYNTHESIS] → LLM Generation → Response
```

---

## 🛠️ **CHUẨN BỊ MÔI TRƯỜNG PHÁT TRIỂN**

### **1. Yêu cầu Phần cứng**
```
CPU: Intel i7/AMD Ryzen 7 (8+ cores) hoặc cao hơn
RAM: 32GB+ (recommend 64GB cho testing với large datasets)
Storage: 1TB+ SSD (NVMe preferred)
GPU: Optional - NVIDIA RTX 4060+ nếu test local LLM
Network: Stable internet cho API calls
```

### **2. Hệ điều hành**
```
Primary: Ubuntu 22.04 LTS / Windows 11 Pro / macOS 13+
Container: Docker Desktop 4.20+
Virtualization: VirtualBox/VMware (nếu cần isolated environment)
```

### **3. Development Environment Setup**

#### **3.1 Core Development Tools**
```bash
# Python Environment
Python 3.11+ (recommend pyenv cho version management)
pip 23.0+
Poetry 1.5+ (dependency management)

# IDE Options (chọn 1)
- Visual Studio Code + Python extensions
- PyCharm Professional 2023.2+
- Cursor AI (recommend cho AI-assisted development)

# Version Control
Git 2.40+
GitHub CLI (optional nhưng recommend)
```

#### **3.2 Specialized AI/ML Tools**
```bash
# Essential Python Libraries
langchain>=0.1.0          # LLM orchestration
openai>=1.0.0             # OpenAI API
anthropic>=0.8.0          # Claude API
jinja2>=3.1.0             # Template engine
tiktoken>=0.5.0           # Token counting
pydantic>=2.0.0           # Data validation
pytest>=7.0.0             # Testing framework
black>=23.0.0             # Code formatting
mypy>=1.5.0               # Type checking

# Performance & Monitoring
prometheus-client>=0.17.0  # Metrics
structlog>=23.0.0         # Structured logging
tenacity>=8.2.0           # Retry mechanisms
```

#### **3.3 Development Infrastructure**
```bash
# Local Development Stack
Docker Desktop 4.20+
Docker Compose v2.20+
Redis 7.0+ (cho caching)
PostgreSQL 15+ (cho metadata)

# Testing & Quality
pytest-cov>=4.0.0        # Coverage testing
pre-commit>=3.3.0        # Git hooks
ruff>=0.0.280             # Fast linter
```

### **4. Project Structure Setup**

```bash
# Tạo project structure
mkdir rag_synthesis_module
cd rag_synthesis_module

# Initialize Python project
poetry init
poetry add langchain openai anthropic jinja2 tiktoken pydantic
poetry add --group dev pytest black mypy ruff pytest-cov

# Tạo cấu trúc thư mục
mkdir -p {
  src/synthesis/{core,templates,handlers,utils},
  tests/{unit,integration,e2e},
  config,
  docs,
  examples,
  scripts
}

# Initialize Git
git init
# Setup pre-commit hooks (sẽ detail ở bước sau)
```

---

## 🏗️ **KIẾN TRÚC MODULE SYNTHESIS**

### **5. Component Architecture**

```mermaid
graph TB
    subgraph "📥 INPUT PROCESSING"
        Input[🔍 Retrieved Documents<br/>+ User Query + Context]
        Validator[✅ Input Validator<br/>Validate & Sanitize]
        Scorer[📊 Relevance Scorer<br/>Score Documents]
    end
    
    subgraph "🧩 CONTEXT BUILDING"
        ContextBuilder[🏗️ Context Builder<br/>Smart Document Selection]
        
        subgraph "Context Strategies"
            TopK[📈 Top-K Strategy<br/>Best N documents]
            Diversity[🎯 Diversity Strategy<br/>Varied sources]
            Threshold[📏 Threshold Strategy<br/>Above confidence score]
        end
        
        ContextOptimizer[⚡ Context Optimizer<br/>Token limit management]
    end
    
    subgraph "📝 TEMPLATE PROCESSING"
        TemplateManager[📋 Template Manager<br/>Load & Manage Templates]
        
        subgraph "Template Types"
            SystemTemplate[🤖 System Templates<br/>Role definitions]
            QueryTemplate[❓ Query Templates<br/>Question formatting]
            ContextTemplate[📚 Context Templates<br/>Document formatting]
            FallbackTemplate[🚨 Fallback Templates<br/>No-result handling]
        end
        
        TemplateRenderer[🎨 Template Renderer<br/>Jinja2 Engine]
    end
    
    subgraph "🔧 PROMPT ASSEMBLY"
        PromptAssembler[🔧 Prompt Assembler<br/>Combine all parts]
        TokenCounter[🔢 Token Counter<br/>Validate length]
        QualityChecker[✅ Quality Checker<br/>Validate prompt quality]
    end
    
    subgraph "📤 OUTPUT GENERATION"
        OutputFormatter[📋 Output Formatter<br/>Structure final output]
        MetadataEnricher[🏷️ Metadata Enricher<br/>Add context info]
        FinalOutput[✨ Structured Prompt<br/>Ready for LLM]
    end
    
    subgraph "🚨 ERROR HANDLING"
        ErrorHandler[❌ Error Handler<br/>Handle failures]
        FallbackGenerator[🔄 Fallback Generator<br/>Default responses]
        LoggingService[📝 Logging Service<br/>Error tracking]
    end
    
    %% Main Flow
    Input --> Validator
    Validator --> Scorer
    Scorer --> ContextBuilder
    
    ContextBuilder --> TopK
    ContextBuilder --> Diversity  
    ContextBuilder --> Threshold
    
    TopK --> ContextOptimizer
    Diversity --> ContextOptimizer
    Threshold --> ContextOptimizer
    
    ContextOptimizer --> TemplateManager
    TemplateManager --> SystemTemplate
    TemplateManager --> QueryTemplate
    TemplateManager --> ContextTemplate
    TemplateManager --> FallbackTemplate
    
    SystemTemplate --> TemplateRenderer
    QueryTemplate --> TemplateRenderer
    ContextTemplate --> TemplateRenderer
    FallbackTemplate --> TemplateRenderer
    
    TemplateRenderer --> PromptAssembler
    PromptAssembler --> TokenCounter
    TokenCounter --> QualityChecker
    QualityChecker --> OutputFormatter
    OutputFormatter --> MetadataEnricher
    MetadataEnricher --> FinalOutput
    
    %% Error Flows (dotted red)
    Validator -.-> ErrorHandler
    ContextBuilder -.-> ErrorHandler  
    TemplateRenderer -.-> ErrorHandler
    TokenCounter -.-> ErrorHandler
    
    ErrorHandler --> FallbackGenerator
    ErrorHandler --> LoggingService
    FallbackGenerator --> FinalOutput
    
    %% Styling
    classDef input fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef context fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef template fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef assembly fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef output fill:#e0f2f1,stroke:#00796b,stroke-width:2px
    classDef error fill:#ffebee,stroke:#c62828,stroke-width:2px
    
    class Input,Validator,Scorer input
    class ContextBuilder,TopK,Diversity,Threshold,ContextOptimizer context
    class TemplateManager,SystemTemplate,QueryTemplate,ContextTemplate,FallbackTemplate,TemplateRenderer template
    class PromptAssembler,TokenCounter,QualityChecker assembly
    class OutputFormatter,MetadataEnricher,FinalOutput output
    class ErrorHandler,FallbackGenerator,LoggingService error
```

---

## 📝 **STEP-BY-STEP IMPLEMENTATION ROADMAP**

### **Phase 1: Environment Setup & Foundation (Week 1)**

#### **Step 1.1: Initialize Development Environment**
```bash
# 1. Clone & setup project
git clone <repository>
cd rag_synthesis_module

# 2. Setup Python environment
pyenv install 3.11.5
pyenv local 3.11.5
poetry install

# 3. Setup pre-commit hooks
cat > .pre-commit-config.yaml << EOF
repos:
  - repo: https://github.com/psf/black
    rev: 23.7.0
    hooks:
      - id: black
  - repo: https://github.com/charliermarsh/ruff-pre-commit
    rev: v0.0.280
    hooks:
      - id: ruff
  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.5.1
    hooks:
      - id: mypy
EOF

pre-commit install

# 4. Setup testing infrastructure
poetry run pytest --version
poetry run black --version
poetry run mypy --version
```

#### **Step 1.2: Core Configuration Setup**
```bash
# Create config structure
mkdir -p config/{development,staging,production}

# Create base config template
cat > config/base_config.py << EOF
from pydantic_settings import BaseSettings
from typing import Dict, Any, List
import os

class SynthesisConfig(BaseSettings):
    # LLM Settings
    openai_api_key: str = ""
    anthropic_api_key: str = ""
    max_tokens: int = 4000
    temperature: float = 0.1
    
    # Context Settings
    max_context_documents: int = 10
    max_context_tokens: int = 8000
    similarity_threshold: float = 0.7
    diversity_threshold: float = 0.8
    
    # Template Settings
    template_directory: str = "templates"
    default_language: str = "vi"
    
    # Performance Settings
    cache_ttl: int = 3600
    request_timeout: int = 30
    max_retries: int = 3
    
    class Config:
        env_file = ".env"
        env_prefix = "SYNTHESIS_"
EOF
```

#### **Step 1.3: Setup Testing Framework**
```bash
# Create test configuration
cat > tests/conftest.py << EOF
import pytest
from unittest.mock import Mock
from src.synthesis.config import SynthesisConfig

@pytest.fixture
def mock_config():
    return SynthesisConfig(
        openai_api_key="test-key",
        max_tokens=1000,
        max_context_documents=5
    )

@pytest.fixture  
def mock_documents():
    return [
        {
            "content": "Document 1 content about AI",
            "metadata": {"source": "doc1.pdf", "score": 0.9},
            "chunk_id": "chunk_1"
        },
        {
            "content": "Document 2 content about machine learning", 
            "metadata": {"source": "doc2.pdf", "score": 0.8},
            "chunk_id": "chunk_2"
        }
    ]

@pytest.fixture
def mock_user_query():
    return {
        "query": "What is artificial intelligence?",
        "user_id": "test_user",
        "department": "engineering",
        "language": "vi"
    }
EOF
```

### **Phase 2: Core Components Implementation (Week 2)**

#### **Step 2.1: Input Validation & Processing**

**🎯 Checklist:**
- [ ] Create input data models với Pydantic
- [ ] Implement input sanitization
- [ ] Add relevance scoring logic
- [ ] Write unit tests cho validation

```python
# File: src/synthesis/core/input_processor.py
from pydantic import BaseModel, Field, validator
from typing import List, Dict, Any, Optional
import re
import logging

class DocumentInput(BaseModel):
    content: str = Field(..., min_length=10)
    metadata: Dict[str, Any] = Field(default_factory=dict)
    chunk_id: str = Field(..., min_length=1)
    relevance_score: Optional[float] = Field(default=0.0, ge=0.0, le=1.0)
    
    @validator('content')
    def sanitize_content(cls, v):
        # Remove excessive whitespace, normalize text
        return re.sub(r'\s+', ' ', v.strip())

class QueryInput(BaseModel):
    query: str = Field(..., min_length=3, max_length=1000)
    user_id: str = Field(..., min_length=1)
    department: Optional[str] = None
    language: str = Field(default="vi")
    context: Dict[str, Any] = Field(default_factory=dict)

class SynthesisInput(BaseModel):
    documents: List[DocumentInput] = Field(..., min_items=0)
    query: QueryInput
    synthesis_config: Dict[str, Any] = Field(default_factory=dict)

# Implementation tasks:
# 1. Implement InputValidator class
# 2. Add document scoring logic  
# 3. Create input preprocessing pipeline
# 4. Add comprehensive error handling
```

**💡 Implementation Hints:**
```bash
# Test validation
poetry run pytest tests/unit/test_input_processor.py -v

# Check type hints
poetry run mypy src/synthesis/core/input_processor.py

# Format code
poetry run black src/synthesis/core/input_processor.py
```

#### **Step 2.2: Context Building Strategies**

**🎯 Checklist:**
- [ ] Implement TopK selection strategy
- [ ] Implement Diversity-based selection  
- [ ] Implement Threshold-based filtering
- [ ] Add token counting and optimization
- [ ] Create context optimization algorithms

```python
# File: src/synthesis/core/context_builder.py
from abc import ABC, abstractmethod
from typing import List, Dict, Any
import tiktoken
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

class ContextSelectionStrategy(ABC):
    @abstractmethod
    def select_documents(
        self, 
        documents: List[DocumentInput], 
        query: QueryInput,
        max_tokens: int
    ) -> List[DocumentInput]:
        pass

class TopKStrategy(ContextSelectionStrategy):
    def __init__(self, k: int = 5):
        self.k = k
    
    def select_documents(self, documents, query, max_tokens):
        # Implementation: Sort by relevance_score, take top K
        # Consider token limits
        pass

class DiversityStrategy(ContextSelectionStrategy):  
    def __init__(self, diversity_threshold: float = 0.8):
        self.diversity_threshold = diversity_threshold
    
    def select_documents(self, documents, query, max_tokens):
        # Implementation: Select diverse documents using embedding similarity
        # Avoid redundant information
        pass

# Implementation tasks:
# 1. Complete each strategy implementation
# 2. Add token counting với tiktoken
# 3. Implement context optimization
# 4. Add performance metrics collection
```

#### **Step 2.3: Template Management System**

**🎯 Checklist:**
- [ ] Setup Jinja2 template engine
- [ ] Create template categories (System, Query, Context, Fallback)
- [ ] Implement template loading and caching
- [ ] Add template validation
- [ ] Create multilingual support

```bash
# Create template directory structure
mkdir -p templates/{system,query,context,fallback}/{vi,en}

# Create base system template
cat > templates/system/vi/default.j2 << EOF
Bạn là một trợ lý AI thông minh chuyên tư vấn về tài liệu nội bộ công ty.

NGUYÊN TẮC HOẠT ĐỘNG:
1. Chỉ trả lời dựa trên thông tin được cung cấp trong CONTEXT bên dưới
2. Nếu không tìm thấy thông tin liên quan, hãy thành thật nói "Tôi không tìm thấy thông tin về vấn đề này trong tài liệu hiện có"
3. Luôn trích dẫn nguồn tài liệu khi đưa ra thông tin
4. Trả lời bằng tiếng Việt, rõ ràng và dễ hiểu
5. Nếu thông tin không chắc chắn, hãy nói rõ mức độ tin cậy

THÔNG TIN NGƯỜI DÙNG:
- Phòng ban: {{ user_department }}
- Cấp độ truy cập: {{ user_access_level }}
EOF
```

### **Phase 3: Template Processing & Prompt Assembly (Week 3)**

#### **Step 3.1: Template Engine Implementation**

**🎯 Checklist:**
- [ ] Setup Jinja2 với custom filters và functions
- [ ] Implement template inheritance
- [ ] Add template caching mechanism  
- [ ] Create template validation system
- [ ] Implement dynamic template selection

```python
# File: src/synthesis/templates/template_manager.py
from jinja2 import Environment, FileSystemLoader, select_autoescape
from typing import Dict, Any, Optional
import os
from functools import lru_cache

class TemplateManager:
    def __init__(self, template_dir: str = "templates"):
        self.env = Environment(
            loader=FileSystemLoader(template_dir),
            autoescape=select_autoescape(['html', 'xml']),
            trim_blocks=True,
            lstrip_blocks=True
        )
        
        # Add custom filters
        self.env.filters['truncate_tokens'] = self._truncate_by_tokens
        self.env.filters['format_citations'] = self._format_citations
        
    @lru_cache(maxsize=128)
    def load_template(self, template_type: str, language: str = "vi") -> str:
        # Implementation: Load and cache templates
        pass
    
    def render_template(
        self, 
        template_name: str, 
        context: Dict[str, Any],
        language: str = "vi"
    ) -> str:
        # Implementation: Render template with context
        pass

# Implementation tasks:
# 1. Complete template loading logic
# 2. Add custom Jinja2 filters for AI-specific formatting
# 3. Implement template validation
# 4. Add error handling for missing templates
```

#### **Step 3.2: Prompt Assembly Engine**

**🎯 Checklist:**
- [ ] Create prompt assembly pipeline
- [ ] Implement token counting and limit management
- [ ] Add prompt quality validation
- [ ] Create prompt optimization algorithms
- [ ] Add structured output formatting

```python
# File: src/synthesis/core/prompt_assembler.py
from typing import List, Dict, Any, Optional
import tiktoken
from dataclasses import dataclass

@dataclass
class AssembledPrompt:
    system_prompt: str
    user_prompt: str
    total_tokens: int
    context_documents: List[Dict[str, Any]]
    metadata: Dict[str, Any]

class PromptAssembler:
    def __init__(self, model_name: str = "gpt-3.5-turbo"):
        self.tokenizer = tiktoken.encoding_for_model(model_name)
        self.max_tokens = 4000  # Configurable
        
    def assemble_prompt(
        self,
        query: QueryInput,
        context_documents: List[DocumentInput],
        templates: Dict[str, str]
    ) -> AssembledPrompt:
        # Implementation: Assemble all components into final prompt
        pass
    
    def optimize_prompt_length(self, prompt: str, max_tokens: int) -> str:
        # Implementation: Truncate intelligently while preserving meaning
        pass

# Implementation tasks:
# 1. Complete prompt assembly logic
# 2. Add intelligent truncation algorithms  
# 3. Implement prompt quality scoring
# 4. Add metadata enrichment
```

### **Phase 4: Error Handling & Fallback Mechanisms (Week 4)**

#### **Step 4.1: Comprehensive Error Handling**

**🎯 Checklist:**
- [ ] Create error taxonomy và handling strategies
- [ ] Implement graceful degradation
- [ ] Add retry mechanisms với backoff
- [ ] Create error logging và monitoring
- [ ] Implement health checks

```python
# File: src/synthesis/core/error_handler.py
from enum import Enum
from typing import Optional, Dict, Any
import structlog
from tenacity import retry, stop_after_attempt, wait_exponential

class SynthesisError(Exception):
    """Base exception for synthesis module"""
    pass

class ErrorType(Enum):
    INVALID_INPUT = "invalid_input"
    NO_RELEVANT_DOCUMENTS = "no_relevant_documents"  
    TEMPLATE_ERROR = "template_error"
    TOKEN_LIMIT_EXCEEDED = "token_limit_exceeded"
    LLM_API_ERROR = "llm_api_error"

class ErrorHandler:
    def __init__(self):
        self.logger = structlog.get_logger()
        
    def handle_error(
        self, 
        error_type: ErrorType, 
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        # Implementation: Route to appropriate error handling strategy
        pass
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10)
    )
    def retry_with_fallback(self, operation, fallback):
        # Implementation: Retry logic with fallback
        pass

# Implementation tasks:
# 1. Complete error handling strategies
# 2. Add comprehensive logging
# 3. Implement circuit breaker pattern
# 4. Add error metrics collection
```

#### **Step 4.2: Fallback Response Generation**

**🎯 Checklist:**
- [ ] Create fallback response templates
- [ ] Implement intelligent fallback selection
- [ ] Add contextual fallback messages
- [ ] Create escalation mechanisms
- [ ] Add user guidance features

```python
# File: src/synthesis/core/fallback_generator.py
from typing import Dict, Any, List
from enum import Enum

class FallbackType(Enum):
    NO_DOCUMENTS_FOUND = "no_documents"
    INSUFFICIENT_PERMISSION = "insufficient_permission"
    QUERY_TOO_VAGUE = "query_too_vague"
    TECHNICAL_ERROR = "technical_error"

class FallbackGenerator:
    def __init__(self, template_manager):
        self.template_manager = template_manager
        
    def generate_fallback_response(
        self,
        fallback_type: FallbackType,
        query: QueryInput,
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        # Implementation: Generate appropriate fallback response
        pass
    
    def suggest_alternatives(self, query: str) -> List[str]:
        # Implementation: Suggest alternative queries or actions
        pass

# Implementation tasks:  
# 1. Create fallback response templates
# 2. Implement smart suggestion algorithms
# 3. Add user guidance mechanisms
# 4. Create escalation workflows
```

---

## 🔄 **WORKFLOW VÀ FAILURE HANDLING**

### **Main Workflow Diagram**

```mermaid
flowchart TD
    Start([🚀 Start Synthesis Process]) --> ValidateInput{✅ Validate Input?}
    
    ValidateInput -->|❌ Invalid| HandleInputError[🚨 Handle Input Error]
    ValidateInput -->|✅ Valid| ScoreDocuments[📊 Score Documents]
    
    HandleInputError --> ReturnError1[❌ Return Error Response]
    
    ScoreDocuments --> HasDocuments{📚 Has Relevant Documents?}
    
    HasDocuments -->|❌ No| GenerateNoResultsFallback[🔄 Generate No Results Fallback]
    HasDocuments -->|✅ Yes| SelectStrategy{🎯 Select Context Strategy}
    
    GenerateNoResultsFallback --> ReturnFallback1[🔄 Return Fallback Response]
    
    SelectStrategy --> TopKStrategy[📈 Top-K Strategy]
    SelectStrategy --> DiversityStrategy[🎯 Diversity Strategy] 
    SelectStrategy --> ThresholdStrategy[📏 Threshold Strategy]
    
    TopKStrategy --> OptimizeContext[⚡ Optimize Context]
    DiversityStrategy --> OptimizeContext
    ThresholdStrategy --> OptimizeContext
    
    OptimizeContext --> TokenCheck{🔢 Within Token Limit?}
    
    TokenCheck -->|❌ Exceed| TruncateContext[✂️ Truncate Context]
    TokenCheck -->|✅ OK| LoadTemplates[📋 Load Templates]
    
    TruncateContext --> LoadTemplates
    
    LoadTemplates --> TemplateExists{📝 Template Exists?}
    
    TemplateExists -->|❌ No| HandleTemplateError[🚨 Handle Template Error]
    TemplateExists -->|✅ Yes| RenderTemplate[🎨 Render Template]
    
    HandleTemplateError --> UseDefaultTemplate[📋 Use Default Template]
    UseDefaultTemplate --> RenderTemplate
    
    RenderTemplate --> RenderSuccess{✅ Render Success?}
    
    RenderSuccess -->|❌ Fail| HandleRenderError[🚨 Handle Render Error]
    RenderSuccess -->|✅ Success| AssemblePrompt[🔧 Assemble Final Prompt]
    
    HandleRenderError --> GenerateBasicFallback[🔄 Generate Basic Fallback]
    GenerateBasicFallback --> ReturnFallback2[🔄 Return Fallback Response]
    
    AssemblePrompt --> ValidatePrompt{✅ Validate Final Prompt?}
    
    ValidatePrompt -->|❌ Invalid| HandlePromptError[🚨 Handle Prompt Error]
    ValidatePrompt -->|✅ Valid| EnrichMetadata[🏷️ Enrich Metadata]
    
    HandlePromptError --> RetryAttempt{🔄 Retry Available?}
    
    RetryAttempt -->|✅ Yes| RetryCount[📊 Increment Retry Counter]
    RetryAttempt -->|❌ No| GenerateFinalFallback[🔄 Generate Final Fallback]
    
    RetryCount --> SelectStrategy
    GenerateFinalFallback --> ReturnFallback3[🔄 Return Fallback Response]
    
    EnrichMetadata --> LogSuccess[📝 Log Success Metrics]
    LogSuccess --> ReturnSuccess[✅ Return Synthesized Prompt]
    
    %% Error Return Points
    ReturnError1 --> End([🏁 End Process])
    ReturnFallback1 --> End
    ReturnFallback2 --> End  
    ReturnFallback3 --> End
    ReturnSuccess --> End
    
    %% Styling
    classDef startEnd fill:#e3f2fd,stroke:#1976d2,stroke-width:3px
    classDef process fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef decision fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef error fill:#ffebee,stroke:#c62828,stroke-width:2px
    classDef fallback fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef success fill:#e0f2f1,stroke:#00796b,stroke-width:2px
    
    class Start,End startEnd
    class ScoreDocuments,OptimizeContext,TruncateContext,LoadTemplates,RenderTemplate,AssemblePrompt,EnrichMetadata,LogSuccess,RetryCount process
    class ValidateInput,HasDocuments,SelectStrategy,TokenCheck,TemplateExists,RenderSuccess,ValidatePrompt,RetryAttempt decision
    class HandleInputError,HandleTemplateError,HandleRenderError,HandlePromptError error
    class GenerateNoResultsFallback,GenerateBasicFallback,GenerateFinalFallback,UseDefaultTemplate,ReturnFallback1,ReturnFallback2,ReturnFallback3 fallback
    class ReturnSuccess success
```

### **Failure Recovery Points**

```mermaid
graph TB
    subgraph "🔄 RECOVERY STRATEGIES"
        subgraph "Level 1: Input Failures"
            InputFail[❌ Input Validation Failed]
            InputRecover[🔧 Sanitize & Retry]
            InputFallback[📋 Request Clarification]
        end
        
        subgraph "Level 2: Context Failures" 
            ContextFail[❌ Context Building Failed]
            ContextRecover[🔧 Reduce Context Size]
            ContextFallback[📚 Use Cached Context]
        end
        
        subgraph "Level 3: Template Failures"
            TemplateFail[❌ Template Loading Failed]
            TemplateRecover[🔧 Load Default Template]  
            TemplateFallback[📝 Use Minimal Template]
        end
        
        subgraph "Level 4: System Failures"
            SystemFail[❌ System Error]
            SystemRecover[🔧 Circuit Breaker]
            SystemFallback[🚨 Emergency Response]
        end
    end
    
    subgraph "📊 MONITORING & ALERTS"
        MetricsCollector[📈 Metrics Collector]
        AlertManager[🚨 Alert Manager]
        HealthChecker[❤️ Health Checker]
    end
    
    %% Recovery Flows
    InputFail --> InputRecover
    InputRecover -->|Success| Normal[✅ Continue Normal Flow]
    InputRecover -->|Fail| InputFallback
    
    ContextFail --> ContextRecover
    ContextRecover -->|Success| Normal
    ContextRecover -->|Fail| ContextFallback
    
    TemplateFail --> TemplateRecover
    TemplateRecover -->|Success| Normal  
    TemplateRecover -->|Fail| TemplateFallback
    
    SystemFail --> SystemRecover
    SystemRecover -->|Success| Normal
    SystemRecover -->|Fail| SystemFallback
    
    %% Monitoring Connections
    InputFail -.-> MetricsCollector
    ContextFail -.-> MetricsCollector
    TemplateFail -.-> MetricsCollector
    SystemFail -.-> AlertManager
    
    MetricsCollector --> HealthChecker
    AlertManager --> HealthChecker
```
---
---

## 🧪 **TESTING STRATEGY & IMPLEMENTATION**

### **Phase 5: Comprehensive Testing (Week 5)**

#### **Step 5.1: Unit Testing Implementation**

**🎯 Checklist:**
- [ ] Test all core components independently
- [ ] Mock external dependencies (LLM APIs, databases)
- [ ] Achieve 90%+ code coverage
- [ ] Test error conditions và edge cases
- [ ] Performance benchmarking

```python
# File: tests/unit/test_context_builder.py
import pytest
from unittest.mock import Mock, patch
from src.synthesis.core.context_builder import TopKStrategy, DiversityStrategy
from src.synthesis.models import DocumentInput, QueryInput

class TestTopKStrategy:
    def test_select_top_k_documents(self, mock_documents, mock_config):
        """Test selecting top K documents by relevance score"""
        strategy = TopKStrategy(k=3)
        
        # Sort mock documents by score
        sorted_docs = sorted(mock_documents, key=lambda x: x.relevance_score, reverse=True)
        
        selected = strategy.select_documents(
            documents=mock_documents,
            query=QueryInput(query="test query", user_id="test"),
            max_tokens=1000
        )
        
        assert len(selected) == 3
        assert selected[0].relevance_score >= selected[1].relevance_score
        
    def test_token_limit_respected(self, mock_documents):
        """Test that token limits are respected"""
        strategy = TopKStrategy(k=10)  # Request more than available
        
        selected = strategy.select_documents(
            documents=mock_documents,
            query=QueryInput(query="test", user_id="test"), 
            max_tokens=500  # Restrictive limit
        )
        
        total_tokens = sum(len(doc.content.split()) for doc in selected)
        assert total_tokens <= 500
        
    @pytest.mark.parametrize("k,expected_count", [
        (1, 1),
        (5, 5),
        (100, 10)  # More than available documents
    ])
    def test_various_k_values(self, mock_documents, k, expected_count):
        """Test strategy với different K values"""
        strategy = TopKStrategy(k=k)
        selected = strategy.select_documents(mock_documents, Mock(), 10000)
        assert len(selected) <= min(k, len(mock_documents))

# File: tests/unit/test_template_manager.py  
class TestTemplateManager:
    def test_template_loading(self, tmp_path):
        """Test template loading từ filesystem"""
        # Create temporary template
        template_dir = tmp_path / "templates"
        template_dir.mkdir()
        (template_dir / "test.j2").write_text("Hello {{ name }}!")
        
        manager = TemplateManager(str(template_dir))
        template = manager.load_template("test")
        
        assert template == "Hello {{ name }}!"
        
    def test_template_rendering(self, tmp_path):
        """Test template rendering với context"""
        template_dir = tmp_path / "templates"  
        template_dir.mkdir()
        (template_dir / "context.j2").write_text("""
        Query: {{ query }}
        Documents:
        {% for doc in documents %}
        - {{ doc.content }}
        {% endfor %}
        """)
        
        manager = TemplateManager(str(template_dir))
        rendered = manager.render_template("context", {
            "query": "What is AI?",
            "documents": [{"content": "AI is intelligence"}]
        })
        
        assert "What is AI?" in rendered
        assert "AI is intelligence" in rendered
        
    def test_missing_template_handling(self):
        """Test handling của missing templates"""
        manager = TemplateManager("/nonexistent/path")
        
        with pytest.raises(TemplateNotFoundError):
            manager.load_template("nonexistent")
```

#### **Step 5.2: Integration Testing**

**🎯 Checklist:**
- [ ] Test end-to-end synthesis pipeline
- [ ] Test integration với external services
- [ ] Test database interactions
- [ ] Validate performance under load
- [ ] Test fallback mechanisms

```python
# File: tests/integration/test_synthesis_pipeline.py
import pytest
from unittest.mock import AsyncMock, patch
from src.synthesis.core.synthesis_engine import SynthesisEngine
from src.synthesis.models import SynthesisInput, QueryInput, DocumentInput

class TestSynthesisPipeline:
    @pytest.fixture
    def synthesis_engine(self, mock_config):
        """Create synthesis engine với mocked dependencies"""
        return SynthesisEngine(config=mock_config)
        
    async def test_full_synthesis_pipeline(self, synthesis_engine):
        """Test complete synthesis từ input đến output"""
        # Prepare input
        synthesis_input = SynthesisInput(
            documents=[
                DocumentInput(
                    content="Python is a programming language",
                    metadata={"source": "python_guide.pdf"},
                    chunk_id="chunk_1",
                    relevance_score=0.95
                )
            ],
            query=QueryInput(
                query="What is Python?",
                user_id="test_user",
                department="engineering"
            )
        )
        
        # Mock external LLM API
        with patch('openai.ChatCompletion.create') as mock_llm:
            mock_llm.return_value = AsyncMock()
            
            result = await synthesis_engine.synthesize(synthesis_input)
            
            assert result.system_prompt is not None
            assert result.user_prompt is not None
            assert "Python" in result.user_prompt
            assert len(result.context_documents) > 0
            
    async def test_no_documents_fallback(self, synthesis_engine):
        """Test fallback khi không có documents"""
        synthesis_input = SynthesisInput(
            documents=[],  # No documents
            query=QueryInput(query="What is AI?", user_id="test")
        )
        
        result = await synthesis_engine.synthesize(synthesis_input)
        
        assert result.is_fallback is True
        assert "không tìm thấy thông tin" in result.user_prompt.lower()
        
    async def test_token_limit_optimization(self, synthesis_engine):
        """Test token limit optimization"""
        # Create documents that exceed token limit
        large_documents = [
            DocumentInput(
                content="Very long content " * 1000,  # Exceed token limit
                metadata={},
                chunk_id=f"chunk_{i}",
                relevance_score=0.8
            ) for i in range(10)
        ]
        
        synthesis_input = SynthesisInput(
            documents=large_documents,
            query=QueryInput(query="Test query", user_id="test")
        )
        
        result = await synthesis_engine.synthesize(synthesis_input)
        
        # Should be within token limits
        assert result.total_tokens <= synthesis_engine.config.max_tokens
        assert len(result.context_documents) < len(large_documents)  # Some truncated
```

#### **Step 5.3: Performance Testing**

```python
# File: tests/performance/test_synthesis_performance.py
import pytest
import asyncio
import time
from concurrent.futures import ThreadPoolExecutor

class TestSynthesisPerformance:
    @pytest.mark.performance
    async def test_synthesis_latency(self, synthesis_engine, mock_documents):
        """Test synthesis latency requirements"""
        synthesis_input = SynthesisInput(
            documents=mock_documents,
            query=QueryInput(query="Performance test", user_id="test")
        )
        
        start_time = time.time()
        result = await synthesis_engine.synthesize(synthesis_input)
        end_time = time.time()
        
        latency = end_time - start_time
        assert latency < 2.0  # Should complete within 2 seconds
        assert result.total_tokens > 0
        
    @pytest.mark.performance  
    async def test_concurrent_synthesis(self, synthesis_engine):
        """Test concurrent synthesis requests"""
        async def single_synthesis():
            synthesis_input = SynthesisInput(
                documents=mock_documents[:5],
                query=QueryInput(query=f"Test query {time.time()}", user_id="test")
            )
            return await synthesis_engine.synthesize(synthesis_input)
            
        # Run 10 concurrent requests
        tasks = [single_synthesis() for _ in range(10)]
        start_time = time.time()
        results = await asyncio.gather(*tasks)
        total_time = time.time() - start_time
        
        assert len(results) == 10
        assert all(r.total_tokens > 0 for r in results)
        assert total_time < 10.0  # All requests within 10 seconds
        
    def test_memory_usage(self, synthesis_engine):
        """Test memory usage under load"""
        import psutil
        import os
        
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss
        
        # Process many documents
        for i in range(100):
            large_input = SynthesisInput(
                documents=[DocumentInput(
                    content=f"Large content block {j}" * 100,
                    metadata={},
                    chunk_id=f"chunk_{i}_{j}",
                    relevance_score=0.5
                ) for j in range(50)],
                query=QueryInput(query=f"Query {i}", user_id="test")
            )
            
            # Force synchronous processing for memory test
            asyncio.run(synthesis_engine.synthesize(large_input))
            
        final_memory = process.memory_info().rss
        memory_growth = (final_memory - initial_memory) / (1024 * 1024)  # MB
        
        assert memory_growth < 500  # Less than 500MB growth
```

---

## 📊 **MONITORING & OBSERVABILITY**

### **Step 6: Production Monitoring Setup**

#### **Step 6.1: Metrics Collection**

```python
# File: src/synthesis/monitoring/metrics.py
from prometheus_client import Counter, Histogram, Gauge, CollectorRegistry
from typing import Dict, Any
import time
import functools

class SynthesisMetrics:
    def __init__(self, registry: CollectorRegistry = None):
        self.registry = registry or CollectorRegistry()
        
        # Counter metrics
        self.synthesis_requests_total = Counter(
            'synthesis_requests_total',
            'Total number of synthesis requests',
            ['status', 'user_department', 'fallback_type'],
            registry=self.registry
        )
        
        # Histogram metrics  
        self.synthesis_duration = Histogram(
            'synthesis_duration_seconds',
            'Time spent on synthesis processing',
            ['stage'],
            registry=self.registry
        )
        
        self.context_documents_count = Histogram(
            'synthesis_context_documents_count',
            'Number of documents in context',
            registry=self.registry
        )
        
        self.prompt_tokens = Histogram(
            'synthesis_prompt_tokens',
            'Number of tokens in generated prompt',
            registry=self.registry
        )
        
        # Gauge metrics
        self.active_synthesis_requests = Gauge(
            'synthesis_active_requests',
            'Number of active synthesis requests',
            registry=self.registry
        )
        
    def record_request(self, status: str, department: str = "", fallback_type: str = ""):
        """Record synthesis request"""
        self.synthesis_requests_total.labels(
            status=status,
            user_department=department, 
            fallback_type=fallback_type
        ).inc()
        
    def record_duration(self, stage: str, duration: float):
        """Record processing duration"""
        self.synthesis_duration.labels(stage=stage).observe(duration)
        
    def record_context_size(self, document_count: int):
        """Record context size"""
        self.context_documents_count.observe(document_count)
        
    def record_prompt_tokens(self, token_count: int):
        """Record prompt token count"""  
        self.prompt_tokens.observe(token_count)

# Decorator for automatic metrics collection
def track_synthesis_metrics(metrics: SynthesisMetrics):
    def decorator(func):
        @functools.wraps(func)
        async def wrapper(*args, **kwargs):
            start_time = time.time()
            metrics.active_synthesis_requests.inc()
            
            try:
                result = await func(*args, **kwargs)
                duration = time.time() - start_time
                
                metrics.record_duration("total", duration)
                metrics.record_request("success")
                
                if hasattr(result, 'context_documents'):
                    metrics.record_context_size(len(result.context_documents))
                if hasattr(result, 'total_tokens'):
                    metrics.record_prompt_tokens(result.total_tokens)
                    
                return result
                
            except Exception as e:
                duration = time.time() - start_time
                metrics.record_duration("error", duration)
                metrics.record_request("error")
                raise
                
            finally:
                metrics.active_synthesis_requests.dec()
                
        return wrapper
    return decorator
```

#### **Step 6.2: Structured Logging**

```python
# File: src/synthesis/monitoring/logging.py
import structlog
import logging.config
from typing import Dict, Any, Optional
import json
from datetime import datetime

def configure_logging(log_level: str = "INFO", log_format: str = "json"):
    """Configure structured logging for synthesis module"""
    
    logging.config.dictConfig({
        "version": 1,
        "disable_existing_loggers": False,
        "formatters": {
            "json": {
                "()": structlog.stdlib.ProcessorFormatter,
                "processor": structlog.dev.ConsoleRenderer(colors=False),
                "foreign_pre_chain": [
                    structlog.stdlib.add_log_level,
                    structlog.stdlib.add_logger_name,
                    structlog.stdlib.ExtraAdder(),
                    structlog.processors.TimeStamper(fmt="ISO"),
                ],
            },
        },
        "handlers": {
            "console": {
                "level": log_level,
                "class": "logging.StreamHandler",
                "formatter": "json",
            },
            "file": {
                "level": log_level,
                "class": "logging.handlers.RotatingFileHandler",
                "filename": "logs/synthesis.log",
                "maxBytes": 10485760,  # 10MB
                "backupCount": 5,
                "formatter": "json",
            },
        },
        "loggers": {
            "synthesis": {
                "handlers": ["console", "file"],
                "level": log_level,
                "propagate": False,
            },
        },
    })
    
    structlog.configure(
        processors=[
            structlog.stdlib.filter_by_level,
            structlog.stdlib.add_logger_name,
            structlog.stdlib.add_log_level,
            structlog.stdlib.PositionalArgumentsFormatter(),
            structlog.processors.StackInfoRenderer(),
            structlog.processors.format_exc_info,
            structlog.processors.UnicodeDecoder(),
            structlog.stdlib.ProcessorFormatter.wrap_for_formatter,
        ],
        context_class=dict,
        logger_factory=structlog.stdlib.LoggerFactory(),
        cache_logger_on_first_use=True,
    )

class SynthesisLogger:
    def __init__(self, component_name: str):
        self.logger = structlog.get_logger("synthesis").bind(component=component_name)
        
    def log_synthesis_start(self, query: str, user_id: str, document_count: int):
        """Log synthesis process start"""
        self.logger.info(
            "synthesis_started",
            query_length=len(query),
            user_id=user_id,
            document_count=document_count,
            timestamp=datetime.utcnow().isoformat()
        )
        
    def log_context_building(self, strategy: str, selected_docs: int, total_tokens: int):
        """Log context building details"""
        self.logger.info(
            "context_built",
            strategy=strategy,
            selected_documents=selected_docs,
            total_tokens=total_tokens
        )
        
    def log_template_processing(self, template_type: str, render_time: float):
        """Log template processing"""
        self.logger.info(
            "template_processed", 
            template_type=template_type,
            render_time_ms=render_time * 1000
        )
        
    def log_synthesis_complete(self, total_time: float, final_tokens: int, is_fallback: bool):
        """Log successful synthesis completion"""
        self.logger.info(
            "synthesis_completed",
            total_time_ms=total_time * 1000,
            final_tokens=final_tokens,
            is_fallback=is_fallback
        )
        
    def log_error(self, error_type: str, error_msg: str, context: Dict[str, Any] = None):
        """Log errors với context"""
        self.logger.error(
            "synthesis_error",
            error_type=error_type,
            error_message=error_msg,
            context=context or {}
        )
        
    def log_fallback_triggered(self, fallback_type: str, reason: str):
        """Log fallback mechanism activation"""
        self.logger.warning(
            "fallback_triggered",
            fallback_type=fallback_type,
            reason=reason
        )
```

#### **Step 6.3: Health Checks & Alerting**

```python
# File: src/synthesis/monitoring/health.py
from typing import Dict, List, Optional
import asyncio
import time
from enum import Enum
from dataclasses import dataclass

class HealthStatus(Enum):
    HEALTHY = "healthy"
    DEGRADED = "degraded"  
    UNHEALTHY = "unhealthy"

@dataclass
class HealthCheck:
    name: str
    status: HealthStatus
    message: str
    duration_ms: float
    timestamp: str

class SynthesisHealthChecker:
    def __init__(self, synthesis_engine):
        self.synthesis_engine = synthesis_engine
        self.health_checks: List[HealthCheck] = []
        
    async def check_template_loading(self) -> HealthCheck:
        """Check if templates can be loaded"""
        start_time = time.time()
        
        try:
            template = self.synthesis_engine.template_manager.load_template(
                "system/vi/default"
            )
            
            if template:
                status = HealthStatus.HEALTHY
                message = "Templates loading successfully"
            else:
                status = HealthStatus.DEGRADED
                message = "Template loaded but empty"
                
        except Exception as e:
            status = HealthStatus.UNHEALTHY
            message = f"Template loading failed: {str(e)}"
            
        duration = (time.time() - start_time) * 1000
        
        return HealthCheck(
            name="template_loading",
            status=status,
            message=message,
            duration_ms=duration,
            timestamp=time.strftime('%Y-%m-%d %H:%M:%S')
        )
        
    async def check_context_building(self) -> HealthCheck:
        """Check context building performance"""
        start_time = time.time()
        
        try:
            # Test với mock documents
            mock_docs = [
                DocumentInput(
                    content="Test document for health check",
                    metadata={"source": "health_check"},
                    chunk_id="health_check_1",
                    relevance_score=0.8
                )
            ]
            
            context_builder = self.synthesis_engine.context_builder
            selected = context_builder.select_documents(
                documents=mock_docs,
                query=QueryInput(query="health check", user_id="system"),
                max_tokens=1000
            )
            
            if selected:
                status = HealthStatus.HEALTHY
                message = f"Context building working, selected {len(selected)} documents"
            else:
                status = HealthStatus.DEGRADED  
                message = "Context building returned no documents"
                
        except Exception as e:
            status = HealthStatus.UNHEALTHY
            message = f"Context building failed: {str(e)}"
            
        duration = (time.time() - start_time) * 1000
        
        return HealthCheck(
            name="context_building",
            status=status,
            message=message, 
            duration_ms=duration,
            timestamp=time.strftime('%Y-%m-%d %H:%M:%S')
        )
        
    async def check_overall_health(self) -> Dict[str, any]:
        """Run all health checks"""
        checks = await asyncio.gather(
            self.check_template_loading(),
            self.check_context_building(),
            return_exceptions=True
        )
        
        # Determine overall status
        statuses = [check.status for check in checks if isinstance(check, HealthCheck)]
        
        if HealthStatus.UNHEALTHY in statuses:
            overall_status = HealthStatus.UNHEALTHY
        elif HealthStatus.DEGRADED in statuses:
            overall_status = HealthStatus.DEGRADED
        else:
            overall_status = HealthStatus.HEALTHY
            
        return {
            "status": overall_status.value,
            "timestamp": time.strftime('%Y-%m-%d %H:%M:%S'),
            "checks": [
                {
                    "name": check.name,
                    "status": check.status.value,
                    "message": check.message,
                    "duration_ms": check.duration_ms
                }
                for check in checks if isinstance(check, HealthCheck)
            ]
        }
```

---

## 🚀 **DEPLOYMENT & PRODUCTION SETUP**

### **Phase 6: Production Deployment (Week 6)**

#### **Step 6.1: Docker Configuration**

```dockerfile
# File: Dockerfile
FROM python:3.11-slim as builder

# Set working directory
WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# Copy dependency files
COPY pyproject.toml poetry.lock ./

# Install poetry và dependencies
RUN pip install poetry==1.5.1 \
    && poetry config virtualenvs.create false \
    && poetry install --no-dev --no-interaction --no-ansi

# Production stage  
FROM python:3.11-slim as production

WORKDIR /app

# Copy installed packages from builder
COPY --from=builder /usr/local/lib/python3.11/site-packages /usr/local/lib/python3.11/site-packages
COPY --from=builder /usr/local/bin /usr/local/bin

# Copy application code
COPY src/ ./src/
COPY templates/ ./templates/
COPY config/ ./config/

# Create logs directory
RUN mkdir -p logs

# Create non-root user
RUN adduser --disabled-password --gecos '' synthesisuser \
    && chown -R synthesisuser:synthesisuser /app

USER synthesisuser

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD python -c "import requests; requests.get('http://localhost:8000/health')"

# Expose port
EXPOSE 8000

# Start application
CMD ["python", "-m", "uvicorn", "src.synthesis.api.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

```yaml
# File: docker-compose.yml
version: '3.8'

services:
  synthesis-service:
    build: .
    ports:
      - "8000:8000"
    environment:
      - SYNTHESIS_OPENAI_API_KEY=${OPENAI_API_KEY}
      - SYNTHESIS_MAX_TOKENS=4000
      - SYNTHESIS_CACHE_TTL=3600
      - REDIS_URL=redis://redis:6379
      - POSTGRES_URL=postgresql://user:pass@postgres:5432/synthesis
    depends_on:
      - redis
      - postgres
    volumes:
      - ./logs:/app/logs
      - ./config:/app/config
    restart: unless-stopped
    
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped
    
  postgres:
    image: postgres:15-alpine
    environment:
      - POSTGRES_DB=synthesis
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=pass
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    restart: unless-stopped
    
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    restart: unless-stopped
    
  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana:/etc/grafana/provisioning
    restart: unless-stopped

volumes:
  redis_data:
  postgres_data:
  prometheus_data:
  grafana_data:
```

#### **Step 6.2: Kubernetes Deployment**

```yaml
# File: k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: synthesis-service
  labels:
    app: synthesis-service
spec:
  replicas: 3
  selector:
    matchLabels:
      app: synthesis-service
  template:
    metadata:
      labels:
        app: synthesis-service
    spec:
      containers:
      - name: synthesis
        image: synthesis-service:latest
        ports:
        - containerPort: 8000
        env:
        - name: SYNTHESIS_OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: synthesis-secrets
              key: openai-api-key
        - name: REDIS_URL
          value: "redis://redis-service:6379"
        - name: POSTGRES_URL
          valueFrom:
            secretKeyRef:
              name: synthesis-secrets
              key: postgres-url
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 2000m
            memory: 4Gi
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
        volumeMounts:
        - name: config
          mountPath: /app/config
          readOnly: true
        - name: logs
          mountPath: /app/logs
      volumes:
      - name: config
        configMap:
          name: synthesis-config
      - name: logs
        emptyDir: {}

---
apiVersion: v1
kind: Service
metadata:
  name: synthesis-service
spec:
  selector:
    app: synthesis-service
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
  type: ClusterIP

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: synthesis-config
data:
  production.yaml: |
    max_tokens: 4000
    max_context_documents: 10
    similarity_threshold: 0.7
    cache_ttl: 3600
    request_timeout: 30
    max_retries: 3

---
apiVersion: v1
kind: Secret
metadata:
  name: synthesis-secrets
type: Opaque
data:
  openai-api-key: <base64-encoded-api-key>
  postgres-url: <base64-encoded-postgres-url>
```

#### **Step 6.3: Production Configuration & Environment Variables**

```python
# File: config/production.py
from src.synthesis.config.base import SynthesisConfig
import os

class ProductionConfig(SynthesisConfig):
    """Production configuration với security & performance optimizations"""
    
    # API Configuration
    openai_api_key: str = os.getenv("SYNTHESIS_OPENAI_API_KEY", "")
    anthropic_api_key: str = os.getenv("SYNTHESIS_ANTHROPIC_API_KEY", "")
    
    # Performance Settings
    max_tokens: int = int(os.getenv("SYNTHESIS_MAX_TOKENS", "4000"))
    max_context_documents: int = int(os.getenv("SYNTHESIS_MAX_CONTEXT_DOCS", "10"))
    max_concurrent_requests: int = int(os.getenv("SYNTHESIS_MAX_CONCURRENT", "100"))
    
    # Cache Configuration  
    redis_url: str = os.getenv("REDIS_URL", "redis://localhost:6379")
    cache_ttl: int = int(os.getenv("SYNTHESIS_CACHE_TTL", "3600"))
    
    # Database Configuration
    postgres_url: str = os.getenv("POSTGRES_URL", "")
    
    # Security Settings
    api_rate_limit: str = os.getenv("SYNTHESIS_RATE_LIMIT", "100/hour")
    cors_origins: list = os.getenv("SYNTHESIS_CORS_ORIGINS", "*").split(",")
    
    # Monitoring & Logging
    log_level: str = os.getenv("LOG_LEVEL", "INFO")
    metrics_enabled: bool = os.getenv("METRICS_ENABLED", "true").lower() == "true"
    
    # Health Check Configuration
    health_check_timeout: int = int(os.getenv("HEALTH_CHECK_TIMEOUT", "5"))
    
    class Config:
        env_file = ".env.production"
        case_sensitive = True
```

---

## 📚 **DOCUMENTATION & HANDOVER**

### **Step 7: Complete Documentation Package**

#### **Step 7.1: API Documentation**

```python
# File: src/synthesis/api/main.py
from fastapi import FastAPI, HTTPException, Depends
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
import structlog

app = FastAPI(
    title="RAG Synthesis Service",
    description="Context synthesis và prompt generation cho RAG system",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Configure properly in production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class SynthesisRequest(BaseModel):
    """Request model cho synthesis endpoint"""
    documents: List[Dict[str, Any]]
    query: Dict[str, Any]
    options: Optional[Dict[str, Any]] = None
    
    class Config:
        schema_extra = {
            "example": {
                "documents": [
                    {
                        "content": "Python is a programming language...",
                        "metadata": {"source": "python_guide.pdf", "page": 1},
                        "chunk_id": "chunk_001",
                        "relevance_score": 0.95
                    }
                ],
                "query": {
                    "query": "What is Python programming language?"
                    "user_id": "user123",
                    "department": "engineering",
                    "language": "vi"
                },
                "options": {
                    "max_context_documents": 5,
                    "strategy": "top_k",
                    "include_citations": True
                }
            }
        }

class SynthesisResponse(BaseModel):
    """Response model cho synthesis endpoint"""
    system_prompt: str
    user_prompt: str
    total_tokens: int
    context_documents: List[Dict[str, Any]]
    metadata: Dict[str, Any]
    is_fallback: bool = False
    processing_time_ms: float
    
    class Config:
        schema_extra = {
            "example": {
                "system_prompt": "Bạn là trợ lý AI chuyên tư vấn tài liệu nội bộ...",
                "user_prompt": "Context: Python là ngôn ngữ lập trình...\nQuestion: Python là gì?",
                "total_tokens": 1250,
                "context_documents": [
                    {
                        "chunk_id": "chunk_001", 
                        "source": "python_guide.pdf",
                        "relevance_score": 0.95
                    }
                ],
                "metadata": {
                    "strategy_used": "top_k",
                    "documents_selected": 3,
                    "documents_available": 10
                },
                "is_fallback": False,
                "processing_time_ms": 245.6
            }
        }

@app.post("/synthesize", response_model=SynthesisResponse)
async def synthesize_context(
    request: SynthesisRequest,
    synthesis_engine = Depends(get_synthesis_engine)
):
    """
    Synthesize context và generate prompt cho LLM từ retrieved documents
    
    - **documents**: Danh sách documents được retrieve từ vector search
    - **query**: User query và context information  
    - **options**: Optional configuration cho synthesis process
    
    Returns structured prompt ready cho LLM processing
    """
    try:
        logger = structlog.get_logger()
        logger.info("synthesis_request_received", 
                   user_id=request.query.get("user_id"),
                   document_count=len(request.documents))
        
        synthesis_input = SynthesisInput(
            documents=[DocumentInput(**doc) for doc in request.documents],
            query=QueryInput(**request.query),
            synthesis_config=request.options or {}
        )
        
        result = await synthesis_engine.synthesize(synthesis_input)
        
        return SynthesisResponse(
            system_prompt=result.system_prompt,
            user_prompt=result.user_prompt,
            total_tokens=result.total_tokens,
            context_documents=result.context_documents,
            metadata=result.metadata,
            is_fallback=result.is_fallback,
            processing_time_ms=result.processing_time_ms
        )
        
    except Exception as e:
        logger.error("synthesis_failed", error=str(e))
        raise HTTPException(status_code=500, detail=f"Synthesis failed: {str(e)}")

@app.get("/health")
async def health_check():
    """Health check endpoint cho monitoring"""
    health_checker = get_health_checker()
    health_status = await health_checker.check_overall_health()
    
    if health_status["status"] == "unhealthy":
        raise HTTPException(status_code=503, detail=health_status)
    
    return health_status

@app.get("/metrics")
async def get_metrics():
    """Prometheus metrics endpoint"""
    from prometheus_client import generate_latest, CONTENT_TYPE_LATEST
    return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)
```

#### **Step 7.2: Developer Guide**

# File: docs/developer_guide.md

# RAG Synthesis Module - Developer Guide

## 🎯 Overview

Module FR-04.2 Synthesis là core component của RAG system, chịu trách nhiệm:
- Xây dựng context từ retrieved documents
- Template hóa prompts cho LLM  
- Xử lý fallback scenarios

## 🏗️ Architecture

### Core Components

```
SynthesisEngine
├── InputProcessor      # Validate & sanitize input
├── ContextBuilder     # Smart document selection  
├── TemplateManager    # Template loading & rendering
├── PromptAssembler    # Final prompt generation
└── ErrorHandler       # Fallback & error management
```

### Data Flow

```
Raw Input → Validation → Context Selection → Template Rendering → Prompt Assembly → Output
     ↓           ↓              ↓                ↓                    ↓
Error Handling → Fallback Generation (nếu cần)
```

## 🛠️ Development Setup

### Prerequisites
```bash
# Required software
Python 3.11+
Poetry 1.5+
Docker Desktop 4.20+
Redis 7.0+
PostgreSQL 15+

# Development tools
VS Code với Python extensions
Git 2.40+
```

### Quick Start
```bash
# Clone và setup
git clone <repository>
cd rag_synthesis_module
poetry install

# Run tests
poetry run pytest

# Start development server  
poetry run uvicorn src.synthesis.api.main:app --reload

# Check API docs
open http://localhost:8000/docs
```

## 📝 Usage Examples

### Basic Synthesis
```python
from src.synthesis.core.synthesis_engine import SynthesisEngine
from src.synthesis.models import SynthesisInput, QueryInput, DocumentInput

# Initialize engine
engine = SynthesisEngine()

# Prepare input
synthesis_input = SynthesisInput(
    documents=[
        DocumentInput(
            content="Python là ngôn ngữ lập trình bậc cao...",
            metadata={"source": "python_guide.pdf"},
            chunk_id="chunk_001",
            relevance_score=0.95
        )
    ],
    query=QueryInput(
        query="Python là gì?",
        user_id="user123",
        department="engineering"
    )
)

# Execute synthesis
result = await engine.synthesize(synthesis_input)

print(f"System Prompt: {result.system_prompt}")
print(f"User Prompt: {result.user_prompt}")
print(f"Total Tokens: {result.total_tokens}")
```

### Custom Context Strategy
```python
from src.synthesis.core.context_builder import DiversityStrategy

# Use diversity-based selection
strategy = DiversityStrategy(diversity_threshold=0.8)
context_builder = ContextBuilder(strategy=strategy)

selected_docs = context_builder.select_documents(
    documents=all_documents,
    query=user_query,
    max_tokens=2000
)
```

### Custom Templates
```python
# Load custom template
template_manager = TemplateManager("templates/")
custom_template = template_manager.load_template(
    "custom/technical_query", 
    language="vi"
)

# Render với context
rendered = template_manager.render_template(
    template_name="custom/technical_query",
    context={
        "user_department": "R&D",
        "query": "Quy trình phát triển sản phẩm",
        "documents": selected_documents
    }
)
```

## 🧪 Testing Guide

### Running Tests
```bash
# All tests
poetry run pytest

# Unit tests only  
poetry run pytest tests/unit/

# Integration tests
poetry run pytest tests/integration/

# Performance tests
poetry run pytest tests/performance/ -m performance

# Coverage report
poetry run pytest --cov=src --cov-report=html
```

### Writing Tests
```python
# Example unit test
def test_top_k_strategy():
    strategy = TopKStrategy(k=3)
    
    mock_docs = [
        DocumentInput(content="doc1", chunk_id="1", relevance_score=0.9),
        DocumentInput(content="doc2", chunk_id="2", relevance_score=0.8),
        DocumentInput(content="doc3", chunk_id="3", relevance_score=0.7),
    ]
    
    selected = strategy.select_documents(
        documents=mock_docs,
        query=QueryInput(query="test", user_id="test"),
        max_tokens=1000
    )
    
    assert len(selected) == 3
    assert selected[0].relevance_score >= selected[1].relevance_score
```

## 🚀 Deployment

### Local Development
```bash
# Using docker-compose
docker-compose up -d

# Check services
docker-compose ps
docker-compose logs synthesis-service
```

### Production Deployment  
```bash
# Build production image
docker build -t synthesis-service:latest .

# Deploy to Kubernetes
kubectl apply -f k8s/

# Check deployment
kubectl get pods -l app=synthesis-service
kubectl logs -l app=synthesis-service
```

## 📊 Monitoring

### Metrics
- `synthesis_requests_total`: Total requests processed
- `synthesis_duration_seconds`: Processing time distribution  
- `synthesis_context_documents_count`: Documents per request
- `synthesis_prompt_tokens`: Token usage distribution

### Health Checks
```bash
# Health endpoint
curl http://localhost:8000/health

# Metrics endpoint  
curl http://localhost:8000/metrics
```

### Logs
```bash
# View structured logs
tail -f logs/synthesis.log | jq .

# Filter by component
tail -f logs/synthesis.log | jq 'select(.component=="context_builder")'
```

## 🐛 Troubleshooting

### Common Issues

**1. Template Loading Errors**
```
Error: Template not found: system/vi/default.j2
Solution: Check template directory structure và file permissions
```

**2. Token Limit Exceeded**
```  
Error: Prompt exceeds max token limit (4000)
Solution: Reduce context documents hoặc implement better truncation
```

**3. Context Selection Returns Empty**
```
Error: No documents selected for context
Solution: Check relevance scores và selection thresholds
```

### Debug Mode
```python
# Enable debug logging
import structlog
structlog.configure(
    processors=[structlog.dev.ConsoleRenderer()],
    wrapper_class=structlog.make_filtering_bound_logger(20),  # INFO level
    logger_factory=structlog.PrintLoggerFactory(),
    cache_logger_on_first_use=True,
)

# Run synthesis với debug info
logger = structlog.get_logger()
logger.setLevel("DEBUG")
```

## 🔧 Configuration

### Environment Variables
```bash
# Core settings
SYNTHESIS_MAX_TOKENS=4000
SYNTHESIS_MAX_CONTEXT_DOCS=10
SYNTHESIS_SIMILARITY_THRESHOLD=0.7

# External services
SYNTHESIS_OPENAI_API_KEY=your_key_here
SYNTHESIS_ANTHROPIC_API_KEY=your_key_here  

# Infrastructure
REDIS_URL=redis://localhost:6379
POSTGRES_URL=postgresql://user:pass@localhost/synthesis

# Performance  
SYNTHESIS_CACHE_TTL=3600
SYNTHESIS_REQUEST_TIMEOUT=30
SYNTHESIS_MAX_RETRIES=3
```

### Template Customization
```jinja2
<!-- templates/system/vi/custom.j2 -->
Bạn là {{ role_name }} chuyên về {{ domain }}.

NGUYÊN TẮC:
{% for principle in principles %}
- {{ principle }}
{% endfor %}

THÔNG TIN NGƯỜI DÙNG:
- Phòng ban: {{ user_department }}
- Cấp độ: {{ user_access_level }}

{% if special_instructions %}
CHỈ DẪN ĐặC BIỆT:
{{ special_instructions }}
{% endif %}
```
```

#### **Step 7.3: Operations Runbook**

```markdown
# File: docs/operations_runbook.md

# RAG Synthesis Module - Operations Runbook

## 🚨 Emergency Response

### Service Down
```bash
# Check service status
kubectl get pods -l app=synthesis-service

# Check logs for errors
kubectl logs -l app=synthesis-service --tail=100

# Restart service
kubectl rollout restart deployment/synthesis-service

# Scale up if needed
kubectl scale deployment synthesis-service --replicas=5
```

### High Error Rate
```bash
# Check error metrics
curl http://synthesis-service/metrics | grep synthesis_requests_total

# Check recent errors  
kubectl logs -l app=synthesis-service | grep ERROR | tail -20

# Check external service status
curl -I https://api.openai.com/v1/models
```

### Memory Issues
```bash
# Check memory usage
kubectl top pods -l app=synthesis-service

# Check for memory leaks
kubectl exec -it synthesis-service-xxx -- ps aux --sort=-%mem

# Restart high memory pods
kubectl delete pod synthesis-service-xxx
```

## 📊 Monitoring Playbooks

### Performance Degradation
1. **Check Response Times**
   ```bash
   # Grafana dashboard: Response Time P95 > 5 seconds
   # Alert threshold: P99 > 10 seconds
   ```

2. **Investigation Steps**
   ```bash
   # Check LLM API latency
   curl -w "@curl-format.txt" -s https://api.openai.com/v1/models
   
   # Check database performance  
   kubectl exec -it postgres-xxx -- psql -c "SELECT * FROM pg_stat_activity;"
   
   # Check cache hit rate
   kubectl exec -it redis-xxx -- redis-cli info stats
   ```

3. **Mitigation Actions**
   ```bash
   # Scale out
   kubectl scale deployment synthesis-service --replicas=10
   
   # Enable circuit breaker
   kubectl patch configmap synthesis-config --patch '{"data":{"enable_circuit_breaker":"true"}}'
   
   # Reduce context size temporarily
   kubectl patch configmap synthesis-config --patch '{"data":{"max_context_documents":"5"}}'
   ```

### High Token Usage
1. **Monitor Token Consumption**
   ```bash
   # Check token usage trends
   prometheus_query: rate(synthesis_prompt_tokens_sum[5m])
   
   # Alert: Token usage > $100/hour
   ```

2. **Cost Control Actions**
   ```bash
   # Reduce max tokens
   kubectl patch configmap synthesis-config --patch '{"data":{"max_tokens":"2000"}}'
   
   # Enable aggressive caching
   kubectl patch configmap synthesis-config --patch '{"data":{"cache_ttl":"7200"}}'
   
   # Switch to smaller model temporarily
   kubectl patch secret synthesis-secrets --patch '{"data":{"model_name":"Z3B0LTMuNS10dXJibw=="}}'  # gpt-3.5-turbo
   ```

## 🔄 Maintenance Procedures

### Weekly Maintenance
```bash
#!/bin/bash
# File: scripts/weekly_maintenance.sh

echo "=== Weekly Synthesis Service Maintenance ==="

# 1. Check disk usage
echo "Checking disk usage..."
kubectl exec -it synthesis-service-xxx -- df -h

# 2. Rotate logs  
echo "Rotating logs..."
kubectl exec -it synthesis-service-xxx -- logrotate /etc/logrotate.d/synthesis

# 3. Clear old cache entries
echo "Clearing old cache..."
kubectl exec -it redis-xxx -- redis-cli EVAL "
  for i, name in ipairs(redis.call('KEYS', 'synthesis:*')) do
    local ttl = redis.call('TTL', name)
    if ttl == -1 then  -- Keys without expiration
      redis.call('EXPIRE', name, 86400)  -- Set 24h expiration
    end
  end
" 0

# 4. Update health check status
echo "Running health checks..."
curl -f http://synthesis-service/health || echo "Health check failed!"

# 5. Check certificate expiry
echo "Checking certificates..."
openssl x509 -in /path/to/cert.pem -noout -dates

echo "Weekly maintenance completed!"
```

### Database Maintenance
```sql
-- File: scripts/database_maintenance.sql

-- Clean up old synthesis logs (older than 30 days)
DELETE FROM synthesis_logs 
WHERE created_at < NOW() - INTERVAL '30 days';

-- Update statistics for query optimization
ANALYZE synthesis_requests;
ANALYZE synthesis_performance;

-- Check for slow queries
SELECT query, calls, total_time, mean_time, rows
FROM pg_stat_statements 
WHERE query LIKE '%synthesis%'
ORDER BY total_time DESC 
LIMIT 10;

-- Vacuum tables to reclaim space
VACUUM ANALYZE synthesis_requests;
VACUUM ANALYZE synthesis_logs;
```

## 📈 Capacity Planning

### Scaling Guidelines
| Metric | Scale Out Threshold | Scale In Threshold | Action |
|--------|-------------------|-------------------|---------|
| CPU Usage | > 70% for 5 min | < 30% for 10 min | +2 replicas |
| Memory Usage | > 80% for 3 min | < 40% for 15 min | +1 replica |
| Response Time P95 | > 5 seconds | < 2 seconds | +3 replicas |
| Error Rate | > 5% | < 1% | Investigate first |
| Queue Depth | > 100 requests | < 10 requests | +5 replicas |

### Resource Recommendations
```yaml
# Small deployment (< 100 requests/hour)
resources:
  requests:
    cpu: 200m
    memory: 512Mi
  limits:
    cpu: 500m  
    memory: 1Gi

# Medium deployment (< 1000 requests/hour)  
resources:
  requests:
    cpu: 500m
    memory: 1Gi
  limits:
    cpu: 1000m
    memory: 2Gi

# Large deployment (< 10000 requests/hour)
resources:
  requests:
    cpu: 1000m
    memory: 2Gi
  limits:
    cpu: 2000m
    memory: 4Gi
```

## 🔐 Security Procedures

### API Key Rotation
```bash
# 1. Generate new API keys từ providers
# 2. Update secrets
kubectl create secret generic synthesis-secrets-new \
  --from-literal=openai-api-key=new_key_here \
  --from-literal=anthropic-api-key=new_key_here

# 3. Update deployment
kubectl patch deployment synthesis-service \
  -p '{"spec":{"template":{"spec":{"volumes":[{"name":"secrets","secret":{"secretName":"synthesis-secrets-new"}}]}}}}'

# 4. Verify service health
kubectl rollout status deployment/synthesis-service

# 5. Delete old secrets  
kubectl delete secret synthesis-secrets
```

### Security Audit
```bash
# Check for vulnerabilities
kubectl exec -it synthesis-service-xxx -- pip audit

# Scan container image
docker scan synthesis-service:latest

# Check RBAC permissions
kubectl auth can-i --list --as=system:serviceaccount:default:synthesis-service

# Review network policies
kubectl get networkpolicy -o yaml
```

## 📋 Backup & Recovery

### Data Backup
```bash
#!/bin/bash
# File: scripts/backup.sh

BACKUP_DATE=$(date +%Y%m%d_%H%M%S)

# Backup configuration
kubectl get configmap synthesis-config -o yaml > "backup/config_${BACKUP_DATE}.yaml"
kubectl get secret synthesis-secrets -o yaml > "backup/secrets_${BACKUP_DATE}.yaml"

# Backup database
kubectl exec -it postgres-xxx -- pg_dump synthesis > "backup/database_${BACKUP_DATE}.sql"

# Backup Redis data
kubectl exec -it redis-xxx -- redis-cli BGSAVE
kubectl cp redis-xxx:/data/dump.rdb "backup/redis_${BACKUP_DATE}.rdb"

# Upload to cloud storage (if configured)
aws s3 cp backup/ s3://synthesis-backups/${BACKUP_DATE}/ --recursive

echo "Backup completed: ${BACKUP_DATE}"
```

### Disaster Recovery
```bash
#!/bin/bash  
# File: scripts/disaster_recovery.sh

echo "=== Disaster Recovery Process ==="

# 1. Deploy infrastructure
kubectl apply -f k8s/infrastructure/

# 2. Restore secrets and config
kubectl apply -f backup/secrets_latest.yaml
kubectl apply -f backup/config_latest.yaml

# 3. Restore database
kubectl exec -it postgres-xxx -- psql synthesis < backup/database_latest.sql

# 4. Restore Redis data  
kubectl cp backup/redis_latest.rdb redis-xxx:/data/dump.rdb
kubectl exec -it redis-xxx -- redis-cli DEBUG RESTART

# 5. Deploy application
kubectl apply -f k8s/application/

# 6. Verify service health
kubectl rollout status deployment/synthesis-service
curl -f http://synthesis-service/health

echo "Disaster recovery completed!"
```

## 📞 Escalation Contacts

| Issue Type | Primary Contact | Secondary Contact | Escalation |
|------------|----------------|------------------|------------|
| Service Down | DevOps Team | Engineering Lead | CTO |
| High Error Rate | Backend Team | Engineering Lead | CTO |  
| Security Incident | Security Team | CISO | CEO |
| Data Loss | DBA Team | Engineering Lead | CTO |
| Performance Issues | Backend Team | Infrastructure Team | Engineering Lead |

## 📚 Additional Resources

- **Monitoring Dashboard**: https://grafana.company.com/synthesis
- **Log Aggregation**: https://kibana.company.com/synthesis  
- **API Documentation**: https://synthesis-service.company.com/docs
- **Runbook Repository**: https://github.com/company/synthesis-runbooks
- **On-call Schedule**: https://pagerduty.com/synthesis-team
```

---

## 🎯 **IMPLEMENTATION CHECKLIST & DELIVERABLES**

### **Final Deliverable Checklist**

#### **📋 Code Deliverables**
- [ ] **Core Components** (100% complete)
  - [ ] InputProcessor với validation logic
  - [ ] ContextBuilder với multiple strategies  
  - [ ] TemplateManager với Jinja2 integration
  - [ ] PromptAssembler với token management
  - [ ] ErrorHandler với comprehensive fallback

- [ ] **API Implementation** (100% complete)
  - [ ] FastAPI application với OpenAPI specs
  - [ ] Request/Response models với Pydantic
  - [ ] Error handling và HTTP status codes
  - [ ] Health check và metrics endpoints

- [ ] **Testing Suite** (90%+ coverage)
  - [ ] Unit tests cho tất cả core components
  - [ ] Integration tests cho end-to-end flows
  - [ ] Performance tests với benchmarks
  - [ ] Error scenario tests

#### **🏗️ Infrastructure Deliverables**  
- [ ] **Containerization**
  - [ ] Production-ready Dockerfile
  - [ ] Docker Compose cho local development
  - [ ] Kubernetes manifests cho production

- [ ] **Configuration Management**
  - [ ] Environment-specific configs
  - [ ] Secret management setup
  - [ ] Template directory structure

#### **📊 Monitoring & Operations**
- [ ] **Observability Stack**
  - [ ] Prometheus metrics collection
  - [ ] Structured logging với structlog
  - [ ] Health check implementation
  - [ ] Performance dashboards

- [ ] **Operational Tools**
  - [ ] Backup và restore scripts
  - [ ] Maintenance automation
  - [ ] Disaster recovery procedures

#### **📚 Documentation Package**
- [ ] **Technical Documentation**
  - [ ] API documentation với examples
  - [ ] Developer setup guide
  - [ ] Architecture decision records
  - [ ] Operations runbook

- [ ] **User Documentation** 
  - [ ] Integration guide cho other teams
  - [ ] Configuration reference
  - [ ] Troubleshooting guide

---

## 🚀 **GO-LIVE CHECKLIST**

### **Pre-Production Validation**
```bash
# 1. Comprehensive Testing
poetry run pytest tests/ --cov=src --cov-report=term-missing
poetry run pytest tests/performance/ -m performance

# 2. Security Scan
docker scan synthesis-service:latest
poetry run safety check

# 3. Load Testing
k6 run tests/load/synthesis_load_test.js

# 4. Integration Testing  
poetry run pytest tests/integration/ -v

# 5. Health Check Validation
curl http://localhost:8000/health
curl http://localhost:8000/metrics
```

### **Production Deployment Steps**
```bash
# 1. Deploy infrastructure
kubectl apply -f k8s/infrastructure/

# 2. Deploy configuration  
kubectl apply -f k8s/config/

# 3. Deploy secrets
kubectl apply -f k8s/secrets/

# 4. Deploy application
kubectl apply -f k8s/application/

# 5. Verify deployment
kubectl rollout status deployment/synthesis-service
kubectl get pods -l app=synthesis-service

# 6. Run smoke tests
curl -X POST http://synthesis-service/synthesize \
  -H "Content-Type: application/json" \
  -d @tests/data/sample_request.json

# 7. Monitor metrics
kubectl port-forward svc/prometheus 9090:9090
# Check metrics at http://localhost:9090
```

### **Post-Deployment Validation**
- [ ] All health checks passing
- [ ] Metrics collection working  
- [ ] Logs being generated properly
- [ ] Error rates < 1%
- [ ] Response times < 2 seconds P95
- [ ] Integration với downstream services working

---

## 🎉 **SUCCESS CRITERIA**

Module FR-04.2 sẽ được consider **thành công** khi:

### **Functional Requirements** ✅
- [ ] Synthesize context từ retrieved documents với accuracy > 95%
- [ ] Generate structured prompts trong < 2 seconds
- [ ] Handle gracefully khi không có relevant documents  
- [ ] Support multiple context selection strategies
- [ ] Template system hoạt động với multilingual support

### **Non-Functional Requirements** ✅  
- [ ] Throughput: Handle 100 concurrent requests
- [ ] Latency: P95 response time < 2 seconds
- [ ] Availability: 99.9% uptime
- [ ] Error rate: < 1% for valid requests
- [ ] Token efficiency: < 5000 tokens average per request

### **Operational Requirements** ✅
- [ ] Comprehensive monitoring và alerting
- [ ] Automated deployment pipeline  
- [ ] Disaster recovery procedures tested
- [ ] Documentation complete và up-to-date
- [ ] Team training completed

---

**🎯 Với roadmap chi tiết này, team kỹ thuật có thể follow step-by-step để implement FR-04.2 Synthesis module một cách systematic và ensure chất lượng cao cho production environment.**

**📞 For support during implementation:** 
- **Technical Questions**: Tham khảo developer guide và API documentation
- **Operational Issues**: Follow operations runbook procedures  
- **Architecture Decisions**: Review architecture diagrams và design rationale

**✅ Ready để start implementation? Begin với Phase 1: Environment Setup & Foundation!**
