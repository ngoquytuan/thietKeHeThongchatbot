B·∫°n ƒë√∫ng r·ªìi! C√°c model b·∫°n li·ªát k√™ ƒë·ªÅu **r·∫•t n·∫∑ng** ƒë·ªÉ ch·∫°y offline. H√£y ph√¢n t√≠ch v√† t√¨m gi·∫£i ph√°p t·ªëi ∆∞u:

## **Ph√¢n t√≠ch c√°c model b·∫°n ƒëang d√πng**

| Model | K√≠ch th∆∞·ªõc | VRAM c·∫ßn | C√≥ th·ªÉ ch·∫°y offline? |
|-------|-----------|----------|---------------------|
| **qwen3-vl-8b** | ~8B params | 16-24GB | ‚ö†Ô∏è Kh√≥ (c·∫ßn GPU cao c·∫•p) |
| **gemini-2.5-flash** | Unknown (Google proprietary) | N/A | ‚ùå Kh√¥ng (API only) |
| **grok-4-fast** | ~314B params | 600GB+ | ‚ùå Kh√¥ng th·ªÉ |
| **gpt-oss-120b** | ~120B params | 240GB+ | ‚ùå Kh√¥ng th·ªÉ |

**K·∫øt lu·∫≠n**: T·∫•t c·∫£ ƒë·ªÅu **KH√îNG ph√π h·ª£p** ƒë·ªÉ ch·∫°y offline tr√™n hardware th√¥ng th∆∞·ªùng c·ªßa doanh nghi·ªáp.

---

## **Gi·∫£i ph√°p t·ªëi ∆∞u cho deployment n·ªôi b·ªô**

### **Chi·∫øn l∆∞·ª£c 1: S·ª≠ d·ª•ng Small Language Models (SLMs) t·ªëi ∆∞u cho ti·∫øng Vi·ªát**

#### **Option 1A: Qwen 2.5 3B/7B (Recommended ‚≠ê)**
```python
# Model: Qwen2.5-7B-Instruct
# VRAM: 8-12GB (c√≥ th·ªÉ ch·∫°y tr√™n RTX 3060/4060)
# Performance: R·∫•t t·ªët cho ti·∫øng Vi·ªát, h·ªó tr·ª£ 128K context

from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "Qwen/Qwen2.5-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Quantization ƒë·ªÉ gi·∫£m VRAM
from transformers import BitsAndBytesConfig

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto"
)
# VRAM gi·∫£m xu·ªëng c√≤n 4-6GB!
```

**∆Øu ƒëi·ªÉm**:
- ‚úÖ Ti·∫øng Vi·ªát t·ªët (ƒë∆∞·ª£c train tr√™n multilingual data)
- ‚úÖ Context window l·ªõn (128K tokens)
- ‚úÖ Ch·∫°y ƒë∆∞·ª£c tr√™n GPU consumer-grade
- ‚úÖ C√≥ th·ªÉ quantize xu·ªëng 4-bit

**Nh∆∞·ª£c ƒëi·ªÉm**:
- ‚ö†Ô∏è Kh√¥ng b·∫±ng GPT-4/Claude v·ªÅ reasoning ph·ª©c t·∫°p
- ‚ö†Ô∏è C·∫ßn fine-tune th√™m cho domain c·ª• th·ªÉ

#### **Option 1B: Vistral-7B (Vietnamese-first model)**
```python
# Model ƒë∆∞·ª£c pre-train v√† fine-tune ƒë·∫∑c bi·ªát cho ti·∫øng Vi·ªát
model_name = "Viet-Mistral/Vistral-7B-Chat"

# T∆∞∆°ng t·ª± setup nh∆∞ Qwen
```

**∆Øu ƒëi·ªÉm**:
- ‚úÖ **ƒê∆∞·ª£c t·ªëi ∆∞u ƒë·∫∑c bi·ªát cho ti·∫øng Vi·ªát**
- ‚úÖ Hi·ªÉu ng·ªØ c·∫£nh vƒÉn h√≥a Vi·ªát Nam t·ªët
- ‚úÖ VRAM requirements t∆∞∆°ng t·ª± Qwen (4-8GB v·ªõi quantization)

---

### **Chi·∫øn l∆∞·ª£c 2: Hybrid Architecture (Best of Both Worlds ‚≠ê‚≠ê‚≠ê)**

```python
"""
K·∫øt h·ª£p: Small model (offline) + Large model (API fallback)
"""

class HybridLLM:
    def __init__(self):
        # Small model ch·∫°y local
        self.local_model = Qwen25_7B()  # 4-8GB VRAM
        
        # Large model qua API (backup)
        self.api_model = OpenRouterAPI()  # gpt-4o-mini, gemini-flash
        
        # Classifier ƒë·ªÉ quy·∫øt ƒë·ªãnh d√πng model n√†o
        self.complexity_classifier = ComplexityClassifier()
    
    def generate(self, query, context):
        # Ph√¢n lo·∫°i ƒë·ªô ph·ª©c t·∫°p c√¢u h·ªèi
        complexity = self.complexity_classifier.predict(query)
        
        if complexity == "simple":
            # C√¢u h·ªèi ƒë∆°n gi·∫£n ‚Üí d√πng local model
            return self.local_model.generate(query, context)
        
        elif complexity == "medium":
            # Th·ª≠ local tr∆∞·ªõc, n·∫øu k√©m ‚Üí fallback API
            local_response = self.local_model.generate(query, context)
            
            if self.is_good_response(local_response):
                return local_response
            else:
                return self.api_model.generate(query, context)
        
        else:  # complex
            # C√¢u h·ªèi ph·ª©c t·∫°p ‚Üí d√πng API lu√¥n
            return self.api_model.generate(query, context)
```

**∆Øu ƒëi·ªÉm**:
- ‚úÖ 80-90% queries ch·∫°y offline (ti·∫øt ki·ªám chi ph√≠)
- ‚úÖ 10-20% queries ph·ª©c t·∫°p d√πng API (quality cao)
- ‚úÖ Ki·ªÉm so√°t ƒë∆∞·ª£c cost
- ‚úÖ Fallback khi local model fail

**Implementation chi ti·∫øt**:

```python
class ComplexityClassifier:
    """Ph√¢n lo·∫°i ƒë·ªô ph·ª©c t·∫°p c·ªßa c√¢u h·ªèi"""
    
    SIMPLE_PATTERNS = [
        r"(c√≥ bao nhi√™u|c√≥ nh·ªØng|li·ªát k√™|danh s√°ch)",
        r"(l√† g√¨|nghƒ©a l√†|ƒë·ªãnh nghƒ©a)",
        r"(khi n√†o|·ªü ƒë√¢u|ai l√†)"
    ]
    
    COMPLEX_PATTERNS = [
        r"(so s√°nh|ƒë√°nh gi√°|ph√¢n t√≠ch)",
        r"(t·∫°i sao|v√¨ sao|nguy√™n nh√¢n)",
        r"(ƒë·ªÅ xu·∫•t|khuy·∫øn ngh·ªã|gi·∫£i ph√°p)",
        r"(d·ª± ƒëo√°n|xu h∆∞·ªõng|t∆∞∆°ng lai)"
    ]
    
    def predict(self, query):
        query_lower = query.lower()
        
        # Check simple patterns
        for pattern in self.SIMPLE_PATTERNS:
            if re.search(pattern, query_lower):
                return "simple"
        
        # Check complex patterns
        for pattern in self.COMPLEX_PATTERNS:
            if re.search(pattern, query_lower):
                return "complex"
        
        # Check query length
        if len(query.split()) < 10:
            return "simple"
        elif len(query.split()) > 30:
            return "complex"
        
        return "medium"
```

---

### **Chi·∫øn l∆∞·ª£c 3: Distillation + Fine-tuning (Long-term ‚≠ê‚≠ê)**

```python
"""
Train m·ªôt model nh·ªè ƒë·ªÉ b·∫Øt ch∆∞·ªõc model l·ªõn tr√™n domain c·ª• th·ªÉ
"""

# Step 1: Collect high-quality responses t·ª´ large model
def collect_training_data():
    large_model = OpenRouterAPI("gpt-4o")
    small_model = Qwen25_3B()
    
    training_data = []
    for query, context in your_dataset:
        # L·∫•y response t·ª´ model l·ªõn
        teacher_response = large_model.generate(query, context)
        
        # L·∫•y response t·ª´ model nh·ªè
        student_response = small_model.generate(query, context)
        
        training_data.append({
            "query": query,
            "context": context,
            "teacher": teacher_response,
            "student": student_response
        })
    
    return training_data

# Step 2: Fine-tune small model ƒë·ªÉ h·ªçc t·ª´ large model
from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir="./qwen-2.5-3b-attech-finetuned",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-5,
    fp16=True,  # Mixed precision training
    logging_steps=10,
)

trainer = Trainer(
    model=small_model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
)

trainer.train()
```

**∆Øu ƒëi·ªÉm**:
- ‚úÖ Model nh·ªè "h·ªçc" ƒë∆∞·ª£c c√°ch tr·∫£ l·ªùi c·ªßa model l·ªõn
- ‚úÖ T·ªëi ∆∞u cho domain c·ª• th·ªÉ (ATTECH)
- ‚úÖ 100% offline sau khi train xong

**Nh∆∞·ª£c ƒëi·ªÉm**:
- ‚ö†Ô∏è C·∫ßn t·∫≠p training data ch·∫•t l∆∞·ª£ng cao (500-1000 samples)
- ‚ö†Ô∏è M·∫•t th·ªùi gian train (v√†i gi·ªù ƒë·∫øn v√†i ng√†y)

---

## **So s√°nh c√°c chi·∫øn l∆∞·ª£c**

| Ti√™u ch√≠ | SLM Only | Hybrid | Distillation |
|----------|----------|--------|--------------|
| **Cost** | üí∞ Th·∫•p nh·∫•t | üí∞üí∞ Trung b√¨nh | üí∞ Th·∫•p (sau khi train) |
| **Quality** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Speed** | ‚ö°‚ö°‚ö° | ‚ö°‚ö° | ‚ö°‚ö°‚ö° |
| **Setup Time** | 1-2 ng√†y | 3-5 ng√†y | 1-2 tu·∫ßn |
| **Maintenance** | Th·∫•p | Trung b√¨nh | Cao (c·∫ßn retrain ƒë·ªãnh k·ª≥) |

---

## **Recommendation c·ª• th·ªÉ cho ATTECH**

### **Phase 1 (Ngay l·∫≠p t·ª©c): Hybrid Architecture**

```python
# docker-compose.yml
services:
  local-llm:
    image: vllm/vllm-openai:latest
    command: >
      --model Qwen/Qwen2.5-7B-Instruct
      --quantization awq
      --gpu-memory-utilization 0.9
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    ports:
      - "8000:8000"
  
  rag-api:
    build: ./backend
    environment:
      - LOCAL_LLM_URL=http://local-llm:8000/v1
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY}
      - LLM_STRATEGY=hybrid  # simple‚Üílocal, complex‚ÜíAPI
    depends_on:
      - local-llm
      - chroma
```

**C·∫•u h√¨nh Hybrid Logic**:
```python
# config/llm_config.py
LLM_CONFIG = {
    "local": {
        "model": "Qwen/Qwen2.5-7B-Instruct",
        "endpoint": "http://localhost:8000/v1",
        "max_tokens": 2048,
        "temperature": 0.2,
    },
    "api": {
        "model": "openai/gpt-4o-mini",  # R·∫ª nh·∫•t, ƒë·ªß t·ªët
        "endpoint": "https://openrouter.ai/api/v1",
        "max_tokens": 4096,
        "temperature": 0.3,
    },
    "routing": {
        "simple_threshold": 0.7,  # >= 70% confidence ‚Üí local
        "api_fallback": True,     # N·∫øu local fail ‚Üí API
        "max_api_calls_per_hour": 100,  # Rate limit
    }
}
```

### **Phase 2 (Sau 1 th√°ng): Fine-tune cho domain**

```python
# Collect real user queries v√† responses t·ªët
# Fine-tune Qwen2.5-3B tr√™n data n√†y
# ‚Üí Model nh·ªè h∆°n (3B), nhanh h∆°n, nh∆∞ng quality t·ªët h∆°n
```

### **Phase 3 (Sau 3 th√°ng): Evaluate & Optimize**

```python
# A/B testing:
# - 50% users d√πng local model
# - 50% users d√πng hybrid
# So s√°nh: quality, latency, cost
```

---

## **Hardware Requirements**

### **Minimum (cho Hybrid setup)**
```yaml
GPU: RTX 3060 12GB ho·∫∑c t∆∞∆°ng ƒë∆∞∆°ng
RAM: 32GB
Storage: 500GB SSD
CPU: 8 cores

Estimated cost: ~$1,000-1,500
```

### **Recommended (cho production)**
```yaml
GPU: RTX 4070 Ti 16GB ho·∫∑c A4000
RAM: 64GB
Storage: 1TB NVMe SSD
CPU: 16 cores

Estimated cost: ~$2,500-3,500
```

### **Optimal (cho scale l·ªõn)**
```yaml
GPU: A100 40GB ho·∫∑c 2x RTX 4090
RAM: 128GB
Storage: 2TB NVMe SSD
CPU: 32 cores

Estimated cost: ~$10,000-15,000
```

---

## **Cost Comparison (Monthly)**

| Setup | Hardware | API Cost | Total |
|-------|----------|----------|-------|
| **100% API** (nh∆∞ hi·ªán t·∫°i) | $0 | $500-1000 | $500-1000 |
| **Hybrid** (80% local) | $2,500 one-time | $100-200 | $300-400/month |
| **100% Local** | $2,500 one-time | $0 | $100/month (electricity) |

**Break-even point**: Sau 3-6 th√°ng, Hybrid/Local s·∫Ω r·∫ª h∆°n 100% API

---

## **Implementation Roadmap**

```mermaid
graph TD
    A[Week 1-2: Setup Hybrid] --> B[Deploy Qwen2.5-7B local]
    B --> C[Implement routing logic]
    C --> D[Week 3-4: Testing]
    D --> E{Quality OK?}
    E -->|Yes| F[Production deployment]
    E -->|No| G[Tune prompts/thresholds]
    G --> D
    F --> H[Month 2-3: Collect data]
    H --> I[Fine-tune Qwen2.5-3B]
    I --> J[Replace 7B with 3B]
    J --> K[Monitor & optimize]
```

---

## **Quick Start Code**

```python
# install.sh
#!/bin/bash

# Install vLLM (serving framework)
pip install vllm

# Download model
python -c "
from transformers import AutoModelForCausalLM, AutoTokenizer
model = AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B-Instruct')
tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen2.5-7B-Instruct')
"

# Start vLLM server
vllm serve Qwen/Qwen2.5-7B-Instruct \
    --host 0.0.0.0 \
    --port 8000 \
    --gpu-memory-utilization 0.9
```

```python
# test_hybrid.py
import openai

# Local model
local_client = openai.OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="dummy"
)

# API model
api_client = openai.OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key="your_key"
)

def generate_hybrid(query, context):
    complexity = classify_complexity(query)
    
    if complexity == "simple":
        response = local_client.chat.completions.create(
            model="Qwen/Qwen2.5-7B-Instruct",
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": f"{context}\n\n{query}"}
            ],
            temperature=0.2
        )
    else:
        response = api_client.chat.completions.create(
            model="openai/gpt-4o-mini",
            messages=[...],
            temperature=0.3
        )
    
    return response.choices[0].message.content
```

---

## **K·∫øt lu·∫≠n**

**TL;DR**: 
1. ‚úÖ **Ngay l·∫≠p t·ª©c**: Deploy **Hybrid (Qwen2.5-7B local + GPT-4o-mini API)**
2. ‚úÖ Sau 1 th√°ng: Fine-tune model nh·ªè h∆°n (Qwen2.5-3B) tr√™n real data
3. ‚úÖ Sau 3 th√°ng: Chuy·ªÉn 90-95% sang local, ch·ªâ 5-10% d√πng API

B·∫°n mu·ªën t√¥i gi√∫p implement ph·∫ßn n√†o tr∆∞·ªõc?
- [ ] Setup vLLM server v·ªõi Qwen2.5-7B
- [ ] Implement routing logic (complexity classifier)
- [ ] Fine-tuning script
- [ ] Cost tracking dashboard
