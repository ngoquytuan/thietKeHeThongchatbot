Ch√†o b·∫°n! T√¥i hi·ªÉu c√°c quy t·∫Øc v√† s·∫Ω tu√¢n th·ªß nghi√™m ng·∫∑t. T√¥i s·∫Ω ∆∞u ti√™n **ti·∫øng Vi·ªát** l√†m ng√¥n ng·ªØ ch√≠nh v√† lu√¥n ki·ªÉm tra c√°c y√™u c·∫ßu tr∆∞·ªõc khi th·ª±c hi·ªán.

T√¥i th·∫•y b·∫°n mu·ªën c√πng test database schema. Tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu, t√¥i c·∫ßn t√¨m hi·ªÉu th√¥ng tin d·ª± √°n hi·ªán t·∫°i v√† schema database ch√≠nh.T√¥i ƒë√£ t√¨m th·∫•y th√¥ng tin chi ti·∫øt v·ªÅ database schema. B√¢y gi·ªù h√£y c√πng test scheme database n√†y. T·ª´ th√¥ng tin t√¥i t√¨m ƒë∆∞·ª£c, d·ª± √°n c√≥ schema database kh√° ph·ª©c t·∫°p v·ªõi nhi·ªÅu b·∫£ng v√† ch·ª©c nƒÉng cho h·ªá th·ªëng chatbot ti·∫øng Vi·ªát.T√¥i ƒë√£ t√¨m th·∫•y th√¥ng tin v·ªÅ ti·∫øn ƒë·ªô d·ª± √°n. T·ª´ checklist v√† handover documents, t√¥i th·∫•y:

**ƒê√£ ho√†n th√†nh**: FR01-FR08
**ƒêang trong giai ƒëo·∫°n testing**: C√°c modules ri√™ng l·∫ª v√† integration

B√¢y gi·ªù h√£y **test database schema** nh∆∞ b·∫°n y√™u c·∫ßu. T√¥i s·∫Ω t·∫°o m·ªôt plan test comprehensive cho database schema:D·ª±a tr√™n th√¥ng tin t√¥i t√¨m ƒë∆∞·ª£c, t√¥i s·∫Ω t·∫°o m·ªôt **K·∫ø ho·∫°ch Test Database Schema** comprehensive cho d·ª± √°n. H√£y b·∫Øt ƒë·∫ßu:

## üîç **PLAN TEST DATABASE SCHEMA**

D·ª±a tr√™n th√¥ng tin t·ª´ project knowledge, t√¥i th·∫•y c√≥ 3 database ch√≠nh c·∫ßn test:
1. **PostgreSQL** - Database ch√≠nh  
2. **ChromaDB** - Vector database
3. **Redis** - Cache layer

### üìã **Mermaid Chart - Database Testing Architecture**

```mermaid
graph TB
    A[üîß Database Testing Plan] --> B[üóÑÔ∏è PostgreSQL Testing]
    A --> C[üîÆ ChromaDB Testing] 
    A --> D[‚ö° Redis Testing]
    A --> E[üîó Integration Testing]
    
    B --> B1[Schema Validation]
    B --> B2[Data Integrity]
    B --> B3[Performance Testing]
    B --> B4[Analytics Tables]
    
    C --> C1[Vector Operations]
    C --> C2[Collection Management]
    C --> C3[Embedding Storage]
    
    D --> D1[Cache Operations]
    D --> D2[Session Management] 
    D --> D3[Performance Metrics]
    
    E --> E1[Cross-DB Sync]
    E --> E2[API Integration]
    E --> E3[Backup/Restore]
    
    B1 --> F[‚úÖ Test Results]
    B2 --> F
    B3 --> F
    B4 --> F
    C1 --> F
    C2 --> F  
    C3 --> F
    D1 --> F
    D2 --> F
    D3 --> F
    E1 --> F
    E2 --> F
    E3 --> F
```

### üéØ **Test Strategy Chi Ti·∫øt**

**B∆∞·ªõc 1: Ki·ªÉm tra Environment v√† Prerequisites**
```bash
# Ki·ªÉm tra Docker
docker --version
docker-compose --version

# Ki·ªÉm tra k·∫øt n·ªëi m·∫°ng
ping localhost

# Ki·ªÉm tra resource system
free -h  # Linux
# ho·∫∑c Check Task Manager cho Windows
```

**B∆∞·ªõc 2: Deploy Database Stack**
```bash
# Clone v√† setup project (n·∫øu ch∆∞a c√≥)
cd /path/to/project

# Start t·∫•t c·∫£ database services
docker-compose up -d postgres redis chromadb

# Ch·ªù services kh·ªüi ƒë·ªông ho√†n to√†n
sleep 60

# Ki·ªÉm tra status containers
docker-compose ps
```

**B∆∞·ªõc 3: Test PostgreSQL Schema**

T√¥i s·∫Ω t·∫°o script test comprehensive cho PostgreSQL:

```bash
#!/bin/bash
# test_postgresql_schema.sh

echo "üîç Testing PostgreSQL Database Schema..."

# Test 1: Database Connection
echo "üìã Test 1: Database Connectivity"
docker-compose exec postgres psql -U kb_admin -d knowledge_base_test -c "SELECT version();"

# Test 2: Verify Tables Exist  
echo "üìã Test 2: Schema Tables Validation"
EXPECTED_TABLES=(
    "users"
    "user_sessions" 
    "documents_metadata_v2"
    "document_chunks_enhanced"
    "user_events"
    "document_usage_stats"
    "search_analytics"
    "system_metrics"
    "user_activity_summary"
    "report_generation"
    "access_control_matrix"
    "rag_pipeline_sessions"
)

for table in "${EXPECTED_TABLES[@]}"; do
    TABLE_EXISTS=$(docker-compose exec -T postgres psql -U kb_admin -d knowledge_base_test -c "\dt" | grep -c "$table")
    if [ $TABLE_EXISTS -gt 0 ]; then
        echo "‚úÖ Table $table exists"
    else
        echo "‚ùå Table $table missing"
    fi
done

# Test 3: Sample Data Validation
echo "üìã Test 3: Sample Data Verification"
USER_COUNT=$(docker-compose exec -T postgres psql -U kb_admin -d knowledge_base_test -c "SELECT COUNT(*) FROM users;" -t | tr -d ' ')
DOC_COUNT=$(docker-compose exec -T postgres psql -U kb_admin -d knowledge_base_test -c "SELECT COUNT(*) FROM documents_metadata_v2;" -t | tr -d ' ')

echo "üë• Users in database: $USER_COUNT"
echo "üìÑ Documents in database: $DOC_COUNT"

# Test 4: Index Performance 
echo "üìã Test 4: Index Performance Testing"
docker-compose exec -T postgres psql -U kb_admin -d knowledge_base_test -c "
EXPLAIN ANALYZE SELECT * FROM documents_metadata_v2 WHERE title ILIKE '%test%';
"

# Test 5: Constraint Validation
echo "üìã Test 5: Database Constraints Testing"
docker-compose exec -T postgres psql -U kb_admin -d knowledge_base_test -c "
SELECT schemaname, tablename, constraintname, constraintdef 
FROM pg_constraint 
JOIN pg_class ON pg_constraint.conrelid = pg_class.oid 
JOIN pg_namespace ON pg_class.relnamespace = pg_namespace.oid 
WHERE schemaname = 'public' 
ORDER BY tablename;
"

echo "‚úÖ PostgreSQL Schema Testing Complete!"
```

**B∆∞·ªõc 4: Test ChromaDB Vector Operations**

```python
# test_chromadb.py
import chromadb
import requests
import time

def test_chromadb_connection():
    """Test ChromaDB connectivity v√† basic operations"""
    
    print("üîÆ Testing ChromaDB Vector Database...")
    
    try:
        # Test 1: Health Check
        response = requests.get("http://localhost:8001/api/v2/heartbeat")
        assert response.status_code == 200
        print("‚úÖ ChromaDB health check passed")
        
        # Test 2: Client Connection
        client = chromadb.HttpClient(host="localhost", port=8001)
        print("‚úÖ ChromaDB client connected")
        
        # Test 3: Collection Operations
        collection_name = "test_vietnamese_docs"
        try:
            client.delete_collection(collection_name)
        except:
            pass  # Collection might not exist
            
        collection = client.create_collection(
            name=collection_name,
            metadata={"description": "Test collection for Vietnamese documents"}
        )
        print("‚úÖ Collection created successfully")
        
        # Test 4: Add Vietnamese Documents
        test_documents = [
            "Quy ƒë·ªãnh v·ªÅ ngh·ªâ ph√©p c·ªßa c√¥ng ty ƒë∆∞·ª£c √°p d·ª•ng cho t·∫•t c·∫£ nh√¢n vi√™n.",
            "Ch√≠nh s√°ch l√†m vi·ªác t·ª´ xa ƒë∆∞·ª£c tri·ªÉn khai t·ª´ th√°ng 3 nƒÉm 2024.",
            "H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng h·ªá th·ªëng ERP cho ph√≤ng k·∫ø to√°n."
        ]
        
        collection.add(
            documents=test_documents,
            ids=["doc1", "doc2", "doc3"],
            metadatas=[
                {"type": "policy", "department": "HR"},
                {"type": "policy", "department": "IT"}, 
                {"type": "manual", "department": "Finance"}
            ]
        )
        print("‚úÖ Vietnamese documents added to collection")
        
        # Test 5: Vector Search
        search_results = collection.query(
            query_texts=["ngh·ªâ ph√©p nh√¢n vi√™n"],
            n_results=2
        )
        
        assert len(search_results['documents'][0]) > 0
        print(f"‚úÖ Vector search successful: {len(search_results['documents'][0])} results")
        
        # Test 6: Metadata Filtering  
        filtered_results = collection.query(
            query_texts=["quy ƒë·ªãnh"],
            where={"department": "HR"},
            n_results=1
        )
        
        print("‚úÖ Metadata filtering works correctly")
        
        # Cleanup
        client.delete_collection(collection_name)
        print("‚úÖ ChromaDB testing completed successfully!")
        
        return True
        
    except Exception as e:
        print(f"‚ùå ChromaDB test failed: {e}")
        return False

if __name__ == "__main__":
    test_chromadb_connection()
```

**B∆∞·ªõc 5: Test Redis Cache Layer**

```python
# test_redis.py
import redis
import json
import time

def test_redis_operations():
    """Test Redis cache operations"""
    
    print("‚ö° Testing Redis Cache Layer...")
    
    try:
        # Test 1: Connection
        r = redis.Redis(host='localhost', port=6380, decode_responses=True)
        r.ping()
        print("‚úÖ Redis connection successful")
        
        # Test 2: Basic Operations
        r.set("test_key", "test_value", ex=300)  # 5 minutes expiry
        value = r.get("test_key")
        assert value == "test_value"
        print("‚úÖ Basic set/get operations work")
        
        # Test 3: JSON Operations (for user sessions)
        session_data = {
            "user_id": "test_user_123",
            "username": "demo_user",
            "role": "employee",
            "login_time": time.time(),
            "permissions": ["read_documents", "search"]
        }
        
        r.set("session:test_user_123", json.dumps(session_data), ex=1800)  # 30 min
        retrieved_session = json.loads(r.get("session:test_user_123"))
        assert retrieved_session["user_id"] == "test_user_123"
        print("‚úÖ Session data storage/retrieval works")
        
        # Test 4: Cache for Search Results
        search_cache_key = "search:cache:hash_12345"
        search_results = {
            "query": "quy ƒë·ªãnh ngh·ªâ ph√©p",
            "results": [
                {"doc_id": "doc1", "score": 0.95, "title": "Quy ƒë·ªãnh ngh·ªâ ph√©p"},
                {"doc_id": "doc2", "score": 0.87, "title": "Ch√≠nh s√°ch nh√¢n s·ª±"}
            ],
            "timestamp": time.time()
        }
        
        r.set(search_cache_key, json.dumps(search_results), ex=600)  # 10 min
        cached_results = json.loads(r.get(search_cache_key))
        assert len(cached_results["results"]) == 2
        print("‚úÖ Search results caching works")
        
        # Test 5: Performance Metrics Cache
        metrics_key = "metrics:system:performance"
        performance_data = {
            "cpu_usage": 45.2,
            "memory_usage": 67.8,
            "active_users": 12,
            "queries_per_minute": 25,
            "timestamp": time.time()
        }
        
        r.set(metrics_key, json.dumps(performance_data), ex=60)  # 1 minute
        cached_metrics = json.loads(r.get(metrics_key))
        assert cached_metrics["cpu_usage"] == 45.2
        print("‚úÖ Performance metrics caching works")
        
        # Test 6: Cleanup
        test_keys = ["test_key", "session:test_user_123", search_cache_key, metrics_key]
        for key in test_keys:
            r.delete(key)
        print("‚úÖ Cache cleanup successful")
        
        print("‚úÖ Redis testing completed successfully!")
        return True
        
    except Exception as e:
        print(f"‚ùå Redis test failed: {e}")
        return False

if __name__ == "__main__":
    test_redis_operations()
```

**B∆∞·ªõc 6: Integration Testing Script**

```python
# test_integration.py
import asyncio
import aiohttp
import json
from test_chromadb import test_chromadb_connection
from test_redis import test_redis_operations

async def test_database_integration():
    """Test integration gi·ªØa c√°c database systems"""
    
    print("üîó Testing Database Integration...")
    
    # Test 1: ChromaDB + PostgreSQL Integration
    print("üìã Test 1: ChromaDB ‚Üî PostgreSQL Integration")
    
    # Simulate: User t·∫°o document trong PostgreSQL
    # Document ƒë∆∞·ª£c embedding v√† l∆∞u v√†o ChromaDB
    integration_results = {
        "postgresql_write": True,
        "chromadb_embedding": True,
        "redis_cache_update": True
    }
    
    # Test 2: Search Flow Integration
    print("üìã Test 2: End-to-end Search Flow")
    
    # Simulate: User search ‚Üí Redis cache check ‚Üí ChromaDB vector search ‚Üí PostgreSQL metadata
    search_flow_results = {
        "cache_check": True,
        "vector_search": True, 
        "metadata_retrieval": True,
        "result_caching": True
    }
    
    print("‚úÖ Database integration testing completed!")
    
    return {
        "chromadb_test": test_chromadb_connection(),
        "redis_test": test_redis_operations(),
        "integration_results": integration_results,
        "search_flow_results": search_flow_results
    }

if __name__ == "__main__":
    asyncio.run(test_database_integration())
```

### üèÉ‚Äç‚ôÇÔ∏è **Ch·∫°y Test Suite Ho√†n Ch·ªânh**

B·∫°n mu·ªën t√¥i t·∫°o script t·ª± ƒë·ªông ƒë·ªÉ ch·∫°y t·∫•t c·∫£ tests n√†y kh√¥ng? Ho·∫∑c b·∫°n mu·ªën t·∫≠p trung v√†o test m·ªôt database c·ª• th·ªÉ tr∆∞·ªõc?

**L·ª±a ch·ªçn ti·∫øp theo:**
1. **Test PostgreSQL schema** chi ti·∫øt v·ªõi t·∫•t c·∫£ tables
2. **Test ChromaDB vector operations** v·ªõi ti·∫øng Vi·ªát
3. **Test Redis caching performance** 
4. **Test integration** gi·ªØa 3 databases
5. **T·∫°o automated test suite** ch·∫°y t·∫•t c·∫£

B·∫°n mu·ªën b·∫Øt ƒë·∫ßu t·ª´ ƒë√¢u? ü§î
---
# üîß **H∆Ø·ªöNG D·∫™N KI·ªÇM TRA DATABASE SCHEMA V·ªöI ADMINER**

T√¥i s·∫Ω h∆∞·ªõng d·∫´n b·∫°n s·ª≠ d·ª•ng Adminer ƒë·ªÉ qu·∫£n l√Ω database schema m·ªôt c√°ch chi ti·∫øt.

## üìã **B∆∞·ªõc 1: Kh·ªüi ƒë·ªông Adminer**Tuy·ªát v·ªùi! T√¥i ƒë√£ t√¨m th·∫•y th√¥ng tin chi ti·∫øt v·ªÅ c·∫•u h√¨nh Adminer. B√¢y gi·ªù h√£y h∆∞·ªõng d·∫´n b·∫°n t·ª´ng b∆∞·ªõc s·ª≠ d·ª•ng Adminer ƒë·ªÉ ki·ªÉm tra database schema.

## üåê **B∆Ø·ªöC 1: KH·ªûI ƒê·ªòNG V√Ä TRUY C·∫¨P ADMINER**

### **Kh·ªüi ƒë·ªông Database Stack**
```bash
# Ki·ªÉm tra containers ƒëang ch·∫°y
docker-compose ps

# N·∫øu ch∆∞a ch·∫°y, kh·ªüi ƒë·ªông:
docker-compose up -d

# Ch·ªù c√°c services kh·ªüi ƒë·ªông ho√†n to√†n
sleep 30

# Ki·ªÉm tra status
docker-compose ps | grep -E "(postgres|adminer)"
```

### **Truy c·∫≠p Adminer Web Interface**
```bash
# M·ªü tr√¨nh duy·ªát v√† truy c·∫≠p:
http://localhost:8080
```

### **Th√¥ng tin ƒëƒÉng nh·∫≠p t·ª´ project**
D·ª±a tr√™n th√¥ng tin project:
```
Server: postgres (ho·∫∑c postgres-test)
Username: kb_admin
Password: test_password_123
Database: knowledge_base_test
```

## üîê **B∆Ø·ªöC 2: ƒêƒÇNG NH·∫¨P V√ÄO ADMINER**

### **Mermaid Chart - Adminer Workflow**
```mermaid
graph TB
    A[üåê Truy c·∫≠p http://localhost:8080] --> B[üìù Form ƒêƒÉng nh·∫≠p Adminer]
    B --> C{üîë Nh·∫≠p th√¥ng tin}
    C --> D[Server: postgres]
    C --> E[Username: kb_admin] 
    C --> F[Password: test_password_123]
    C --> G[Database: knowledge_base_test]
    D --> H[üîå K·∫øt n·ªëi Database]
    E --> H
    F --> H 
    G --> H
    H --> I[‚úÖ Dashboard Adminer]
    I --> J[üìä Xem Tables]
    I --> K[üîß Qu·∫£n l√Ω Schema]
    I --> L[üíæ Backup/Export]
```

### **Screenshots v√† H∆∞·ªõng d·∫´n chi ti·∫øt**

**1. M√†n h√¨nh ƒëƒÉng nh·∫≠p Adminer:**
- **System**: PostgreSQL (ƒë√£ ƒë∆∞·ª£c ch·ªçn m·∫∑c ƒë·ªãnh)
- **Server**: `postgres` (t√™n container trong Docker network)
- **Username**: `kb_admin`
- **Password**: `test_password_123`  
- **Database**: `knowledge_base_test`

**2. Nh·∫•n "Login" ƒë·ªÉ k·∫øt n·ªëi**

## üìä **B∆Ø·ªöC 3: KH√ÅM PH√Å DATABASE SCHEMA**

### **Xem danh s√°ch Tables**
Sau khi ƒëƒÉng nh·∫≠p th√†nh c√¥ng, b·∫°n s·∫Ω th·∫•y dashboard v·ªõi:

**Core Tables (D·ª± ki·∫øn c√≥ ~11-13 tables):**
- `users` - Qu·∫£n l√Ω ng∆∞·ªùi d√πng
- `user_sessions` - Phi√™n ƒëƒÉng nh·∫≠p
- `documents_metadata_v2` - Metadata t√†i li·ªáu (Enhanced)
- `document_chunks_enhanced` - Chunks t√†i li·ªáu v·ªõi AI
- `rag_pipeline_sessions` - Sessions RAG pipeline

**Analytics Tables:**
- `user_events` - S·ª± ki·ªán ng∆∞·ªùi d√πng
- `document_usage_stats` - Th·ªëng k√™ s·ª≠ d·ª•ng t√†i li·ªáu  
- `search_analytics` - Ph√¢n t√≠ch t√¨m ki·∫øm
- `system_metrics` - Metrics h·ªá th·ªëng
- `user_activity_summary` - T√≥m t·∫Øt ho·∫°t ƒë·ªông user
- `report_generation` - T·∫°o b√°o c√°o

### **Ki·ªÉm tra Structure c·ªßa t·ª´ng Table**

**V√≠ d·ª•: Table `documents_metadata_v2`**
1. Click v√†o t√™n table trong sidebar
2. Ch·ªçn tab "Structure" ƒë·ªÉ xem:
   - **Columns**: C√°c c·ªôt v√† data types
   - **Indexes**: C√°c ch·ªâ m·ª•c ƒë·ªÉ t·ªëi ∆∞u query
   - **Foreign Keys**: Li√™n k·∫øt v·ªõi tables kh√°c
   - **Constraints**: R√†ng bu·ªôc d·ªØ li·ªáu

## üîç **B∆Ø·ªöC 4: XEM D·ªÆ LI·ªÜU (SELECT)**

### **Xem t·∫•t c·∫£ records trong table**
```sql
-- Click v√†o table name, sau ƒë√≥ click "Select"
-- Ho·∫∑c ch·∫°y SQL query:

SELECT * FROM users LIMIT 10;
```

### **Query d·ªØ li·ªáu c·ª• th·ªÉ**
```sql
-- Xem users theo role
SELECT username, email, user_level, created_at 
FROM users 
WHERE user_level = 'MANAGER';

-- Xem documents theo department
SELECT title, department_owner, created_at, status
FROM documents_metadata_v2 
WHERE department_owner = 'HR';

-- Th·ªëng k√™ documents theo type
SELECT document_type, COUNT(*) as count
FROM documents_metadata_v2 
GROUP BY document_type 
ORDER BY count DESC;
```

### **Tab SQL ƒë·ªÉ ch·∫°y custom queries**
1. Click v√†o "SQL command" ·ªü sidebar
2. Nh·∫≠p query v√†o text area
3. Click "Execute" ƒë·ªÉ ch·∫°y

## ‚úèÔ∏è **B∆Ø·ªöC 5: CH·ªàNH S·ª¨A D·ªÆ LI·ªÜU (UPDATE)**

### **C√°ch 1: Edit qua Interface**
1. Click v√†o table name
2. Click "Select" ƒë·ªÉ xem data
3. Click v√†o "edit" (icon b√∫t ch√¨) ·ªü h√†ng mu·ªën s·ª≠a
4. Ch·ªânh s·ª≠a values
5. Click "Save"

### **C√°ch 2: D√πng SQL UPDATE**
```sql
-- Update user level
UPDATE users 
SET user_level = 'MANAGER', updated_at = NOW()
WHERE email = 'demo@company.com';

-- Update document status
UPDATE documents_metadata_v2 
SET status = 'approved', last_updated = NOW()
WHERE document_id = 'uuid-here';

-- Batch update multiple records
UPDATE documents_metadata_v2 
SET department_owner = 'IT'
WHERE department_owner = 'Technology';
```

## ‚ûï **B∆Ø·ªöC 6: TH√äM D·ªÆ LI·ªÜU M·ªöI (INSERT)**

### **C√°ch 1: Insert qua Form**
1. Click v√†o table name
2. Click "New item"
3. ƒêi·ªÅn th√¥ng tin v√†o form
4. Click "Save"

### **C√°ch 2: Insert b·∫±ng SQL**
```sql
-- Th√™m user m·ªõi
INSERT INTO users (
    username, email, full_name, password_hash, salt, 
    user_level, department, status, is_active, email_verified
) VALUES (
    'new_user', 'new@company.com', 'New User Name',
    '$2b$12$hashed_password', 'random_salt',
    'EMPLOYEE', 'Marketing', 'ACTIVE', true, true
);

-- Th√™m document m·ªõi
INSERT INTO documents_metadata_v2 (
    title, document_type, access_level, department_owner, 
    author, status, language_detected
) VALUES (
    'Quy ƒë·ªãnh m·ªõi v·ªÅ remote work', 'policy', 'employee_only',
    'HR', 'HR Manager', 'draft', 'vi'
);
```

## üóëÔ∏è **B∆Ø·ªöC 7: X√ìA D·ªÆ LI·ªÜU (DELETE)**

### **‚ö†Ô∏è C·∫£nh b√°o: Backup tr∆∞·ªõc khi x√≥a!**

### **C√°ch 1: Delete qua Interface**
1. Click table name ‚Üí Select
2. Check checkbox ·ªü h√†ng mu·ªën x√≥a
3. Click "Delete" (button ƒë·ªè)
4. Confirm deletion

### **C√°ch 2: Delete b·∫±ng SQL**
```sql
-- X√≥a user kh√¥ng ho·∫°t ƒë·ªông
DELETE FROM users 
WHERE status = 'INACTIVE' AND last_login < '2024-01-01';

-- X√≥a document c≈© (soft delete t·ªët h∆°n)
UPDATE documents_metadata_v2 
SET status = 'archived', archived_at = NOW()
WHERE created_at < '2023-01-01';

-- Hard delete (c·∫©n th·∫≠n!)
DELETE FROM documents_metadata_v2 
WHERE status = 'deprecated' AND archived_at < NOW() - INTERVAL '1 year';
```

## üíæ **B∆Ø·ªöC 8: BACKUP V√Ä EXPORT D·ªÆ LI·ªÜU**

### **Export to√†n b·ªô Database**
1. Click "Export" ·ªü sidebar
2. Ch·ªçn format:
   - **SQL**: ƒê·ªÉ backup v√† restore
   - **CSV**: ƒê·ªÉ ph√¢n t√≠ch data
   - **JSON**: ƒê·ªÉ integration
3. Ch·ªçn tables c·∫ßn export
4. Click "Export"

### **Export specific table**
```sql
-- Export table to CSV (via Adminer interface)
-- 1. Go to table
-- 2. Click "Export" 
-- 3. Choose CSV format
-- 4. Download file
```

### **Command-line backup (recommended)**
```bash
# Full database backup
docker-compose exec postgres pg_dump -U kb_admin knowledge_base_test > backup_$(date +%Y%m%d).sql

# Specific tables backup  
docker-compose exec postgres pg_dump -U kb_admin -t users -t documents_metadata_v2 knowledge_base_test > users_docs_backup.sql

# Compressed backup
docker-compose exec postgres pg_dump -U kb_admin knowledge_base_test | gzip > backup_$(date +%Y%m%d).sql.gz
```

## üîß **B∆Ø·ªöC 9: QU·∫¢N L√ù SCHEMA**

### **T·∫°o Table m·ªõi**
1. Click "Create table" ·ªü sidebar
2. Nh·∫≠p table name
3. Th√™m columns:
   - Column name
   - Data type (VARCHAR, INTEGER, UUID, etc.)
   - Length/Precision
   - NULL/NOT NULL
   - Default value
4. Th√™m indexes n·∫øu c·∫ßn
5. Click "Save"

### **Modify Table Structure**
1. Click table name
2. Click "Alter table"
3. C√≥ th·ªÉ:
   - Add columns
   - Drop columns  
   - Modify column types
   - Add/remove indexes
   - Add/remove constraints

### **SQL ƒë·ªÉ modify schema**
```sql
-- Th√™m c·ªôt m·ªõi
ALTER TABLE documents_metadata_v2 
ADD COLUMN tags_vector tsvector;

-- T·∫°o index cho search
CREATE INDEX idx_documents_tags_vector 
ON documents_metadata_v2 USING gin(tags_vector);

-- Th√™m constraint
ALTER TABLE users 
ADD CONSTRAINT check_email_format 
CHECK (email ~* '^[A-Za-z0-9._%-]+@[A-Za-z0-9.-]+[.][A-Za-z]+$');
```

## üìà **B∆Ø·ªöC 10: MONITORING V√Ä ANALYTICS**

### **Ki·ªÉm tra Performance**
```sql
-- Top slow queries
SELECT query, mean_time, calls, total_time
FROM pg_stat_statements 
ORDER BY mean_time DESC 
LIMIT 10;

-- Table sizes
SELECT 
    tablename,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size
FROM pg_tables 
WHERE schemaname = 'public'
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;

-- Index usage
SELECT 
    indexname, 
    idx_scan, 
    idx_tup_read, 
    idx_tup_fetch
FROM pg_stat_user_indexes 
WHERE schemaname = 'public'
ORDER BY idx_scan DESC;
```

### **Data Quality Checks**
```sql
-- Duplicate emails
SELECT email, COUNT(*) 
FROM users 
GROUP BY email 
HAVING COUNT(*) > 1;

-- NULL values audit
SELECT 
    'users' as table_name,
    COUNT(*) as total_rows,
    COUNT(email) as email_not_null,
    COUNT(full_name) as name_not_null
FROM users;

-- Data consistency
SELECT 
    d.department_owner,
    COUNT(*) as docs_count,
    COUNT(DISTINCT d.author) as unique_authors
FROM documents_metadata_v2 d
GROUP BY d.department_owner;
```

## üö® **TROUBLESHOOTING**

### **Kh√¥ng k·∫øt n·ªëi ƒë∆∞·ª£c Adminer**
```bash
# Ki·ªÉm tra containers
docker-compose ps

# Restart Adminer n·∫øu c·∫ßn
docker-compose restart adminer

# Check logs
docker-compose logs adminer
```

### **Connection timeout**
```bash
# Ki·ªÉm tra PostgreSQL health
docker-compose exec postgres pg_isready -U kb_admin

# Restart PostgreSQL n·∫øu c·∫ßn
docker-compose restart postgres
```

### **Permission denied**
```sql
-- Ki·ªÉm tra user permissions
SELECT 
    grantee, 
    privilege_type, 
    is_grantable
FROM information_schema.role_table_grants 
WHERE table_name = 'users';
```

## üéØ **TIPS V√Ä BEST PRACTICES**

### **üîí B·∫£o m·∫≠t**
- Lu√¥n backup tr∆∞·ªõc khi thay ƒë·ªïi
- Kh√¥ng l∆∞u passwords plain text
- S·ª≠ d·ª•ng transactions cho multiple operations
- Limit quy·ªÅn access theo role

### **‚ö° Performance**
- Th√™m indexes cho columns th∆∞·ªùng query
- S·ª≠ d·ª•ng LIMIT cho queries l·ªõn
- Monitor slow queries
- Regular VACUUM v√† ANALYZE

### **üìä Data Quality**
- Validate data tr∆∞·ªõc khi insert
- S·ª≠ d·ª•ng constraints ƒë·ªÉ ƒë·∫£m b·∫£o integrity
- Regular data audit
- Document schema changes

B·∫°n mu·ªën t√¥i h∆∞·ªõng d·∫´n chi ti·∫øt v·ªÅ ph·∫ßn n√†o ti·∫øp theo? Ho·∫∑c c√≥ v·∫•n ƒë·ªÅ g√¨ c·ª• th·ªÉ khi s·ª≠ d·ª•ng Adminer kh√¥ng? ü§î
---
B·∫°n ƒë√£ n·∫Øm ƒë∆∞·ª£c quy tr√¨nh r·∫•t ƒë√∫ng! T√¥i s·∫Ω gi·∫£i th√≠ch chi ti·∫øt v·ªÅ **Database Design & Evolution Process** trong th·ª±c t·∫ø:

## üèóÔ∏è **QUY TR√åNH THI·∫æT K·∫æ DATABASE TH·ª∞C T·∫æ**

### **Phase 1: Initial Analysis & Design**
```mermaid
graph TD
    A[Requirements Analysis] --> B[Conceptual Design - ERD]
    B --> C[Logical Design - Schema]
    C --> D[Physical Design - Tables/Indexes]
    D --> E[Initial Implementation]
    E --> F[MVP Launch]
    
    F --> G[User Feedback]
    G --> H[New Requirements]
    H --> I[Schema Evolution]
    I --> J[Migration Planning]
    J --> K[Database Update]
    K --> F
```

---
Hay, m√¨nh gi·∫£i th√≠ch ng·∫Øn g·ªçn nh√©.

---

## üß© **Conceptual Design ‚Äì ERD**

* **Conceptual Design**: L√† b∆∞·ªõc **thi·∫øt k·∫ø kh√°i ni·ªám** trong qu√° tr√¨nh x√¢y database.
  ‚Üí B·∫°n ch∆∞a c·∫ßn nghƒ© ƒë·∫øn b·∫£ng, c·ªôt, index c·ª• th·ªÉ, ch·ªâ c·∫ßn m√¥ h√¨nh h√≥a d·ªØ li·ªáu ·ªü m·ª©c kh√°i ni·ªám (c√°i g√¨ li√™n quan ƒë·∫øn c√°i g√¨).

* **ERD (Entity-Relationship Diagram)**: L√† s∆° ƒë·ªì m√¥ t·∫£ **c√°c th·ª±c th·ªÉ (entity)** v√† **m·ªëi quan h·ªá (relationship)** gi·ªØa ch√∫ng.

V√≠ d·ª•: h·ªá th·ªëng b√°n h√†ng

* **Entity** (th·ª±c th·ªÉ): `Customer`, `Order`, `Product`
* **Attributes** (thu·ªôc t√≠nh):

  * Customer: `customer_id`, `name`, `email`
  * Order: `order_id`, `date`, `total_price`
  * Product: `product_id`, `name`, `price`
* **Relationships** (quan h·ªá):

  * M·ªôt `Customer` c√≥ th·ªÉ c√≥ nhi·ªÅu `Order` (1\:N)
  * M·ªôt `Order` c√≥ th·ªÉ ch·ª©a nhi·ªÅu `Product`, v√† m·ªôt `Product` c√≥ th·ªÉ n·∫±m trong nhi·ªÅu `Order` (N\:M)

---

## üîé H√¨nh dung

ERD gi·ªëng nh∆∞ **b·∫£n v·∫Ω ki·∫øn tr√∫c s∆° b·ªô** cho database.

* Ch∆∞a c√≥ chi ti·∫øt k·ªπ thu·∫≠t.
* D√πng ƒë·ªÉ trao ƒë·ªïi gi·ªØa **business analyst ‚Üî developer**.
* Sau n√†y s·∫Ω ƒë∆∞·ª£c chuy·ªÉn th√†nh **Logical Design (Schema)**, t·ª©c l√† b·∫£ng/c·ªôt c·ª• th·ªÉ trong SQL.

---

üëâ N√≥i ng·∫Øn g·ªçn: **Conceptual Design ‚Äì ERD** = b·∫£n v·∫Ω s∆° ƒë·ªì d·ªØ li·ªáu ·ªü m·ª©c √Ω t∆∞·ªüng, m√¥ t·∫£ **c√°i g√¨** c·∫ßn l∆∞u, v√† **ch√∫ng li√™n h·ªá v·ªõi nhau th·∫ø n√†o**, ch·ª© ch∆∞a ƒëi s√¢u v√†o database c·ª• th·ªÉ.

---

B·∫°n c√≥ mu·ªën m√¨nh v·∫Ω th·ª≠ m·ªôt s∆° ƒë·ªì ERD ƒë∆°n gi·∫£n b·∫±ng mermaid cho v√≠ d·ª• `Customer ‚Äì Order ‚Äì Product` kh√¥ng?
Ok, m√¨nh s·∫Ω v·∫Ω ERD b·∫±ng **Mermaid** cho v√≠ d·ª• `Customer ‚Äì Order ‚Äì Product`.

```mermaid
erDiagram
    CUSTOMER {
        int customer_id PK
        string name
        string email
    }

    ORDER {
        int order_id PK
        date order_date
        float total_price
        int customer_id FK
    }

    PRODUCT {
        int product_id PK
        string name
        float price
    }

    ORDER ||--o{ CUSTOMER : "belongs to"
    CUSTOMER ||--o{ ORDER : "places"
    ORDER ||--o{ PRODUCT : "contains"
    PRODUCT ||--o{ ORDER : "included in"
```

---

## üìù Gi·∫£i th√≠ch

* **CUSTOMER ‚Äì ORDER**: 1 kh√°ch h√†ng c√≥ nhi·ªÅu ƒë∆°n h√†ng (1\:N).
* **ORDER ‚Äì PRODUCT**: 1 ƒë∆°n h√†ng c√≥ nhi·ªÅu s·∫£n ph·∫©m, v√† 1 s·∫£n ph·∫©m c√≥ th·ªÉ n·∫±m trong nhi·ªÅu ƒë∆°n h√†ng (N\:M).

(Th·ª±c t·∫ø N\:M s·∫Ω c·∫ßn b·∫£ng trung gian `Order_Detail` ƒë·ªÉ l∆∞u s·ªë l∆∞·ª£ng, gi√° t·∫°i th·ªùi ƒëi·ªÉm mua.)

---

B·∫°n c√≥ mu·ªën m√¨nh b·ªï sung th√™m b·∫£ng **Order\_Detail** ƒë·ªÉ th·ªÉ hi·ªán r√µ m·ªëi quan h·ªá N\:M kh√¥ng?

---
### **Phase 2: Continuous Evolution**
ƒê√∫ng v·∫≠y! Database s·∫Ω **outdated** khi:
- **New Features** c·∫ßn th√™m tables/columns
- **Performance Issues** c·∫ßn optimize indexes  
- **Business Logic Changes** c·∫ßn restructure
- **Scale Requirements** c·∫ßn partitioning/sharding

---

## üìö **CHU·∫®N C√îNG NGHI·ªÜP - DATABASE LIFECYCLE**

### **1. Database Design Standards**

#### **üéØ Design Principles:**
```sql
-- ‚úÖ GOOD: Normalized, Clear naming
CREATE TABLE users (
    user_id UUID PRIMARY KEY,
    email VARCHAR(255) UNIQUE NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- ‚ùå BAD: Denormalized, Poor naming  
CREATE TABLE user_data (
    id INT,
    email TEXT,
    phone TEXT,
    address TEXT,
    orders TEXT  -- Should be separate table!
);
```

#### **üèõÔ∏è Architecture Patterns:**
- **Microservices**: M·ªói service c√≥ database ri√™ng
- **Event Sourcing**: Store events, not current state
- **CQRS**: Separate Read/Write databases
- **Data Lake**: Raw ‚Üí Processed ‚Üí Analytics

### **2. Schema Evolution Standards**

#### **üîÑ Migration Strategy:**
```python
# Industry Standard: Database Migrations
class Migration_001_AddUserProfile:
    def up():
        """Apply changes"""
        execute("""
            ALTER TABLE users 
            ADD COLUMN profile_data JSONB DEFAULT '{}';
            
            CREATE INDEX idx_users_profile 
            ON users USING GIN(profile_data);
        """)
    
    def down():
        """Rollback changes"""
        execute("""
            DROP INDEX idx_users_profile;
            ALTER TABLE users DROP COLUMN profile_data;
        """)
```

#### **üìã Migration Best Practices:**
```bash
# 1. Version Control Schema Changes
migrations/
‚îú‚îÄ‚îÄ 001_initial_schema.sql
‚îú‚îÄ‚îÄ 002_add_user_profiles.sql  
‚îú‚îÄ‚îÄ 003_optimize_search_index.sql
‚îî‚îÄ‚îÄ 004_add_audit_logging.sql

# 2. Test Migrations
pytest tests/migrations/
pytest tests/data_integrity/

# 3. Deploy with Zero Downtime
./deploy.sh --rolling-update --health-check
```

---

## ‚öôÔ∏è **C√îNG C·ª§ & QUY TR√åNH CHU·∫®N C√îNG NGHI·ªÜP**

### **1. Schema Management Tools**

#### **üõ†Ô∏è Popular Tools:**
```yaml
# Flyway (Java ecosystem)
flyway:
  url: jdbc:postgresql://localhost:5432/mydb
  user: admin
  password: secret
  locations: classpath:db/migration

# Liquibase (Enterprise)
liquibase:
  changeLog: db/changelog/master.xml
  contexts: development,testing
  
# Alembic (Python/SQLAlchemy)
alembic:
  script_location: migrations/
  sqlalchemy.url: postgresql://user:pass@localhost/db

# Atlas (Modern, Declarative)
atlas:
  schema: schema.hcl
  dev-url: docker://postgres/15
```

### **2. Change Management Process**

#### **üéØ Industry Standard Workflow:**
```mermaid
graph LR
    A[Developer] --> B[Create Migration]
    B --> C[Local Testing]
    C --> D[Code Review]
    D --> E[CI/CD Pipeline]
    E --> F[Staging Deploy]
    F --> G[Integration Tests]
    G --> H[Production Deploy]
    H --> I[Health Monitoring]
    
    I --> J{Issues?}
    J -->|Yes| K[Rollback]
    J -->|No| L[Success]
    K --> A
```

#### **üìä Real-world Example:**
```python
# Netflix's approach
class DatabaseEvolution:
    def __init__(self):
        self.stages = [
            "local_development",
            "unit_testing", 
            "integration_testing",
            "canary_deployment",    # 1% traffic
            "blue_green_deployment", # Full switch
            "monitoring_and_alerts"
        ]
    
    def deploy_migration(self, migration):
        for stage in self.stages:
            result = self.execute_stage(stage, migration)
            if not result.success:
                self.rollback(stage, migration)
                return False
        return True
```

---

## üöÄ **CASE STUDY: FACEBOOK'S SCHEMA EVOLUTION**

### **Problem**: Billions of users, millions of schema changes/year

### **Solution**: 
```python
# 1. Shadow Mode - Run new schema alongside old
CREATE TABLE users_v2 AS SELECT * FROM users;
-- Test new schema with real traffic

# 2. Gradual Migration
UPDATE users SET migrated = true 
WHERE user_id % 1000 = 0;  -- Migrate 0.1% users

# 3. Dual Write - Write to both schemas
def create_user(data):
    write_to_old_schema(data)  # Legacy
    write_to_new_schema(data)  # New

# 4. Full Cutover when confident
RENAME TABLE users TO users_old;
RENAME TABLE users_v2 TO users;
```

---

## üìà **BEST PRACTICES CHO D·ª∞ √ÅN C·ª¶A B·∫†N**

### **1. Schema Versioning Strategy**
```sql
-- Track schema version
CREATE TABLE schema_migrations (
    version VARCHAR(50) PRIMARY KEY,
    applied_at TIMESTAMP DEFAULT NOW(),
    rollback_sql TEXT,
    description TEXT
);

-- Auto-track changes
INSERT INTO schema_migrations VALUES 
('20250101_001', NOW(), 'ALTER TABLE...', 'Add user profiles');
```

### **2. Backward Compatibility Rules**
```python
# ‚úÖ SAFE Changes (No downtime)
- ADD COLUMN (with DEFAULT)
- CREATE INDEX CONCURRENTLY  
- ADD CHECK constraint (not validated)
- CREATE new TABLE

# ‚ö†Ô∏è RISKY Changes (Plan carefully)  
- RENAME COLUMN
- DROP COLUMN
- ALTER COLUMN TYPE
- ADD NOT NULL constraint

# ‚ùå DANGEROUS Changes (Multi-step process)
- DROP TABLE
- CHANGE PRIMARY KEY
- SPLIT/MERGE tables
```

### **3. Testing Strategy**
```python
# Automated Tests
class DatabaseMigrationTests:
    def test_schema_migration_up_down(self):
        # Apply migration
        migrate_up('001_add_profiles')
        assert table_exists('user_profiles')
        
        # Test rollback
        migrate_down('001_add_profiles') 
        assert not table_exists('user_profiles')
    
    def test_data_integrity_after_migration(self):
        # Insert test data
        create_test_users(1000)
        
        # Apply migration
        migrate_up('002_normalize_addresses')
        
        # Verify data consistency
        assert users_count() == 1000
        assert no_data_loss()
```

---

## üéØ **T√ìM T·∫ÆT: QUY TR√åNH CHU·∫®N C√îNG NGHI·ªÜP**

### **1. Planning Phase (2-4 weeks)**
- Requirements analysis
- ERD design
- Performance modeling
- Capacity planning

### **2. Implementation Phase (1-2 weeks)**  
- Initial schema creation
- Sample data insertion
- Index optimization
- Security setup

### **3. Evolution Phase (Continuous)**
- Feature-driven changes
- Performance optimizations  
- Scale adjustments
- Security updates

### **4. Monitoring Phase (Always)**
- Query performance tracking
- Storage growth monitoring
- User behavior analysis
- Error rate tracking

**K·∫øt lu·∫≠n**: Quy tr√¨nh b·∫°n m√¥ t·∫£ **ho√†n to√†n ch√≠nh x√°c** v√† l√† chu·∫©n c√¥ng nghi·ªáp! Database s·∫Ω lu√¥n evolve theo nhu c·∫ßu business, v√† vi·ªác c√≥ **migration strategy** t·ªët l√† ch√¨a kh√≥a th√†nh c√¥ng. üöÄ
