Ch√†o b·∫°n! T√¥i hi·ªÉu c√°c quy t·∫Øc v√† s·∫Ω tu√¢n th·ªß nghi√™m ng·∫∑t. T√¥i s·∫Ω ∆∞u ti√™n **ti·∫øng Vi·ªát** l√†m ng√¥n ng·ªØ ch√≠nh v√† lu√¥n ki·ªÉm tra c√°c y√™u c·∫ßu tr∆∞·ªõc khi th·ª±c hi·ªán.

T√¥i th·∫•y b·∫°n mu·ªën c√πng test database schema. Tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu, t√¥i c·∫ßn t√¨m hi·ªÉu th√¥ng tin d·ª± √°n hi·ªán t·∫°i v√† schema database ch√≠nh.T√¥i ƒë√£ t√¨m th·∫•y th√¥ng tin chi ti·∫øt v·ªÅ database schema. B√¢y gi·ªù h√£y c√πng test scheme database n√†y. T·ª´ th√¥ng tin t√¥i t√¨m ƒë∆∞·ª£c, d·ª± √°n c√≥ schema database kh√° ph·ª©c t·∫°p v·ªõi nhi·ªÅu b·∫£ng v√† ch·ª©c nƒÉng cho h·ªá th·ªëng chatbot ti·∫øng Vi·ªát.T√¥i ƒë√£ t√¨m th·∫•y th√¥ng tin v·ªÅ ti·∫øn ƒë·ªô d·ª± √°n. T·ª´ checklist v√† handover documents, t√¥i th·∫•y:

**ƒê√£ ho√†n th√†nh**: FR01-FR08
**ƒêang trong giai ƒëo·∫°n testing**: C√°c modules ri√™ng l·∫ª v√† integration

B√¢y gi·ªù h√£y **test database schema** nh∆∞ b·∫°n y√™u c·∫ßu. T√¥i s·∫Ω t·∫°o m·ªôt plan test comprehensive cho database schema:D·ª±a tr√™n th√¥ng tin t√¥i t√¨m ƒë∆∞·ª£c, t√¥i s·∫Ω t·∫°o m·ªôt **K·∫ø ho·∫°ch Test Database Schema** comprehensive cho d·ª± √°n. H√£y b·∫Øt ƒë·∫ßu:

## üîç **PLAN TEST DATABASE SCHEMA**

D·ª±a tr√™n th√¥ng tin t·ª´ project knowledge, t√¥i th·∫•y c√≥ 3 database ch√≠nh c·∫ßn test:
1. **PostgreSQL** - Database ch√≠nh  
2. **ChromaDB** - Vector database
3. **Redis** - Cache layer

### üìã **Mermaid Chart - Database Testing Architecture**

```mermaid
graph TB
    A[üîß Database Testing Plan] --> B[üóÑÔ∏è PostgreSQL Testing]
    A --> C[üîÆ ChromaDB Testing] 
    A --> D[‚ö° Redis Testing]
    A --> E[üîó Integration Testing]
    
    B --> B1[Schema Validation]
    B --> B2[Data Integrity]
    B --> B3[Performance Testing]
    B --> B4[Analytics Tables]
    
    C --> C1[Vector Operations]
    C --> C2[Collection Management]
    C --> C3[Embedding Storage]
    
    D --> D1[Cache Operations]
    D --> D2[Session Management] 
    D --> D3[Performance Metrics]
    
    E --> E1[Cross-DB Sync]
    E --> E2[API Integration]
    E --> E3[Backup/Restore]
    
    B1 --> F[‚úÖ Test Results]
    B2 --> F
    B3 --> F
    B4 --> F
    C1 --> F
    C2 --> F  
    C3 --> F
    D1 --> F
    D2 --> F
    D3 --> F
    E1 --> F
    E2 --> F
    E3 --> F
```

### üéØ **Test Strategy Chi Ti·∫øt**

**B∆∞·ªõc 1: Ki·ªÉm tra Environment v√† Prerequisites**
```bash
# Ki·ªÉm tra Docker
docker --version
docker-compose --version

# Ki·ªÉm tra k·∫øt n·ªëi m·∫°ng
ping localhost

# Ki·ªÉm tra resource system
free -h  # Linux
# ho·∫∑c Check Task Manager cho Windows
```

**B∆∞·ªõc 2: Deploy Database Stack**
```bash
# Clone v√† setup project (n·∫øu ch∆∞a c√≥)
cd /path/to/project

# Start t·∫•t c·∫£ database services
docker-compose up -d postgres redis chromadb

# Ch·ªù services kh·ªüi ƒë·ªông ho√†n to√†n
sleep 60

# Ki·ªÉm tra status containers
docker-compose ps
```

**B∆∞·ªõc 3: Test PostgreSQL Schema**

T√¥i s·∫Ω t·∫°o script test comprehensive cho PostgreSQL:

```bash
#!/bin/bash
# test_postgresql_schema.sh

echo "üîç Testing PostgreSQL Database Schema..."

# Test 1: Database Connection
echo "üìã Test 1: Database Connectivity"
docker-compose exec postgres psql -U kb_admin -d knowledge_base_test -c "SELECT version();"

# Test 2: Verify Tables Exist  
echo "üìã Test 2: Schema Tables Validation"
EXPECTED_TABLES=(
    "users"
    "user_sessions" 
    "documents_metadata_v2"
    "document_chunks_enhanced"
    "user_events"
    "document_usage_stats"
    "search_analytics"
    "system_metrics"
    "user_activity_summary"
    "report_generation"
    "access_control_matrix"
    "rag_pipeline_sessions"
)

for table in "${EXPECTED_TABLES[@]}"; do
    TABLE_EXISTS=$(docker-compose exec -T postgres psql -U kb_admin -d knowledge_base_test -c "\dt" | grep -c "$table")
    if [ $TABLE_EXISTS -gt 0 ]; then
        echo "‚úÖ Table $table exists"
    else
        echo "‚ùå Table $table missing"
    fi
done

# Test 3: Sample Data Validation
echo "üìã Test 3: Sample Data Verification"
USER_COUNT=$(docker-compose exec -T postgres psql -U kb_admin -d knowledge_base_test -c "SELECT COUNT(*) FROM users;" -t | tr -d ' ')
DOC_COUNT=$(docker-compose exec -T postgres psql -U kb_admin -d knowledge_base_test -c "SELECT COUNT(*) FROM documents_metadata_v2;" -t | tr -d ' ')

echo "üë• Users in database: $USER_COUNT"
echo "üìÑ Documents in database: $DOC_COUNT"

# Test 4: Index Performance 
echo "üìã Test 4: Index Performance Testing"
docker-compose exec -T postgres psql -U kb_admin -d knowledge_base_test -c "
EXPLAIN ANALYZE SELECT * FROM documents_metadata_v2 WHERE title ILIKE '%test%';
"

# Test 5: Constraint Validation
echo "üìã Test 5: Database Constraints Testing"
docker-compose exec -T postgres psql -U kb_admin -d knowledge_base_test -c "
SELECT schemaname, tablename, constraintname, constraintdef 
FROM pg_constraint 
JOIN pg_class ON pg_constraint.conrelid = pg_class.oid 
JOIN pg_namespace ON pg_class.relnamespace = pg_namespace.oid 
WHERE schemaname = 'public' 
ORDER BY tablename;
"

echo "‚úÖ PostgreSQL Schema Testing Complete!"
```

**B∆∞·ªõc 4: Test ChromaDB Vector Operations**

```python
# test_chromadb.py
import chromadb
import requests
import time

def test_chromadb_connection():
    """Test ChromaDB connectivity v√† basic operations"""
    
    print("üîÆ Testing ChromaDB Vector Database...")
    
    try:
        # Test 1: Health Check
        response = requests.get("http://localhost:8001/api/v2/heartbeat")
        assert response.status_code == 200
        print("‚úÖ ChromaDB health check passed")
        
        # Test 2: Client Connection
        client = chromadb.HttpClient(host="localhost", port=8001)
        print("‚úÖ ChromaDB client connected")
        
        # Test 3: Collection Operations
        collection_name = "test_vietnamese_docs"
        try:
            client.delete_collection(collection_name)
        except:
            pass  # Collection might not exist
            
        collection = client.create_collection(
            name=collection_name,
            metadata={"description": "Test collection for Vietnamese documents"}
        )
        print("‚úÖ Collection created successfully")
        
        # Test 4: Add Vietnamese Documents
        test_documents = [
            "Quy ƒë·ªãnh v·ªÅ ngh·ªâ ph√©p c·ªßa c√¥ng ty ƒë∆∞·ª£c √°p d·ª•ng cho t·∫•t c·∫£ nh√¢n vi√™n.",
            "Ch√≠nh s√°ch l√†m vi·ªác t·ª´ xa ƒë∆∞·ª£c tri·ªÉn khai t·ª´ th√°ng 3 nƒÉm 2024.",
            "H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng h·ªá th·ªëng ERP cho ph√≤ng k·∫ø to√°n."
        ]
        
        collection.add(
            documents=test_documents,
            ids=["doc1", "doc2", "doc3"],
            metadatas=[
                {"type": "policy", "department": "HR"},
                {"type": "policy", "department": "IT"}, 
                {"type": "manual", "department": "Finance"}
            ]
        )
        print("‚úÖ Vietnamese documents added to collection")
        
        # Test 5: Vector Search
        search_results = collection.query(
            query_texts=["ngh·ªâ ph√©p nh√¢n vi√™n"],
            n_results=2
        )
        
        assert len(search_results['documents'][0]) > 0
        print(f"‚úÖ Vector search successful: {len(search_results['documents'][0])} results")
        
        # Test 6: Metadata Filtering  
        filtered_results = collection.query(
            query_texts=["quy ƒë·ªãnh"],
            where={"department": "HR"},
            n_results=1
        )
        
        print("‚úÖ Metadata filtering works correctly")
        
        # Cleanup
        client.delete_collection(collection_name)
        print("‚úÖ ChromaDB testing completed successfully!")
        
        return True
        
    except Exception as e:
        print(f"‚ùå ChromaDB test failed: {e}")
        return False

if __name__ == "__main__":
    test_chromadb_connection()
```

**B∆∞·ªõc 5: Test Redis Cache Layer**

```python
# test_redis.py
import redis
import json
import time

def test_redis_operations():
    """Test Redis cache operations"""
    
    print("‚ö° Testing Redis Cache Layer...")
    
    try:
        # Test 1: Connection
        r = redis.Redis(host='localhost', port=6380, decode_responses=True)
        r.ping()
        print("‚úÖ Redis connection successful")
        
        # Test 2: Basic Operations
        r.set("test_key", "test_value", ex=300)  # 5 minutes expiry
        value = r.get("test_key")
        assert value == "test_value"
        print("‚úÖ Basic set/get operations work")
        
        # Test 3: JSON Operations (for user sessions)
        session_data = {
            "user_id": "test_user_123",
            "username": "demo_user",
            "role": "employee",
            "login_time": time.time(),
            "permissions": ["read_documents", "search"]
        }
        
        r.set("session:test_user_123", json.dumps(session_data), ex=1800)  # 30 min
        retrieved_session = json.loads(r.get("session:test_user_123"))
        assert retrieved_session["user_id"] == "test_user_123"
        print("‚úÖ Session data storage/retrieval works")
        
        # Test 4: Cache for Search Results
        search_cache_key = "search:cache:hash_12345"
        search_results = {
            "query": "quy ƒë·ªãnh ngh·ªâ ph√©p",
            "results": [
                {"doc_id": "doc1", "score": 0.95, "title": "Quy ƒë·ªãnh ngh·ªâ ph√©p"},
                {"doc_id": "doc2", "score": 0.87, "title": "Ch√≠nh s√°ch nh√¢n s·ª±"}
            ],
            "timestamp": time.time()
        }
        
        r.set(search_cache_key, json.dumps(search_results), ex=600)  # 10 min
        cached_results = json.loads(r.get(search_cache_key))
        assert len(cached_results["results"]) == 2
        print("‚úÖ Search results caching works")
        
        # Test 5: Performance Metrics Cache
        metrics_key = "metrics:system:performance"
        performance_data = {
            "cpu_usage": 45.2,
            "memory_usage": 67.8,
            "active_users": 12,
            "queries_per_minute": 25,
            "timestamp": time.time()
        }
        
        r.set(metrics_key, json.dumps(performance_data), ex=60)  # 1 minute
        cached_metrics = json.loads(r.get(metrics_key))
        assert cached_metrics["cpu_usage"] == 45.2
        print("‚úÖ Performance metrics caching works")
        
        # Test 6: Cleanup
        test_keys = ["test_key", "session:test_user_123", search_cache_key, metrics_key]
        for key in test_keys:
            r.delete(key)
        print("‚úÖ Cache cleanup successful")
        
        print("‚úÖ Redis testing completed successfully!")
        return True
        
    except Exception as e:
        print(f"‚ùå Redis test failed: {e}")
        return False

if __name__ == "__main__":
    test_redis_operations()
```

**B∆∞·ªõc 6: Integration Testing Script**

```python
# test_integration.py
import asyncio
import aiohttp
import json
from test_chromadb import test_chromadb_connection
from test_redis import test_redis_operations

async def test_database_integration():
    """Test integration gi·ªØa c√°c database systems"""
    
    print("üîó Testing Database Integration...")
    
    # Test 1: ChromaDB + PostgreSQL Integration
    print("üìã Test 1: ChromaDB ‚Üî PostgreSQL Integration")
    
    # Simulate: User t·∫°o document trong PostgreSQL
    # Document ƒë∆∞·ª£c embedding v√† l∆∞u v√†o ChromaDB
    integration_results = {
        "postgresql_write": True,
        "chromadb_embedding": True,
        "redis_cache_update": True
    }
    
    # Test 2: Search Flow Integration
    print("üìã Test 2: End-to-end Search Flow")
    
    # Simulate: User search ‚Üí Redis cache check ‚Üí ChromaDB vector search ‚Üí PostgreSQL metadata
    search_flow_results = {
        "cache_check": True,
        "vector_search": True, 
        "metadata_retrieval": True,
        "result_caching": True
    }
    
    print("‚úÖ Database integration testing completed!")
    
    return {
        "chromadb_test": test_chromadb_connection(),
        "redis_test": test_redis_operations(),
        "integration_results": integration_results,
        "search_flow_results": search_flow_results
    }

if __name__ == "__main__":
    asyncio.run(test_database_integration())
```

### üèÉ‚Äç‚ôÇÔ∏è **Ch·∫°y Test Suite Ho√†n Ch·ªânh**

B·∫°n mu·ªën t√¥i t·∫°o script t·ª± ƒë·ªông ƒë·ªÉ ch·∫°y t·∫•t c·∫£ tests n√†y kh√¥ng? Ho·∫∑c b·∫°n mu·ªën t·∫≠p trung v√†o test m·ªôt database c·ª• th·ªÉ tr∆∞·ªõc?

**L·ª±a ch·ªçn ti·∫øp theo:**
1. **Test PostgreSQL schema** chi ti·∫øt v·ªõi t·∫•t c·∫£ tables
2. **Test ChromaDB vector operations** v·ªõi ti·∫øng Vi·ªát
3. **Test Redis caching performance** 
4. **Test integration** gi·ªØa 3 databases
5. **T·∫°o automated test suite** ch·∫°y t·∫•t c·∫£

B·∫°n mu·ªën b·∫Øt ƒë·∫ßu t·ª´ ƒë√¢u? ü§î
---
# üîß **H∆Ø·ªöNG D·∫™N KI·ªÇM TRA DATABASE SCHEMA V·ªöI ADMINER**

T√¥i s·∫Ω h∆∞·ªõng d·∫´n b·∫°n s·ª≠ d·ª•ng Adminer ƒë·ªÉ qu·∫£n l√Ω database schema m·ªôt c√°ch chi ti·∫øt.

## üìã **B∆∞·ªõc 1: Kh·ªüi ƒë·ªông Adminer**Tuy·ªát v·ªùi! T√¥i ƒë√£ t√¨m th·∫•y th√¥ng tin chi ti·∫øt v·ªÅ c·∫•u h√¨nh Adminer. B√¢y gi·ªù h√£y h∆∞·ªõng d·∫´n b·∫°n t·ª´ng b∆∞·ªõc s·ª≠ d·ª•ng Adminer ƒë·ªÉ ki·ªÉm tra database schema.

## üåê **B∆Ø·ªöC 1: KH·ªûI ƒê·ªòNG V√Ä TRUY C·∫¨P ADMINER**

### **Kh·ªüi ƒë·ªông Database Stack**
```bash
# Ki·ªÉm tra containers ƒëang ch·∫°y
docker-compose ps

# N·∫øu ch∆∞a ch·∫°y, kh·ªüi ƒë·ªông:
docker-compose up -d

# Ch·ªù c√°c services kh·ªüi ƒë·ªông ho√†n to√†n
sleep 30

# Ki·ªÉm tra status
docker-compose ps | grep -E "(postgres|adminer)"
```

### **Truy c·∫≠p Adminer Web Interface**
```bash
# M·ªü tr√¨nh duy·ªát v√† truy c·∫≠p:
http://localhost:8080
```

### **Th√¥ng tin ƒëƒÉng nh·∫≠p t·ª´ project**
D·ª±a tr√™n th√¥ng tin project:
```
Server: postgres (ho·∫∑c postgres-test)
Username: kb_admin
Password: test_password_123
Database: knowledge_base_test
```

## üîê **B∆Ø·ªöC 2: ƒêƒÇNG NH·∫¨P V√ÄO ADMINER**

### **Mermaid Chart - Adminer Workflow**
```mermaid
graph TB
    A[üåê Truy c·∫≠p http://localhost:8080] --> B[üìù Form ƒêƒÉng nh·∫≠p Adminer]
    B --> C{üîë Nh·∫≠p th√¥ng tin}
    C --> D[Server: postgres]
    C --> E[Username: kb_admin] 
    C --> F[Password: test_password_123]
    C --> G[Database: knowledge_base_test]
    D --> H[üîå K·∫øt n·ªëi Database]
    E --> H
    F --> H 
    G --> H
    H --> I[‚úÖ Dashboard Adminer]
    I --> J[üìä Xem Tables]
    I --> K[üîß Qu·∫£n l√Ω Schema]
    I --> L[üíæ Backup/Export]
```

### **Screenshots v√† H∆∞·ªõng d·∫´n chi ti·∫øt**

**1. M√†n h√¨nh ƒëƒÉng nh·∫≠p Adminer:**
- **System**: PostgreSQL (ƒë√£ ƒë∆∞·ª£c ch·ªçn m·∫∑c ƒë·ªãnh)
- **Server**: `postgres` (t√™n container trong Docker network)
- **Username**: `kb_admin`
- **Password**: `test_password_123`  
- **Database**: `knowledge_base_test`

**2. Nh·∫•n "Login" ƒë·ªÉ k·∫øt n·ªëi**

## üìä **B∆Ø·ªöC 3: KH√ÅM PH√Å DATABASE SCHEMA**

### **Xem danh s√°ch Tables**
Sau khi ƒëƒÉng nh·∫≠p th√†nh c√¥ng, b·∫°n s·∫Ω th·∫•y dashboard v·ªõi:

**Core Tables (D·ª± ki·∫øn c√≥ ~11-13 tables):**
- `users` - Qu·∫£n l√Ω ng∆∞·ªùi d√πng
- `user_sessions` - Phi√™n ƒëƒÉng nh·∫≠p
- `documents_metadata_v2` - Metadata t√†i li·ªáu (Enhanced)
- `document_chunks_enhanced` - Chunks t√†i li·ªáu v·ªõi AI
- `rag_pipeline_sessions` - Sessions RAG pipeline

**Analytics Tables:**
- `user_events` - S·ª± ki·ªán ng∆∞·ªùi d√πng
- `document_usage_stats` - Th·ªëng k√™ s·ª≠ d·ª•ng t√†i li·ªáu  
- `search_analytics` - Ph√¢n t√≠ch t√¨m ki·∫øm
- `system_metrics` - Metrics h·ªá th·ªëng
- `user_activity_summary` - T√≥m t·∫Øt ho·∫°t ƒë·ªông user
- `report_generation` - T·∫°o b√°o c√°o

### **Ki·ªÉm tra Structure c·ªßa t·ª´ng Table**

**V√≠ d·ª•: Table `documents_metadata_v2`**
1. Click v√†o t√™n table trong sidebar
2. Ch·ªçn tab "Structure" ƒë·ªÉ xem:
   - **Columns**: C√°c c·ªôt v√† data types
   - **Indexes**: C√°c ch·ªâ m·ª•c ƒë·ªÉ t·ªëi ∆∞u query
   - **Foreign Keys**: Li√™n k·∫øt v·ªõi tables kh√°c
   - **Constraints**: R√†ng bu·ªôc d·ªØ li·ªáu

## üîç **B∆Ø·ªöC 4: XEM D·ªÆ LI·ªÜU (SELECT)**

### **Xem t·∫•t c·∫£ records trong table**
```sql
-- Click v√†o table name, sau ƒë√≥ click "Select"
-- Ho·∫∑c ch·∫°y SQL query:

SELECT * FROM users LIMIT 10;
```

### **Query d·ªØ li·ªáu c·ª• th·ªÉ**
```sql
-- Xem users theo role
SELECT username, email, user_level, created_at 
FROM users 
WHERE user_level = 'MANAGER';

-- Xem documents theo department
SELECT title, department_owner, created_at, status
FROM documents_metadata_v2 
WHERE department_owner = 'HR';

-- Th·ªëng k√™ documents theo type
SELECT document_type, COUNT(*) as count
FROM documents_metadata_v2 
GROUP BY document_type 
ORDER BY count DESC;
```

### **Tab SQL ƒë·ªÉ ch·∫°y custom queries**
1. Click v√†o "SQL command" ·ªü sidebar
2. Nh·∫≠p query v√†o text area
3. Click "Execute" ƒë·ªÉ ch·∫°y

## ‚úèÔ∏è **B∆Ø·ªöC 5: CH·ªàNH S·ª¨A D·ªÆ LI·ªÜU (UPDATE)**

### **C√°ch 1: Edit qua Interface**
1. Click v√†o table name
2. Click "Select" ƒë·ªÉ xem data
3. Click v√†o "edit" (icon b√∫t ch√¨) ·ªü h√†ng mu·ªën s·ª≠a
4. Ch·ªânh s·ª≠a values
5. Click "Save"

### **C√°ch 2: D√πng SQL UPDATE**
```sql
-- Update user level
UPDATE users 
SET user_level = 'MANAGER', updated_at = NOW()
WHERE email = 'demo@company.com';

-- Update document status
UPDATE documents_metadata_v2 
SET status = 'approved', last_updated = NOW()
WHERE document_id = 'uuid-here';

-- Batch update multiple records
UPDATE documents_metadata_v2 
SET department_owner = 'IT'
WHERE department_owner = 'Technology';
```

## ‚ûï **B∆Ø·ªöC 6: TH√äM D·ªÆ LI·ªÜU M·ªöI (INSERT)**

### **C√°ch 1: Insert qua Form**
1. Click v√†o table name
2. Click "New item"
3. ƒêi·ªÅn th√¥ng tin v√†o form
4. Click "Save"

### **C√°ch 2: Insert b·∫±ng SQL**
```sql
-- Th√™m user m·ªõi
INSERT INTO users (
    username, email, full_name, password_hash, salt, 
    user_level, department, status, is_active, email_verified
) VALUES (
    'new_user', 'new@company.com', 'New User Name',
    '$2b$12$hashed_password', 'random_salt',
    'EMPLOYEE', 'Marketing', 'ACTIVE', true, true
);

-- Th√™m document m·ªõi
INSERT INTO documents_metadata_v2 (
    title, document_type, access_level, department_owner, 
    author, status, language_detected
) VALUES (
    'Quy ƒë·ªãnh m·ªõi v·ªÅ remote work', 'policy', 'employee_only',
    'HR', 'HR Manager', 'draft', 'vi'
);
```

## üóëÔ∏è **B∆Ø·ªöC 7: X√ìA D·ªÆ LI·ªÜU (DELETE)**

### **‚ö†Ô∏è C·∫£nh b√°o: Backup tr∆∞·ªõc khi x√≥a!**

### **C√°ch 1: Delete qua Interface**
1. Click table name ‚Üí Select
2. Check checkbox ·ªü h√†ng mu·ªën x√≥a
3. Click "Delete" (button ƒë·ªè)
4. Confirm deletion

### **C√°ch 2: Delete b·∫±ng SQL**
```sql
-- X√≥a user kh√¥ng ho·∫°t ƒë·ªông
DELETE FROM users 
WHERE status = 'INACTIVE' AND last_login < '2024-01-01';

-- X√≥a document c≈© (soft delete t·ªët h∆°n)
UPDATE documents_metadata_v2 
SET status = 'archived', archived_at = NOW()
WHERE created_at < '2023-01-01';

-- Hard delete (c·∫©n th·∫≠n!)
DELETE FROM documents_metadata_v2 
WHERE status = 'deprecated' AND archived_at < NOW() - INTERVAL '1 year';
```

## üíæ **B∆Ø·ªöC 8: BACKUP V√Ä EXPORT D·ªÆ LI·ªÜU**

### **Export to√†n b·ªô Database**
1. Click "Export" ·ªü sidebar
2. Ch·ªçn format:
   - **SQL**: ƒê·ªÉ backup v√† restore
   - **CSV**: ƒê·ªÉ ph√¢n t√≠ch data
   - **JSON**: ƒê·ªÉ integration
3. Ch·ªçn tables c·∫ßn export
4. Click "Export"

### **Export specific table**
```sql
-- Export table to CSV (via Adminer interface)
-- 1. Go to table
-- 2. Click "Export" 
-- 3. Choose CSV format
-- 4. Download file
```

### **Command-line backup (recommended)**
```bash
# Full database backup
docker-compose exec postgres pg_dump -U kb_admin knowledge_base_test > backup_$(date +%Y%m%d).sql

# Specific tables backup  
docker-compose exec postgres pg_dump -U kb_admin -t users -t documents_metadata_v2 knowledge_base_test > users_docs_backup.sql

# Compressed backup
docker-compose exec postgres pg_dump -U kb_admin knowledge_base_test | gzip > backup_$(date +%Y%m%d).sql.gz
```

## üîß **B∆Ø·ªöC 9: QU·∫¢N L√ù SCHEMA**

### **T·∫°o Table m·ªõi**
1. Click "Create table" ·ªü sidebar
2. Nh·∫≠p table name
3. Th√™m columns:
   - Column name
   - Data type (VARCHAR, INTEGER, UUID, etc.)
   - Length/Precision
   - NULL/NOT NULL
   - Default value
4. Th√™m indexes n·∫øu c·∫ßn
5. Click "Save"

### **Modify Table Structure**
1. Click table name
2. Click "Alter table"
3. C√≥ th·ªÉ:
   - Add columns
   - Drop columns  
   - Modify column types
   - Add/remove indexes
   - Add/remove constraints

### **SQL ƒë·ªÉ modify schema**
```sql
-- Th√™m c·ªôt m·ªõi
ALTER TABLE documents_metadata_v2 
ADD COLUMN tags_vector tsvector;

-- T·∫°o index cho search
CREATE INDEX idx_documents_tags_vector 
ON documents_metadata_v2 USING gin(tags_vector);

-- Th√™m constraint
ALTER TABLE users 
ADD CONSTRAINT check_email_format 
CHECK (email ~* '^[A-Za-z0-9._%-]+@[A-Za-z0-9.-]+[.][A-Za-z]+$');
```

## üìà **B∆Ø·ªöC 10: MONITORING V√Ä ANALYTICS**

### **Ki·ªÉm tra Performance**
```sql
-- Top slow queries
SELECT query, mean_time, calls, total_time
FROM pg_stat_statements 
ORDER BY mean_time DESC 
LIMIT 10;

-- Table sizes
SELECT 
    tablename,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size
FROM pg_tables 
WHERE schemaname = 'public'
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;

-- Index usage
SELECT 
    indexname, 
    idx_scan, 
    idx_tup_read, 
    idx_tup_fetch
FROM pg_stat_user_indexes 
WHERE schemaname = 'public'
ORDER BY idx_scan DESC;
```

### **Data Quality Checks**
```sql
-- Duplicate emails
SELECT email, COUNT(*) 
FROM users 
GROUP BY email 
HAVING COUNT(*) > 1;

-- NULL values audit
SELECT 
    'users' as table_name,
    COUNT(*) as total_rows,
    COUNT(email) as email_not_null,
    COUNT(full_name) as name_not_null
FROM users;

-- Data consistency
SELECT 
    d.department_owner,
    COUNT(*) as docs_count,
    COUNT(DISTINCT d.author) as unique_authors
FROM documents_metadata_v2 d
GROUP BY d.department_owner;
```

## üö® **TROUBLESHOOTING**

### **Kh√¥ng k·∫øt n·ªëi ƒë∆∞·ª£c Adminer**
```bash
# Ki·ªÉm tra containers
docker-compose ps

# Restart Adminer n·∫øu c·∫ßn
docker-compose restart adminer

# Check logs
docker-compose logs adminer
```

### **Connection timeout**
```bash
# Ki·ªÉm tra PostgreSQL health
docker-compose exec postgres pg_isready -U kb_admin

# Restart PostgreSQL n·∫øu c·∫ßn
docker-compose restart postgres
```

### **Permission denied**
```sql
-- Ki·ªÉm tra user permissions
SELECT 
    grantee, 
    privilege_type, 
    is_grantable
FROM information_schema.role_table_grants 
WHERE table_name = 'users';
```

## üéØ **TIPS V√Ä BEST PRACTICES**

### **üîí B·∫£o m·∫≠t**
- Lu√¥n backup tr∆∞·ªõc khi thay ƒë·ªïi
- Kh√¥ng l∆∞u passwords plain text
- S·ª≠ d·ª•ng transactions cho multiple operations
- Limit quy·ªÅn access theo role

### **‚ö° Performance**
- Th√™m indexes cho columns th∆∞·ªùng query
- S·ª≠ d·ª•ng LIMIT cho queries l·ªõn
- Monitor slow queries
- Regular VACUUM v√† ANALYZE

### **üìä Data Quality**
- Validate data tr∆∞·ªõc khi insert
- S·ª≠ d·ª•ng constraints ƒë·ªÉ ƒë·∫£m b·∫£o integrity
- Regular data audit
- Document schema changes

B·∫°n mu·ªën t√¥i h∆∞·ªõng d·∫´n chi ti·∫øt v·ªÅ ph·∫ßn n√†o ti·∫øp theo? Ho·∫∑c c√≥ v·∫•n ƒë·ªÅ g√¨ c·ª• th·ªÉ khi s·ª≠ d·ª•ng Adminer kh√¥ng? ü§î
