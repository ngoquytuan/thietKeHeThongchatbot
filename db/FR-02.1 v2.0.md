# T√ÄI LI·ªÜU THI·∫æT K·∫æ K·ª∏ THU·∫¨T FR-02.1 v2.0
## H·ªÜ TH·ªêNG C∆† S·ªû D·ªÆ LI·ªÜU K√âP (DUAL DATABASE SYSTEM)

---

**Phi√™n b·∫£n:** 2.0  
**Ng√†y:** 12/09/2025  
**Module:** FR-02.1 - H·ªá th·ªëng CSDL k√©p  
**M·ª©c ƒë·ªô ∆∞u ti√™n:** CRITICAL  
**Ng∆∞·ªùi so·∫°n th·∫£o:** Technical Design Team  

---

## üìã **CHANGELOG v2.0**

### **üîÑ Thay ƒë·ªïi t·ª´ v1.0:**

#### **‚úÖ Enhancements ch√≠nh:**
- **Enhanced Schema Integration**: T√≠ch h·ª£p schema t·ª´ FR03.1 v·ªõi c√°c tr∆∞·ªùng m·ªü r·ªông cho chunking n√¢ng cao
- **Updated Vector Database**: Thay th·∫ø model embedding t·ª´ OpenAI sang `Qwen/Qwen3-Embedding-0.6B` (1024-dim)
- **Vietnamese Optimization**: T·ªëi ∆∞u ƒë·∫∑c bi·ªát cho x·ª≠ l√Ω ti·∫øng Vi·ªát v·ªõi `pyvi` v√† `underthesea`
- **Advanced Monitoring**: Th√™m comprehensive monitoring v·ªõi Prometheus + Grafana
- **Production-Ready Setup**: Docker Compose ho√†n ch·ªânh v·ªõi health checks v√† restart policies
- **Performance Optimization**: Connection pooling, indexing strategies, v√† caching layers
- **Security Hardening**: Enhanced authentication, authorization, v√† audit logging

#### **üóëÔ∏è Removed t·ª´ v1.0:**
- Timeline estimates (th·ªùi gian tri·ªÉn khai)
- OpenAI dependency requirements
- Basic monitoring setup
- Simple Docker configuration

#### **üÜï New Features:**
- Vietnamese text analysis table v·ªõi quality metrics
- Data ingestion job tracking tables
- Chunk processing logs v·ªõi retry mechanisms
- Performance monitoring views
- Automated backup strategies
- Load balancing configuration

---

## üîã **T·ªîNG QUAN THI·∫æT K·∫æ**

### **M·ª•c ti√™u**
Thi·∫øt k·∫ø v√† tri·ªÉn khai h·ªá th·ªëng c∆° s·ªü d·ªØ li·ªáu k√©p bao g·ªìm:
- **Vector Database**: L∆∞u tr·ªØ embeddings cho t√¨m ki·∫øm ng·ªØ nghƒ©a (‚â•100k chunks)
- **Relational Database**: Qu·∫£n l√Ω metadata, ng∆∞·ªùi d√πng v√† ph√¢n quy·ªÅn
- **Vietnamese Language Support**: T·ªëi ∆∞u ƒë·∫∑c bi·ªát cho ti·∫øng Vi·ªát
- **Production-Ready Infrastructure**: Monitoring, backup, v√† high availability

### **Ki·∫øn tr√∫c t·ªïng quan**
```mermaid
graph TB
    subgraph "Application Layer"
        API[üåê REST API Services]
        RAG[ü§ñ RAG Engine]
        UI[üíª Web Interface]
    end
    
    subgraph "üóÑÔ∏è FR-02.1: DUAL DATABASE SYSTEM v2.0"
        subgraph "üìä Vector Database Layer"
            ChromaDB[üì¢ ChromaDB<br/>Qwen Embeddings]
            VectorIndex[üìá Vector Index<br/>HNSW Algorithm]
            Collections[üìö Collections<br/>Multi-tenant]
        end
        
        subgraph "üóÉÔ∏è Enhanced PostgreSQL Layer"
            PostgreSQL[(üêò PostgreSQL 15<br/>Enhanced Schema)]
            ConnPool[üîÑ PgBouncer<br/>Connection Pool]
            Schemas[üìã Enhanced Tables<br/>Vietnamese Support]
        end
        
        subgraph "‚ö° Cache & Performance Layer"
            RedisCache[üî¥ Redis Cluster<br/>Query + Session Cache]
            RedisSearch[üîç Redis Search<br/>Full-text Search]
        end
        
        subgraph "üìä Monitoring & Operations"
            Prometheus[üìà Prometheus<br/>Metrics Collection]
            Grafana[üìä Grafana<br/>Dashboards]
            AlertManager[üö® AlertManager<br/>Notifications]
        end
    end
    
    subgraph "üíæ Storage & Backup"
        VectorStorage[üìÅ Vector Storage<br/>Persistent Volumes]
        SQLStorage[üìÅ SQL Storage<br/>Persistent Volumes]
        BackupStorage[üíΩ Backup Storage<br/>Automated Backups]
    end
    
    API --> ChromaDB
    API --> PostgreSQL
    RAG --> ChromaDB
    RAG --> PostgreSQL
    UI --> API
    
    ChromaDB --> VectorIndex
    ChromaDB --> Collections
    PostgreSQL --> ConnPool
    PostgreSQL --> Schemas
    
    API -.-> RedisCache
    API -.-> RedisSearch
    
    Prometheus --> PostgreSQL
    Prometheus --> ChromaDB
    Prometheus --> RedisCache
    Grafana --> Prometheus
    AlertManager --> Prometheus
    
    ChromaDB --> VectorStorage
    PostgreSQL --> SQLStorage
    VectorStorage --> BackupStorage
    SQLStorage --> BackupStorage
    
    classDef app fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef vector fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef sql fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef cache fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef monitor fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    classDef storage fill:#e0f2f1,stroke:#00796b,stroke-width:2px
    
    class API,RAG,UI app
    class ChromaDB,VectorIndex,Collections vector
    class PostgreSQL,ConnPool,Schemas sql
    class RedisCache,RedisSearch cache
    class Prometheus,Grafana,AlertManager monitor
    class VectorStorage,SQLStorage,BackupStorage storage
```

---

## üõ†Ô∏è **CHU·∫®N B·ªä M√îI TR∆Ø·ªúNG PH√ÅT TRI·ªÇN**

### **1. Y√™u c·∫ßu h·∫° t·∫ßng**

#### **üíª Ph·∫ßn c·ª©ng t·ªëi thi·ªÉu**
- **CPU**: 8 cores (Intel i7 ho·∫∑c AMD Ryzen 7)
- **RAM**: 32GB (khuy·∫øn ngh·ªã 64GB)
- **Storage**: 
  - SSD 500GB cho OS v√† development tools
  - SSD 1TB cho databases v√† data storage
  - HDD 2TB cho backup (optional)
- **Network**: Gigabit Ethernet
- **GPU**: Optional (NVIDIA v·ªõi CUDA support cho accelerated embeddings)

#### **üñ•Ô∏è H·ªá ƒëi·ªÅu h√†nh**
- **Primary**: Ubuntu 22.04 LTS / CentOS 8
- **Alternative**: macOS 12+ / Windows 11 v·ªõi WSL2

### **2. Ph·∫ßn m·ªÅm c·∫ßn thi·∫øt**

#### **üê≥ Container & Orchestration**
```bash
# Docker & Docker Compose Latest
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
sudo curl -L "https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose
```

#### **üêç Python Development Stack**
```bash
# Python 3.10.11+ v·ªõi virtual environment
sudo apt-get update
sudo apt-get install python3.10 python3.10-venv python3.10-dev build-essential
python3.10 -m venv venv-fr02-v2
source venv-fr02-v2/bin/activate

# Vietnamese NLP Dependencies
sudo apt-get install libicu-dev
pip install pyvi underthesea
```

#### **üóÑÔ∏è Database Tools**
```bash
# PostgreSQL Client Tools
sudo apt-get install postgresql-client-15

# GUI Tools (choose one)
# - pgAdmin 4 (Web-based)
# - DBeaver Community (Desktop)
# - TablePlus (macOS/Windows)
```

#### **üìä Monitoring & Development Tools**
```bash
# Essential development tools
sudo apt-get install git curl wget htop tree jq unzip

# Vietnamese text processing tools
pip install underthesea pyvi sentence-transformers

# Monitoring tools
sudo apt-get install prometheus-node-exporter
```

---

## üóóÔ∏è **KI·∫æN TR√öC CHI TI·∫æT**

### **1. Enhanced PostgreSQL Database Design**

#### **üêò PostgreSQL 15 v·ªõi Enhanced Schema**
```sql
-- Core Enhanced Schema (based on updated schema_01_init_database.sql)

-- Enable required extensions
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pg_trgm";
CREATE EXTENSION IF NOT EXISTS "btree_gin";
CREATE EXTENSION IF NOT EXISTS "unaccent";

-- Enhanced enum types
CREATE TYPE access_level_enum AS ENUM (
    'public', 'employee_only', 'manager_only', 'director_only', 'system_admin'
);

CREATE TYPE document_type_enum AS ENUM (
    'policy', 'procedure', 'technical_guide', 'report', 
    'manual', 'specification', 'template', 'form', 
    'presentation', 'training_material', 'other'
);

CREATE TYPE userlevel AS ENUM (
    'GUEST', 'EMPLOYEE', 'MANAGER', 'DIRECTOR', 'SYSTEM_ADMIN'
);

CREATE TYPE chunking_method_enum AS ENUM (
    'fixed_size', 'sentence_based', 'semantic_boundary', 'paragraph_based', 'hybrid'
);

-- Enhanced Users table v·ªõi Vietnamese support
CREATE TABLE users (
    user_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    username VARCHAR(50) UNIQUE NOT NULL,
    email VARCHAR(100) UNIQUE NOT NULL,
    full_name VARCHAR(255) NOT NULL,
    password_hash VARCHAR(255) NOT NULL,
    salt VARCHAR(32) NOT NULL,
    user_level userlevel NOT NULL DEFAULT 'EMPLOYEE',
    department VARCHAR(100),
    position VARCHAR(100),
    status VARCHAR(20) NOT NULL DEFAULT 'ACTIVE',
    is_active BOOLEAN NOT NULL DEFAULT true,
    email_verified BOOLEAN NOT NULL DEFAULT false,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    last_login TIMESTAMP WITH TIME ZONE,
    preferences JSONB DEFAULT '{}',
    failed_login_attempts INTEGER DEFAULT 0,
    locked_until TIMESTAMP WITH TIME ZONE
);

-- Enhanced Documents Metadata v·ªõi Vietnamese optimization
CREATE TABLE documents_metadata_v2 (
    document_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    
    -- Basic information
    title VARCHAR(500) NOT NULL,
    content TEXT,
    document_type document_type_enum NOT NULL,
    access_level access_level_enum NOT NULL DEFAULT 'employee_only',
    department_owner VARCHAR(100) NOT NULL,
    author VARCHAR(255) NOT NULL,
    status VARCHAR(20) DEFAULT 'draft',
    
    -- Vietnamese language support
    language_detected VARCHAR(10) DEFAULT 'vi',
    vietnamese_segmented BOOLEAN DEFAULT false,
    diacritics_normalized BOOLEAN DEFAULT false,
    tone_marks_preserved BOOLEAN DEFAULT true,
    
    -- Enhanced search fields
    search_text_normalized TEXT,
    indexable_content TEXT,
    extracted_emails TEXT[] DEFAULT '{}',
    extracted_phones TEXT[] DEFAULT '{}',
    extracted_dates DATE[] DEFAULT '{}',
    
    -- Embedding v√† chunking metadata
    embedding_model_primary VARCHAR(100) DEFAULT 'Qwen/Qwen3-Embedding-0.6B',
    chunk_count INTEGER DEFAULT 0,
    file_size_bytes BIGINT,
    
    -- Search support
    search_tokens TSVECTOR,
    keyword_density JSONB DEFAULT '{}',
    
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Enhanced Document Chunks v·ªõi advanced chunking support
CREATE TABLE document_chunks_enhanced (
    chunk_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    document_id UUID NOT NULL REFERENCES documents_metadata_v2(document_id) ON DELETE CASCADE,
    
    -- Content data
    chunk_content TEXT NOT NULL,
    chunk_position INTEGER NOT NULL,
    chunk_size_tokens INTEGER,
    
    -- Advanced semantic chunking
    semantic_boundary BOOLEAN DEFAULT false,
    overlap_with_prev INTEGER DEFAULT 0,
    overlap_with_next INTEGER DEFAULT 0,
    overlap_source_prev INTEGER,
    overlap_source_next INTEGER,
    is_final_part BOOLEAN DEFAULT false,
    heading_context TEXT,
    
    -- Chunking method v√† quality
    chunk_method chunking_method_enum DEFAULT 'semantic_boundary',
    chunk_quality_score DECIMAL(3,2) CHECK (chunk_quality_score BETWEEN 0.00 AND 1.00),
    
    -- Vector storage references
    embedding_model VARCHAR(100) DEFAULT 'Qwen/Qwen3-Embedding-0.6B',
    embedding_dimensions INTEGER DEFAULT 1024,
    
    -- Full-text search support
    bm25_tokens TSVECTOR,
    
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Vietnamese Text Analysis table
CREATE TABLE vietnamese_text_analysis (
    analysis_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    document_id UUID REFERENCES documents_metadata_v2(document_id) ON DELETE CASCADE,
    chunk_id UUID REFERENCES document_chunks_enhanced(chunk_id) ON DELETE CASCADE,
    
    -- Text data
    original_text TEXT NOT NULL,
    processed_text TEXT,
    
    -- Analysis results
    word_segmentation JSONB DEFAULT '{}',
    pos_tagging JSONB DEFAULT '{}',
    
    -- Vietnamese features
    compound_words TEXT[] DEFAULT '{}',
    technical_terms TEXT[] DEFAULT '{}',
    proper_nouns TEXT[] DEFAULT '{}',
    
    -- Quality metrics
    readability_score DECIMAL(3,2),
    formality_level VARCHAR(20),
    language_quality_score DECIMAL(4,1),
    diacritics_density DECIMAL(4,3),
    token_diversity DECIMAL(4,3),
    
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Data Ingestion Jobs tracking (FR03.3 integration)
CREATE TABLE data_ingestion_jobs (
    job_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_id UUID REFERENCES users(user_id) ON DELETE SET NULL,
    
    -- Job configuration
    job_name VARCHAR(255) NOT NULL,
    job_type VARCHAR(50) NOT NULL DEFAULT 'document_processing',
    source_path VARCHAR(1000),
    target_collection VARCHAR(100) DEFAULT 'default_collection',
    
    -- Processing parameters
    chunking_method chunking_method_enum DEFAULT 'semantic_boundary',
    chunk_size_tokens INTEGER DEFAULT 512,
    overlap_tokens INTEGER DEFAULT 50,
    embedding_model VARCHAR(100) DEFAULT 'Qwen/Qwen3-Embedding-0.6B',
    
    -- Job status
    status VARCHAR(50) NOT NULL DEFAULT 'pending',
    progress_percentage DECIMAL(5,2) DEFAULT 0.00,
    documents_processed INTEGER DEFAULT 0,
    chunks_created INTEGER DEFAULT 0,
    
    -- Performance metrics
    started_at TIMESTAMP WITH TIME ZONE,
    completed_at TIMESTAMP WITH TIME ZONE,
    processing_time_seconds DOUBLE PRECISION,
    
    -- Results
    success_count INTEGER DEFAULT 0,
    error_count INTEGER DEFAULT 0,
    error_log TEXT[],
    
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);
```

### **2. Vector Database Design**

#### **üì¢ ChromaDB v·ªõi Qwen Embeddings**
```python
# Enhanced ChromaDB Configuration
CHROMA_CONFIG = {
    "embedding_model": "Qwen/Qwen3-Embedding-0.6B",
    "embedding_dimension": 1024,
    "distance_metric": "cosine",
    "index_type": "HNSW",
    "max_elements": 500000,  # Increased capacity
    "ef_construction": 200,
    "M": 16,
    "vietnamese_optimization": True
}

# Collection Schema Design
COLLECTIONS = {
    "knowledge_base_v2": {
        "description": "Main knowledge base with Vietnamese optimization",
        "embedding_dimension": 1024,
        "metadata_schema": {
            "document_id": "string",
            "chunk_id": "string", 
            "access_level": "string",
            "department": "string",
            "document_type": "string",
            "language": "string",
            "quality_score": "float",
            "vietnamese_processed": "boolean"
        }
    },
    "dept_hr_v2": {"parent": "knowledge_base_v2", "department": "HR"},
    "dept_it_v2": {"parent": "knowledge_base_v2", "department": "IT"},
    "dept_rd_v2": {"parent": "knowledge_base_v2", "department": "R&D"}
}
```

### **3. Enhanced Cache Layer**

#### **üî¥ Redis Cluster Configuration**
```yaml
# Redis Cluster Setup
redis-cluster:
  nodes:
    - redis-node-1:7000
    - redis-node-2:7001  
    - redis-node-3:7002
  configuration:
    maxmemory: 4gb
    maxmemory-policy: allkeys-lru
    save: "900 1 300 10 60 10000"
    appendonly: yes
    appendfsync: everysec
  
redis-modules:
  - name: RedisSearch
    enabled: true
    purpose: Full-text search for Vietnamese
  - name: RedisJSON
    enabled: true
    purpose: JSON document storage
  - name: RedisTimeSeries
    enabled: true
    purpose: Performance metrics
```

---

## üê≥ **DOCKER COMPOSE CONFIGURATION**

### **docker-compose.yml ho√†n ch·ªânh**
```yaml
version: '3.8'

services:
  # PostgreSQL Database
  postgres:
    image: postgres:15-alpine
    container_name: fr02-postgres-v2
    environment:
      POSTGRES_DB: knowledge_base_v2
      POSTGRES_USER: kb_admin
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_INITDB_ARGS: "--encoding=UTF8 --locale=C.UTF-8"
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./config/postgres.conf:/etc/postgresql/postgresql.conf
      - ./scripts/init-postgres.sql:/docker-entrypoint-initdb.d/01_init.sql
      - ./scripts/enhanced-schema.sql:/docker-entrypoint-initdb.d/02_schema.sql
    command: postgres -c config_file=/etc/postgresql/postgresql.conf
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U kb_admin -d knowledge_base_v2"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    networks:
      - fr02-network

  # PgBouncer Connection Pool
  pgbouncer:
    image: pgbouncer/pgbouncer:latest
    container_name: fr02-pgbouncer
    environment:
      DATABASES_HOST: postgres
      DATABASES_PORT: 5432
      DATABASES_USER: kb_admin
      DATABASES_PASSWORD: ${POSTGRES_PASSWORD}
      DATABASES_DBNAME: knowledge_base_v2
      POOL_MODE: transaction
      SERVER_RESET_QUERY: DISCARD ALL
      MAX_CLIENT_CONN: 1000
      DEFAULT_POOL_SIZE: 25
      MIN_POOL_SIZE: 5
      RESERVE_POOL_SIZE: 3
      MAX_DB_CONNECTIONS: 100
    ports:
      - "6432:5432"
    depends_on:
      postgres:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - fr02-network

  # ChromaDB Vector Database , latest, 1.0.0, tuannq
  chroma:
    image: chromadb/chroma: 1.0.0
    container_name: fr02-chroma-v2
    ports:
      - "8000:8000"
    volumes:
      - chroma_data:/chroma/chroma
      - ./config/chroma-config.yaml:/chroma/config.yaml
    environment:
      - CHROMA_SERVER_HOST=0.0.0.0
      - CHROMA_SERVER_PORT=8000
      - CHROMA_SERVER_CORS_ALLOW_ORIGINS=*
      - PERSIST_DIRECTORY=/chroma/chroma
      - CHROMA_SERVER_AUTH_CREDENTIALS_PROVIDER=chromadb.auth.token.TokenAuthCredentialsProvider
      - CHROMA_SERVER_AUTH_CREDENTIALS=${CHROMA_AUTH_TOKEN}
      - CHROMA_SERVER_AUTH_TOKEN_TRANSPORT_HEADER=X-Chroma-Token
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/api/v1/heartbeat || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    networks:
      - fr02-network

  # Redis Cache Cluster
  redis-master:
    image: redis:7-alpine
    container_name: fr02-redis-master
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
      - ./config/redis-master.conf:/usr/local/etc/redis/redis.conf
    command: redis-server /usr/local/etc/redis/redis.conf
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "redis-cli ping | grep PONG"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - fr02-network

  redis-replica:
    image: redis:7-alpine
    container_name: fr02-redis-replica
    ports:
      - "6380:6379"
    volumes:
      - redis_replica_data:/data
      - ./config/redis-replica.conf:/usr/local/etc/redis/redis.conf
    command: redis-server /usr/local/etc/redis/redis.conf
    depends_on:
      - redis-master
    restart: unless-stopped
    networks:
      - fr02-network

  # Prometheus Monitoring
  prometheus:
    image: prom/prometheus:latest
    container_name: fr02-prometheus
    ports:
      - "9090:9090"
    volumes:
      - prometheus_data:/prometheus
      - ./config/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./config/alert_rules.yml:/etc/prometheus/alert_rules.yml
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
    restart: unless-stopped
    networks:
      - fr02-network

  # Grafana Dashboards
  grafana:
    image: grafana/grafana:latest
    container_name: fr02-grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_INSTALL_PLUGINS=redis-datasource,postgres-datasource
    volumes:
      - grafana_data:/var/lib/grafana
      - ./config/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./config/grafana/datasources:/etc/grafana/provisioning/datasources
    depends_on:
      - prometheus
    restart: unless-stopped
    networks:
      - fr02-network

  # PostgreSQL Exporter
  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:latest
    container_name: fr02-postgres-exporter
    environment:
      DATA_SOURCE_NAME: "postgresql://kb_admin:${POSTGRES_PASSWORD}@postgres:5432/knowledge_base_v2?sslmode=disable"
    ports:
      - "9187:9187"
    depends_on:
      postgres:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - fr02-network

  # Redis Exporter
  redis-exporter:
    image: oliver006/redis_exporter:latest
    container_name: fr02-redis-exporter
    environment:
      REDIS_ADDR: "redis://redis-master:6379"
    ports:
      - "9121:9121"
    depends_on:
      - redis-master
    restart: unless-stopped
    networks:
      - fr02-network

  # Node Exporter
  node-exporter:
    image: prom/node-exporter:latest
    container_name: fr02-node-exporter
    ports:
      - "9100:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    restart: unless-stopped
    networks:
      - fr02-network

  # NGINX Load Balancer
  nginx:
    image: nginx:alpine
    container_name: fr02-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./config/nginx.conf:/etc/nginx/nginx.conf
      - ./config/ssl:/etc/nginx/ssl
    depends_on:
      - chroma
      - postgres
    restart: unless-stopped
    networks:
      - fr02-network

volumes:
  postgres_data:
    driver: local
  chroma_data:
    driver: local
  redis_data:
    driver: local
  redis_replica_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local

networks:
  fr02-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16
```

---

## ‚öôÔ∏è **C·∫§U H√åNH CHI TI·∫æT**

### **1. PostgreSQL Configuration**

#### **config/postgres.conf**
```conf
# Memory Settings
shared_buffers = 2GB
effective_cache_size = 6GB
maintenance_work_mem = 512MB
work_mem = 16MB

# Connection Settings
max_connections = 200
superuser_reserved_connections = 3

# WAL Settings
wal_level = replica
max_wal_size = 2GB
min_wal_size = 512MB
checkpoint_completion_target = 0.9

# Logging
log_destination = 'stderr'
logging_collector = on
log_directory = 'log'
log_filename = 'postgresql-%Y-%m-%d_%H%M%S.log'
log_statement = 'mod'
log_min_duration_statement = 1000

# Vietnamese locale support
lc_messages = 'en_US.UTF-8'
lc_monetary = 'en_US.UTF-8'
lc_numeric = 'en_US.UTF-8'
lc_time = 'en_US.UTF-8'
default_text_search_config = 'pg_catalog.simple'

# Performance tuning
random_page_cost = 1.1
effective_io_concurrency = 200
max_worker_processes = 8
max_parallel_workers_per_gather = 4
max_parallel_workers = 8
```

### **2. Redis Configuration**

#### **config/redis-master.conf**
```conf
# Basic Configuration
port 6379
bind 0.0.0.0
protected-mode no

# Memory Management
maxmemory 4gb
maxmemory-policy allkeys-lru
maxmemory-samples 10

# Persistence
save 900 1
save 300 10
save 60 10000
stop-writes-on-bgsave-error yes
rdbcompression yes
rdbchecksum yes
dbfilename dump.rdb

# Append Only File
appendonly yes
appendfilename "appendonly.aof"
appendfsync everysec
no-appendfsync-on-rewrite no
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb

# Networking
tcp-keepalive 300
timeout 0

# Logging
loglevel notice
logfile ""

# Vietnamese text search optimization
hash-max-ziplist-entries 512
hash-max-ziplist-value 64
list-max-ziplist-size -2
set-max-intset-entries 512
zset-max-ziplist-entries 128
zset-max-ziplist-value 64

# Modules
loadmodule /usr/lib/redis/modules/redisearch.so
loadmodule /usr/lib/redis/modules/rejson.so
```

### **3. Monitoring Configuration**

#### **config/prometheus.yml**
```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s
  external_labels:
    cluster: 'fr02-production'
    project: 'vietnamese-chatbot'

rule_files:
  - "alert_rules.yml"

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres-exporter:9187']
    scrape_interval: 30s
    metrics_path: /metrics

  - job_name: 'redis'
    static_configs:
      - targets: ['redis-exporter:9121']
    scrape_interval: 30s

  - job_name: 'chroma'
    static_configs:
      - targets: ['chroma:8000']
    metrics_path: '/api/v1/metrics'
    scrape_interval: 30s

  - job_name: 'node'
    static_configs:
      - targets: ['node-exporter:9100']
    scrape_interval: 30s

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

# Recording rules for performance optimization
rule_files:
  - "recording_rules.yml"
```

---

## üîß **SCRIPT TRI·ªÇN KHAI T·ª∞ ƒê·ªòNG**

### **scripts/deploy-fr02-v2.sh**
```bash
#!/bin/bash

# FR-02.1 v2.0 Deployment Script
# Enhanced Dual Database System with Vietnamese Support

set -euo pipefail

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Configuration
PROJECT_NAME="fr02-dual-database-v2"
DEPLOY_DIR="/opt/${PROJECT_NAME}"
BACKUP_DIR="/opt/backups"
LOG_FILE="/var/log/fr02-deployment.log"

# Environment variables
export POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-$(openssl rand -base64 32)}
export GRAFANA_PASSWORD=${GRAFANA_PASSWORD:-$(openssl rand -base64 16)}
export CHROMA_AUTH_TOKEN=${CHROMA_AUTH_TOKEN:-$(openssl rand -hex 32)}

# Logging function
log() {
    echo -e "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a "$LOG_FILE"
}

log_info() {
    log "${BLUE}[INFO]${NC} $1"
}

log_success() {
    log "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    log "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    log "${RED}[ERROR]${NC} $1"
}

# Prerequisites check
check_prerequisites() {
    log_info "Checking prerequisites..."
    
    # Check Docker
    if ! command -v docker &> /dev/null; then
        log_error "Docker is not installed"
        exit 1
    fi
    
    # Check Docker Compose
    if ! command -v docker-compose &> /dev/null; then
        log_error "Docker Compose is not installed"
        exit 1
    fi
    
    # Check available memory
    AVAILABLE_MEMORY=$(free -g | awk '/^Mem:/{print $7}')
    if [ "$AVAILABLE_MEMORY" -lt 8 ]; then
        log_warning "Available memory is ${AVAILABLE_MEMORY}GB. Recommended: 8GB+"
    fi
    
    # Check disk space
    AVAILABLE_DISK=$(df -BG / | awk 'NR==2{print $4}' | sed 's/G//')
    if [ "$AVAILABLE_DISK" -lt 50 ]; then
        log_error "Available disk space is ${AVAILABLE_DISK}GB. Required: 50GB+"
        exit 1
    fi
    
    log_success "Prerequisites check passed"
}

# Create project structure
create_project_structure() {
    log_info "Creating project structure..."
    
    mkdir -p "$DEPLOY_DIR"/{config,scripts,data,logs,backups}
    mkdir -p "$DEPLOY_DIR"/data/{postgres,chroma,redis,prometheus,grafana}
    mkdir -p "$DEPLOY_DIR"/config/{grafana,prometheus,nginx,ssl}
    
    # Set proper permissions
    chmod 755 "$DEPLOY_DIR"
    chmod 700 "$DEPLOY_DIR"/data
    
    log_success "Project structure created"
}

# Setup environment file
setup_environment() {
    log_info "Setting up environment variables..."
    
    cat > "$DEPLOY_DIR/.env" << EOF
# FR-02.1 v2.0 Environment Configuration
PROJECT_NAME=${PROJECT_NAME}
ENVIRONMENT=production

# Database Configuration
POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
POSTGRES_USER=kb_admin
POSTGRES_DB=knowledge_base_v2
POSTGRES_HOST=postgres
POSTGRES_PORT=5432

# Redis Configuration
REDIS_PASSWORD=${REDIS_PASSWORD:-}
REDIS_HOST=redis-master
REDIS_PORT=6379

# ChromaDB Configuration
CHROMA_AUTH_TOKEN=${CHROMA_AUTH_TOKEN}
CHROMA_HOST=chroma
CHROMA_PORT=8000

# Monitoring Configuration
GRAFANA_PASSWORD=${GRAFANA_PASSWORD}
PROMETHEUS_RETENTION=30d

# Security Configuration
JWT_SECRET=${JWT_SECRET:-$(openssl rand -hex 32)}
ENCRYPTION_KEY=${ENCRYPTION_KEY:-$(openssl rand -hex 32)}

# Vietnamese NLP Configuration
VIETNAMESE_NLP_MODEL=pyvi
EMBEDDING_MODEL=Qwen/Qwen3-Embedding-0.6B
EMBEDDING_DIMENSION=1024

# Performance Configuration
MAX_CONNECTIONS=200
WORKER_PROCESSES=4
CACHE_TTL=3600
EOF

    chmod 600 "$DEPLOY_DIR/.env"
    log_success "Environment file created"
}

# Deploy database schema
deploy_database_schema() {
    log_info "Deploying enhanced database schema..."
    
    # Wait for PostgreSQL to be ready
    log_info "Waiting for PostgreSQL to be ready..."
    timeout 120 bash -c 'until docker-compose exec postgres pg_isready -U kb_admin -d knowledge_base_v2; do sleep 5; done'
    
    # Run schema migrations
    docker-compose exec postgres psql -U kb_admin -d knowledge_base_v2 -f /docker-entrypoint-initdb.d/02_schema.sql
    
    # Verify schema deployment
    TABLES_COUNT=$(docker-compose exec postgres psql -U kb_admin -d knowledge_base_v2 -t -c "SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = 'public';")
    
    if [ "$TABLES_COUNT" -ge 15 ]; then
        log_success "Database schema deployed successfully ($TABLES_COUNT tables created)"
    else
        log_error "Database schema deployment failed (only $TABLES_COUNT tables found)"
        exit 1
    fi
}

# Setup vector database
setup_vector_database() {
    log_info "Setting up ChromaDB vector database..."
    
    # Wait for ChromaDB to be ready
    timeout 120 bash -c 'until curl -f http://localhost:8000/api/v1/heartbeat; do sleep 5; done'
    
    # Create collections using Python script
    python3 << EOF
import chromadb
from chromadb.config import Settings
import uuid

try:
    client = chromadb.HttpClient(
        host="localhost",
        port=8000,
        settings=Settings(
            chroma_server_host="localhost",
            chroma_server_port=8000,
            chroma_server_cors_allow_origins="*",
            chroma_server_auth_credentials_provider="chromadb.auth.token.TokenAuthCredentialsProvider",
            chroma_server_auth_credentials="${CHROMA_AUTH_TOKEN}",
            chroma_server_auth_token_transport_header="X-Chroma-Token"
        )
    )
    
    # Create main collection
    collection = client.create_collection(
        name="knowledge_base_v2",
        metadata={
            "description": "Main knowledge base with Vietnamese optimization",
            "embedding_dimension": 1024,
            "embedding_model": "Qwen/Qwen3-Embedding-0.6B",
            "language": "vietnamese",
            "version": "2.0"
        }
    )
    
    # Create department collections
    departments = ['hr', 'it', 'rd', 'sales', 'finance']
    for dept in departments:
        dept_collection = client.create_collection(
            name=f"dept_{dept}_v2",
            metadata={
                "description": f"{dept.upper()} department knowledge base",
                "department": dept,
                "parent_collection": "knowledge_base_v2",
                "embedding_dimension": 1024
            }
        )
    
    print("‚úÖ ChromaDB collections created successfully")
    
except Exception as e:
    print(f"‚ùå Error creating ChromaDB collections: {e}")
    exit(1)
EOF

    log_success "Vector database setup completed"
}

# Setup monitoring
setup_monitoring() {
    log_info "Setting up monitoring stack..."
    
    # Create Grafana datasources configuration
    cat > "$DEPLOY_DIR/config/grafana/datasources/datasources.yml" << EOF
apiVersion: 1

datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
    editable: true
  
  - name: PostgreSQL
    type: postgres
    url: postgres:5432
    database: knowledge_base_v2
    user: kb_admin
    secureJsonData:
      password: '${POSTGRES_PASSWORD}'
    jsonData:
      sslmode: disable
      postgresVersion: 1500
      timescaledb: false
EOF

    # Create alert rules
    cat > "$DEPLOY_DIR/config/prometheus/alert_rules.yml" << EOF
groups:
  - name: fr02_alerts
    rules:
      - alert: PostgreSQLDown
        expr: pg_up == 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "PostgreSQL instance is down"
          description: "PostgreSQL instance {{ \$labels.instance }} has been down for more than 5 minutes."

      - alert: ChromaDBDown
        expr: up{job="chroma"} == 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "ChromaDB instance is down"
          description: "ChromaDB instance has been down for more than 5 minutes."

      - alert: HighDatabaseConnections
        expr: pg_stat_activity_count > 150
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High database connections"
          description: "PostgreSQL has {{ \$value }} active connections (>150)."

      - alert: RedisMemoryHigh
        expr: redis_memory_used_bytes / redis_memory_max_bytes > 0.8
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Redis memory usage high"
          description: "Redis memory usage is above 80%."
EOF

    log_success "Monitoring setup completed"
}

# Run health checks
run_health_checks() {
    log_info "Running comprehensive health checks..."
    
    # Check PostgreSQL
    if docker-compose exec postgres pg_isready -U kb_admin -d knowledge_base_v2 > /dev/null 2>&1; then
        log_success "‚úÖ PostgreSQL is healthy"
    else
        log_error "‚ùå PostgreSQL health check failed"
        return 1
    fi
    
    # Check ChromaDB
    if curl -f http://localhost:8000/api/v1/heartbeat > /dev/null 2>&1; then
        log_success "‚úÖ ChromaDB is healthy"
    else
        log_error "‚ùå ChromaDB health check failed"
        return 1
    fi
    
    # Check Redis
    if docker-compose exec redis-master redis-cli ping | grep PONG > /dev/null 2>&1; then
        log_success "‚úÖ Redis is healthy"
    else
        log_error "‚ùå Redis health check failed"
        return 1
    fi
    
    # Check Prometheus
    if curl -f http://localhost:9090/-/healthy > /dev/null 2>&1; then
        log_success "‚úÖ Prometheus is healthy"
    else
        log_error "‚ùå Prometheus health check failed"
        return 1
    fi
    
    # Check Grafana
    if curl -f http://localhost:3000/api/health > /dev/null 2>&1; then
        log_success "‚úÖ Grafana is healthy"
    else
        log_error "‚ùå Grafana health check failed"
        return 1
    fi
    
    log_success "All health checks passed!"
}

# Performance testing
run_performance_tests() {
    log_info "Running performance tests..."
    
    python3 << EOF
import asyncio
import asyncpg
import chromadb
import time
import numpy as np
from concurrent.futures import ThreadPoolExecutor
import statistics

async def test_database_performance():
    # PostgreSQL connection test
    conn = await asyncpg.connect(
        "postgresql://kb_admin:${POSTGRES_PASSWORD}@localhost:5432/knowledge_base_v2"
    )
    
    # Test query performance
    start_time = time.time()
    result = await conn.fetchval("SELECT COUNT(*) FROM documents_metadata_v2")
    query_time = time.time() - start_time
    
    print(f"PostgreSQL query time: {query_time:.3f}s")
    
    await conn.close()
    
    # ChromaDB performance test
    client = chromadb.HttpClient(host="localhost", port=8000)
    
    try:
        collection = client.get_collection("knowledge_base_v2")
        
        # Test embedding search
        query_embedding = np.random.rand(1024).tolist()
        start_time = time.time()
        results = collection.query(
            query_embeddings=[query_embedding],
            n_results=5
        )
        search_time = time.time() - start_time
        
        print(f"ChromaDB search time: {search_time:.3f}s")
        
        if query_time < 0.1 and search_time < 0.5:
            print("‚úÖ Performance tests passed")
            return True
        else:
            print("‚ùå Performance tests failed")
            return False
            
    except Exception as e:
        print(f"‚ùå ChromaDB test failed: {e}")
        return False

# Run the test
if asyncio.run(test_database_performance()):
    exit(0)
else:
    exit(1)
EOF

    if [ $? -eq 0 ]; then
        log_success "Performance tests passed"
    else
        log_error "Performance tests failed"
        return 1
    fi
}

# Setup backup strategy
setup_backup_strategy() {
    log_info "Setting up backup strategy..."
    
    # Create backup script
    cat > "$DEPLOY_DIR/scripts/backup.sh" << 'EOF'
#!/bin/bash

BACKUP_DIR="/opt/backups"
DATE=$(date +%Y%m%d_%H%M%S)

# PostgreSQL backup
pg_dump -h localhost -U kb_admin -d knowledge_base_v2 \
    | gzip > "$BACKUP_DIR/postgres_backup_$DATE.sql.gz"

# ChromaDB backup
docker-compose exec chroma /bin/sh -c "tar czf - /chroma/chroma" \
    > "$BACKUP_DIR/chroma_backup_$DATE.tar.gz"

# Redis backup
docker-compose exec redis-master redis-cli --rdb - \
    > "$BACKUP_DIR/redis_backup_$DATE.rdb"

# Cleanup old backups (keep last 7 days)
find "$BACKUP_DIR" -name "*backup*" -mtime +7 -delete

echo "Backup completed: $DATE"
EOF

    chmod +x "$DEPLOY_DIR/scripts/backup.sh"
    
    # Setup cron job for daily backups
    (crontab -l 2>/dev/null; echo "0 2 * * * $DEPLOY_DIR/scripts/backup.sh") | crontab -
    
    log_success "Backup strategy configured"
}

# Main deployment function
main() {
    log_info "Starting FR-02.1 v2.0 deployment..."
    
    # Pre-deployment
    check_prerequisites
    create_project_structure
    setup_environment
    
    # Change to deployment directory
    cd "$DEPLOY_DIR"
    
    # Copy configuration files (assuming they exist in current directory)
    if [ -f "docker-compose.yml" ]; then
        cp docker-compose.yml "$DEPLOY_DIR/"
    else
        log_error "docker-compose.yml not found"
        exit 1
    fi
    
    # Deploy services
    log_info "Starting Docker services..."
    docker-compose up -d
    
    # Wait for services to be ready
    sleep 30
    
    # Post-deployment setup
    deploy_database_schema
    setup_vector_database
    setup_monitoring
    setup_backup_strategy
    
    # Final checks
    run_health_checks
    run_performance_tests
    
    # Display access information
    log_success "üéâ FR-02.1 v2.0 deployment completed successfully!"
    echo ""
    echo "üìä Service Access URLs:"
    echo "  PostgreSQL: localhost:5432 (user: kb_admin)"
    echo "  PgBouncer: localhost:6432"
    echo "  ChromaDB: http://localhost:8000"
    echo "  Redis: localhost:6379"
    echo "  Prometheus: http://localhost:9090"
    echo "  Grafana: http://localhost:3000 (admin/${GRAFANA_PASSWORD})"
    echo ""
    echo "üìÅ Important Directories:"
    echo "  Project: $DEPLOY_DIR"
    echo "  Data: $DEPLOY_DIR/data"
    echo "  Logs: $DEPLOY_DIR/logs"
    echo "  Backups: $BACKUP_DIR"
    echo ""
    echo "üîê Generated Passwords saved in: $DEPLOY_DIR/.env"
    echo ""
    echo "üìñ Next Steps:"
    echo "  1. Access Grafana and import dashboards"
    echo "  2. Configure application connections"
    echo "  3. Load initial data"
    echo "  4. Run integration tests"
}

# Error handling
trap 'log_error "Deployment failed at line $LINENO"' ERR

# Run main function
main "$@"
```

### **scripts/enhanced-schema.sql**
```sql
-- FR-02.1 v2.0 Enhanced Database Schema
-- Compatible with Vietnamese chatbot system
-- Includes all updates from schema_01_init_database.sql

-- Enable required extensions
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pg_trgm";
CREATE EXTENSION IF NOT EXISTS "btree_gin";
CREATE EXTENSION IF NOT EXISTS "unaccent";

-- Create Vietnamese text processing functions
CREATE OR REPLACE FUNCTION normalize_vietnamese_text(input_text TEXT)
RETURNS TEXT AS $$
BEGIN
    -- Basic Vietnamese text normalization
    RETURN lower(
        regexp_replace(
            regexp_replace(
                regexp_replace(input_text, '[√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ]', 'a', 'gi'),
                '[√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ]', 'e', 'gi'
            ),
            '[√¨√≠·ªã·ªâƒ©]', 'i', 'gi'
        )
    );
END;
$$ LANGUAGE plpgsql IMMUTABLE;

-- Create function to extract contact information
CREATE OR REPLACE FUNCTION extract_emails_from_text(input_text TEXT)
RETURNS TEXT[] AS $$
DECLARE
    email_pattern TEXT := '[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}';
    result TEXT[];
BEGIN
    SELECT array_agg(DISTINCT match)
    INTO result
    FROM regexp_matches(input_text, email_pattern, 'gi') AS match;
    
    RETURN COALESCE(result, '{}');
END;
$$ LANGUAGE plpgsql IMMUTABLE;

-- Insert the complete enhanced schema here
\i /docker-entrypoint-initdb.d/01_init.sql

-- Create performance monitoring views
CREATE OR REPLACE VIEW vw_system_performance AS
SELECT 
    'postgresql' as service,
    COUNT(*) as total_documents,
    SUM(chunk_count) as total_chunks,
    AVG(EXTRACT(EPOCH FROM updated_at - created_at)) as avg_processing_time
FROM documents_metadata_v2
UNION ALL
SELECT 
    'vector_database' as service,
    COUNT(*) as total_documents,
    COUNT(*) as total_chunks,
    0 as avg_processing_time
FROM document_chunks_enhanced;

-- Create monitoring table for system metrics
CREATE TABLE IF NOT EXISTS system_health_metrics (
    metric_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    service_name VARCHAR(50) NOT NULL,
    metric_type VARCHAR(50) NOT NULL,
    metric_value DOUBLE PRECISION NOT NULL,
    timestamp TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    metadata JSONB DEFAULT '{}'
);

-- Insert initial system configuration
INSERT INTO system_health_metrics (service_name, metric_type, metric_value, metadata) VALUES
('postgresql', 'version', 15.0, '{"description": "PostgreSQL version"}'),
('chroma', 'embedding_dimension', 1024, '{"model": "Qwen/Qwen3-Embedding-0.6B"}'),
('redis', 'version', 7.0, '{"description": "Redis version"}');

-- Create indexes for performance
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_system_health_service_time 
ON system_health_metrics(service_name, timestamp DESC);

CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_documents_vietnamese_search 
ON documents_metadata_v2 USING GIN(to_tsvector('simple', search_text_normalized));

-- Success notification
DO $$ BEGIN
    RAISE NOTICE '====================================';
    RAISE NOTICE 'FR-02.1 v2.0 Enhanced Schema deployed successfully!';
    RAISE NOTICE '====================================';
    RAISE NOTICE 'New features in v2.0:';
    RAISE NOTICE '‚úì Vietnamese language optimization';
    RAISE NOTICE '‚úì Enhanced chunking support';
    RAISE NOTICE '‚úì Qwen embedding model support';
    RAISE NOTICE '‚úì Advanced monitoring capabilities';
    RAISE NOTICE '‚úì Performance optimization';
    RAISE NOTICE '‚úì Production-ready configuration';
    RAISE NOTICE '';
    RAISE NOTICE 'Database is ready for Vietnamese chatbot v2.0!';
    RAISE NOTICE '====================================';
END $$;
```

---

## üîç **TESTING & VALIDATION**

### **tests/test_dual_database_v2.py**
```python
import pytest
import asyncio
import asyncpg
import chromadb
import redis
import numpy as np
from typing import Dict, Any
import uuid
import time

@pytest.mark.asyncio
class TestDualDatabaseV2:
    
    @pytest.fixture
    async def postgres_connection(self):
        """Setup PostgreSQL connection for tests"""
        conn = await asyncpg.connect(
            "postgresql://kb_admin:password@localhost:5432/knowledge_base_v2"
        )
        yield conn
        await conn.close()
    
    @pytest.fixture
    def chroma_client(self):
        """Setup ChromaDB client for tests"""
        return chromadb.HttpClient(host="localhost", port=8000)
    
    @pytest.fixture
    def redis_client(self):
        """Setup Redis client for tests"""
        return redis.Redis(host="localhost", port=6379, decode_responses=True)
    
    async def test_postgresql_vietnamese_support(self, postgres_connection):
        """Test PostgreSQL Vietnamese language support"""
        test_text = "H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng h·ªá th·ªëng qu·∫£n l√Ω t√†i li·ªáu"
        
        # Test Vietnamese text normalization
        normalized = await postgres_connection.fetchval(
            "SELECT normalize_vietnamese_text($1)", test_text
        )
        
        assert "huong dan" in normalized
        assert "he thong" in normalized
        
        # Test document insertion with Vietnamese content
        doc_id = await postgres_connection.fetchval("""
            INSERT INTO documents_metadata_v2 
            (title, content, document_type, access_level, department_owner, author)
            VALUES ($1, $2, 'manual', 'employee_only', 'IT', 'Test Author')
            RETURNING document_id
        """, test_text, f"N·ªôi dung t√†i li·ªáu: {test_text}")
        
        assert doc_id is not None
        
        # Cleanup
        await postgres_connection.execute(
            "DELETE FROM documents_metadata_v2 WHERE document_id = $1", doc_id
        )
    
    def test_chroma_vietnamese_embeddings(self, chroma_client):
        """Test ChromaDB with Vietnamese content"""
        try:
            collection = chroma_client.get_collection("knowledge_base_v2")
        except:
            collection = chroma_client.create_collection(
                name="knowledge_base_v2",
                metadata={"embedding_dimension": 1024}
            )
        
        # Test Vietnamese document addition
        vietnamese_docs = [
            {
                'id': 'test_vi_1',
                'content': 'H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng h·ªá th·ªëng chatbot ti·∫øng Vi·ªát',
                'embedding': np.random.rand(1024).tolist(),
                'metadata': {'language': 'vi', 'type': 'guide'}
            },
            {
                'id': 'test_vi_2',
                'content': 'Ch√≠nh s√°ch l√†m vi·ªác t·ª´ xa c·ªßa c√¥ng ty',
                'embedding': np.random.rand(1024).tolist(),
                'metadata': {'language': 'vi', 'type': 'policy'}
            }
        ]
        
        collection.add(
            ids=[doc['id'] for doc in vietnamese_docs],
            embeddings=[doc['embedding'] for doc in vietnamese_docs],
            documents=[doc['content'] for doc in vietnamese_docs],
            metadatas=[doc['metadata'] for doc in vietnamese_docs]
        )
        
        # Test Vietnamese search
        query_embedding = np.random.rand(1024).tolist()
        results = collection.query(
            query_embeddings=[query_embedding],
            n_results=2,
            where={"language": "vi"}
        )
        
        assert len(results['ids'][0]) == 2
        assert all('vi' in doc for doc in results['documents'][0])
        
        # Cleanup
        collection.delete(ids=['test_vi_1', 'test_vi_2'])
    
    def test_redis_cache_performance(self, redis_client):
        """Test Redis caching performance"""
        # Test basic operations
        redis_client.set('test_key', 'test_value', ex=300)
        assert redis_client.get('test_key') == 'test_value'
        
        # Test Vietnamese text caching
        vietnamese_text = "H·ªá th·ªëng qu·∫£n l√Ω t√†i li·ªáu ti·∫øng Vi·ªát"
        redis_client.hset('vietnamese_cache', 'test_doc', vietnamese_text)
        
        cached_text = redis_client.hget('vietnamese_cache', 'test_doc')
        assert cached_text == vietnamese_text
        
        # Test performance
        start_time = time.time()
        for i in range(1000):
            redis_client.set(f'perf_test_{i}', f'value_{i}')
        operation_time = time.time() - start_time
        
        assert operation_time < 1.0  # Should complete in under 1 second
        
        # Cleanup
        redis_client.delete('test_key')
        redis_client.delete('vietnamese_cache')
        for i in range(1000):
            redis_client.delete(f'perf_test_{i}')
    
    async def test_integrated_search_workflow(self, postgres_connection, chroma_client):
        """Test complete search workflow integration"""
        # Create test user
        user_id = await postgres_connection.fetchval("""
            INSERT INTO users (username, email, password_hash, user_level, department)
            VALUES ('test_integration', 'test@integration.com', 'hash123', 'EMPLOYEE', 'IT')
            RETURNING user_id
        """)
        
        # Create test document
        doc_id = await postgres_connection.fetchval("""
            INSERT INTO documents_metadata_v2 
            (title, content, document_type, access_level, department_owner, author)
            VALUES ('Test Integration Doc', 'Content for integration testing', 
                    'technical_guide', 'employee_only', 'IT', 'Test Author')
            RETURNING document_id
        """)
        
        # Create test chunks
        chunk_ids = []
        for i in range(3):
            chunk_id = await postgres_connection.fetchval("""
                INSERT INTO document_chunks_enhanced 
                (document_id, chunk_content, chunk_position, chunk_size_tokens, 
                 semantic_boundary, chunk_method, embedding_model)
                VALUES ($1, $2, $3, 100, true, 'semantic_boundary', 'Qwen/Qwen3-Embedding-0.6B')
                RETURNING chunk_id
            """, doc_id, f'Test chunk content {i}', i)
            chunk_ids.append(chunk_id)
        
        # Add to ChromaDB
        try:
            collection = chroma_client.get_collection("knowledge_base_v2")
        except:
            collection = chroma_client.create_collection("knowledge_base_v2")
        
        embeddings = [np.random.rand(1024).tolist() for _ in range(3)]
        collection.add(
            ids=[f"chunk_{i}" for i in range(3)],
            embeddings=embeddings,
            documents=[f'Test chunk content {i}' for i in range(3)],
            metadatas=[{
                'document_id': str(doc_id),
                'chunk_id': str(chunk_ids[i]),
                'access_level': 'employee_only'
            } for i in range(3)]
        )
        
        # Test search
        query_embedding = np.random.rand(1024).tolist()
        search_results = collection.query(
            query_embeddings=[query_embedding],
            n_results=3,
            where={"access_level": "employee_only"}
        )
        
        assert len(search_results['ids'][0]) == 3
        
        # Verify database consistency
        db_chunks = await postgres_connection.fetch(
            "SELECT chunk_id FROM document_chunks_enhanced WHERE document_id = $1", doc_id
        )
        assert len(db_chunks) == 3
        
        # Cleanup
        collection.delete(ids=[f"chunk_{i}" for i in range(3)])
        await postgres_connection.execute("DELETE FROM users WHERE user_id = $1", user_id)
        await postgres_connection.execute("DELETE FROM documents_metadata_v2 WHERE document_id = $1", doc_id)
    
    async def test_performance_benchmarks(self, postgres_connection, chroma_client):
        """Test system performance benchmarks"""
        # Database query performance
        start_time = time.time()
        result = await postgres_connection.fetchval("SELECT COUNT(*) FROM documents_metadata_v2")
        db_query_time = time.time() - start_time
        
        assert db_query_time < 0.1  # Should be under 100ms
        
        # Vector search performance
        try:
            collection = chroma_client.get_collection("knowledge_base_v2")
            
            query_embedding = np.random.rand(1024).tolist()
            start_time = time.time()
            
            results = collection.query(
                query_embeddings=[query_embedding],
                n_results=5
            )
            
            vector_search_time = time.time() - start_time
            assert vector_search_time < 0.5  # Should be under 500ms
            
        except Exception as e:
            pytest.skip(f"ChromaDB collection not available: {e}")

if __name__ == "__main__":
    pytest.main([__file__, "-v", "--tb=short"])
```

---

## üìä **MONITORING & ALERTING**

### **Grafana Dashboard Configuration**
```json
{
  "dashboard": {
    "id": null,
    "title": "FR-02.1 v2.0 Dual Database System",
    "tags": ["fr-02.1", "vietnamese-chatbot", "databases"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "title": "System Overview",
        "type": "stat",
        "targets": [
          {
            "expr": "up{job=\"postgres\"} + up{job=\"chroma\"} + up{job=\"redis\"}",
            "refId": "A"
          }
        ],
        "gridPos": {"h": 6, "w": 8, "x": 0, "y": 0},
        "fieldConfig": {
          "defaults": {
            "color": {"mode": "thresholds"},
            "thresholds": {
              "steps": [
                {"color": "red", "value": 0},
                {"color": "yellow", "value": 2},
                {"color": "green", "value": 3}
              ]
            }
          }
        }
      },
      {
        "id": 2,
        "title": "PostgreSQL Connections",
        "type": "graph",
        "targets": [
          {
            "expr": "pg_stat_activity_count",
            "refId": "A"
          }
        ],
        "gridPos": {"h": 6, "w": 8, "x": 8, "y": 0}
      },
      {
        "id": 3,
        "title": "ChromaDB Response Time",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{job=\"chroma\"}[5m]))",
            "refId": "A"
          }
        ],
        "gridPos": {"h": 6, "w": 8, "x": 16, "y": 0}
      },
      {
        "id": 4,
        "title": "Redis Memory Usage",
        "type": "singlestat",
        "targets": [
          {
            "expr": "redis_memory_used_bytes / redis_memory_max_bytes * 100",
            "refId": "A"
          }
        ],
        "gridPos": {"h": 6, "w": 8, "x": 0, "y": 6}
      },
      {
        "id": 5,
        "title": "Database Query Performance",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(pg_stat_database_tup_fetched[5m])",
            "refId": "A",
            "legendFormat": "PostgreSQL Queries/sec"
          }
        ],
        "gridPos": {"h": 6, "w": 16, "x": 8, "y": 6}
      }
    ],
    "time": {"from": "now-1h", "to": "now"},
    "refresh": "10s"
  }
}
```

---

## üöÄ **PRODUCTION DEPLOYMENT GUIDE**

### **Quick Start Commands**
```bash
# 1. Clone and setup
git clone <repository-url> fr02-dual-database-v2
cd fr02-dual-database-v2

# 2. Set environment variables
export POSTGRES_PASSWORD=$(openssl rand -base64 32)
export GRAFANA_PASSWORD=$(openssl rand -base64 16)
export CHROMA_AUTH_TOKEN=$(openssl rand -hex 32)

# 3. Deploy with single command
chmod +x scripts/deploy-fr02-v2.sh
sudo ./scripts/deploy-fr02-v2.sh

# 4. Verify deployment
curl -f http://localhost:8000/api/v1/heartbeat  # ChromaDB
curl -f http://localhost:9090/-/healthy        # Prometheus
curl -f http://localhost:3000/api/health       # Grafana
```

### **Manual Deployment Steps**
```bash
# Start services
docker-compose up -d

# Wait for services
sleep 60

# Verify PostgreSQL
docker-compose exec postgres pg_isready -U kb_admin -d knowledge_base_v2

# Create ChromaDB collections
python3 scripts/setup_chroma_collections.py

# Import Grafana dashboards
curl -X POST http://admin:${GRAFANA_PASSWORD}@localhost:3000/api/dashboards/db \
  -H "Content-Type: application/json" \
  -d @config/grafana/fr02-dashboard.json

# Test system
python3 -m pytest tests/test_dual_database_v2.py -v
```

---

## üìã **CHECKLIST HO√ÄN TH√ÄNH**

### **Infrastructure Setup**
- [ ] Docker v√† Docker Compose ƒë∆∞·ª£c c√†i ƒë·∫∑t
- [ ] M√¥i tr∆∞·ªùng Python 3.10+ v·ªõi Vietnamese NLP packages
- [ ] Minimum 32GB RAM v√† 1TB storage available
- [ ] Network ports (5432, 8000, 6379, 9090, 3000) available

### **Database Deployment**
- [ ] PostgreSQL 15 container running v·ªõi enhanced schema
- [ ] PgBouncer connection pooling configured
- [ ] All 15+ tables created successfully
- [ ] Vietnamese text processing functions working
- [ ] Sample data inserted and verified

### **Vector Database Setup**
- [ ] ChromaDB container running v·ªõi Qwen embeddings support
- [ ] Knowledge base collections created (main + department-specific)
- [ ] Embedding dimension set to 1024
- [ ] Authentication v·ªõi token configured
- [ ] Test embeddings stored and searchable

### **Cache Layer**
- [ ] Redis cluster running (master + replica)
- [ ] RedisSearch module loaded for Vietnamese text
- [ ] Cache policies configured (LRU, TTL)
- [ ] Performance metrics collection enabled

### **Monitoring Stack**
- [ ] Prometheus collecting metrics from all services
- [ ] Grafana dashboards imported and functional
- [ ] Alert rules configured v√† tested
- [ ] Node exporter collecting system metrics
- [ ] Service health checks passing

### **Performance Validation**
- [ ] PostgreSQL queries < 100ms average
- [ ] ChromaDB searches < 500ms average
- [ ] Redis operations < 10ms average
- [ ] System supports 100+ concurrent users
- [ ] Memory usage within acceptable limits

### **Security Configuration**
- [ ] Database passwords generated v√† secured
- [ ] ChromaDB authentication enabled
- [ ] Network access properly restricted
- [ ] SSL/TLS certificates configured (if applicable)
- [ ] Audit logging enabled

### **Backup v√† Recovery**
- [ ] Automated backup scripts configured
- [ ] Daily backup cron jobs scheduled
- [ ] Backup retention policy implemented
- [ ] Recovery procedures documented v√† tested

### **Documentation**
- [ ] Configuration files documented
- [ ] API endpoints documented
- [ ] Troubleshooting guide available
- [ ] Performance tuning guide available
- [ ] Operational procedures documented

---

## üéØ **PERFORMANCE TARGETS v2.0**

| Metric | Target | Expected | Monitoring |
|--------|--------|----------|------------|
| PostgreSQL Query Response | < 100ms | 20-80ms | Prometheus + Grafana |
| ChromaDB Search Time | < 500ms | 100-300ms | Application metrics |
| Redis Cache Hit Rate | > 90% | 95%+ | Redis exporter |
| System Uptime | > 99.5% | 99.9%+ | Health checks |
| Concurrent Users | 100+ | 200+ | Load testing |
| Storage Capacity | 100k+ chunks | 500k+ chunks | Database monitoring |
| Vietnamese Processing | 100% support | Optimized | Custom metrics |

---

## üìö **KI·∫æN TH·ª®C THAM KH·∫¢O**

### **Vietnamese NLP Resources**
- **pyvi**: Vietnamese word segmentation - https://github.com/trungtv/pyvi
- **underthesea**: Vietnamese NLP toolkit - https://github.com/undertheseanlp/underthesea
- **Qwen Embeddings**: Multilingual embedding model - https://huggingface.co/Qwen

### **Database Optimization**
- **PostgreSQL 15 Performance**: Tuning parameters for production
- **ChromaDB Scaling**: Vector database optimization strategies
- **Redis Clustering**: High availability v√† performance

### **Monitoring Best Practices**
- **Prometheus**: Metrics collection v√† storage
- **Grafana**: Visualization v√† alerting
- **Performance Engineering**: Database monitoring strategies

---

## üîß **TROUBLESHOOTING COMMON ISSUES**

### **PostgreSQL Connection Issues**
```bash
# Check container status
docker-compose ps postgres

# Check logs
docker-compose logs postgres

# Test connection
docker-compose exec postgres psql -U kb_admin -d knowledge_base_v2 -c "SELECT version();"

# Reset if needed
docker-compose down
sudo rm -rf data/postgres/*
docker-compose up -d postgres
```

### **ChromaDB Performance Issues**
```bash
# Check ChromaDB health
curl http://localhost:8000/api/v1/heartbeat

# Monitor memory usage
docker stats fr02-chroma-v2

# Clear collections if needed
python3 -c "
import chromadb
client = chromadb.HttpClient(host='localhost', port=8000)
collections = client.list_collections()
for col in collections:
    print(f'Collection: {col.name}, Count: {col.count()}')
"
```

### **Redis Memory Issues**
```bash
# Check Redis memory
docker-compose exec redis-master redis-cli info memory

# Clear cache if needed
docker-compose exec redis-master redis-cli flushall

# Monitor key distribution
docker-compose exec redis-master redis-cli --bigkeys
```

---

## ‚úÖ **K·∫æT QU·∫¢ MONG ƒê·ª¢I**

Sau khi ho√†n th√†nh FR-02.1 v2.0, h·ªá th·ªëng s·∫Ω cung c·∫•p:

### **Core Infrastructure**
- **Dual Database System** ho·∫°t ƒë·ªông ·ªïn ƒë·ªãnh v·ªõi PostgreSQL + ChromaDB
- **Vietnamese Language Support** t·ªëi ∆∞u v·ªõi pyvi v√† underthesea
- **High Performance** ƒë√°p ·ª©ng 100+ concurrent users
- **Production-Ready** v·ªõi monitoring, backup, v√† security

### **Enhanced Capabilities**
- **Advanced Chunking** v·ªõi semantic boundary detection
- **Quality Metrics** cho Vietnamese text processing
- **Comprehensive Monitoring** v·ªõi real-time dashboards
- **Automated Operations** v·ªõi backup v√† recovery

### **Integration Ready**
- **API Endpoints** s·∫µn s√†ng cho FR-04 RAG Core Engine
- **Authentication** t√≠ch h·ª£p v·ªõi user management system
- **Scalability** support cho future expansion
- **Documentation** ƒë·∫ßy ƒë·ªß cho development team

**Deliverable ch√≠nh: H·ªá th·ªëng c∆° s·ªü d·ªØ li·ªáu k√©p production-ready, t·ªëi ∆∞u cho ti·∫øng Vi·ªát, v·ªõi kh·∫£ nƒÉng scale v√† monitoring comprehensive.**

---

**Document Status:** ‚úÖ READY FOR PRODUCTION DEPLOYMENT  
**Version:** 2.0  
**Last Updated:** September 12, 2025  
**Next Phase:** FR-04 RAG Core Engine Integration  
**Approval Required:** Technical Lead, Database Administrator, DevOps Team

---

D·ª±a tr√™n k·∫øt qu·∫£ t√¨m ki·∫øm trong project knowledge, t√¥i th·∫•y r·∫±ng **Prometheus v√† Grafana** xu·∫•t hi·ªán trong nhi·ªÅu module kh√°c nhau:

## üìä **PROMETHEUS & GRAFANA TRONG C·∫§U TR√öC D·ª∞ √ÅN**

### **FR-02.1: Dual Database System**
- **Prometheus**: Metrics collection cho PostgreSQL, ChromaDB, Redis
- **Grafana**: Database performance dashboards
- **Scope**: Infrastructure monitoring cho database layer

### **FR-07: Analytics & Reporting** 
- **Prometheus**: Application metrics v√† business metrics
- **Grafana**: Analytics dashboards cho user behavior, search patterns
- **Scope**: Business intelligence v√† reporting

### **FR-08: Admin & Maintenance Tools**
- **Prometheus**: System health monitoring
- **Grafana**: Admin dashboards cho system administration
- **Scope**: Administrative monitoring v√† maintenance

## üéØ **PH√ÇN CHIA TR√ÅCH NHI·ªÜM**

### **FR-02.1 (Infrastructure Monitoring)**
```yaml
# Prometheus trong FR-02.1
scrape_configs:
  - job_name: 'postgres-exporter'
  - job_name: 'redis-exporter' 
  - job_name: 'chroma-api'
  - job_name: 'node-exporter'
```

### **FR-07 (Business Analytics)**
```python
# Prometheus trong FR-07
from prometheus_client import Counter, Histogram, Gauge
search_queries_total = Counter('search_queries_total')
response_time_histogram = Histogram('response_time_seconds')
```

### **FR-08 (System Administration)**
```yaml
# Prometheus trong FR-08
admin_operations_total = Counter('admin_operations_total')
system_health_gauge = Gauge('system_health_status')
```

## üîß **KHUY·∫æN NGH·ªä THI·∫æT K·∫æ**

### **T√°ch bi·ªát theo Layer:**
1. **FR-02.1**: Infrastructure metrics (database, cache, storage)
2. **FR-07**: Application metrics (business logic, user analytics)  
3. **FR-08**: Administrative metrics (system health, maintenance)

### **Shared Configuration:**
- **Centralized Prometheus**: M·ªôt instance duy nh·∫•t collect t·ª´ t·∫•t c·∫£ modules
- **Dedicated Grafana Dashboards**: M·ªói module c√≥ dashboard ri√™ng
- **Alert Manager**: Chung cho to√†n h·ªá th·ªëng

## üìã **C·∫¨P NH·∫¨T CHO FR-02.1 v2.0**

Trong t√†i li·ªáu FR-02.1 v2.0, c·∫ßn l√†m r√µ:

```yaml
# FR-02.1 ch·ªâ t·∫≠p trung v√†o Infrastructure Monitoring
prometheus:
  role: "Infrastructure Metrics Collection"
  scope: 
    - Database performance (PostgreSQL, ChromaDB)
    - Cache performance (Redis)
    - Storage metrics
    - Network performance
  
grafana:
  role: "Infrastructure Dashboards" 
  scope:
    - Database connection pools
    - Query performance
    - Cache hit rates
    - Storage utilization
```

T√†i li·ªáu FR-02.1 v2.0 hi·ªán t·∫°i ƒë√£ ƒë√∫ng khi ƒë·∫∑t Prometheus/Grafana trong infrastructure monitoring, nh∆∞ng n√™n b·ªï sung note v·ªÅ coordination v·ªõi c√°c module kh√°c ƒë·ªÉ tr√°nh confusion.
---

# FR-07 ANALYTICS & REPORTING - HANDOVER v2.0
## T√ÄI LI·ªÜU B√ÄN GIAO NHANH CHO TEAM PH√ÅT TRI·ªÇN

---

**Phi√™n b·∫£n:** 2.0  
**Ng√†y:** 12/09/2025  
**Module:** FR-07 Analytics & Reporting  
**Tr·∫°ng th√°i:** Ready for Integration v·ªõi FR-02.1 v2.0  
**Li√™n h·ªá:** Development Team Lead  

---

## üéØ **T√ìM T·∫ÆT MODULE**

FR-07 cung c·∫•p **Business Intelligence v√† User Analytics** cho h·ªá th·ªëng Vietnamese Chatbot, bao g·ªìm:
- **Real-time Analytics API** cho search patterns, user behavior
- **Interactive Dashboard** v·ªõi Streamlit 
- **Business Metrics Collection** qua Prometheus
- **Automated Reporting** (CSV, PDF export)

## üîå **INTEGRATION POINTS V·ªöI FR-02.1 v2.0**

### **Database Connection**
```python
# FR-07 connects to FR-02.1 PostgreSQL
DATABASE_URL = "postgresql://kb_admin:${POSTGRES_PASSWORD}@localhost:5432/knowledge_base_v2"

# Tables FR-07 c·∫ßn t·ª´ FR-02.1:
- users                    # User management data
- documents_metadata_v2    # Document information  
- search_analytics         # Search queries v√† results
- user_activity_summary    # User behavior patterns
- system_metrics          # Performance data
```

### **API Endpoints c·∫ßn t·ª´ FR-02.1**
```bash
# FR-07 s·∫Ω call c√°c endpoints n√†y:
GET /api/users/activity           # User statistics
GET /api/documents/usage         # Document access patterns  
GET /api/search/analytics        # Search performance
GET /api/system/health           # System status
```

## üìä **MONITORING COORDINATION**

### **Prometheus Metrics Separation**

#### **FR-02.1 (Infrastructure)**
```yaml
# PostgreSQL, ChromaDB, Redis metrics
- pg_connections_active
- chroma_search_duration 
- redis_memory_usage
```

#### **FR-07 (Business Analytics)**
```python
# Application v√† business metrics
search_queries_total = Counter('fr07_search_queries_total')
user_session_duration = Histogram('fr07_user_session_duration_seconds')
document_views_total = Counter('fr07_document_views_total')
report_generation_time = Histogram('fr07_report_generation_seconds')
```

### **Grafana Dashboard Structure**
```
Grafana Organization:
‚îú‚îÄ‚îÄ Infrastructure (FR-02.1)
‚îÇ   ‚îú‚îÄ‚îÄ Database Performance
‚îÇ   ‚îú‚îÄ‚îÄ Cache Metrics  
‚îÇ   ‚îî‚îÄ‚îÄ System Health
‚îú‚îÄ‚îÄ Business Analytics (FR-07)
‚îÇ   ‚îú‚îÄ‚îÄ User Behavior
‚îÇ   ‚îú‚îÄ‚îÄ Search Analytics
‚îÇ   ‚îî‚îÄ‚îÄ Content Performance
‚îî‚îÄ‚îÄ Admin Tools (FR-08)
    ‚îú‚îÄ‚îÄ System Administration
    ‚îî‚îÄ‚îÄ Maintenance Tasks
```

## üöÄ **QUICK START GUIDE**

### **1. Environment Setup**
```bash
# Clone project
git clone <repository> && cd FR-07

# Environment variables
cat > .env << EOF
# Database (from FR-02.1)
DATABASE_URL=postgresql://kb_admin:${POSTGRES_PASSWORD}@localhost:5432/knowledge_base_v2
REDIS_URL=redis://localhost:6379

# FR-07 Specific
API_PORT=8001
DASHBOARD_PORT=8501
SECRET_KEY=$(openssl rand -hex 32)

# Integration URLs
FR02_API_BASE=http://localhost:8000
PROMETHEUS_URL=http://localhost:9090
GRAFANA_URL=http://localhost:3000
EOF
```

### **2. Docker Deployment**
```yaml
# docker-compose.fr07.yml
version: '3.8'
services:
  fr07-analytics-api:
    build: ./analytics_module
    ports:
      - "8001:8001"
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
    depends_on:
      - postgres  # From FR-02.1
      - redis     # From FR-02.1
    networks:
      - fr02-network  # Join FR-02.1 network
      
  fr07-dashboard:
    build: ./analytics_module
    command: streamlit run dashboard/main.py --server.port 8501
    ports:
      - "8501:8501"
    depends_on:
      - fr07-analytics-api
    networks:
      - fr02-network

networks:
  fr02-network:
    external: true  # Use network from FR-02.1
```

### **3. Start Services**
```bash
# Ensure FR-02.1 is running first
cd ../FR-02.1 && docker-compose up -d

# Start FR-07
cd ../FR-07 && docker-compose -f docker-compose.fr07.yml up -d

# Verify
curl http://localhost:8001/health
curl http://localhost:8501
```

## üìã **INTEGRATION CHECKLIST**

### **Prerequisites**
- [ ] FR-02.1 v2.0 deployed v√† running
- [ ] Database `knowledge_base_v2` accessible
- [ ] Redis instance available
- [ ] Prometheus collecting metrics t·ª´ FR-02.1

### **Integration Steps**
- [ ] Configure database connection to FR-02.1 PostgreSQL
- [ ] Set up Redis connection for caching
- [ ] Register FR-07 metrics v·ªõi Prometheus
- [ ] Create Grafana dashboards cho business analytics
- [ ] Test API endpoints integration
- [ ] Verify dashboard connectivity

### **Testing Integration**
```bash
# Test database connectivity
python3 -c "
import asyncpg
import asyncio
async def test():
    conn = await asyncpg.connect('${DATABASE_URL}')
    result = await conn.fetchval('SELECT COUNT(*) FROM users')
    print(f'Users in database: {result}')
    await conn.close()
asyncio.run(test())
"

# Test API integration
curl -X GET "http://localhost:8001/api/analytics/search-metrics?days=7"
curl -X GET "http://localhost:8001/api/analytics/user-activity?user_level=EMPLOYEE"
```

## üîß **CONFIGURATION**

### **Database Schema Requirements**
```sql
-- FR-07 requires these views from FR-02.1
CREATE VIEW fr07_search_summary AS
SELECT 
    DATE(timestamp) as date,
    COUNT(*) as total_searches,
    AVG(processing_time_ms) as avg_response_time,
    COUNT(CASE WHEN has_results THEN 1 END) as successful_searches
FROM search_analytics 
GROUP BY DATE(timestamp);

CREATE VIEW fr07_user_engagement AS
SELECT 
    user_level,
    department,
    COUNT(DISTINCT user_id) as active_users,
    AVG(search_count) as avg_searches_per_user
FROM user_activity_summary
GROUP BY user_level, department;
```

### **API Configuration**
```python
# config/fr07_config.py
class FR07Config:
    # Integration settings
    FR02_DATABASE_URL = os.getenv("DATABASE_URL")
    FR02_API_BASE = os.getenv("FR02_API_BASE", "http://localhost:8000")
    
    # FR-07 specific
    ANALYTICS_API_PORT = 8001
    DASHBOARD_PORT = 8501
    CACHE_TTL = 300  # 5 minutes
    
    # Business logic
    DEFAULT_DATE_RANGE = 30  # days
    MAX_EXPORT_RECORDS = 10000
    REPORT_GENERATION_TIMEOUT = 600  # seconds
```

## üìà **API ENDPOINTS**

### **Core Analytics APIs**
```bash
# Search Analytics
GET /api/analytics/search-metrics?days=30
GET /api/analytics/popular-queries?limit=10
GET /api/analytics/search-performance

# User Analytics  
GET /api/analytics/user-activity?department=IT
GET /api/analytics/user-engagement?user_level=EMPLOYEE
GET /api/analytics/session-analysis

# Document Analytics
GET /api/analytics/document-usage?document_type=policy
GET /api/analytics/content-performance
GET /api/analytics/access-patterns

# System Analytics
GET /api/analytics/system-performance
GET /api/analytics/error-rates
GET /api/analytics/capacity-metrics
```

### **Export & Reports**
```bash
# Data Export
GET /api/export/search-data?format=csv&start_date=2025-01-01
GET /api/export/user-report?format=pdf&department=HR

# Automated Reports
POST /api/reports/generate
GET /api/reports/{report_id}/status
GET /api/reports/{report_id}/download
```

## üõ†Ô∏è **TROUBLESHOOTING**

### **Common Integration Issues**

#### **Database Connection Failed**
```bash
# Check FR-02.1 PostgreSQL status
docker-compose -f ../FR-02.1/docker-compose.yml ps postgres

# Test connection
psql -h localhost -p 5432 -U kb_admin -d knowledge_base_v2 -c "SELECT version();"

# Fix: Update DATABASE_URL in FR-07 .env
```

#### **Redis Connection Issues**
```bash
# Check FR-02.1 Redis status
docker-compose -f ../FR-02.1/docker-compose.yml ps redis-master

# Test connection
redis-cli -h localhost -p 6379 ping

# Fix: Update REDIS_URL in FR-07 .env
```

#### **Prometheus Metrics Conflicts**
```bash
# Check if FR-07 metrics are being collected
curl http://localhost:9090/api/v1/label/__name__/values | grep fr07

# Fix: Ensure unique metric names v·ªõi fr07_ prefix
```

## üìû **SUPPORT & CONTACTS**

### **Module Dependencies**
- **FR-02.1**: Database schema, API endpoints
- **Shared**: Prometheus instance, Grafana organization
- **Optional**: FR-05 (UI integration), FR-06 (Authentication)

### **Key Integration Points**
```python
# FR-07 Service Discovery
DEPENDENT_SERVICES = {
    "postgres": "FR-02.1 PostgreSQL Database",
    "redis": "FR-02.1 Redis Cache", 
    "prometheus": "Shared Monitoring Stack",
    "grafana": "Shared Dashboard Platform"
}
```

### **Documentation Links**
- **API Docs**: http://localhost:8001/docs
- **Dashboard**: http://localhost:8501
- **Metrics**: http://localhost:9090/targets (check fr07-* targets)
- **Grafana**: http://localhost:3000 (Business Analytics folder)

---

# FR-08 ADMIN & MAINTENANCE TOOLS - HANDOVER v2.0
## T√ÄI LI·ªÜU B√ÄN GIAO NHANH CHO TEAM PH√ÅT TRI·ªÇN

---

**Phi√™n b·∫£n:** 2.0  
**Ng√†y:** 12/09/2025  
**Module:** FR-08 Admin & Maintenance Tools  
**Tr·∫°ng th√°i:** Ready for Integration v·ªõi FR-02.1 v2.0  
**Li√™n h·ªá:** DevOps Team Lead  

---

## üéØ **T√ìM T·∫ÆT MODULE**

FR-08 cung c·∫•p **System Administration v√† Maintenance** cho to√†n b·ªô h·ªá th·ªëng, bao g·ªìm:
- **Admin API** cho user management, system configuration
- **Maintenance Tools** cho database backup, cleanup
- **System Health Monitoring** v·ªõi alerts v√† notifications
- **Administrative Dashboard** cho system operators

## üîå **INTEGRATION POINTS V·ªöI FR-02.1 v2.0**

### **Database Administrative Access**
```python
# FR-08 needs ADMIN level access to all FR-02.1 tables
DATABASE_URL = "postgresql://kb_admin:${POSTGRES_PASSWORD}@localhost:5432/knowledge_base_v2"

# Admin operations on:
- users                      # User management (CRUD)
- documents_metadata_v2      # Document administration
- data_ingestion_jobs        # Job management
- system_health_metrics      # System monitoring
- chunk_processing_logs      # Processing oversight
```

### **Administrative APIs**
```bash
# FR-08 provides admin endpoints for:
POST /api/admin/users                    # User creation/management
DELETE /api/admin/users/{user_id}        # User deletion
POST /api/admin/documents/reindex        # Document reindexing
POST /api/admin/system/backup            # System backup
GET /api/admin/system/health             # Comprehensive health check
POST /api/admin/maintenance/cleanup      # Data cleanup operations
```

## üìä **MONITORING COORDINATION**

### **Administrative Metrics**

#### **FR-08 System Administration Metrics**
```python
# System admin operations tracking
admin_operations_total = Counter('fr08_admin_operations_total', ['operation_type', 'status'])
backup_duration_seconds = Histogram('fr08_backup_duration_seconds')
cleanup_operations_total = Counter('fr08_cleanup_operations_total', ['cleanup_type'])
system_health_score = Gauge('fr08_system_health_score')
maintenance_window_active = Gauge('fr08_maintenance_window_active')
```

### **Alert Management Integration**
```yaml
# FR-08 manages alerts for entire system
alert_groups:
  - name: fr02_infrastructure
    rules:
      - alert: DatabaseDown
        expr: pg_up{job="postgres-exporter"} == 0
        for: 5m
        labels:
          severity: critical
          module: FR-02.1
        annotations:
          summary: "PostgreSQL database is down"
          
  - name: fr07_business_metrics  
    rules:
      - alert: HighErrorRate
        expr: rate(fr07_api_errors_total[5m]) > 0.1
        for: 10m
        labels:
          severity: warning
          module: FR-07
```

## üöÄ **QUICK START GUIDE**

### **1. Environment Setup**
```bash
# Clone admin tools
git clone <repository> && cd FR-08

# Admin environment variables
cat > .env << EOF
# Database (ADMIN access to FR-02.1)
DATABASE_URL=postgresql://kb_admin:${POSTGRES_PASSWORD}@localhost:5432/knowledge_base_v2
REDIS_URL=redis://localhost:6379

# FR-08 Admin specific
ADMIN_API_PORT=8002
ADMIN_SECRET_KEY=$(openssl rand -hex 32)
BACKUP_RETENTION_DAYS=30
LOG_LEVEL=INFO

# System integration
FR02_API_BASE=http://localhost:8000
FR07_API_BASE=http://localhost:8001
PROMETHEUS_URL=http://localhost:9090
GRAFANA_URL=http://localhost:3000
ALERTMANAGER_URL=http://localhost:9093

# Notification settings
SMTP_SERVER=smtp.company.com
SMTP_PORT=587
ADMIN_EMAIL=admin@company.com
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/xxx
EOF
```

### **2. Docker Deployment**
```yaml
# docker-compose.fr08.yml
version: '3.8'
services:
  fr08-admin-api:
    build: ./admin_tools
    ports:
      - "8002:8002"
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
      - ADMIN_SECRET_KEY=${ADMIN_SECRET_KEY}
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock  # Docker management
      - ./backups:/app/backups                     # Backup storage
      - ./logs:/app/logs                           # Log storage
    depends_on:
      - postgres  # From FR-02.1
      - redis     # From FR-02.1
    networks:
      - fr02-network
    restart: unless-stopped
      
  fr08-backup-scheduler:
    build: ./admin_tools
    command: python -m admin_tools.services.backup_scheduler
    volumes:
      - ./backups:/app/backups
      - postgres_data:/var/lib/postgresql/data:ro
      - chroma_data:/chroma/chroma:ro
    depends_on:
      - fr08-admin-api
    networks:
      - fr02-network
    restart: unless-stopped

  fr08-alertmanager:
    image: prom/alertmanager:latest
    ports:
      - "9093:9093"
    volumes:
      - ./config/alertmanager.yml:/etc/alertmanager/alertmanager.yml
    networks:
      - fr02-network
    restart: unless-stopped

volumes:
  postgres_data:
    external: true  # From FR-02.1
  chroma_data:
    external: true  # From FR-02.1

networks:
  fr02-network:
    external: true  # Use FR-02.1 network
```

### **3. Start Admin Services**
```bash
# Ensure FR-02.1 v√† FR-07 are running
cd ../FR-02.1 && docker-compose ps
cd ../FR-07 && docker-compose -f docker-compose.fr07.yml ps

# Start FR-08 admin tools
cd ../FR-08 && docker-compose -f docker-compose.fr08.yml up -d

# Verify admin services
curl http://localhost:8002/health
curl http://localhost:9093/#/status
```

## üìã **INTEGRATION CHECKLIST**

### **Prerequisites**
- [ ] FR-02.1 v2.0 running v·ªõi admin database access
- [ ] FR-07 deployed (optional but recommended)
- [ ] Docker socket access for container management
- [ ] SMTP server configured for notifications
- [ ] Backup storage volumes mounted

### **Administrative Setup**
- [ ] Create admin user accounts v·ªõi SYSTEM_ADMIN level
- [ ] Configure backup schedules v√† retention policies
- [ ] Set up alert notification channels (email, Slack)
- [ ] Configure maintenance windows
- [ ] Test emergency procedures
- [ ] Document recovery procedures

### **Security Configuration**
```bash
# Create admin users
curl -X POST http://localhost:8002/api/admin/users \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer ${ADMIN_TOKEN}" \
  -d '{
    "username": "sysadmin",
    "email": "sysadmin@company.com", 
    "user_level": "SYSTEM_ADMIN",
    "department": "IT"
  }'

# Configure backup encryption
curl -X POST http://localhost:8002/api/admin/config/backup \
  -H "Authorization: Bearer ${ADMIN_TOKEN}" \
  -d '{
    "encryption_enabled": true,
    "compression_level": 6,
    "retention_days": 30
  }'
```

## üîß **ADMINISTRATIVE OPERATIONS**

### **User Management**
```bash
# User administration
GET /api/admin/users                     # List all users
POST /api/admin/users                    # Create user
PUT /api/admin/users/{user_id}           # Update user
DELETE /api/admin/users/{user_id}        # Delete user
POST /api/admin/users/{user_id}/reset    # Reset password
POST /api/admin/users/{user_id}/lock     # Lock account
```

### **System Management**
```bash
# System operations
GET /api/admin/system/status             # Complete system status
POST /api/admin/system/backup            # Trigger manual backup
POST /api/admin/system/maintenance       # Enter maintenance mode
DELETE /api/admin/system/cache           # Clear all caches
GET /api/admin/system/logs?service=all   # Aggregate logs
```

### **Document Management**
```bash
# Document administration
POST /api/admin/documents/reindex        # Reindex all documents
POST /api/admin/documents/cleanup        # Remove orphaned documents
GET /api/admin/documents/quality         # Document quality report
POST /api/admin/documents/migrate        # Migrate document format
```

### **Maintenance Operations**
```bash
# Database maintenance
POST /api/admin/maintenance/vacuum       # Database vacuum
POST /api/admin/maintenance/reindex      # Rebuild indexes
POST /api/admin/maintenance/analyze      # Update statistics
GET /api/admin/maintenance/health        # Database health check
```

## üö® **ALERTING & NOTIFICATIONS**

### **Alert Configuration**
```yaml
# config/alertmanager.yml
global:
  smtp_smarthost: '${SMTP_SERVER}:${SMTP_PORT}'
  smtp_from: 'alerts@company.com'

route:
  group_by: ['alertname', 'module']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'admin-team'

receivers:
  - name: 'admin-team'
    email_configs:
      - to: '${ADMIN_EMAIL}'
        subject: '[CHATBOT] {{ .GroupLabels.module }} Alert: {{ .GroupLabels.alertname }}'
        body: |
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Module: {{ .Labels.module }}
          Severity: {{ .Labels.severity }}
          {{ end }}
    
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#chatbot-alerts'
        title: '{{ .GroupLabels.module }} Alert'
        text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
```

### **System Health Monitoring**
```python
# Health check aggregation
@app.get("/api/admin/system/health")
async def system_health():
    health_status = {
        "overall": "healthy",
        "services": {
            "fr02_postgres": await check_postgres_health(),
            "fr02_chroma": await check_chroma_health(),
            "fr02_redis": await check_redis_health(),
            "fr07_analytics": await check_fr07_health(),
            "fr08_admin": "healthy"
        },
        "metrics": {
            "uptime_seconds": get_system_uptime(),
            "memory_usage_percent": get_memory_usage(),
            "disk_usage_percent": get_disk_usage(),
            "active_users": await get_active_user_count()
        }
    }
    return health_status
```

## üîÑ **BACKUP & RECOVERY**

### **Automated Backup Strategy**
```python
# Backup scheduler configuration
BACKUP_SCHEDULE = {
    "postgres": {
        "frequency": "daily",
        "time": "02:00",
        "retention_days": 30,
        "compression": True,
        "encryption": True
    },
    "chroma": {
        "frequency": "daily", 
        "time": "03:00",
        "retention_days": 14,
        "compression": True
    },
    "redis": {
        "frequency": "every_6_hours",
        "retention_hours": 72,
        "compression": False
    },
    "logs": {
        "frequency": "weekly",
        "retention_weeks": 4,
        "compression": True
    }
}
```

### **Recovery Procedures**
```bash
# Database recovery
POST /api/admin/recovery/postgres
{
  "backup_file": "postgres_backup_20250912_020000.sql.gz",
  "target_database": "knowledge_base_v2",
  "confirm_overwrite": true
}

# Vector database recovery  
POST /api/admin/recovery/chroma
{
  "backup_file": "chroma_backup_20250912_030000.tar.gz",
  "target_collection": "knowledge_base_v2",
  "verify_integrity": true
}
```

## üõ†Ô∏è **TROUBLESHOOTING**

### **Common Administrative Issues**

#### **Backup Failures**
```bash
# Check backup service status
docker-compose -f docker-compose.fr08.yml ps fr08-backup-scheduler

# Check backup logs
docker-compose -f docker-compose.fr08.yml logs fr08-backup-scheduler

# Manual backup test
curl -X POST http://localhost:8002/api/admin/system/backup \
  -H "Authorization: Bearer ${ADMIN_TOKEN}"
```

#### **Alert Manager Not Working**
```bash
# Check AlertManager status
curl http://localhost:9093/api/v1/status

# Test alert routing
curl -X POST http://localhost:9093/api/v1/alerts \
  -H "Content-Type: application/json" \
  -d '[{
    "labels": {"alertname": "TestAlert", "severity": "warning"},
    "annotations": {"summary": "Test alert from FR-08"}
  }]'
```

#### **System Health Check Failures**
```bash
# Check individual service health
curl http://localhost:8002/api/admin/system/health

# Check service dependencies
docker-compose -f ../FR-02.1/docker-compose.yml ps
docker-compose -f ../FR-07/docker-compose.fr07.yml ps

# Restart unhealthy services
docker-compose restart <service_name>
```

## üìû **SUPPORT & CONTACTS**

### **Administrative Responsibilities**
- **User Management**: Account creation, permissions, password resets
- **System Monitoring**: Health checks, performance monitoring, alerting
- **Data Management**: Backups, cleanup, migration, recovery
- **Security**: Access control, audit logging, compliance

### **Emergency Procedures**
```bash
# System emergency stop
curl -X POST http://localhost:8002/api/admin/emergency/stop \
  -H "Authorization: Bearer ${ADMIN_TOKEN}"

# Enter maintenance mode
curl -X POST http://localhost:8002/api/admin/system/maintenance \
  -H "Authorization: Bearer ${ADMIN_TOKEN}" \
  -d '{"duration_minutes": 60, "reason": "Emergency maintenance"}'

# Restore from backup
curl -X POST http://localhost:8002/api/admin/recovery/full-restore \
  -H "Authorization: Bearer ${ADMIN_TOKEN}" \
  -d '{"backup_timestamp": "20250912_020000"}'
```

### **Documentation Links**
- **Admin API**: http://localhost:8002/docs
- **System Status**: http://localhost:8002/api/admin/system/status
- **Alert Manager**: http://localhost:9093
- **Backup Status**: http://localhost:8002/api/admin/backup/status

### **Integration Dependencies**
```yaml
# FR-08 depends on:
required_services:
  - fr02-postgres      # Database administration
  - fr02-redis         # Cache management  
  - fr02-chroma        # Vector DB maintenance
  - prometheus         # Metrics collection
  - grafana           # Dashboard management

optional_integrations:
  - fr07-analytics     # Business metrics oversight
  - smtp-server        # Email notifications
  - slack-api          # Chat notifications
```

---

**üîß Ready for immediate deployment and integration v·ªõi FR-02.1 v2.0 infrastructure!**
