# T√ÄI LI·ªÜU THI·∫æT K·∫æ K·ª∏ THU·∫¨T FR-02.1 v2.0
## H·ªÜ TH·ªêNG C∆† S·ªû D·ªÆ LI·ªÜU K√âP (DUAL DATABASE SYSTEM)

---

**Phi√™n b·∫£n:** 2.0  
**Ng√†y:** 12/09/2025  
**Module:** FR-02.1 - H·ªá th·ªëng CSDL k√©p  
**M·ª©c ƒë·ªô ∆∞u ti√™n:** CRITICAL  
**Ng∆∞·ªùi so·∫°n th·∫£o:** Technical Design Team  

---

## üìã **CHANGELOG v2.0**

### **üîÑ Thay ƒë·ªïi t·ª´ v1.0:**

#### **‚úÖ Enhancements ch√≠nh:**
- **Enhanced Schema Integration**: T√≠ch h·ª£p schema t·ª´ FR03.1 v·ªõi c√°c tr∆∞·ªùng m·ªü r·ªông cho chunking n√¢ng cao
- **Updated Vector Database**: Thay th·∫ø model embedding t·ª´ OpenAI sang `Qwen/Qwen3-Embedding-0.6B` (1024-dim)
- **Vietnamese Optimization**: T·ªëi ∆∞u ƒë·∫∑c bi·ªát cho x·ª≠ l√Ω ti·∫øng Vi·ªát v·ªõi `pyvi` v√† `underthesea`
- **Advanced Monitoring**: Th√™m comprehensive monitoring v·ªõi Prometheus + Grafana
- **Production-Ready Setup**: Docker Compose ho√†n ch·ªânh v·ªõi health checks v√† restart policies
- **Performance Optimization**: Connection pooling, indexing strategies, v√† caching layers
- **Security Hardening**: Enhanced authentication, authorization, v√† audit logging

#### **üóëÔ∏è Removed t·ª´ v1.0:**
- Timeline estimates (th·ªùi gian tri·ªÉn khai)
- OpenAI dependency requirements
- Basic monitoring setup
- Simple Docker configuration

#### **üÜï New Features:**
- Vietnamese text analysis table v·ªõi quality metrics
- Data ingestion job tracking tables
- Chunk processing logs v·ªõi retry mechanisms
- Performance monitoring views
- Automated backup strategies
- Load balancing configuration

---

## üîã **T·ªîNG QUAN THI·∫æT K·∫æ**

### **M·ª•c ti√™u**
Thi·∫øt k·∫ø v√† tri·ªÉn khai h·ªá th·ªëng c∆° s·ªü d·ªØ li·ªáu k√©p bao g·ªìm:
- **Vector Database**: L∆∞u tr·ªØ embeddings cho t√¨m ki·∫øm ng·ªØ nghƒ©a (‚â•100k chunks)
- **Relational Database**: Qu·∫£n l√Ω metadata, ng∆∞·ªùi d√πng v√† ph√¢n quy·ªÅn
- **Vietnamese Language Support**: T·ªëi ∆∞u ƒë·∫∑c bi·ªát cho ti·∫øng Vi·ªát
- **Production-Ready Infrastructure**: Monitoring, backup, v√† high availability

### **Ki·∫øn tr√∫c t·ªïng quan**
```mermaid
graph TB
    subgraph "Application Layer"
        API[üåê REST API Services]
        RAG[ü§ñ RAG Engine]
        UI[üíª Web Interface]
    end
    
    subgraph "üóÑÔ∏è FR-02.1: DUAL DATABASE SYSTEM v2.0"
        subgraph "üìä Vector Database Layer"
            ChromaDB[üì¢ ChromaDB<br/>Qwen Embeddings]
            VectorIndex[üìá Vector Index<br/>HNSW Algorithm]
            Collections[üìö Collections<br/>Multi-tenant]
        end
        
        subgraph "üóÉÔ∏è Enhanced PostgreSQL Layer"
            PostgreSQL[(üêò PostgreSQL 15<br/>Enhanced Schema)]
            ConnPool[üîÑ PgBouncer<br/>Connection Pool]
            Schemas[üìã Enhanced Tables<br/>Vietnamese Support]
        end
        
        subgraph "‚ö° Cache & Performance Layer"
            RedisCache[üî¥ Redis Cluster<br/>Query + Session Cache]
            RedisSearch[üîç Redis Search<br/>Full-text Search]
        end
        
        subgraph "üìä Monitoring & Operations"
            Prometheus[üìà Prometheus<br/>Metrics Collection]
            Grafana[üìä Grafana<br/>Dashboards]
            AlertManager[üö® AlertManager<br/>Notifications]
        end
    end
    
    subgraph "üíæ Storage & Backup"
        VectorStorage[üìÅ Vector Storage<br/>Persistent Volumes]
        SQLStorage[üìÅ SQL Storage<br/>Persistent Volumes]
        BackupStorage[üíΩ Backup Storage<br/>Automated Backups]
    end
    
    API --> ChromaDB
    API --> PostgreSQL
    RAG --> ChromaDB
    RAG --> PostgreSQL
    UI --> API
    
    ChromaDB --> VectorIndex
    ChromaDB --> Collections
    PostgreSQL --> ConnPool
    PostgreSQL --> Schemas
    
    API -.-> RedisCache
    API -.-> RedisSearch
    
    Prometheus --> PostgreSQL
    Prometheus --> ChromaDB
    Prometheus --> RedisCache
    Grafana --> Prometheus
    AlertManager --> Prometheus
    
    ChromaDB --> VectorStorage
    PostgreSQL --> SQLStorage
    VectorStorage --> BackupStorage
    SQLStorage --> BackupStorage
    
    classDef app fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef vector fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef sql fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef cache fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef monitor fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    classDef storage fill:#e0f2f1,stroke:#00796b,stroke-width:2px
    
    class API,RAG,UI app
    class ChromaDB,VectorIndex,Collections vector
    class PostgreSQL,ConnPool,Schemas sql
    class RedisCache,RedisSearch cache
    class Prometheus,Grafana,AlertManager monitor
    class VectorStorage,SQLStorage,BackupStorage storage
```

---

## üõ†Ô∏è **CHU·∫®N B·ªä M√îI TR∆Ø·ªúNG PH√ÅT TRI·ªÇN**

### **1. Y√™u c·∫ßu h·∫° t·∫ßng**

#### **üíª Ph·∫ßn c·ª©ng t·ªëi thi·ªÉu**
- **CPU**: 8 cores (Intel i7 ho·∫∑c AMD Ryzen 7)
- **RAM**: 32GB (khuy·∫øn ngh·ªã 64GB)
- **Storage**: 
  - SSD 500GB cho OS v√† development tools
  - SSD 1TB cho databases v√† data storage
  - HDD 2TB cho backup (optional)
- **Network**: Gigabit Ethernet
- **GPU**: Optional (NVIDIA v·ªõi CUDA support cho accelerated embeddings)

#### **üñ•Ô∏è H·ªá ƒëi·ªÅu h√†nh**
- **Primary**: Ubuntu 22.04 LTS / CentOS 8
- **Alternative**: macOS 12+ / Windows 11 v·ªõi WSL2

### **2. Ph·∫ßn m·ªÅm c·∫ßn thi·∫øt**

#### **üê≥ Container & Orchestration**
```bash
# Docker & Docker Compose Latest
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
sudo curl -L "https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose
```

#### **üêç Python Development Stack**
```bash
# Python 3.10.11+ v·ªõi virtual environment
sudo apt-get update
sudo apt-get install python3.10 python3.10-venv python3.10-dev build-essential
python3.10 -m venv venv-fr02-v2
source venv-fr02-v2/bin/activate

# Vietnamese NLP Dependencies
sudo apt-get install libicu-dev
pip install pyvi underthesea
```

#### **üóÑÔ∏è Database Tools**
```bash
# PostgreSQL Client Tools
sudo apt-get install postgresql-client-15

# GUI Tools (choose one)
# - pgAdmin 4 (Web-based)
# - DBeaver Community (Desktop)
# - TablePlus (macOS/Windows)
```

#### **üìä Monitoring & Development Tools**
```bash
# Essential development tools
sudo apt-get install git curl wget htop tree jq unzip

# Vietnamese text processing tools
pip install underthesea pyvi sentence-transformers

# Monitoring tools
sudo apt-get install prometheus-node-exporter
```

---

## üóóÔ∏è **KI·∫æN TR√öC CHI TI·∫æT**

### **1. Enhanced PostgreSQL Database Design**

#### **üêò PostgreSQL 15 v·ªõi Enhanced Schema**
```sql
-- Core Enhanced Schema (based on updated schema_01_init_database.sql)

-- Enable required extensions
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pg_trgm";
CREATE EXTENSION IF NOT EXISTS "btree_gin";
CREATE EXTENSION IF NOT EXISTS "unaccent";

-- Enhanced enum types
CREATE TYPE access_level_enum AS ENUM (
    'public', 'employee_only', 'manager_only', 'director_only', 'system_admin'
);

CREATE TYPE document_type_enum AS ENUM (
    'policy', 'procedure', 'technical_guide', 'report', 
    'manual', 'specification', 'template', 'form', 
    'presentation', 'training_material', 'other'
);

CREATE TYPE userlevel AS ENUM (
    'GUEST', 'EMPLOYEE', 'MANAGER', 'DIRECTOR', 'SYSTEM_ADMIN'
);

CREATE TYPE chunking_method_enum AS ENUM (
    'fixed_size', 'sentence_based', 'semantic_boundary', 'paragraph_based', 'hybrid'
);

-- Enhanced Users table v·ªõi Vietnamese support
CREATE TABLE users (
    user_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    username VARCHAR(50) UNIQUE NOT NULL,
    email VARCHAR(100) UNIQUE NOT NULL,
    full_name VARCHAR(255) NOT NULL,
    password_hash VARCHAR(255) NOT NULL,
    salt VARCHAR(32) NOT NULL,
    user_level userlevel NOT NULL DEFAULT 'EMPLOYEE',
    department VARCHAR(100),
    position VARCHAR(100),
    status VARCHAR(20) NOT NULL DEFAULT 'ACTIVE',
    is_active BOOLEAN NOT NULL DEFAULT true,
    email_verified BOOLEAN NOT NULL DEFAULT false,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    last_login TIMESTAMP WITH TIME ZONE,
    preferences JSONB DEFAULT '{}',
    failed_login_attempts INTEGER DEFAULT 0,
    locked_until TIMESTAMP WITH TIME ZONE
);

-- Enhanced Documents Metadata v·ªõi Vietnamese optimization
CREATE TABLE documents_metadata_v2 (
    document_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    
    -- Basic information
    title VARCHAR(500) NOT NULL,
    content TEXT,
    document_type document_type_enum NOT NULL,
    access_level access_level_enum NOT NULL DEFAULT 'employee_only',
    department_owner VARCHAR(100) NOT NULL,
    author VARCHAR(255) NOT NULL,
    status VARCHAR(20) DEFAULT 'draft',
    
    -- Vietnamese language support
    language_detected VARCHAR(10) DEFAULT 'vi',
    vietnamese_segmented BOOLEAN DEFAULT false,
    diacritics_normalized BOOLEAN DEFAULT false,
    tone_marks_preserved BOOLEAN DEFAULT true,
    
    -- Enhanced search fields
    search_text_normalized TEXT,
    indexable_content TEXT,
    extracted_emails TEXT[] DEFAULT '{}',
    extracted_phones TEXT[] DEFAULT '{}',
    extracted_dates DATE[] DEFAULT '{}',
    
    -- Embedding v√† chunking metadata
    embedding_model_primary VARCHAR(100) DEFAULT 'Qwen/Qwen3-Embedding-0.6B',
    chunk_count INTEGER DEFAULT 0,
    file_size_bytes BIGINT,
    
    -- Search support
    search_tokens TSVECTOR,
    keyword_density JSONB DEFAULT '{}',
    
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Enhanced Document Chunks v·ªõi advanced chunking support
CREATE TABLE document_chunks_enhanced (
    chunk_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    document_id UUID NOT NULL REFERENCES documents_metadata_v2(document_id) ON DELETE CASCADE,
    
    -- Content data
    chunk_content TEXT NOT NULL,
    chunk_position INTEGER NOT NULL,
    chunk_size_tokens INTEGER,
    
    -- Advanced semantic chunking
    semantic_boundary BOOLEAN DEFAULT false,
    overlap_with_prev INTEGER DEFAULT 0,
    overlap_with_next INTEGER DEFAULT 0,
    overlap_source_prev INTEGER,
    overlap_source_next INTEGER,
    is_final_part BOOLEAN DEFAULT false,
    heading_context TEXT,
    
    -- Chunking method v√† quality
    chunk_method chunking_method_enum DEFAULT 'semantic_boundary',
    chunk_quality_score DECIMAL(3,2) CHECK (chunk_quality_score BETWEEN 0.00 AND 1.00),
    
    -- Vector storage references
    embedding_model VARCHAR(100) DEFAULT 'Qwen/Qwen3-Embedding-0.6B',
    embedding_dimensions INTEGER DEFAULT 1024,
    
    -- Full-text search support
    bm25_tokens TSVECTOR,
    
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Vietnamese Text Analysis table
CREATE TABLE vietnamese_text_analysis (
    analysis_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    document_id UUID REFERENCES documents_metadata_v2(document_id) ON DELETE CASCADE,
    chunk_id UUID REFERENCES document_chunks_enhanced(chunk_id) ON DELETE CASCADE,
    
    -- Text data
    original_text TEXT NOT NULL,
    processed_text TEXT,
    
    -- Analysis results
    word_segmentation JSONB DEFAULT '{}',
    pos_tagging JSONB DEFAULT '{}',
    
    -- Vietnamese features
    compound_words TEXT[] DEFAULT '{}',
    technical_terms TEXT[] DEFAULT '{}',
    proper_nouns TEXT[] DEFAULT '{}',
    
    -- Quality metrics
    readability_score DECIMAL(3,2),
    formality_level VARCHAR(20),
    language_quality_score DECIMAL(4,1),
    diacritics_density DECIMAL(4,3),
    token_diversity DECIMAL(4,3),
    
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Data Ingestion Jobs tracking (FR03.3 integration)
CREATE TABLE data_ingestion_jobs (
    job_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_id UUID REFERENCES users(user_id) ON DELETE SET NULL,
    
    -- Job configuration
    job_name VARCHAR(255) NOT NULL,
    job_type VARCHAR(50) NOT NULL DEFAULT 'document_processing',
    source_path VARCHAR(1000),
    target_collection VARCHAR(100) DEFAULT 'default_collection',
    
    -- Processing parameters
    chunking_method chunking_method_enum DEFAULT 'semantic_boundary',
    chunk_size_tokens INTEGER DEFAULT 512,
    overlap_tokens INTEGER DEFAULT 50,
    embedding_model VARCHAR(100) DEFAULT 'Qwen/Qwen3-Embedding-0.6B',
    
    -- Job status
    status VARCHAR(50) NOT NULL DEFAULT 'pending',
    progress_percentage DECIMAL(5,2) DEFAULT 0.00,
    documents_processed INTEGER DEFAULT 0,
    chunks_created INTEGER DEFAULT 0,
    
    -- Performance metrics
    started_at TIMESTAMP WITH TIME ZONE,
    completed_at TIMESTAMP WITH TIME ZONE,
    processing_time_seconds DOUBLE PRECISION,
    
    -- Results
    success_count INTEGER DEFAULT 0,
    error_count INTEGER DEFAULT 0,
    error_log TEXT[],
    
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);
```

### **2. Vector Database Design**

#### **üì¢ ChromaDB v·ªõi Qwen Embeddings**
```python
# Enhanced ChromaDB Configuration
CHROMA_CONFIG = {
    "embedding_model": "Qwen/Qwen3-Embedding-0.6B",
    "embedding_dimension": 1024,
    "distance_metric": "cosine",
    "index_type": "HNSW",
    "max_elements": 500000,  # Increased capacity
    "ef_construction": 200,
    "M": 16,
    "vietnamese_optimization": True
}

# Collection Schema Design
COLLECTIONS = {
    "knowledge_base_v2": {
        "description": "Main knowledge base with Vietnamese optimization",
        "embedding_dimension": 1024,
        "metadata_schema": {
            "document_id": "string",
            "chunk_id": "string", 
            "access_level": "string",
            "department": "string",
            "document_type": "string",
            "language": "string",
            "quality_score": "float",
            "vietnamese_processed": "boolean"
        }
    },
    "dept_hr_v2": {"parent": "knowledge_base_v2", "department": "HR"},
    "dept_it_v2": {"parent": "knowledge_base_v2", "department": "IT"},
    "dept_rd_v2": {"parent": "knowledge_base_v2", "department": "R&D"}
}
```

### **3. Enhanced Cache Layer**

#### **üî¥ Redis Cluster Configuration**
```yaml
# Redis Cluster Setup
redis-cluster:
  nodes:
    - redis-node-1:7000
    - redis-node-2:7001  
    - redis-node-3:7002
  configuration:
    maxmemory: 4gb
    maxmemory-policy: allkeys-lru
    save: "900 1 300 10 60 10000"
    appendonly: yes
    appendfsync: everysec
  
redis-modules:
  - name: RedisSearch
    enabled: true
    purpose: Full-text search for Vietnamese
  - name: RedisJSON
    enabled: true
    purpose: JSON document storage
  - name: RedisTimeSeries
    enabled: true
    purpose: Performance metrics
```

---

## üê≥ **DOCKER COMPOSE CONFIGURATION**

### **docker-compose.yml ho√†n ch·ªânh**
```yaml
version: '3.8'

services:
  # PostgreSQL Database
  postgres:
    image: postgres:15-alpine
    container_name: fr02-postgres-v2
    environment:
      POSTGRES_DB: knowledge_base_v2
      POSTGRES_USER: kb_admin
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_INITDB_ARGS: "--encoding=UTF8 --locale=C.UTF-8"
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./config/postgres.conf:/etc/postgresql/postgresql.conf
      - ./scripts/init-postgres.sql:/docker-entrypoint-initdb.d/01_init.sql
      - ./scripts/enhanced-schema.sql:/docker-entrypoint-initdb.d/02_schema.sql
    command: postgres -c config_file=/etc/postgresql/postgresql.conf
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U kb_admin -d knowledge_base_v2"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    networks:
      - fr02-network

  # PgBouncer Connection Pool
  pgbouncer:
    image: pgbouncer/pgbouncer:latest
    container_name: fr02-pgbouncer
    environment:
      DATABASES_HOST: postgres
      DATABASES_PORT: 5432
      DATABASES_USER: kb_admin
      DATABASES_PASSWORD: ${POSTGRES_PASSWORD}
      DATABASES_DBNAME: knowledge_base_v2
      POOL_MODE: transaction
      SERVER_RESET_QUERY: DISCARD ALL
      MAX_CLIENT_CONN: 1000
      DEFAULT_POOL_SIZE: 25
      MIN_POOL_SIZE: 5
      RESERVE_POOL_SIZE: 3
      MAX_DB_CONNECTIONS: 100
    ports:
      - "6432:5432"
    depends_on:
      postgres:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - fr02-network

  # ChromaDB Vector Database , latest, 1.0.0, tuannq
  chroma:
    image: chromadb/chroma: 1.0.0
    container_name: fr02-chroma-v2
    ports:
      - "8000:8000"
    volumes:
      - chroma_data:/chroma/chroma
      - ./config/chroma-config.yaml:/chroma/config.yaml
    environment:
      - CHROMA_SERVER_HOST=0.0.0.0
      - CHROMA_SERVER_PORT=8000
      - CHROMA_SERVER_CORS_ALLOW_ORIGINS=*
      - PERSIST_DIRECTORY=/chroma/chroma
      - CHROMA_SERVER_AUTH_CREDENTIALS_PROVIDER=chromadb.auth.token.TokenAuthCredentialsProvider
      - CHROMA_SERVER_AUTH_CREDENTIALS=${CHROMA_AUTH_TOKEN}
      - CHROMA_SERVER_AUTH_TOKEN_TRANSPORT_HEADER=X-Chroma-Token
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/api/v1/heartbeat || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    networks:
      - fr02-network

  # Redis Cache Cluster
  redis-master:
    image: redis:7-alpine
    container_name: fr02-redis-master
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
      - ./config/redis-master.conf:/usr/local/etc/redis/redis.conf
    command: redis-server /usr/local/etc/redis/redis.conf
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "redis-cli ping | grep PONG"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - fr02-network

  redis-replica:
    image: redis:7-alpine
    container_name: fr02-redis-replica
    ports:
      - "6380:6379"
    volumes:
      - redis_replica_data:/data
      - ./config/redis-replica.conf:/usr/local/etc/redis/redis.conf
    command: redis-server /usr/local/etc/redis/redis.conf
    depends_on:
      - redis-master
    restart: unless-stopped
    networks:
      - fr02-network

  # Prometheus Monitoring
  prometheus:
    image: prom/prometheus:latest
    container_name: fr02-prometheus
    ports:
      - "9090:9090"
    volumes:
      - prometheus_data:/prometheus
      - ./config/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./config/alert_rules.yml:/etc/prometheus/alert_rules.yml
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
    restart: unless-stopped
    networks:
      - fr02-network

  # Grafana Dashboards
  grafana:
    image: grafana/grafana:latest
    container_name: fr02-grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_INSTALL_PLUGINS=redis-datasource,postgres-datasource
    volumes:
      - grafana_data:/var/lib/grafana
      - ./config/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./config/grafana/datasources:/etc/grafana/provisioning/datasources
    depends_on:
      - prometheus
    restart: unless-stopped
    networks:
      - fr02-network

  # PostgreSQL Exporter
  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:latest
    container_name: fr02-postgres-exporter
    environment:
      DATA_SOURCE_NAME: "postgresql://kb_admin:${POSTGRES_PASSWORD}@postgres:5432/knowledge_base_v2?sslmode=disable"
    ports:
      - "9187:9187"
    depends_on:
      postgres:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - fr02-network

  # Redis Exporter
  redis-exporter:
    image: oliver006/redis_exporter:latest
    container_name: fr02-redis-exporter
    environment:
      REDIS_ADDR: "redis://redis-master:6379"
    ports:
      - "9121:9121"
    depends_on:
      - redis-master
    restart: unless-stopped
    networks:
      - fr02-network

  # Node Exporter
  node-exporter:
    image: prom/node-exporter:latest
    container_name: fr02-node-exporter
    ports:
      - "9100:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    restart: unless-stopped
    networks:
      - fr02-network

  # NGINX Load Balancer
  nginx:
    image: nginx:alpine
    container_name: fr02-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./config/nginx.conf:/etc/nginx/nginx.conf
      - ./config/ssl:/etc/nginx/ssl
    depends_on:
      - chroma
      - postgres
    restart: unless-stopped
    networks:
      - fr02-network

volumes:
  postgres_data:
    driver: local
  chroma_data:
    driver: local
  redis_data:
    driver: local
  redis_replica_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local

networks:
  fr02-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16
```

---

## ‚öôÔ∏è **C·∫§U H√åNH CHI TI·∫æT**

### **1. PostgreSQL Configuration**

#### **config/postgres.conf**
```conf
# Memory Settings
shared_buffers = 2GB
effective_cache_size = 6GB
maintenance_work_mem = 512MB
work_mem = 16MB

# Connection Settings
max_connections = 200
superuser_reserved_connections = 3

# WAL Settings
wal_level = replica
max_wal_size = 2GB
min_wal_size = 512MB
checkpoint_completion_target = 0.9

# Logging
log_destination = 'stderr'
logging_collector = on
log_directory = 'log'
log_filename = 'postgresql-%Y-%m-%d_%H%M%S.log'
log_statement = 'mod'
log_min_duration_statement = 1000

# Vietnamese locale support
lc_messages = 'en_US.UTF-8'
lc_monetary = 'en_US.UTF-8'
lc_numeric = 'en_US.UTF-8'
lc_time = 'en_US.UTF-8'
default_text_search_config = 'pg_catalog.simple'

# Performance tuning
random_page_cost = 1.1
effective_io_concurrency = 200
max_worker_processes = 8
max_parallel_workers_per_gather = 4
max_parallel_workers = 8
```

### **2. Redis Configuration**

#### **config/redis-master.conf**
```conf
# Basic Configuration
port 6379
bind 0.0.0.0
protected-mode no

# Memory Management
maxmemory 4gb
maxmemory-policy allkeys-lru
maxmemory-samples 10

# Persistence
save 900 1
save 300 10
save 60 10000
stop-writes-on-bgsave-error yes
rdbcompression yes
rdbchecksum yes
dbfilename dump.rdb

# Append Only File
appendonly yes
appendfilename "appendonly.aof"
appendfsync everysec
no-appendfsync-on-rewrite no
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb

# Networking
tcp-keepalive 300
timeout 0

# Logging
loglevel notice
logfile ""

# Vietnamese text search optimization
hash-max-ziplist-entries 512
hash-max-ziplist-value 64
list-max-ziplist-size -2
set-max-intset-entries 512
zset-max-ziplist-entries 128
zset-max-ziplist-value 64

# Modules
loadmodule /usr/lib/redis/modules/redisearch.so
loadmodule /usr/lib/redis/modules/rejson.so
```

### **3. Monitoring Configuration**

#### **config/prometheus.yml**
```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s
  external_labels:
    cluster: 'fr02-production'
    project: 'vietnamese-chatbot'

rule_files:
  - "alert_rules.yml"

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres-exporter:9187']
    scrape_interval: 30s
    metrics_path: /metrics

  - job_name: 'redis'
    static_configs:
      - targets: ['redis-exporter:9121']
    scrape_interval: 30s

  - job_name: 'chroma'
    static_configs:
      - targets: ['chroma:8000']
    metrics_path: '/api/v1/metrics'
    scrape_interval: 30s

  - job_name: 'node'
    static_configs:
      - targets: ['node-exporter:9100']
    scrape_interval: 30s

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

# Recording rules for performance optimization
rule_files:
  - "recording_rules.yml"
```

---

## üîß **SCRIPT TRI·ªÇN KHAI T·ª∞ ƒê·ªòNG**

### **scripts/deploy-fr02-v2.sh**
```bash
#!/bin/bash

# FR-02.1 v2.0 Deployment Script
# Enhanced Dual Database System with Vietnamese Support

set -euo pipefail

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Configuration
PROJECT_NAME="fr02-dual-database-v2"
DEPLOY_DIR="/opt/${PROJECT_NAME}"
BACKUP_DIR="/opt/backups"
LOG_FILE="/var/log/fr02-deployment.log"

# Environment variables
export POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-$(openssl rand -base64 32)}
export GRAFANA_PASSWORD=${GRAFANA_PASSWORD:-$(openssl rand -base64 16)}
export CHROMA_AUTH_TOKEN=${CHROMA_AUTH_TOKEN:-$(openssl rand -hex 32)}

# Logging function
log() {
    echo -e "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a "$LOG_FILE"
}

log_info() {
    log "${BLUE}[INFO]${NC} $1"
}

log_success() {
    log "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    log "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    log "${RED}[ERROR]${NC} $1"
}

# Prerequisites check
check_prerequisites() {
    log_info "Checking prerequisites..."
    
    # Check Docker
    if ! command -v docker &> /dev/null; then
        log_error "Docker is not installed"
        exit 1
    fi
    
    # Check Docker Compose
    if ! command -v docker-compose &> /dev/null; then
        log_error "Docker Compose is not installed"
        exit 1
    fi
    
    # Check available memory
    AVAILABLE_MEMORY=$(free -g | awk '/^Mem:/{print $7}')
    if [ "$AVAILABLE_MEMORY" -lt 8 ]; then
        log_warning "Available memory is ${AVAILABLE_MEMORY}GB. Recommended: 8GB+"
    fi
    
    # Check disk space
    AVAILABLE_DISK=$(df -BG / | awk 'NR==2{print $4}' | sed 's/G//')
    if [ "$AVAILABLE_DISK" -lt 50 ]; then
        log_error "Available disk space is ${AVAILABLE_DISK}GB. Required: 50GB+"
        exit 1
    fi
    
    log_success "Prerequisites check passed"
}

# Create project structure
create_project_structure() {
    log_info "Creating project structure..."
    
    mkdir -p "$DEPLOY_DIR"/{config,scripts,data,logs,backups}
    mkdir -p "$DEPLOY_DIR"/data/{postgres,chroma,redis,prometheus,grafana}
    mkdir -p "$DEPLOY_DIR"/config/{grafana,prometheus,nginx,ssl}
    
    # Set proper permissions
    chmod 755 "$DEPLOY_DIR"
    chmod 700 "$DEPLOY_DIR"/data
    
    log_success "Project structure created"
}

# Setup environment file
setup_environment() {
    log_info "Setting up environment variables..."
    
    cat > "$DEPLOY_DIR/.env" << EOF
# FR-02.1 v2.0 Environment Configuration
PROJECT_NAME=${PROJECT_NAME}
ENVIRONMENT=production

# Database Configuration
POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
POSTGRES_USER=kb_admin
POSTGRES_DB=knowledge_base_v2
POSTGRES_HOST=postgres
POSTGRES_PORT=5432

# Redis Configuration
REDIS_PASSWORD=${REDIS_PASSWORD:-}
REDIS_HOST=redis-master
REDIS_PORT=6379

# ChromaDB Configuration
CHROMA_AUTH_TOKEN=${CHROMA_AUTH_TOKEN}
CHROMA_HOST=chroma
CHROMA_PORT=8000

# Monitoring Configuration
GRAFANA_PASSWORD=${GRAFANA_PASSWORD}
PROMETHEUS_RETENTION=30d

# Security Configuration
JWT_SECRET=${JWT_SECRET:-$(openssl rand -hex 32)}
ENCRYPTION_KEY=${ENCRYPTION_KEY:-$(openssl rand -hex 32)}

# Vietnamese NLP Configuration
VIETNAMESE_NLP_MODEL=pyvi
EMBEDDING_MODEL=Qwen/Qwen3-Embedding-0.6B
EMBEDDING_DIMENSION=1024

# Performance Configuration
MAX_CONNECTIONS=200
WORKER_PROCESSES=4
CACHE_TTL=3600
EOF

    chmod 600 "$DEPLOY_DIR/.env"
    log_success "Environment file created"
}

# Deploy database schema
deploy_database_schema() {
    log_info "Deploying enhanced database schema..."
    
    # Wait for PostgreSQL to be ready
    log_info "Waiting for PostgreSQL to be ready..."
    timeout 120 bash -c 'until docker-compose exec postgres pg_isready -U kb_admin -d knowledge_base_v2; do sleep 5; done'
    
    # Run schema migrations
    docker-compose exec postgres psql -U kb_admin -d knowledge_base_v2 -f /docker-entrypoint-initdb.d/02_schema.sql
    
    # Verify schema deployment
    TABLES_COUNT=$(docker-compose exec postgres psql -U kb_admin -d knowledge_base_v2 -t -c "SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = 'public';")
    
    if [ "$TABLES_COUNT" -ge 15 ]; then
        log_success "Database schema deployed successfully ($TABLES_COUNT tables created)"
    else
        log_error "Database schema deployment failed (only $TABLES_COUNT tables found)"
        exit 1
    fi
}

# Setup vector database
setup_vector_database() {
    log_info "Setting up ChromaDB vector database..."
    
    # Wait for ChromaDB to be ready
    timeout 120 bash -c 'until curl -f http://localhost:8000/api/v1/heartbeat; do sleep 5; done'
    
    # Create collections using Python script
    python3 << EOF
import chromadb
from chromadb.config import Settings
import uuid

try:
    client = chromadb.HttpClient(
        host="localhost",
        port=8000,
        settings=Settings(
            chroma_server_host="localhost",
            chroma_server_port=8000,
            chroma_server_cors_allow_origins="*",
            chroma_server_auth_credentials_provider="chromadb.auth.token.TokenAuthCredentialsProvider",
            chroma_server_auth_credentials="${CHROMA_AUTH_TOKEN}",
            chroma_server_auth_token_transport_header="X-Chroma-Token"
        )
    )
    
    # Create main collection
    collection = client.create_collection(
        name="knowledge_base_v2",
        metadata={
            "description": "Main knowledge base with Vietnamese optimization",
            "embedding_dimension": 1024,
            "embedding_model": "Qwen/Qwen3-Embedding-0.6B",
            "language": "vietnamese",
            "version": "2.0"
        }
    )
    
    # Create department collections
    departments = ['hr', 'it', 'rd', 'sales', 'finance']
    for dept in departments:
        dept_collection = client.create_collection(
            name=f"dept_{dept}_v2",
            metadata={
                "description": f"{dept.upper()} department knowledge base",
                "department": dept,
                "parent_collection": "knowledge_base_v2",
                "embedding_dimension": 1024
            }
        )
    
    print("‚úÖ ChromaDB collections created successfully")
    
except Exception as e:
    print(f"‚ùå Error creating ChromaDB collections: {e}")
    exit(1)
EOF

    log_success "Vector database setup completed"
}

# Setup monitoring
setup_monitoring() {
    log_info "Setting up monitoring stack..."
    
    # Create Grafana datasources configuration
    cat > "$DEPLOY_DIR/config/grafana/datasources/datasources.yml" << EOF
apiVersion: 1

datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
    editable: true
  
  - name: PostgreSQL
    type: postgres
    url: postgres:5432
    database: knowledge_base_v2
    user: kb_admin
    secureJsonData:
      password: '${POSTGRES_PASSWORD}'
    jsonData:
      sslmode: disable
      postgresVersion: 1500
      timescaledb: false
EOF

    # Create alert rules
    cat > "$DEPLOY_DIR/config/prometheus/alert_rules.yml" << EOF
groups:
  - name: fr02_alerts
    rules:
      - alert: PostgreSQLDown
        expr: pg_up == 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "PostgreSQL instance is down"
          description: "PostgreSQL instance {{ \$labels.instance }} has been down for more than 5 minutes."

      - alert: ChromaDBDown
        expr: up{job="chroma"} == 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "ChromaDB instance is down"
          description: "ChromaDB instance has been down for more than 5 minutes."

      - alert: HighDatabaseConnections
        expr: pg_stat_activity_count > 150
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High database connections"
          description: "PostgreSQL has {{ \$value }} active connections (>150)."

      - alert: RedisMemoryHigh
        expr: redis_memory_used_bytes / redis_memory_max_bytes > 0.8
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Redis memory usage high"
          description: "Redis memory usage is above 80%."
EOF

    log_success "Monitoring setup completed"
}

# Run health checks
run_health_checks() {
    log_info "Running comprehensive health checks..."
    
    # Check PostgreSQL
    if docker-compose exec postgres pg_isready -U kb_admin -d knowledge_base_v2 > /dev/null 2>&1; then
        log_success "‚úÖ PostgreSQL is healthy"
    else
        log_error "‚ùå PostgreSQL health check failed"
        return 1
    fi
    
    # Check ChromaDB
    if curl -f http://localhost:8000/api/v1/heartbeat > /dev/null 2>&1; then
        log_success "‚úÖ ChromaDB is healthy"
    else
        log_error "‚ùå ChromaDB health check failed"
        return 1
    fi
    
    # Check Redis
    if docker-compose exec redis-master redis-cli ping | grep PONG > /dev/null 2>&1; then
        log_success "‚úÖ Redis is healthy"
    else
        log_error "‚ùå Redis health check failed"
        return 1
    fi
    
    # Check Prometheus
    if curl -f http://localhost:9090/-/healthy > /dev/null 2>&1; then
        log_success "‚úÖ Prometheus is healthy"
    else
        log_error "‚ùå Prometheus health check failed"
        return 1
    fi
    
    # Check Grafana
    if curl -f http://localhost:3000/api/health > /dev/null 2>&1; then
        log_success "‚úÖ Grafana is healthy"
    else
        log_error "‚ùå Grafana health check failed"
        return 1
    fi
    
    log_success "All health checks passed!"
}

# Performance testing
run_performance_tests() {
    log_info "Running performance tests..."
    
    python3 << EOF
import asyncio
import asyncpg
import chromadb
import time
import numpy as np
from concurrent.futures import ThreadPoolExecutor
import statistics

async def test_database_performance():
    # PostgreSQL connection test
    conn = await asyncpg.connect(
        "postgresql://kb_admin:${POSTGRES_PASSWORD}@localhost:5432/knowledge_base_v2"
    )
    
    # Test query performance
    start_time = time.time()
    result = await conn.fetchval("SELECT COUNT(*) FROM documents_metadata_v2")
    query_time = time.time() - start_time
    
    print(f"PostgreSQL query time: {query_time:.3f}s")
    
    await conn.close()
    
    # ChromaDB performance test
    client = chromadb.HttpClient(host="localhost", port=8000)
    
    try:
        collection = client.get_collection("knowledge_base_v2")
        
        # Test embedding search
        query_embedding = np.random.rand(1024).tolist()
        start_time = time.time()
        results = collection.query(
            query_embeddings=[query_embedding],
            n_results=5
        )
        search_time = time.time() - start_time
        
        print(f"ChromaDB search time: {search_time:.3f}s")
        
        if query_time < 0.1 and search_time < 0.5:
            print("‚úÖ Performance tests passed")
            return True
        else:
            print("‚ùå Performance tests failed")
            return False
            
    except Exception as e:
        print(f"‚ùå ChromaDB test failed: {e}")
        return False

# Run the test
if asyncio.run(test_database_performance()):
    exit(0)
else:
    exit(1)
EOF

    if [ $? -eq 0 ]; then
        log_success "Performance tests passed"
    else
        log_error "Performance tests failed"
        return 1
    fi
}

# Setup backup strategy
setup_backup_strategy() {
    log_info "Setting up backup strategy..."
    
    # Create backup script
    cat > "$DEPLOY_DIR/scripts/backup.sh" << 'EOF'
#!/bin/bash

BACKUP_DIR="/opt/backups"
DATE=$(date +%Y%m%d_%H%M%S)

# PostgreSQL backup
pg_dump -h localhost -U kb_admin -d knowledge_base_v2 \
    | gzip > "$BACKUP_DIR/postgres_backup_$DATE.sql.gz"

# ChromaDB backup
docker-compose exec chroma /bin/sh -c "tar czf - /chroma/chroma" \
    > "$BACKUP_DIR/chroma_backup_$DATE.tar.gz"

# Redis backup
docker-compose exec redis-master redis-cli --rdb - \
    > "$BACKUP_DIR/redis_backup_$DATE.rdb"

# Cleanup old backups (keep last 7 days)
find "$BACKUP_DIR" -name "*backup*" -mtime +7 -delete

echo "Backup completed: $DATE"
EOF

    chmod +x "$DEPLOY_DIR/scripts/backup.sh"
    
    # Setup cron job for daily backups
    (crontab -l 2>/dev/null; echo "0 2 * * * $DEPLOY_DIR/scripts/backup.sh") | crontab -
    
    log_success "Backup strategy configured"
}

# Main deployment function
main() {
    log_info "Starting FR-02.1 v2.0 deployment..."
    
    # Pre-deployment
    check_prerequisites
    create_project_structure
    setup_environment
    
    # Change to deployment directory
    cd "$DEPLOY_DIR"
    
    # Copy configuration files (assuming they exist in current directory)
    if [ -f "docker-compose.yml" ]; then
        cp docker-compose.yml "$DEPLOY_DIR/"
    else
        log_error "docker-compose.yml not found"
        exit 1
    fi
    
    # Deploy services
    log_info "Starting Docker services..."
    docker-compose up -d
    
    # Wait for services to be ready
    sleep 30
    
    # Post-deployment setup
    deploy_database_schema
    setup_vector_database
    setup_monitoring
    setup_backup_strategy
    
    # Final checks
    run_health_checks
    run_performance_tests
    
    # Display access information
    log_success "üéâ FR-02.1 v2.0 deployment completed successfully!"
    echo ""
    echo "üìä Service Access URLs:"
    echo "  PostgreSQL: localhost:5432 (user: kb_admin)"
    echo "  PgBouncer: localhost:6432"
    echo "  ChromaDB: http://localhost:8000"
    echo "  Redis: localhost:6379"
    echo "  Prometheus: http://localhost:9090"
    echo "  Grafana: http://localhost:3000 (admin/${GRAFANA_PASSWORD})"
    echo ""
    echo "üìÅ Important Directories:"
    echo "  Project: $DEPLOY_DIR"
    echo "  Data: $DEPLOY_DIR/data"
    echo "  Logs: $DEPLOY_DIR/logs"
    echo "  Backups: $BACKUP_DIR"
    echo ""
    echo "üîê Generated Passwords saved in: $DEPLOY_DIR/.env"
    echo ""
    echo "üìñ Next Steps:"
    echo "  1. Access Grafana and import dashboards"
    echo "  2. Configure application connections"
    echo "  3. Load initial data"
    echo "  4. Run integration tests"
}

# Error handling
trap 'log_error "Deployment failed at line $LINENO"' ERR

# Run main function
main "$@"
```

### **scripts/enhanced-schema.sql**
```sql
-- FR-02.1 v2.0 Enhanced Database Schema
-- Compatible with Vietnamese chatbot system
-- Includes all updates from schema_01_init_database.sql

-- Enable required extensions
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pg_trgm";
CREATE EXTENSION IF NOT EXISTS "btree_gin";
CREATE EXTENSION IF NOT EXISTS "unaccent";

-- Create Vietnamese text processing functions
CREATE OR REPLACE FUNCTION normalize_vietnamese_text(input_text TEXT)
RETURNS TEXT AS $$
BEGIN
    -- Basic Vietnamese text normalization
    RETURN lower(
        regexp_replace(
            regexp_replace(
                regexp_replace(input_text, '[√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ]', 'a', 'gi'),
                '[√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ]', 'e', 'gi'
            ),
            '[√¨√≠·ªã·ªâƒ©]', 'i', 'gi'
        )
    );
END;
$$ LANGUAGE plpgsql IMMUTABLE;

-- Create function to extract contact information
CREATE OR REPLACE FUNCTION extract_emails_from_text(input_text TEXT)
RETURNS TEXT[] AS $$
DECLARE
    email_pattern TEXT := '[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}';
    result TEXT[];
BEGIN
    SELECT array_agg(DISTINCT match)
    INTO result
    FROM regexp_matches(input_text, email_pattern, 'gi') AS match;
    
    RETURN COALESCE(result, '{}');
END;
$$ LANGUAGE plpgsql IMMUTABLE;

-- Insert the complete enhanced schema here
\i /docker-entrypoint-initdb.d/01_init.sql

-- Create performance monitoring views
CREATE OR REPLACE VIEW vw_system_performance AS
SELECT 
    'postgresql' as service,
    COUNT(*) as total_documents,
    SUM(chunk_count) as total_chunks,
    AVG(EXTRACT(EPOCH FROM updated_at - created_at)) as avg_processing_time
FROM documents_metadata_v2
UNION ALL
SELECT 
    'vector_database' as service,
    COUNT(*) as total_documents,
    COUNT(*) as total_chunks,
    0 as avg_processing_time
FROM document_chunks_enhanced;

-- Create monitoring table for system metrics
CREATE TABLE IF NOT EXISTS system_health_metrics (
    metric_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    service_name VARCHAR(50) NOT NULL,
    metric_type VARCHAR(50) NOT NULL,
    metric_value DOUBLE PRECISION NOT NULL,
    timestamp TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    metadata JSONB DEFAULT '{}'
);

-- Insert initial system configuration
INSERT INTO system_health_metrics (service_name, metric_type, metric_value, metadata) VALUES
('postgresql', 'version', 15.0, '{"description": "PostgreSQL version"}'),
('chroma', 'embedding_dimension', 1024, '{"model": "Qwen/Qwen3-Embedding-0.6B"}'),
('redis', 'version', 7.0, '{"description": "Redis version"}');

-- Create indexes for performance
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_system_health_service_time 
ON system_health_metrics(service_name, timestamp DESC);

CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_documents_vietnamese_search 
ON documents_metadata_v2 USING GIN(to_tsvector('simple', search_text_normalized));

-- Success notification
DO $$ BEGIN
    RAISE NOTICE '====================================';
    RAISE NOTICE 'FR-02.1 v2.0 Enhanced Schema deployed successfully!';
    RAISE NOTICE '====================================';
    RAISE NOTICE 'New features in v2.0:';
    RAISE NOTICE '‚úì Vietnamese language optimization';
    RAISE NOTICE '‚úì Enhanced chunking support';
    RAISE NOTICE '‚úì Qwen embedding model support';
    RAISE NOTICE '‚úì Advanced monitoring capabilities';
    RAISE NOTICE '‚úì Performance optimization';
    RAISE NOTICE '‚úì Production-ready configuration';
    RAISE NOTICE '';
    RAISE NOTICE 'Database is ready for Vietnamese chatbot v2.0!';
    RAISE NOTICE '====================================';
END $$;
```

---

## üîç **TESTING & VALIDATION**

### **tests/test_dual_database_v2.py**
```python
import pytest
import asyncio
import asyncpg
import chromadb
import redis
import numpy as np
from typing import Dict, Any
import uuid
import time

@pytest.mark.asyncio
class TestDualDatabaseV2:
    
    @pytest.fixture
    async def postgres_connection(self):
        """Setup PostgreSQL connection for tests"""
        conn = await asyncpg.connect(
            "postgresql://kb_admin:password@localhost:5432/knowledge_base_v2"
        )
        yield conn
        await conn.close()
    
    @pytest.fixture
    def chroma_client(self):
        """Setup ChromaDB client for tests"""
        return chromadb.HttpClient(host="localhost", port=8000)
    
    @pytest.fixture
    def redis_client(self):
        """Setup Redis client for tests"""
        return redis.Redis(host="localhost", port=6379, decode_responses=True)
    
    async def test_postgresql_vietnamese_support(self, postgres_connection):
        """Test PostgreSQL Vietnamese language support"""
        test_text = "H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng h·ªá th·ªëng qu·∫£n l√Ω t√†i li·ªáu"
        
        # Test Vietnamese text normalization
        normalized = await postgres_connection.fetchval(
            "SELECT normalize_vietnamese_text($1)", test_text
        )
        
        assert "huong dan" in normalized
        assert "he thong" in normalized
        
        # Test document insertion with Vietnamese content
        doc_id = await postgres_connection.fetchval("""
            INSERT INTO documents_metadata_v2 
            (title, content, document_type, access_level, department_owner, author)
            VALUES ($1, $2, 'manual', 'employee_only', 'IT', 'Test Author')
            RETURNING document_id
        """, test_text, f"N·ªôi dung t√†i li·ªáu: {test_text}")
        
        assert doc_id is not None
        
        # Cleanup
        await postgres_connection.execute(
            "DELETE FROM documents_metadata_v2 WHERE document_id = $1", doc_id
        )
    
    def test_chroma_vietnamese_embeddings(self, chroma_client):
        """Test ChromaDB with Vietnamese content"""
        try:
            collection = chroma_client.get_collection("knowledge_base_v2")
        except:
            collection = chroma_client.create_collection(
                name="knowledge_base_v2",
                metadata={"embedding_dimension": 1024}
            )
        
        # Test Vietnamese document addition
        vietnamese_docs = [
            {
                'id': 'test_vi_1',
                'content': 'H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng h·ªá th·ªëng chatbot ti·∫øng Vi·ªát',
                'embedding': np.random.rand(1024).tolist(),
                'metadata': {'language': 'vi', 'type': 'guide'}
            },
            {
                'id': 'test_vi_2',
                'content': 'Ch√≠nh s√°ch l√†m vi·ªác t·ª´ xa c·ªßa c√¥ng ty',
                'embedding': np.random.rand(1024).tolist(),
                'metadata': {'language': 'vi', 'type': 'policy'}
            }
        ]
        
        collection.add(
            ids=[doc['id'] for doc in vietnamese_docs],
            embeddings=[doc['embedding'] for doc in vietnamese_docs],
            documents=[doc['content'] for doc in vietnamese_docs],
            metadatas=[doc['metadata'] for doc in vietnamese_docs]
        )
        
        # Test Vietnamese search
        query_embedding = np.random.rand(1024).tolist()
        results = collection.query(
            query_embeddings=[query_embedding],
            n_results=2,
            where={"language": "vi"}
        )
        
        assert len(results['ids'][0]) == 2
        assert all('vi' in doc for doc in results['documents'][0])
        
        # Cleanup
        collection.delete(ids=['test_vi_1', 'test_vi_2'])
    
    def test_redis_cache_performance(self, redis_client):
        """Test Redis caching performance"""
        # Test basic operations
        redis_client.set('test_key', 'test_value', ex=300)
        assert redis_client.get('test_key') == 'test_value'
        
        # Test Vietnamese text caching
        vietnamese_text = "H·ªá th·ªëng qu·∫£n l√Ω t√†i li·ªáu ti·∫øng Vi·ªát"
        redis_client.hset('vietnamese_cache', 'test_doc', vietnamese_text)
        
        cached_text = redis_client.hget('vietnamese_cache', 'test_doc')
        assert cached_text == vietnamese_text
        
        # Test performance
        start_time = time.time()
        for i in range(1000):
            redis_client.set(f'perf_test_{i}', f'value_{i}')
        operation_time = time.time() - start_time
        
        assert operation_time < 1.0  # Should complete in under 1 second
        
        # Cleanup
        redis_client.delete('test_key')
        redis_client.delete('vietnamese_cache')
        for i in range(1000):
            redis_client.delete(f'perf_test_{i}')
    
    async def test_integrated_search_workflow(self, postgres_connection, chroma_client):
        """Test complete search workflow integration"""
        # Create test user
        user_id = await postgres_connection.fetchval("""
            INSERT INTO users (username, email, password_hash, user_level, department)
            VALUES ('test_integration', 'test@integration.com', 'hash123', 'EMPLOYEE', 'IT')
            RETURNING user_id
        """)
        
        # Create test document
        doc_id = await postgres_connection.fetchval("""
            INSERT INTO documents_metadata_v2 
            (title, content, document_type, access_level, department_owner, author)
            VALUES ('Test Integration Doc', 'Content for integration testing', 
                    'technical_guide', 'employee_only', 'IT', 'Test Author')
            RETURNING document_id
        """)
        
        # Create test chunks
        chunk_ids = []
        for i in range(3):
            chunk_id = await postgres_connection.fetchval("""
                INSERT INTO document_chunks_enhanced 
                (document_id, chunk_content, chunk_position, chunk_size_tokens, 
                 semantic_boundary, chunk_method, embedding_model)
                VALUES ($1, $2, $3, 100, true, 'semantic_boundary', 'Qwen/Qwen3-Embedding-0.6B')
                RETURNING chunk_id
            """, doc_id, f'Test chunk content {i}', i)
            chunk_ids.append(chunk_id)
        
        # Add to ChromaDB
        try:
            collection = chroma_client.get_collection("knowledge_base_v2")
        except:
            collection = chroma_client.create_collection("knowledge_base_v2")
        
        embeddings = [np.random.rand(1024).tolist() for _ in range(3)]
        collection.add(
            ids=[f"chunk_{i}" for i in range(3)],
            embeddings=embeddings,
            documents=[f'Test chunk content {i}' for i in range(3)],
            metadatas=[{
                'document_id': str(doc_id),
                'chunk_id': str(chunk_ids[i]),
                'access_level': 'employee_only'
            } for i in range(3)]
        )
        
        # Test search
        query_embedding = np.random.rand(1024).tolist()
        search_results = collection.query(
            query_embeddings=[query_embedding],
            n_results=3,
            where={"access_level": "employee_only"}
        )
        
        assert len(search_results['ids'][0]) == 3
        
        # Verify database consistency
        db_chunks = await postgres_connection.fetch(
            "SELECT chunk_id FROM document_chunks_enhanced WHERE document_id = $1", doc_id
        )
        assert len(db_chunks) == 3
        
        # Cleanup
        collection.delete(ids=[f"chunk_{i}" for i in range(3)])
        await postgres_connection.execute("DELETE FROM users WHERE user_id = $1", user_id)
        await postgres_connection.execute("DELETE FROM documents_metadata_v2 WHERE document_id = $1", doc_id)
    
    async def test_performance_benchmarks(self, postgres_connection, chroma_client):
        """Test system performance benchmarks"""
        # Database query performance
        start_time = time.time()
        result = await postgres_connection.fetchval("SELECT COUNT(*) FROM documents_metadata_v2")
        db_query_time = time.time() - start_time
        
        assert db_query_time < 0.1  # Should be under 100ms
        
        # Vector search performance
        try:
            collection = chroma_client.get_collection("knowledge_base_v2")
            
            query_embedding = np.random.rand(1024).tolist()
            start_time = time.time()
            
            results = collection.query(
                query_embeddings=[query_embedding],
                n_results=5
            )
            
            vector_search_time = time.time() - start_time
            assert vector_search_time < 0.5  # Should be under 500ms
            
        except Exception as e:
            pytest.skip(f"ChromaDB collection not available: {e}")

if __name__ == "__main__":
    pytest.main([__file__, "-v", "--tb=short"])
```

---

## üìä **MONITORING & ALERTING**

### **Grafana Dashboard Configuration**
```json
{
  "dashboard": {
    "id": null,
    "title": "FR-02.1 v2.0 Dual Database System",
    "tags": ["fr-02.1", "vietnamese-chatbot", "databases"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "title": "System Overview",
        "type": "stat",
        "targets": [
          {
            "expr": "up{job=\"postgres\"} + up{job=\"chroma\"} + up{job=\"redis\"}",
            "refId": "A"
          }
        ],
        "gridPos": {"h": 6, "w": 8, "x": 0, "y": 0},
        "fieldConfig": {
          "defaults": {
            "color": {"mode": "thresholds"},
            "thresholds": {
              "steps": [
                {"color": "red", "value": 0},
                {"color": "yellow", "value": 2},
                {"color": "green", "value": 3}
              ]
            }
          }
        }
      },
      {
        "id": 2,
        "title": "PostgreSQL Connections",
        "type": "graph",
        "targets": [
          {
            "expr": "pg_stat_activity_count",
            "refId": "A"
          }
        ],
        "gridPos": {"h": 6, "w": 8, "x": 8, "y": 0}
      },
      {
        "id": 3,
        "title": "ChromaDB Response Time",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{job=\"chroma\"}[5m]))",
            "refId": "A"
          }
        ],
        "gridPos": {"h": 6, "w": 8, "x": 16, "y": 0}
      },
      {
        "id": 4,
        "title": "Redis Memory Usage",
        "type": "singlestat",
        "targets": [
          {
            "expr": "redis_memory_used_bytes / redis_memory_max_bytes * 100",
            "refId": "A"
          }
        ],
        "gridPos": {"h": 6, "w": 8, "x": 0, "y": 6}
      },
      {
        "id": 5,
        "title": "Database Query Performance",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(pg_stat_database_tup_fetched[5m])",
            "refId": "A",
            "legendFormat": "PostgreSQL Queries/sec"
          }
        ],
        "gridPos": {"h": 6, "w": 16, "x": 8, "y": 6}
      }
    ],
    "time": {"from": "now-1h", "to": "now"},
    "refresh": "10s"
  }
}
```

---

## üöÄ **PRODUCTION DEPLOYMENT GUIDE**

### **Quick Start Commands**
```bash
# 1. Clone and setup
git clone <repository-url> fr02-dual-database-v2
cd fr02-dual-database-v2

# 2. Set environment variables
export POSTGRES_PASSWORD=$(openssl rand -base64 32)
export GRAFANA_PASSWORD=$(openssl rand -base64 16)
export CHROMA_AUTH_TOKEN=$(openssl rand -hex 32)

# 3. Deploy with single command
chmod +x scripts/deploy-fr02-v2.sh
sudo ./scripts/deploy-fr02-v2.sh

# 4. Verify deployment
curl -f http://localhost:8000/api/v1/heartbeat  # ChromaDB
curl -f http://localhost:9090/-/healthy        # Prometheus
curl -f http://localhost:3000/api/health       # Grafana
```

### **Manual Deployment Steps**
```bash
# Start services
docker-compose up -d

# Wait for services
sleep 60

# Verify PostgreSQL
docker-compose exec postgres pg_isready -U kb_admin -d knowledge_base_v2

# Create ChromaDB collections
python3 scripts/setup_chroma_collections.py

# Import Grafana dashboards
curl -X POST http://admin:${GRAFANA_PASSWORD}@localhost:3000/api/dashboards/db \
  -H "Content-Type: application/json" \
  -d @config/grafana/fr02-dashboard.json

# Test system
python3 -m pytest tests/test_dual_database_v2.py -v
```

---

## üìã **CHECKLIST HO√ÄN TH√ÄNH**

### **Infrastructure Setup**
- [ ] Docker v√† Docker Compose ƒë∆∞·ª£c c√†i ƒë·∫∑t
- [ ] M√¥i tr∆∞·ªùng Python 3.10+ v·ªõi Vietnamese NLP packages
- [ ] Minimum 32GB RAM v√† 1TB storage available
- [ ] Network ports (5432, 8000, 6379, 9090, 3000) available

### **Database Deployment**
- [ ] PostgreSQL 15 container running v·ªõi enhanced schema
- [ ] PgBouncer connection pooling configured
- [ ] All 15+ tables created successfully
- [ ] Vietnamese text processing functions working
- [ ] Sample data inserted and verified

### **Vector Database Setup**
- [ ] ChromaDB container running v·ªõi Qwen embeddings support
- [ ] Knowledge base collections created (main + department-specific)
- [ ] Embedding dimension set to 1024
- [ ] Authentication v·ªõi token configured
- [ ] Test embeddings stored and searchable

### **Cache Layer**
- [ ] Redis cluster running (master + replica)
- [ ] RedisSearch module loaded for Vietnamese text
- [ ] Cache policies configured (LRU, TTL)
- [ ] Performance metrics collection enabled

### **Monitoring Stack**
- [ ] Prometheus collecting metrics from all services
- [ ] Grafana dashboards imported and functional
- [ ] Alert rules configured v√† tested
- [ ] Node exporter collecting system metrics
- [ ] Service health checks passing

### **Performance Validation**
- [ ] PostgreSQL queries < 100ms average
- [ ] ChromaDB searches < 500ms average
- [ ] Redis operations < 10ms average
- [ ] System supports 100+ concurrent users
- [ ] Memory usage within acceptable limits

### **Security Configuration**
- [ ] Database passwords generated v√† secured
- [ ] ChromaDB authentication enabled
- [ ] Network access properly restricted
- [ ] SSL/TLS certificates configured (if applicable)
- [ ] Audit logging enabled

### **Backup v√† Recovery**
- [ ] Automated backup scripts configured
- [ ] Daily backup cron jobs scheduled
- [ ] Backup retention policy implemented
- [ ] Recovery procedures documented v√† tested

### **Documentation**
- [ ] Configuration files documented
- [ ] API endpoints documented
- [ ] Troubleshooting guide available
- [ ] Performance tuning guide available
- [ ] Operational procedures documented

---

## üéØ **PERFORMANCE TARGETS v2.0**

| Metric | Target | Expected | Monitoring |
|--------|--------|----------|------------|
| PostgreSQL Query Response | < 100ms | 20-80ms | Prometheus + Grafana |
| ChromaDB Search Time | < 500ms | 100-300ms | Application metrics |
| Redis Cache Hit Rate | > 90% | 95%+ | Redis exporter |
| System Uptime | > 99.5% | 99.9%+ | Health checks |
| Concurrent Users | 100+ | 200+ | Load testing |
| Storage Capacity | 100k+ chunks | 500k+ chunks | Database monitoring |
| Vietnamese Processing | 100% support | Optimized | Custom metrics |

---

## üìö **KI·∫æN TH·ª®C THAM KH·∫¢O**

### **Vietnamese NLP Resources**
- **pyvi**: Vietnamese word segmentation - https://github.com/trungtv/pyvi
- **underthesea**: Vietnamese NLP toolkit - https://github.com/undertheseanlp/underthesea
- **Qwen Embeddings**: Multilingual embedding model - https://huggingface.co/Qwen

### **Database Optimization**
- **PostgreSQL 15 Performance**: Tuning parameters for production
- **ChromaDB Scaling**: Vector database optimization strategies
- **Redis Clustering**: High availability v√† performance

### **Monitoring Best Practices**
- **Prometheus**: Metrics collection v√† storage
- **Grafana**: Visualization v√† alerting
- **Performance Engineering**: Database monitoring strategies

---

## üîß **TROUBLESHOOTING COMMON ISSUES**

### **PostgreSQL Connection Issues**
```bash
# Check container status
docker-compose ps postgres

# Check logs
docker-compose logs postgres

# Test connection
docker-compose exec postgres psql -U kb_admin -d knowledge_base_v2 -c "SELECT version();"

# Reset if needed
docker-compose down
sudo rm -rf data/postgres/*
docker-compose up -d postgres
```

### **ChromaDB Performance Issues**
```bash
# Check ChromaDB health
curl http://localhost:8000/api/v1/heartbeat

# Monitor memory usage
docker stats fr02-chroma-v2

# Clear collections if needed
python3 -c "
import chromadb
client = chromadb.HttpClient(host='localhost', port=8000)
collections = client.list_collections()
for col in collections:
    print(f'Collection: {col.name}, Count: {col.count()}')
"
```

### **Redis Memory Issues**
```bash
# Check Redis memory
docker-compose exec redis-master redis-cli info memory

# Clear cache if needed
docker-compose exec redis-master redis-cli flushall

# Monitor key distribution
docker-compose exec redis-master redis-cli --bigkeys
```

---

## ‚úÖ **K·∫æT QU·∫¢ MONG ƒê·ª¢I**

Sau khi ho√†n th√†nh FR-02.1 v2.0, h·ªá th·ªëng s·∫Ω cung c·∫•p:

### **Core Infrastructure**
- **Dual Database System** ho·∫°t ƒë·ªông ·ªïn ƒë·ªãnh v·ªõi PostgreSQL + ChromaDB
- **Vietnamese Language Support** t·ªëi ∆∞u v·ªõi pyvi v√† underthesea
- **High Performance** ƒë√°p ·ª©ng 100+ concurrent users
- **Production-Ready** v·ªõi monitoring, backup, v√† security

### **Enhanced Capabilities**
- **Advanced Chunking** v·ªõi semantic boundary detection
- **Quality Metrics** cho Vietnamese text processing
- **Comprehensive Monitoring** v·ªõi real-time dashboards
- **Automated Operations** v·ªõi backup v√† recovery

### **Integration Ready**
- **API Endpoints** s·∫µn s√†ng cho FR-04 RAG Core Engine
- **Authentication** t√≠ch h·ª£p v·ªõi user management system
- **Scalability** support cho future expansion
- **Documentation** ƒë·∫ßy ƒë·ªß cho development team

**Deliverable ch√≠nh: H·ªá th·ªëng c∆° s·ªü d·ªØ li·ªáu k√©p production-ready, t·ªëi ∆∞u cho ti·∫øng Vi·ªát, v·ªõi kh·∫£ nƒÉng scale v√† monitoring comprehensive.**

---

**Document Status:** ‚úÖ READY FOR PRODUCTION DEPLOYMENT  
**Version:** 2.0  
**Last Updated:** September 12, 2025  
**Next Phase:** FR-04 RAG Core Engine Integration  
**Approval Required:** Technical Lead, Database Administrator, DevOps Team
