```txt
TÃ´i chá»‘t láº¡i cÃ¡c Ä‘iá»ƒm máº¥u chá»‘t nhá»¯ng pháº§n chÃºng ta Ä‘Ã£ lÃ m, báº¡n xem Ä‘Ãºng khÃ´ng nhÃ©. HÃ£y bá»• sung ngáº¯n gá»n vÃ o nhá»¯ng Ä‘iá»ƒm tÃ´i chÆ°a hiá»ƒu.
- Database PostgreSQL Ä‘Ã£ táº¡o cÃ³ 5 báº£ng chÃ­nh, 7 báº£ng cÃ²n láº¡i báº¡n kÃª ra tÆ°Æ¡ng tá»± cho tÃ´i nhÃ©
1. documents_metadata_v2
ÄÃ¢y lÃ  báº£ng quan trá»ng nháº¥t, dÃ¹ng Ä‘á»ƒ lÆ°u trá»¯ thÃ´ng tin tá»•ng quan (siÃªu dá»¯ liá»‡u) vá» má»—i tÃ i liá»‡u gá»‘c.
Chá»©c nÄƒng chÃ­nh: Quáº£n lÃ½ cÃ¡c tÃ i liá»‡u nhÆ° chÃ­nh sÃ¡ch, quy trÃ¬nh, hÆ°á»›ng dáº«n...
2. document_chunks_enhanced
Báº£ng nÃ y chá»©a cÃ¡c "máº©u" hoáº·c "Ä‘oáº¡n" vÄƒn báº£n (chunks) Ä‘Æ°á»£c chia nhá» ra tá»« tÃ i liá»‡u gá»‘c trong báº£ng documents_metadata_v2.
Chá»©c nÄƒng chÃ­nh: Chia nhá» tÃ i liá»‡u Ä‘á»ƒ dá»… dÃ ng xá»­ lÃ½, nhÃºng (embedding) vÃ  tÃ¬m kiáº¿m ngá»¯ nghÄ©a (semantic search).
 3. document_bm25_index
Báº£ng nÃ y há»— trá»£ cho viá»‡c tÃ¬m kiáº¿m tá»« khÃ³a truyá»n thá»‘ng báº±ng thuáº­t toÃ¡n BM25.
Chá»©c nÄƒng chÃ­nh: LÆ°u trá»¯ chá»‰ má»¥c (index) cÃ¡c tá»« khÃ³a Ä‘á»ƒ tÄƒng tá»‘c Ä‘á»™ vÃ  Ä‘á»™ chÃ­nh xÃ¡c cá»§a tÃ¬m kiáº¿m lai (hybrid search), káº¿t há»£p cáº£ tÃ¬m kiáº¿m tá»« khÃ³a vÃ  tÃ¬m kiáº¿m ngá»¯ nghÄ©a.
  4. rag_pipeline_sessions
Báº£ng nÃ y dÃ¹ng Ä‘á»ƒ theo dÃµi vÃ  ghi láº¡i lá»‹ch sá»­ cÃ¡c phiÃªn truy váº¥n cá»§a ngÆ°á»i dÃ¹ng hoáº·c há»‡ thá»‘ng.
Chá»©c nÄƒng chÃ­nh: GiÃ¡m sÃ¡t hiá»‡u suáº¥t cá»§a há»‡ thá»‘ng RAG (Retrieval-Augmented Generation), phÃ¢n tÃ­ch cÃ¡c cÃ¢u há»i vÃ  cháº¥t lÆ°á»£ng cÃ¢u tráº£ lá»i.
5. vietnamese_text_analysis
Báº£ng nÃ y Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘áº·c biá»‡t Ä‘á»ƒ lÆ°u káº¿t quáº£ phÃ¢n tÃ­ch sÃ¢u vá» vÄƒn báº£n tiáº¿ng Viá»‡t.
Chá»©c nÄƒng chÃ­nh: Há»— trá»£ xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn (NLP) cho tiáº¿ng Viá»‡t, nhÆ° tÃ¡ch tá»«, gáº¯n tháº» tá»« loáº¡i, nháº­n diá»‡n danh tá»« riÃªng...
ChromaDB : má»™t báº£n sao cá»§a PostgreSQL Ä‘á»ƒ seach cho nhanh, nhÆ°ng seach theo cÃ¡ch nÃ o dÃ¹ng ChromaDB thÃ¬ tÃ´i chÆ°a rÃµ.
- Document embeddings:Ä‘Ã¢y lÃ  gÃ¬?
- Chunk embeddings:Ä‘Ã¢y lÃ  gÃ¬?
- HNSW index overhead:Ä‘Ã¢y lÃ  gÃ¬?
Redis, bá»™ nhá»› cache lÆ°u á»Ÿ Ä‘Ã¢u? dá»¯ liá»‡u Ä‘Æ°á»£c tÃ¬m kiáº¿m nhiá»u sáº½ cÃ³ chá»‰ sá»‘ popular Ä‘á»ƒ lÆ°u vÃ o Ä‘Ã¢y, chá»‰ sá»‘ popular nÃ y lÆ°u á»Ÿ PostgreSQL Ä‘Ãºng khÃ´ng?
- Session data : Ä‘Ã¢y lÃ  dá»¯ liá»‡u chat táº¡m thá»i?
- Embedding cache: Ä‘Ã¢y lÃ  gÃ¬?
- Search results cache: Ä‘Ã¢y lÃ  nhá»¯ng dá»¯ liá»‡u hay Ä‘Æ°á»£c tÃ¬m Ä‘Ãºng khÃ´ng?
- NLP processing cache:Ä‘Ã¢y lÃ  gÃ¬?VÃ  JSONL náº±m á»Ÿ Ä‘Ã¢u trong há»‡ thá»‘ng nÃ y: Data Flow Between Components
```

Báº¡n hiá»ƒu ráº¥t tá»‘t! TÃ´i sáº½ bá»• sung vÃ  lÃ m rÃµ nhá»¯ng Ä‘iá»ƒm báº¡n chÆ°a rÃµ:

## ğŸ˜ **POSTGRESQL - 12 Báº¢NG HOÃ€N CHá»ˆNH**

### **âœ… 5 báº£ng chÃ­nh báº¡n Ä‘Ã£ hiá»ƒu:**
1. **documents_metadata_v2** - Master documents
2. **document_chunks_enhanced** - Text chunks  
3. **document_bm25_index** - Keyword search index
4. **rag_pipeline_sessions** - Query tracking
5. **vietnamese_text_analysis** - Vietnamese NLP

### **ğŸ“‹ 7 báº£ng cÃ²n láº¡i:**

**6. context_refinement_log**
- **Chá»©c nÄƒng**: Theo dÃµi viá»‡c "tinh chá»‰nh ngá»¯ cáº£nh" - khi há»‡ thá»‘ng láº¥y quÃ¡ nhiá»u chunk, cáº§n nÃ©n láº¡i
- **Táº¡i sao cáº§n**: RAG thÆ°á»ng láº¥y 10-20 chunks, nhÆ°ng LLM chá»‰ cáº§n 3-5 chunks tá»‘t nháº¥t
- **VD**: Query "nghá»‰ phÃ©p" â†’ tÃ¬m Ä‘Æ°á»£c 15 chunks â†’ tinh chá»‰nh cÃ²n 5 chunks relevant nháº¥t

**7. knowledge_graph_edges**  
- **Chá»©c nÄƒng**: LÆ°u má»‘i quan há»‡ giá»¯a cÃ¡c chunks (chunk A tham chiáº¿u Ä‘áº¿n chunk B)
- **Táº¡i sao cáº§n**: TÃ i liá»‡u cÃ³ thá»ƒ tham chiáº¿u láº«n nhau ("theo quy Ä‘á»‹nh táº¡i má»¥c 2.1")
- **VD**: Chunk "quy trÃ¬nh nghá»‰ phÃ©p" â†’ liÃªn káº¿t â†’ chunk "máº«u Ä‘Æ¡n nghá»‰ phÃ©p"

**8. query_performance_metrics**
- **Chá»©c nÄƒng**: Äo lÆ°á»ng chi tiáº¿t hiá»‡u suáº¥t tá»«ng bÆ°á»›c xá»­ lÃ½ query
- **Táº¡i sao cáº§n**: Tá»‘i Æ°u hÃ³a tá»‘c Ä‘á»™ - biáº¿t bÆ°á»›c nÃ o cháº­m nháº¥t
- **VD**: Query máº¥t 2.5s â†’ embedding: 0.1s, search: 0.5s, LLM: 1.9s

**9. embedding_model_benchmarks**
- **Chá»©c nÄƒng**: So sÃ¡nh hiá»‡u suáº¥t cÃ¡c model embedding khÃ¡c nhau
- **Táº¡i sao cáº§n**: Chá»n model tá»‘t nháº¥t cho tiáº¿ng Viá»‡t (OpenAI vs Google vs local model)
- **VD**: OpenAI: 87% accuracy, Google: 83%, Vietnamese-BERT: 91%

**10. jsonl_exports**
- **Chá»©c nÄƒng**: Theo dÃµi cÃ¡c láº§n export/import dá»¯ liá»‡u ra file JSONL
- **Táº¡i sao cáº§n**: Backup, migrate data, integrate vá»›i FlashRAG
- **VD**: Exported 1000 docs â†’ production_backup.jsonl.gz (50MB)

**11. vietnamese_terminology**
- **Chá»©c nÄƒng**: Tá»« Ä‘iá»ƒn thuáº­t ngá»¯ tiáº¿ng Viá»‡t cá»§a cÃ´ng ty
- **Táº¡i sao cáº§n**: "WFH" = "lÃ m viá»‡c tá»« xa", "OT" = "tÄƒng ca"
- **VD**: Chuáº©n hÃ³a thuáº­t ngá»¯ Ä‘á»ƒ search chÃ­nh xÃ¡c hÆ¡n

**12. system_metrics_log**
- **Chá»©c nÄƒng**: Monitor há»‡ thá»‘ng - CPU, RAM, disk usage
- **Táº¡i sao cáº§n**: PhÃ¡t hiá»‡n sá»›m khi há»‡ thá»‘ng quÃ¡ táº£i
- **VD**: RAM usage 85% â†’ cáº£nh bÃ¡o cáº§n upgrade server

## ğŸŸ¢ **CHROMADB - VECTOR SEARCH ENGINE**

```mermaid
graph LR
    subgraph "ğŸŸ¢ ChromaDB Search Process"
        Query[ğŸ‘¤ User Query<br/>Quy trÃ¬nh nghá»‰ phÃ©p]
        
        Embed[ğŸ§® Convert to Vector<br/>0.2, 0.7, 0.1, ...]
        
        ChromaSearch[ğŸ” Vector Similarity<br/>Cosine distance]
        
        Results[ğŸ“„ Similar Chunks<br/>chunk_001: 0.95<br/>chunk_002: 0.87]
    end
    
    Query --> Embed --> ChromaSearch --> Results
```

### **Document embeddings vs Chunk embeddings:**
- **Document embeddings**: Vector cá»§a TOÃ€N Bá»˜ tÃ i liá»‡u
  - VD: "Quy trÃ¬nh xin nghá»‰ phÃ©p" (3000 tá»«) â†’ 1 vector [1536 dimensions]
  
- **Chunk embeddings**: Vector cá»§a Tá»ªNG ÄOáº N nhá»
  - VD: "BÆ°á»›c 1: Äiá»n Ä‘Æ¡n xin nghá»‰" (200 tá»«) â†’ 1 vector [1536 dimensions]

### **HNSW index overhead:**
- **HNSW** = Hierarchical Navigable Small World
- **LÃ  gÃ¬**: Cáº¥u trÃºc dá»¯ liá»‡u Ä‘á»ƒ tÃ¬m kiáº¿m vector nhanh hÆ¡n
- **Táº¡i sao cáº§n**: Thay vÃ¬ so sÃ¡nh vá»›i 100,000 vectors â†’ chá»‰ cáº§n so sÃ¡nh vá»›i 1,000 vectors
- **Overhead**: Chiáº¿m thÃªm ~50% storage Ä‘á»ƒ lÆ°u index

```mermaid
graph TB
    subgraph "Vector Search: Brute Force vs HNSW"
        subgraph "âŒ Brute Force (Cháº­m)"
            Query1[Query Vector] --> Compare1[So sÃ¡nh vá»›i<br/>100,000 vectors]
            Compare1 --> Time1[â±ï¸ 500ms]
        end
        
        subgraph "âœ… HNSW Index (Nhanh)"
            Query2[Query Vector] --> HNSW[HNSW Index<br/>Smart routing]
            HNSW --> Compare2[So sÃ¡nh vá»›i<br/>1,000 vectors]
            Compare2 --> Time2[â±ï¸ 50ms]
        end
    end
```

## ğŸ”´ **REDIS CACHE SYSTEM**

### **Redis lÆ°u á»Ÿ Ä‘Ã¢u?**
- **RAM** cá»§a server (in-memory database)
- **Persistence**: CÃ³ thá»ƒ save xuá»‘ng disk Ä‘á»‹nh ká»³
- **Docker**: LÆ°u trong volume `redis_test_data`

### **Popular data tracking:**
```sql
-- Popular score ÄÆ¯á»¢C TÃNH trong PostgreSQL:
UPDATE documents_metadata_v2 SET 
    view_count = view_count + 1,
    last_accessed = NOW()
WHERE document_id = 'doc_123';

-- Sau Ä‘Ã³ documents popular sáº½ Ä‘Æ°á»£c cache trong Redis
```

### **Chi tiáº¿t cÃ¡c loáº¡i cache:**

**Session data**: 
- **LÃ  gÃ¬**: ThÃ´ng tin user Ä‘ang login (KHÃ”NG pháº£i chat history)
- **VD**: `user:session:user_001` â†’ {username: "nguyen.van.a", department: "HR", permissions: ["read", "search"]}

**Embedding cache**:
```python
# Khi user search "nghá»‰ phÃ©p":
# 1. Convert text â†’ vector (expensive operation)
# 2. Cache vector trong Redis Ä‘á»ƒ láº§n sau khÃ´ng cáº§n convert láº¡i

"embedding:openai:hash123": {
    "text": "quy trÃ¬nh nghá»‰ phÃ©p", 
    "vector": [0.1, 0.2, 0.3, ...],
    "ttl": 7_days
}
```

**Search results cache**:
- **ÄÃºng rá»“i**: Káº¿t quáº£ search hay Ä‘Æ°á»£c tÃ¬m
- **VD**: Query "nghá»‰ phÃ©p" Ä‘Æ°á»£c search 50 láº§n/ngÃ y â†’ cache result 30 phÃºt

**NLP processing cache**:
```python
# Vietnamese text processing ráº¥t cháº­m:
# "Quy trÃ¬nh xin nghá»‰ phÃ©p táº¡i cÃ´ng ty" 
# â†’ TÃ¡ch tá»«: ["Quy_trÃ¬nh", "xin", "nghá»‰_phÃ©p", "táº¡i", "cÃ´ng_ty"]
# â†’ Cache káº¿t quáº£ Ä‘á»ƒ khÃ´ng cáº§n process láº¡i

"vn:nlp:hash456": {
    "original": "Quy trÃ¬nh xin nghá»‰ phÃ©p táº¡i cÃ´ng ty",
    "segmented": ["Quy_trÃ¬nh", "xin", "nghá»‰_phÃ©p", "táº¡i", "cÃ´ng_ty"],
    "pos_tags": [{"word": "Quy_trÃ¬nh", "tag": "N"}, ...],
    "ttl": 24_hours
}
```

## ğŸ“„ **JSONL TRONG DATA FLOW**

```mermaid
graph TB
    subgraph "ğŸ”„ Complete Data Flow"
        subgraph "ğŸ“¥ INPUT"
            UserUpload[ğŸ‘¤ User Upload<br/>PDF, DOCX]
            ExternalData[ğŸŒ External JSONL<br/>From other systems]
        end
        
        subgraph "ğŸ˜ PostgreSQL (Master)"
            PGStore[ğŸ’¾ Permanent Storage<br/>All document data]
        end
        
        subgraph "ğŸŸ¢ ChromaDB (Search)"
            VectorStore[ğŸ§® Vector Search<br/>Embeddings only]
        end
        
        subgraph "ğŸ”´ Redis (Cache)"
            CacheLayer[âš¡ Fast Access<br/>Popular data]
        end
        
        subgraph "ğŸ“ File System"
            JSONLFiles[ğŸ“„ JSONL Files<br/>backup_20240315.jsonl.gz<br/>external_import.jsonl]
        end
        
        %% Data flow
        UserUpload --> PGStore
        ExternalData --> PGStore
        
        PGStore -->|Generate embeddings| VectorStore
        PGStore -->|Cache popular queries| CacheLayer
        PGStore -->|Export snapshots| JSONLFiles
        
        JSONLFiles -->|Import/restore| PGStore
        
        %% Search flow
        CacheLayer -.->|Cache miss| VectorStore
        VectorStore -.->|Get full content| PGStore
    end
```

**JSONL vá»‹ trÃ­ trong há»‡ thá»‘ng:**
- **KhÃ´ng pháº£i database** - chá»‰ lÃ  file format
- **Náº±m trÃªn file system** (disk storage)
- **Vai trÃ²**: Import/Export bridge giá»¯a systems
- **Khi nÃ o táº¡o**: Manual export, scheduled backup, system integration

## ğŸ¯ **SEARCH WORKFLOW THá»°C Táº¾**

```mermaid
sequenceDiagram
    participant User
    participant Redis
    participant ChromaDB  
    participant PostgreSQL
    
    User->>Redis: "Quy trÃ¬nh nghá»‰ phÃ©p"
    
    alt Cache Hit
        Redis-->>User: Cached results (50ms)
    else Cache Miss
        Redis->>ChromaDB: Vector search
        ChromaDB->>ChromaDB: Find similar chunks
        ChromaDB-->>Redis: chunk_001, chunk_002, chunk_003
        
        Redis->>PostgreSQL: Get full content
        PostgreSQL-->>Redis: Full text + metadata
        
        Redis->>Redis: Cache results (30 min)
        Redis-->>User: Complete results (300ms)
    end
    
    Note over Redis: Popular queries cached
    Note over ChromaDB: Semantic similarity only  
    Note over PostgreSQL: Complete data source
```

**Báº¡n Ä‘Ã£ hiá»ƒu Ä‘Ãºng 95%! Nhá»¯ng Ä‘iá»ƒm bá»• sung nÃ y sáº½ giÃºp báº¡n cÃ³ cÃ¡i nhÃ¬n hoÃ n chá»‰nh vá» architecture** ğŸ‰
---
```txt
HÃ£y quay vá» thiáº¿t káº¿. TÃ´i nháº­n Ä‘á»‹nh Project nÃ y Ä‘á»ƒ thÃªm docs vÃ o database theo chuáº©n cá»§a database vÃ  cÃ³ cÃ¡c tÃ­nh nÄƒng sau:  Vietnamese analysis ,document storage , search. TÃ”i nháº­n Ä‘á»‹nh Ä‘Ãºng khÃ´ng? TÃ”i cÃ³ bá» lá»¡ gÃ¬ khÃ´ng? táº¡i sao pháº£i Vietnamese analysis á»Ÿ bÆ°á»›c nÃ y nhá»‰?
```
Báº¡n nháº­n Ä‘á»‹nh Ä‘Ãºng vá» 3 tÃ­nh nÄƒng chÃ­nh, nhÆ°ng cÃ³ má»™t sá»‘ Ä‘iá»ƒm cáº§n lÃ m rÃµ vÃ  bá»• sung:

## **PhÃ¢n tÃ­ch thiáº¿t káº¿ hiá»‡n táº¡i:**

### **3 tÃ­nh nÄƒng chÃ­nh báº¡n xÃ¡c Ä‘á»‹nh:**
1. **Document Storage** - LÆ°u trá»¯ tÃ i liá»‡u vÃ o database
2. **Vietnamese Analysis** - PhÃ¢n tÃ­ch ngÃ´n ngá»¯ tiáº¿ng Viá»‡t 
3. **Search** - TÃ¬m kiáº¿m tÃ i liá»‡u

### **TÃ­nh nÄƒng bá»‹ bá» lá»¡:**
4. **Semantic Chunking** - Chia tÃ i liá»‡u thÃ nh chunks cÃ³ nghÄ©a
5. **Vector Embeddings** - Táº¡o embedding vectors cho semantic search
6. **ChromaDB Integration** - Vector database cho similarity search
7. **Full-text Search** - PostgreSQL search vá»›i tsvector

## **Táº¡i sao Vietnamese Analysis á»Ÿ bÆ°á»›c nÃ y?**

Vietnamese Analysis **khÃ´ng thá»±c sá»± cáº§n thiáº¿t** á»Ÿ bÆ°á»›c ingestion nÃ y. ÄÃ¢y lÃ  **over-engineering** vÃ¬:

### **LÃ½ do Ä‘Æ°á»£c thÃªm vÃ o:**
- Word segmentation cho better chunking
- POS tagging Ä‘á»ƒ extract proper nouns
- Compound word detection cho better search
- Readability scoring cho content quality

### **Táº¡i sao cÃ³ thá»ƒ bá» qua:**
- **Modern embedding models** (nhÆ° multilingual BERT) Ä‘Ã£ handle Vietnamese tá»‘t mÃ  khÃ´ng cáº§n preprocessing
- **Semantic search** qua embeddings hiá»‡u quáº£ hÆ¡n keyword search
- **Maintenance overhead** - thÃªm dependencies vÃ  complexity
- **Performance impact** - cháº­m quÃ¡ trÃ¬nh ingestion

## **Thiáº¿t káº¿ tá»‘i giáº£n hÆ¡n:**

```python
# Simplified Document Ingestion Pipeline
class SimpleDocumentProcessor:
    def process_document(self, file_path, metadata):
        # 1. Extract text
        content = self.extract_text(file_path)
        
        # 2. Semantic chunking (language-agnostic)
        chunks = self.chunk_text(content)
        
        # 3. Generate embeddings
        embeddings = self.generate_embeddings(chunks)
        
        # 4. Store in databases
        doc_id = self.store_document(content, metadata)
        self.store_chunks_and_embeddings(doc_id, chunks, embeddings)
        
        return doc_id
```

## **Recommendation cho project:**

### **Keep (Essential):**
- Document storage vá»›i metadata
- Semantic chunking
- Vector embeddings
- Dual search (full-text + semantic)

### **Remove/Optional (Over-engineered):**
- Vietnamese NLP analysis
- POS tagging
- Compound word extraction
- Language-specific preprocessing

### **LÃ½ do:**
1. **Simpler = More Reliable** - Ãt bugs, dá»… maintain
2. **Better Performance** - Nhanh hÆ¡n mÃ  khÃ´ng máº¥t cháº¥t lÆ°á»£ng
3. **Language Agnostic** - Hoáº¡t Ä‘á»™ng vá»›i má»i ngÃ´n ngá»¯
4. **Modern Approach** - Embedding models Ä‘Ã£ handle multilingual tá»‘t

Báº¡n cÃ³ muá»‘n tÃ´i táº¡o version simplified nÃ y khÃ´ng? NÃ³ sáº½ á»•n Ä‘á»‹nh hÆ¡n vÃ  táº­p trung vÃ o core functionality.
---
Tuyá»‡t vá»i! Database Ä‘Ã£ setup thÃ nh cÃ´ng. BÃ¢y giá» tÃ´i sáº½ hÆ°á»›ng dáº«n báº¡n tá»«ng bÆ°á»›c Ä‘á»ƒ thÃªm tÃ i liá»‡u vÃ o há»‡ thá»‘ng. TÃ´i sáº½ táº¡o má»™t tool Ä‘Æ¡n giáº£n Ä‘á»ƒ báº¡n cÃ³ thá»ƒ upload vÃ  xá»­ lÃ½ tÃ i liá»‡u.

## ğŸ› ï¸ **TOOL THÃŠM TÃ€I LIá»†U - DOCUMENT INGESTION TOOL**

### **BÆ°á»›c 1: Táº¡o cáº¥u trÃºc thÆ° má»¥c cho tool**

Trong thÆ° má»¥c `chatbot-enhanced-db`, táº¡o thÃªm:

```cmd
mkdir tools
mkdir tools\document_ingestion
mkdir tools\document_ingestion\uploads
mkdir tools\document_ingestion\processed
mkdir requirements
```

### **BÆ°á»›c 2: Táº¡o Requirements cho tool**

Táº¡o file `requirements/tool_requirements.txt`:

```txt
# requirements/tool_requirements.txt
asyncpg==0.29.0
python-multipart==0.0.6
fastapi==0.104.1
uvicorn==0.24.0
streamlit==1.28.2
python-docx==1.1.0
PyPDF2==3.0.1
openpyxl==3.1.2
pandas==2.0.3
sentence-transformers==2.2.2
chromadb==0.4.15
redis==5.0.1
pyvi==0.1.1
underthesea==6.7.0
numpy==1.24.3
```

### **BÆ°á»›c 3: Táº¡o Document Processor**

Táº¡o file `tools/document_ingestion/document_processor.py`:

```python
# tools/document_ingestion/document_processor.py
import asyncio
import asyncpg
import os
import hashlib
import uuid
from pathlib import Path
import json
from typing import List, Dict, Optional, Tuple
import logging
from datetime import datetime

# Document processing imports
from docx import Document
import PyPDF2
import pandas as pd

# Vietnamese NLP
import pyvi
from underthesea import word_tokenize, pos_tag

# Embedding
from sentence_transformers import SentenceTransformer
import numpy as np

# Vector DB
import chromadb
from chromadb.config import Settings

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DocumentProcessor:
    def __init__(self):
        """Initialize document processor with all required connections"""
        self.db_pool = None
        self.chroma_client = None
        self.embedding_model = None
        self.setup_complete = False
        
    async def setup(self):
        """Setup all connections and models"""
        logger.info("ğŸ”§ Setting up Document Processor...")
        
        # Database connection
        await self._setup_database()
        
        # Vector database connection  
        await self._setup_vector_db()
        
        # Embedding model
        await self._setup_embedding_model()
        
        self.setup_complete = True
        logger.info("âœ… Document Processor setup complete!")
    
    async def _setup_database(self):
        """Setup PostgreSQL connection"""
        db_config = {
            'host': 'localhost',
            'port': 5433,
            'database': 'knowledge_base_test',
            'user': 'kb_admin',
            'password': 'test_password_123'
        }
        
        try:
            self.db_pool = await asyncpg.create_pool(
                min_size=2,
                max_size=10,
                **db_config
            )
            logger.info("âœ… Database connection established")
        except Exception as e:
            logger.error(f"âŒ Database connection failed: {e}")
            raise
    
    async def _setup_vector_db(self):
        """Setup ChromaDB connection"""
        try:
            self.chroma_client = chromadb.HttpClient(
                host='localhost',
                port=8001,
                settings=Settings(allow_reset=True)
            )
            
            # Test connection
            collections = self.chroma_client.list_collections()
            logger.info(f"âœ… ChromaDB connected. Collections: {len(collections)}")
            
        except Exception as e:
            logger.error(f"âŒ ChromaDB connection failed: {e}")
            # For demo, we'll continue without vector DB
            self.chroma_client = None
    
    async def _setup_embedding_model(self):
        """Setup embedding model for Vietnamese"""
        try:
            # Use a multilingual model that works well with Vietnamese
            model_name = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'
            self.embedding_model = SentenceTransformer(model_name)
            logger.info(f"âœ… Embedding model loaded: {model_name}")
        except Exception as e:
            logger.error(f"âŒ Embedding model loading failed: {e}")
            raise
    
    def extract_text_from_file(self, file_path: str) -> Tuple[str, Dict]:
        """Extract text from various file formats"""
        file_path = Path(file_path)
        file_ext = file_path.suffix.lower()
        
        metadata = {
            'file_name': file_path.name,
            'file_size': file_path.stat().st_size,
            'file_extension': file_ext
        }
        
        try:
            if file_ext == '.txt':
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
            elif file_ext == '.docx':
                doc = Document(file_path)
                content = '\n'.join([paragraph.text for paragraph in doc.paragraphs])
                metadata['page_count'] = len(doc.paragraphs)
                
            elif file_ext == '.pdf':
                with open(file_path, 'rb') as f:
                    pdf_reader = PyPDF2.PdfReader(f)
                    content = ''
                    for page in pdf_reader.pages:
                        content += page.extract_text() + '\n'
                    metadata['page_count'] = len(pdf_reader.pages)
                    
            elif file_ext in ['.xlsx', '.xls']:
                df = pd.read_excel(file_path)
                content = df.to_string()
                metadata['rows'] = len(df)
                metadata['columns'] = len(df.columns)
                
            else:
                raise ValueError(f"Unsupported file format: {file_ext}")
            
            # Calculate file hash
            content_hash = hashlib.sha256(content.encode()).hexdigest()
            metadata['content_hash'] = content_hash
            metadata['word_count'] = len(content.split())
            
            logger.info(f"âœ… Extracted text from {file_path.name}: {len(content)} characters")
            return content, metadata
            
        except Exception as e:
            logger.error(f"âŒ Text extraction failed for {file_path}: {e}")
            raise
    
    def process_vietnamese_text(self, text: str) -> Dict:
        """Process Vietnamese text with NLP"""
        try:
            # Word segmentation
            words = word_tokenize(text)
            
            # POS tagging
            pos_tags = pos_tag(text)
            
            # Pyvi tokenization
            pyvi_tokens = pyvi.ViTokenizer.tokenize(text).split()
            
            # Extract compound words (words with underscores from pyvi)
            compound_words = [word for word in pyvi_tokens if '_' in word]
            
            # Extract proper nouns
            proper_nouns = [word for word, tag in pos_tags if tag in ['Np', 'N']]
            
            # Simple readability score
            avg_word_length = sum(len(w) for w in words) / len(words) if words else 0
            readability = max(0.0, min(1.0, 1.0 - (avg_word_length - 3.0) / 5.0))
            
            analysis = {
                'word_segmentation': {
                    'words': words,
                    'count': len(words)
                },
                'pos_tagging': {
                    'tagged_words': pos_tags
                },
                'compound_words': compound_words,
                'proper_nouns': list(set(proper_nouns)),
                'readability_score': round(readability, 2),
                'pyvi_tokens': pyvi_tokens
            }
            
            logger.info(f"âœ… Vietnamese analysis: {len(words)} words, {len(compound_words)} compounds")
            return analysis
            
        except Exception as e:
            logger.error(f"âŒ Vietnamese processing failed: {e}")
            return {}
    
    def semantic_chunking(self, text: str, max_chunk_size: int = 1000) -> List[Dict]:
        """Split text into semantic chunks"""
        try:
            # Simple sentence-based chunking for Vietnamese
            sentences = text.split('.')
            sentences = [s.strip() for s in sentences if s.strip()]
            
            chunks = []
            current_chunk = ""
            current_position = 0
            
            for sentence in sentences:
                # If adding this sentence would exceed max size, save current chunk
                if len(current_chunk) + len(sentence) > max_chunk_size and current_chunk:
                    chunk_data = {
                        'content': current_chunk.strip(),
                        'position': current_position,
                        'size_tokens': len(current_chunk.split()),
                        'size_chars': len(current_chunk),
                        'semantic_boundary': True
                    }
                    chunks.append(chunk_data)
                    
                    current_chunk = sentence
                    current_position += 1
                else:
                    current_chunk += sentence + ". "
            
            # Add final chunk
            if current_chunk.strip():
                chunk_data = {
                    'content': current_chunk.strip(),
                    'position': current_position,
                    'size_tokens': len(current_chunk.split()),
                    'size_chars': len(current_chunk),
                    'semantic_boundary': True
                }
                chunks.append(chunk_data)
            
            logger.info(f"âœ… Created {len(chunks)} semantic chunks")
            return chunks
            
        except Exception as e:
            logger.error(f"âŒ Chunking failed: {e}")
            return [{'content': text, 'position': 0, 'size_tokens': len(text.split()), 'size_chars': len(text), 'semantic_boundary': False}]
    
    def generate_embeddings(self, texts: List[str]) -> List[np.ndarray]:
        """Generate embeddings for text chunks"""
        try:
            embeddings = self.embedding_model.encode(texts)
            logger.info(f"âœ… Generated embeddings for {len(texts)} chunks")
            return embeddings
        except Exception as e:
            logger.error(f"âŒ Embedding generation failed: {e}")
            return []
    
    async def store_document(self, content: str, file_metadata: Dict, 
                           document_info: Dict, vietnamese_analysis: Dict, 
                           chunks: List[Dict]) -> str:
        """Store document and chunks in database"""
        try:
            async with self.db_pool.acquire() as conn:
                # Insert main document
                document_id = await conn.fetchval("""
                    INSERT INTO documents_metadata_v2 (
                        title, content, document_type, access_level,
                        department_owner, author, status,
                        language_detected, vietnamese_segmented,
                        file_size_bytes, embedding_model_primary,
                        chunk_count, flashrag_collection, jsonl_export_ready
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14)
                    RETURNING document_id
                """,
                document_info['title'],
                content,
                document_info['document_type'],
                document_info['access_level'],
                document_info['department_owner'],
                document_info['author'],
                'approved',  # Auto-approve for demo
                'vi',
                True,
                file_metadata.get('file_size', 0),
                'paraphrase-multilingual-MiniLM-L12-v2',
                len(chunks),
                'default_collection',
                True
                )
                
                # Store Vietnamese analysis
                if vietnamese_analysis:
                    await conn.execute("""
                        INSERT INTO vietnamese_text_analysis (
                            document_id, original_text, processed_text,
                            word_segmentation, pos_tagging,
                            compound_words, proper_nouns,
                            readability_score, formality_level
                        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
                    """,
                    document_id,
                    content,
                    content,  # For demo, same as original
                    json.dumps(vietnamese_analysis.get('word_segmentation', {})),
                    json.dumps(vietnamese_analysis.get('pos_tagging', {})),
                    vietnamese_analysis.get('compound_words', []),
                    vietnamese_analysis.get('proper_nouns', []),
                    vietnamese_analysis.get('readability_score', 0.0),
                    'formal'  # Default
                    )
                
                # Store chunks
                chunk_texts = []
                for chunk in chunks:
                    chunk_id = await conn.fetchval("""
                        INSERT INTO document_chunks_enhanced (
                            document_id, chunk_content, chunk_position,
                            chunk_size_tokens, chunk_method, semantic_boundary,
                            chunk_quality_score, embedding_model
                        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
                        RETURNING chunk_id
                    """,
                    document_id,
                    chunk['content'],
                    chunk['position'],
                    chunk['size_tokens'],
                    'semantic',
                    chunk['semantic_boundary'],
                    0.85,  # Default quality score
                    'paraphrase-multilingual-MiniLM-L12-v2'
                    )
                    
                    chunk_texts.append(chunk['content'])
                
                # Generate and store embeddings (if ChromaDB is available)
                if self.chroma_client and chunk_texts:
                    try:
                        # Get or create collection
                        try:
                            collection = self.chroma_client.get_collection("knowledge_base")
                        except:
                            collection = self.chroma_client.create_collection("knowledge_base")
                        
                        # Generate embeddings
                        embeddings = self.generate_embeddings(chunk_texts)
                        
                        # Store in ChromaDB
                        chunk_ids = [f"{document_id}_{i}" for i in range(len(chunks))]
                        metadatas = [
                            {
                                'document_id': str(document_id),
                                'chunk_position': chunk['position'],
                                'document_title': document_info['title']
                            }
                            for chunk in chunks
                        ]
                        
                        collection.add(
                            embeddings=embeddings.tolist(),
                            documents=chunk_texts,
                            metadatas=metadatas,
                            ids=chunk_ids
                        )
                        
                        logger.info(f"âœ… Stored embeddings in ChromaDB for {len(chunks)} chunks")
                        
                    except Exception as e:
                        logger.warning(f"âš ï¸ ChromaDB storage failed: {e}")
                
                # Update search tokens for full-text search
                await conn.execute("""
                    UPDATE documents_metadata_v2 
                    SET search_tokens = to_tsvector('simple', title || ' ' || COALESCE(content, ''))
                    WHERE document_id = $1
                """, document_id)
                
                logger.info(f"âœ… Document stored with ID: {document_id}")
                return str(document_id)
                
        except Exception as e:
            logger.error(f"âŒ Document storage failed: {e}")
            raise
    
    async def process_document(self, file_path: str, document_info: Dict) -> str:
        """Complete document processing pipeline"""
        if not self.setup_complete:
            await self.setup()
        
        logger.info(f"ğŸ”„ Processing document: {file_path}")
        
        try:
            # 1. Extract text from file
            content, file_metadata = self.extract_text_from_file(file_path)
            
            # 2. Process Vietnamese text
            vietnamese_analysis = self.process_vietnamese_text(content)
            
            # 3. Create semantic chunks
            chunks = self.semantic_chunking(content)
            
            # 4. Store everything in database
            document_id = await self.store_document(
                content, file_metadata, document_info, 
                vietnamese_analysis, chunks
            )
            
            logger.info(f"ğŸ‰ Document processing completed! Document ID: {document_id}")
            return document_id
            
        except Exception as e:
            logger.error(f"âŒ Document processing failed: {e}")
            raise

# Utility functions for the tool
async def get_document_stats():
    """Get current database statistics"""
    db_config = {
        'host': 'localhost',
        'port': 5433,
        'database': 'knowledge_base_test', 
        'user': 'kb_admin',
        'password': 'test_password_123'
    }
    
    try:
        conn = await asyncpg.connect(**db_config)
        
        stats = await conn.fetchrow("""
            SELECT 
                COUNT(*) as total_documents,
                COUNT(*) FILTER (WHERE language_detected = 'vi') as vietnamese_docs,
                COUNT(*) FILTER (WHERE status = 'approved') as approved_docs,
                SUM(chunk_count) as total_chunks
            FROM documents_metadata_v2
        """)
        
        await conn.close()
        return dict(stats)
        
    except Exception as e:
        logger.error(f"Error getting stats: {e}")
        return {}

if __name__ == "__main__":
    # Quick test
    async def test_processor():
        processor = DocumentProcessor()
        await processor.setup()
        
        stats = await get_document_stats()
        print("Current database stats:", stats)
    
    asyncio.run(test_processor())
```

### **BÆ°á»›c 4: Táº¡o Streamlit Interface**

Táº¡o file `tools/document_ingestion/streamlit_app.py`:

```python
# tools/document_ingestion/streamlit_app.py
import streamlit as st
import asyncio
import os
from pathlib import Path
import time
from document_processor import DocumentProcessor, get_document_stats

# Page configuration
st.set_page_config(
    page_title="ğŸ“„ Document Ingestion Tool",
    page_icon="ğŸ“„",
    layout="wide"
)

# Custom CSS
st.markdown("""
<style>
    .success-box {
        padding: 1rem;
        border-radius: 0.5rem;
        background-color: #d4edda;
        border: 1px solid #c3e6cb;
        color: #155724;
        margin: 1rem 0;
    }
    .error-box {
        padding: 1rem;
        border-radius: 0.5rem;
        background-color: #f8d7da;
        border: 1px solid #f5c6cb;
        color: #721c24;
        margin: 1rem 0;
    }
    .info-box {
        padding: 1rem;
        border-radius: 0.5rem;
        background-color: #cce7ff;
        border: 1px solid #99d6ff;
        color: #004085;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

# Initialize session state
if 'processor' not in st.session_state:
    st.session_state.processor = None
if 'stats' not in st.session_state:
    st.session_state.stats = {}

def init_processor():
    """Initialize document processor"""
    if st.session_state.processor is None:
        with st.spinner("ğŸ”§ Initializing Document Processor..."):
            st.session_state.processor = DocumentProcessor()
            # Run async setup
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            loop.run_until_complete(st.session_state.processor.setup())
        st.success("âœ… Document Processor initialized!")
    return st.session_state.processor

def get_stats():
    """Get database statistics"""
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    return loop.run_until_complete(get_document_stats())

# Main interface
st.title("ğŸ“„ Enhanced Database Document Ingestion Tool")
st.markdown("Upload vÃ  xá»­ lÃ½ tÃ i liá»‡u tiáº¿ng Viá»‡t vá»›i enhanced database architecture")

# Sidebar with database stats
with st.sidebar:
    st.header("ğŸ“Š Database Statistics")
    
    if st.button("ğŸ”„ Refresh Stats"):
        st.session_state.stats = get_stats()
    
    if not st.session_state.stats:
        st.session_state.stats = get_stats()
    
    if st.session_state.stats:
        col1, col2 = st.columns(2)
        with col1:
            st.metric("ğŸ“„ Total Documents", st.session_state.stats.get('total_documents', 0))
            st.metric("âœ… Approved", st.session_state.stats.get('approved_docs', 0))
        with col2:
            st.metric("ğŸ‡»ğŸ‡³ Vietnamese", st.session_state.stats.get('vietnamese_docs', 0))
            st.metric("ğŸ”— Total Chunks", st.session_state.stats.get('total_chunks', 0))
    
    st.markdown("---")
    st.header("ğŸ”— Database Access")
    st.markdown("""
    **Adminer**: http://localhost:8080  
    **ChromaDB**: http://localhost:8001  
    **Redis**: localhost:6380
    
    **DB Connection:**  
    - Server: postgres-test  
    - User: kb_admin  
    - Password: test_password_123
    """)

# Main content tabs
tab1, tab2, tab3 = st.tabs(["ğŸ“¤ Upload Document", "ğŸ” Search Test", "ğŸ“Š Analytics"])

with tab1:
    st.header("ğŸ“¤ Upload New Document")
    
    # File upload
    uploaded_file = st.file_uploader(
        "Chá»n tÃ i liá»‡u Ä‘á»ƒ upload",
        type=['txt', 'docx', 'pdf', 'xlsx', 'xls'],
        help="Há»— trá»£: .txt, .docx, .pdf, .xlsx, .xls"
    )
    
    if uploaded_file:
        # Document information form
        with st.form("document_info_form"):
            st.subheader("ğŸ“ ThÃ´ng tin tÃ i liá»‡u")
            
            col1, col2 = st.columns(2)
            with col1:
                title = st.text_input("TiÃªu Ä‘á» *", value=uploaded_file.name)
                document_type = st.selectbox("Loáº¡i tÃ i liá»‡u *", [
                    'policy', 'procedure', 'technical_guide', 'report',
                    'manual', 'specification', 'template', 'form',
                    'presentation', 'training_material', 'other'
                ])
                access_level = st.selectbox("Cáº¥p Ä‘á»™ truy cáº­p *", [
                    'employee_only', 'manager_only', 'director_only', 'public'
                ])
            
            with col2:
                department_owner = st.text_input("PhÃ²ng ban sá»Ÿ há»¯u *", value="IT")
                author = st.text_input("TÃ¡c giáº£ *", value="System Admin")
                
            description = st.text_area("MÃ´ táº£", placeholder="MÃ´ táº£ ngáº¯n vá» tÃ i liá»‡u...")
            
            # Submit button
            submit_button = st.form_submit_button("ğŸš€ Process Document", use_container_width=True)
            
            if submit_button:
                if not all([title, document_type, access_level, department_owner, author]):
                    st.error("âŒ Vui lÃ²ng Ä‘iá»n Ä‘áº§y Ä‘á»§ cÃ¡c trÆ°á»ng báº¯t buá»™c (*)")
                else:
                    # Save uploaded file
                    uploads_dir = Path("tools/document_ingestion/uploads")
                    uploads_dir.mkdir(exist_ok=True)
                    file_path = uploads_dir / uploaded_file.name
                    
                    with open(file_path, "wb") as f:
                        f.write(uploaded_file.getbuffer())
                    
                    # Prepare document info
                    document_info = {
                        'title': title,
                        'document_type': document_type,
                        'access_level': access_level,
                        'department_owner': department_owner,
                        'author': author,
                        'description': description
                    }
                    
                    # Process document
                    try:
                        # Initialize processor if not already done
                        processor = init_processor()
                        
                        with st.spinner("ğŸ”„ Processing document... This may take a few minutes."):
                            # Create progress bar
                            progress_bar = st.progress(0)
                            status_text = st.empty()
                            
                            status_text.text("ğŸ“„ Extracting text...")
                            progress_bar.progress(20)
                            time.sleep(1)
                            
                            status_text.text("ğŸ‡»ğŸ‡³ Processing Vietnamese text...")
                            progress_bar.progress(40)
                            time.sleep(1)
                            
                            status_text.text("âœ‚ï¸ Creating semantic chunks...")
                            progress_bar.progress(60)
                            time.sleep(1)
                            
                            status_text.text("ğŸ”¢ Generating embeddings...")
                            progress_bar.progress(80)
                            time.sleep(1)
                            
                            status_text.text("ğŸ’¾ Storing in database...")
                            progress_bar.progress(90)
                            
                            # Run async processing
                            loop = asyncio.new_event_loop()
                            asyncio.set_event_loop(loop)
                            document_id = loop.run_until_complete(
                                processor.process_document(str(file_path), document_info)
                            )
                            
                            progress_bar.progress(100)
                            status_text.text("âœ… Complete!")
                        
                        # Success message
                        st.markdown(f"""
                        <div class="success-box">
                            <h4>ğŸ‰ Document processed successfully!</h4>
                            <p><strong>Document ID:</strong> {document_id}</p>
                            <p><strong>Title:</strong> {title}</p>
                            <p><strong>Type:</strong> {document_type}</p>
                            <p><strong>Author:</strong> {author}</p>
                        </div>
                        """, unsafe_allow_html=True)
                        
                        # Refresh stats
                        st.session_state.stats = get_stats()
                        st.rerun()
                        
                    except Exception as e:
                        st.markdown(f"""
                        <div class="error-box">
                            <h4>âŒ Processing failed!</h4>
                            <p><strong>Error:</strong> {str(e)}</p>
                        </div>
                        """, unsafe_allow_html=True)
                        
                        # Clean up uploaded file
                        if file_path.exists():
                            file_path.unlink()

with tab2:
    st.header("ğŸ” Test Document Search")
    
    # Search interface
    search_query = st.text_input("TÃ¬m kiáº¿m tÃ i liá»‡u:", placeholder="VÃ­ dá»¥: quy trÃ¬nh nghá»‰ phÃ©p")
    
    if st.button("ğŸ” Search") and search_query:
        try:
            # Simple search in database
            import asyncpg
            
            async def search_documents(query):
                conn = await asyncpg.connect(
                    host='localhost', port=5433, database='knowledge_base_test',
                    user='kb_admin', password='test_password_123'
                )
                
                results = await conn.fetch("""
                    SELECT 
                        document_id, title, author, document_type,
                        ts_rank(search_tokens, plainto_tsquery('simple', $1)) as rank,
                        substring(content, 1, 200) as preview
                    FROM documents_metadata_v2
                    WHERE search_tokens @@ plainto_tsquery('simple', $1)
                    OR title ILIKE '%' || $1 || '%'
                    OR content ILIKE '%' || $1 || '%'
                    ORDER BY rank DESC, created_at DESC
                    LIMIT 10
                """, search_query)
                
                await conn.close()
                return results
            
            # Run search
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            search_results = loop.run_until_complete(search_documents(search_query))
            
            if search_results:
                st.success(f"âœ… Found {len(search_results)} documents")
                
                for result in search_results:
                    with st.expander(f"ğŸ“„ {result['title']} ({result['document_type']})"):
                        col1, col2 = st.columns([2, 1])
                        with col1:
                            st.write(f"**Author:** {result['author']}")
                            st.write(f"**Preview:** {result['preview']}...")
                        with col2:
                            st.write(f"**ID:** {result['document_id']}")
                            st.write(f"**Rank:** {result['rank']:.3f}")
            else:
                st.warning("âš ï¸ No documents found")
                
        except Exception as e:
            st.error(f"âŒ Search failed: {e}")

with tab3:
    st.header("ğŸ“Š Database Analytics")
    
    if st.button("ğŸ“Š Generate Report"):
        try:
            # Get comprehensive stats
            import asyncpg
            
            async def get_analytics():
                conn = await asyncpg.connect(
                    host='localhost', port=5433, database='knowledge_base_test',
                    user='kb_admin', password='test_password_123'
                )
                
                # Document stats by type
                doc_types = await conn.fetch("""
                    SELECT document_type, COUNT(*) as count
                    FROM documents_metadata_v2
                    GROUP BY document_type
                    ORDER BY count DESC
                """)
                
                # Recent documents
                recent_docs = await conn.fetch("""
                    SELECT title, author, created_at, chunk_count
                    FROM documents_metadata_v2
                    ORDER BY created_at DESC
                    LIMIT 5
                """)
                
                # Vietnamese analysis stats
                vn_stats = await conn.fetchrow("""
                    SELECT 
                        COUNT(*) as analyzed_docs,
                        AVG(readability_score) as avg_readability
                    FROM vietnamese_text_analysis
                """)
                
                # Chunk statistics
                chunk_stats = await conn.fetchrow("""
                    SELECT 
                        COUNT(*) as total_chunks,
                        AVG(chunk_size_tokens) as avg_tokens,
                        AVG(chunk_quality_score) as avg_quality
                    FROM document_chunks_enhanced
                """)
                
                await conn.close()
                return doc_types, recent_docs, vn_stats, chunk_stats
            
            # Run analytics
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            doc_types, recent_docs, vn_stats, chunk_stats = loop.run_until_complete(get_analytics())
            
            # Display results
            col1, col2 = st.columns(2)
            
            with col1:
                st.subheader("ğŸ“Š Document Types")
                if doc_types:
                    import pandas as pd
                    df_types = pd.DataFrame(doc_types)
                    st.bar_chart(df_types.set_index('document_type'))
                
                st.subheader("ğŸ‡»ğŸ‡³ Vietnamese Analysis")
                if vn_stats:
                    st.metric("Analyzed Documents", vn_stats['analyzed_docs'] or 0)
                    st.metric("Avg Readability", f"{vn_stats['avg_readability'] or 0:.2f}")
            
            with col2:
                st.subheader("ğŸ”— Chunk Statistics")
                if chunk_stats:
                    st.metric("Total Chunks", chunk_stats['total_chunks'] or 0)
                    st.metric("Avg Tokens/Chunk", f"{chunk_stats['avg_tokens'] or 0:.1f}")
                    st.metric("Avg Quality Score", f"{chunk_stats['avg_quality'] or 0:.2f}")
                
                st.subheader("ğŸ“„ Recent Documents")
                if recent_docs:
                    for doc in recent_docs:
                        st.write(f"**{doc['title']}** by {doc['author']}")
                        st.write(f"Created: {doc['created_at'].strftime('%Y-%m-%d %H:%M')}, Chunks: {doc['chunk_count']}")
                        st.write("---")
        
        except Exception as e:
            st.error(f"âŒ Analytics failed: {e}")

# Footer
st.markdown("---")
st.markdown("""
<div style="text-align: center; color: #666;">
    <p>ğŸ“„ Enhanced Database Document Ingestion Tool</p>
    <p>Há»— trá»£ xá»­ lÃ½ tÃ i liá»‡u tiáº¿ng Viá»‡t vá»›i semantic chunking vÃ  vector embeddings</p>
</div>
""", unsafe_allow_html=True)
```

### **BÆ°á»›c 5: Táº¡o Docker cho Tool**

Táº¡o file `docker-compose.tool.yml`:

```yaml
# docker-compose.tool.yml
version: '3.8'

services:
  # Document ingestion tool
  document-tool:
    build:
      context: .
      dockerfile: docker/Dockerfile.tool
    container_name: chatbot-document-tool
    environment:
      DB_HOST: postgres-test
      DB_PORT: 5432
      DB_NAME: knowledge_base_test
      DB_USER: kb_admin
      DB_PASSWORD: test_password_123
      CHROMA_HOST: chromadb-test
      CHROMA_PORT: 8000
    volumes:
      - ./tools:/app/tools
      - ./logs:/app/logs
    ports:
      - "8501:8501"
    depends_on:
      - postgres-test
      - chromadb-test
    networks:
      - chatbot-test-network
    command: streamlit run tools/document_ingestion/streamlit_app.py --server.port=8501 --server.address=0.0.0.0

  # Extend existing services
  postgres-test:
    image: postgres:15-alpine
    container_name: chatbot-postgres-test
    environment:
      POSTGRES_DB: knowledge_base_test
      POSTGRES_USER: kb_admin
      POSTGRES_PASSWORD: test_password_123
    volumes:
      - postgres_test_data:/var/lib/postgresql/data
      - ./scripts/migrations:/docker-entrypoint-initdb.d:ro
    ports:
      - "5433:5432"
    networks:
      - chatbot-test-network

  redis-test:
    image: redis:7-alpine
    container_name: chatbot-redis-test
    ports:
      - "6380:6379"
    volumes:
      - redis_test_data:/data
    networks:
      - chatbot-test-network

  chromadb-test:
    image: chromadb/chroma:latest
    container_name: chatbot-chroma-test
    environment:
      CHROMA_SERVER_HOST: 0.0.0.0
      CHROMA_SERVER_HTTP_PORT: 8000
    volumes:
      - chromadb_test_data:/chroma/chroma
    ports:
      - "8001:8000"
    networks:
      - chatbot-test-network

  adminer:
    image: adminer
    container_name: chatbot-adminer
    ports:
      - "8080:8080"
    environment:
      ADMINER_DEFAULT_SERVER: postgres-test
    networks:
      - chatbot-test-network

volumes:
  postgres_test_data:
  redis_test_data:
  chromadb_test_data:

networks:
  chatbot-test-network:
    driver: bridge
```

### **BÆ°á»›c 6: Táº¡o Dockerfile cho Tool**

Táº¡o file `docker/Dockerfile.tool`:

```dockerfile
# docker/Dockerfile.tool
FROM python:3.9

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    postgresql-client \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements/tool_requirements.txt /app/
RUN pip install --no-cache-dir -r tool_requirements.txt

# Download Vietnamese NLP models
RUN python -c "import underthesea; underthesea.word_tokenize('test')" || echo "underthesea models downloaded"
RUN python -c "from sentence_transformers import SentenceTransformer; SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')" || echo "embedding model downloaded"

# Copy application code
COPY tools/ /app/tools/

# Create directories
RUN mkdir -p /app/logs /app/tools/document_ingestion/uploads /app/tools/document_ingestion/processed

# Set permissions
RUN chmod -R 755 /app/tools

# Expose Streamlit port
EXPOSE 8501

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8501/_stcore/health || exit 1

CMD ["streamlit", "run", "tools/document_ingestion/streamlit_app.py", "--server.port=8501", "--server.address=0.0.0.0"]
```

### **BÆ°á»›c 7: Simple Quick Start Script**

Táº¡o file `start_document_tool.bat` (Windows batch file):

```batch
@echo off
echo ğŸš€ Starting Enhanced Document Ingestion Tool

echo ğŸ“¦ Building and starting containers...
docker-compose -f docker-compose.tool.yml up --build -d

echo â³ Waiting for services to start...
timeout /t 30 /nobreak

echo ğŸ“Š Checking service status...
docker-compose -f docker-compose.tool.yml ps

echo âœ… Services started successfully!
echo.
echo ğŸŒ Access points:
echo   ğŸ“„ Document Tool:  http://localhost:8501
echo   ğŸ—„ï¸ Database Tool:  http://localhost:8080
echo   ğŸ”¢ ChromaDB API:   http://localhost:8001
echo.
echo ğŸ“‹ Database Connection Info:
echo   Server: postgres-test
echo   Username: kb_admin  
echo   Password: test_password_123
echo   Database: knowledge_base_test
echo.
echo Press any key to view tool logs...
pause
docker logs -f chatbot-document-tool
```

### **BÆ°á»›c 8: Táº¡o Sample Documents Ä‘á»ƒ Test**

Táº¡o má»™t sá»‘ file máº«u trong `data/sample_documents/`:

**File 1: `quy_trinh_nghi_phep.txt`**
```txt
QUY TRÃŒNH XIN NGHá»ˆ PHÃ‰P

1. Má»¤C ÄÃCH
Quy trÃ¬nh nÃ y nháº±m hÆ°á»›ng dáº«n nhÃ¢n viÃªn thá»±c hiá»‡n viá»‡c xin nghá»‰ phÃ©p má»™t cÃ¡ch chÃ­nh xÃ¡c vÃ  Ä‘áº§y Ä‘á»§.

2. PHáº M VI ÃP Dá»¤NG
Ãp dá»¥ng cho táº¥t cáº£ nhÃ¢n viÃªn chÃ­nh thá»©c cá»§a cÃ´ng ty.

3. QUY TRÃŒNH CHI TIáº¾T

BÆ°á»›c 1: NhÃ¢n viÃªn Ä‘iá»n Ä‘Æ¡n xin nghá»‰ phÃ©p
- Sá»­ dá»¥ng form chuáº©n cá»§a cÃ´ng ty
- Ghi rÃµ lÃ½ do nghá»‰ phÃ©p
- Thá»i gian nghá»‰ tá»« ngÃ y Ä‘áº¿n ngÃ y

BÆ°á»›c 2: Gá»­i Ä‘Æ¡n cho quáº£n lÃ½ trá»±c tiáº¿p
- Gá»­i trÆ°á»›c Ã­t nháº¥t 3 ngÃ y lÃ m viá»‡c
- TrÆ°á»ng há»£p kháº©n cáº¥p: thÃ´ng bÃ¡o qua Ä‘iá»‡n thoáº¡i

BÆ°á»›c 3: Quáº£n lÃ½ xem xÃ©t vÃ  phÃª duyá»‡t
- Thá»i gian phÃª duyá»‡t: tá»‘i Ä‘a 2 ngÃ y lÃ m viá»‡c
- Xem xÃ©t tÃ¬nh hÃ¬nh cÃ´ng viá»‡c vÃ  nhÃ¢n sá»±

BÆ°á»›c 4: ThÃ´ng bÃ¡o káº¿t quáº£
- HR cáº­p nháº­t vÃ o há»‡ thá»‘ng
- ThÃ´ng bÃ¡o cho nhÃ¢n viÃªn qua email

4. GHI CHÃš
- Nghá»‰ phÃ©p quÃ¡ 3 ngÃ y liÃªn tiáº¿p cáº§n cÃ³ giáº¥y tá» chá»©ng minh
- Nghá»‰ á»‘m cáº§n cÃ³ giáº¥y khÃ¡m bá»‡nh
```

**File 2: `chinh_sach_wfh.txt`**
```txt
CHÃNH SÃCH LÃ€M VIá»†C Tá»ª XA (WORK FROM HOME)

1. Tá»”NG QUAN
Nháº±m tÄƒng sá»± linh hoáº¡t trong cÃ´ng viá»‡c vÃ  cÃ¢n báº±ng cuá»™c sá»‘ng, cÃ´ng ty Ã¡p dá»¥ng chÃ­nh sÃ¡ch lÃ m viá»‡c tá»« xa.

2. QUY Äá»ŠNH CHUNG

2.1 Thá»i gian lÃ m viá»‡c tá»« xa
- Tá»‘i Ä‘a 3 ngÃ y/tuáº§n
- Cáº§n Ä‘Äƒng kÃ½ trÆ°á»›c Ã­t nháº¥t 1 ngÃ y
- KhÃ´ng Ã¡p dá»¥ng trong tuáº§n Ä‘áº§u tiÃªn cá»§a thÃ¡ng

2.2 Äiá»u kiá»‡n lÃ m viá»‡c tá»« xa
- CÃ³ mÃ´i trÆ°á»ng lÃ m viá»‡c á»•n Ä‘á»‹nh táº¡i nhÃ 
- Káº¿t ná»‘i internet á»•n Ä‘á»‹nh (tá»‘i thiá»ƒu 50Mbps)
- CÃ³ thiáº¿t bá»‹ lÃ m viá»‡c phÃ¹ há»£p

2.3 TrÃ¡ch nhiá»‡m cá»§a nhÃ¢n viÃªn
- Tham gia Ä‘áº§y Ä‘á»§ cÃ¡c cuá»™c há»p online
- BÃ¡o cÃ¡o tiáº¿n Ä‘á»™ cÃ´ng viá»‡c hÃ ng ngÃ y
- CÃ³ máº·t online trong giá» hÃ nh chÃ­nh
- Äáº£m báº£o tÃ­nh báº£o máº­t thÃ´ng tin

3. QUY TRÃŒNH ÄÄ‚NG KÃ
- Gá»­i yÃªu cáº§u qua há»‡ thá»‘ng HR
- Quáº£n lÃ½ trá»±c tiáº¿p phÃª duyá»‡t
- ThÃ´ng bÃ¡o cho team vá» lá»‹ch WFH

4. GIÃM SÃT VÃ€ ÄÃNH GIÃ
- Check-in hÃ ng ngÃ y lÃºc 9:00 AM
- BÃ¡o cÃ¡o cÃ´ng viá»‡c lÃºc 5:00 PM
- ÄÃ¡nh giÃ¡ hiá»‡u suáº¥t Ä‘á»‹nh ká»³
```

### **BÆ°á»›c 9: Cháº¡y Tool vÃ  Test**

1. **Khá»Ÿi Ä‘á»™ng tool:**
```cmd
# Cháº¡y batch file
start_document_tool.bat

# Hoáº·c cháº¡y báº±ng docker-compose
docker-compose -f docker-compose.tool.yml up --build
```

2. **Truy cáº­p cÃ¡c interface:**
- **Document Tool**: http://localhost:8501
- **Database Browser**: http://localhost:8080  
- **ChromaDB**: http://localhost:8001

3. **Test upload document:**
   - Má»Ÿ http://localhost:8501
   - Upload file tá»« `data/sample_documents/`
   - Äiá»n thÃ´ng tin tÃ i liá»‡u
   - Nháº¥n "Process Document"

### **BÆ°á»›c 10: Troubleshooting náº¿u cÃ³ lá»—i**

**Script kiá»ƒm tra:**
```batch
@echo off
echo ğŸ” Checking Document Tool Status

echo ğŸ“Š Docker containers status:
docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"

echo.
echo ğŸŒ Testing endpoints:

curl -s http://localhost:8501 >nul 2>&1
if %errorlevel%==0 (
    echo âœ… Document Tool: http://localhost:8501 - OK
) else (
    echo âŒ Document Tool: http://localhost:8501 - FAILED
)

curl -s http://localhost:8080 >nul 2>&1  
if %errorlevel%==0 (
    echo âœ… Adminer: http://localhost:8080 - OK
) else (
    echo âŒ Adminer: http://localhost:8080 - FAILED
)

curl -s http://localhost:8001/api/v1/heartbeat >nul 2>&1
if %errorlevel%==0 (
    echo âœ… ChromaDB: http://localhost:8001 - OK  
) else (
    echo âŒ ChromaDB: http://localhost:8001 - FAILED
)

echo.
echo ğŸ“‹ Recent logs:
echo --- Document Tool Logs ---
docker logs --tail 5 chatbot-document-tool
echo --- Database Logs ---  
docker logs --tail 5 chatbot-postgres-test

pause
```

## ğŸ‰ **Káº¿t quáº£ mong Ä‘á»£i**

Sau khi setup thÃ nh cÃ´ng, báº¡n sáº½ cÃ³:

1. **âœ… Web Interface** táº¡i http://localhost:8501 Ä‘á»ƒ:
   - Upload tÃ i liá»‡u (txt, docx, pdf, xlsx)
   - Xem thá»‘ng kÃª database
   - Test tÃ¬m kiáº¿m
   - Xem analytics

2. **âœ… Tá»± Ä‘á»™ng xá»­ lÃ½:**
   - TrÃ­ch xuáº¥t text tá»« file
   - PhÃ¢n tÃ­ch tiáº¿ng Viá»‡t vá»›i pyvi/underthesea
   - Táº¡o semantic chunks
   - Generate embeddings
   - LÆ°u vÃ o PostgreSQL + ChromaDB

3. **âœ… Database Browser** táº¡i http://localhost:8080
4. **âœ… Monitoring** vÃ  stats real-time

**BÃ¢y giá» báº¡n cÃ³ thá»ƒ:**
- Upload tÃ i liá»‡u tiáº¿ng Viá»‡t 
- Xem chÃºng Ä‘Æ°á»£c xá»­ lÃ½ tá»± Ä‘á»™ng
- Test tÃ¬m kiáº¿m full-text
- Xem database Ä‘Æ°á»£c populate vá»›i enhanced schema

Báº¡n cÃ³ muá»‘n test vá»›i tÃ i liá»‡u cá»¥ thá»ƒ nÃ o khÃ´ng? TÃ´i cÃ³ thá»ƒ hÆ°á»›ng dáº«n thÃªm!

---
Báº¡n Ä‘Ãºng rá»“i, tÃ´i giáº£i thÃ­ch ngáº¯n gá»n:

## ğŸ¯ **Má»¤C TIÃŠU**
Táº¡o tool Ä‘á»ƒ thÃªm tÃ i liá»‡u vÃ o database enhanced Ä‘Ã£ cÃ³ sáºµn.

## ğŸ—ï¸ **2 CÃCH TRIá»‚N KHAI**

### **CÃ¡ch 1: Tool trong Docker (nhÆ° tÃ´i code)**
**Æ¯u Ä‘iá»ƒm:**
- âœ… Tá»± Ä‘á»™ng káº¿t ná»‘i vá»›i database containers
- âœ… KhÃ´ng cáº§n cÃ i Python/dependencies trÃªn mÃ¡y Windows
- âœ… Isolated, khÃ´ng conflict
- âœ… Dá»… deploy vÃ  share

**NhÆ°á»£c Ä‘iá»ƒm:**  
- âŒ Phá»©c táº¡p hÆ¡n
- âŒ Tá»‘n resource Docker

### **CÃ¡ch 2: Tool ngoÃ i Docker (Ä‘Æ¡n giáº£n hÆ¡n)**
**Æ¯u Ä‘iá»ƒm:**
- âœ… ÄÆ¡n giáº£n, cháº¡y trá»±c tiáº¿p
- âœ… Debug dá»… hÆ¡n
- âœ… Ãt phá»¥ thuá»™c

**NhÆ°á»£c Ä‘iá»ƒm:**
- âŒ Pháº£i cÃ i Python + dependencies
- âŒ Config connection manual

## ğŸ¤” **KHUYáº¾N NGHá»Š**

**Náº¿u báº¡n má»›i há»c:** Chá»n **CÃ¡ch 2** (ngoÃ i Docker)
- Chá»‰ cáº§n 1 file Python Ä‘Æ¡n giáº£n 
- Cháº¡y: `python upload_tool.py`
- Connect tá»›i database qua port 5433

**Vá» embedding model:** ÄÃºng, báº¡n chá»‰ cáº§n Ä‘á»•i tÃªn model trong code tá»« `'paraphrase-multilingual-MiniLM-L12-v2'` thÃ nh `'Qwen/Qwen3-Embedding-0.6B'`.

## â“ **Báº N MUá»N**
1. Tool Ä‘Æ¡n giáº£n ngoÃ i Docker?
2. Hay giá»¯ nguyÃªn trong Docker?

TÃ´i sáº½ hÆ°á»›ng dáº«n theo cÃ¡ch báº¡n chá»n!
