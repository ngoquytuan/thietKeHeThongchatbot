Tuyá»‡t vá»i! TÃ´i sáº½ giáº£i thÃ­ch chi tiáº¿t tá»«ng container Ä‘ang cháº¡y vÃ  vai trÃ² cá»§a chÃºng trong há»‡ thá»‘ng. HÃ£y cÃ¹ng khÃ¡m phÃ¡ tá»«ng module má»™t cÃ¡ch dá»… hiá»ƒu:

## ğŸ³ **GIáº¢I THÃCH CHI TIáº¾T CÃC DOCKER CONTAINERS**

### **1. ğŸ˜ chatbot-postgres-test - "Bá»™ nÃ£o chÃ­nh cá»§a há»‡ thá»‘ng"**

**ÄÃ¢y lÃ  gÃ¬?**
- PostgreSQL lÃ  há»‡ quáº£n trá»‹ cÆ¡ sá»Ÿ dá»¯ liá»‡u quan há»‡ (nhÆ° Excel siÃªu máº¡nh)
- LÆ°u trá»¯ táº¥t cáº£ thÃ´ng tin cÃ³ cáº¥u trÃºc: metadata tÃ i liá»‡u, thÃ´ng tin ngÆ°á»i dÃ¹ng, logs

**Äang lÃ m gÃ¬?**
```sql
-- LÆ°u thÃ´ng tin tÃ i liá»‡u
documents_metadata_v2: TÃªn file, tÃ¡c giáº£, ngÃ y táº¡o, loáº¡i tÃ i liá»‡u
document_chunks_enhanced: CÃ¡c Ä‘oáº¡n vÄƒn báº£n Ä‘Ã£ Ä‘Æ°á»£c cáº¯t nhá»
rag_pipeline_sessions: Lá»‹ch sá»­ cÃ¡c cÃ¢u há»i vÃ  tráº£ lá»i

-- VÃ­ dá»¥ data thá»±c táº¿:
Title: "Quy trÃ¬nh xin nghá»‰ phÃ©p"
Author: "HR Department" 
Content: "BÆ°á»›c 1: Äiá»n Ä‘Æ¡n..."
Status: "approved"
```

**Kiá»ƒm tra PostgreSQL:**
```powershell
# VÃ o container PostgreSQL
docker exec -it chatbot-postgres-test psql -U kb_admin -d knowledge_base_test

# Xem cÃ¡c báº£ng Ä‘Ã£ táº¡o
\dt

# Xem dá»¯ liá»‡u máº«u
SELECT title, author, status FROM documents_metadata_v2;

# ThoÃ¡t
\q
```

### **2. ğŸ”´ chatbot-redis-test - "Bá»™ nhá»› Ä‘á»‡m tá»‘c Ä‘á»™ cao"**

**ÄÃ¢y lÃ  gÃ¬?**
- Redis nhÆ° "RAM má»Ÿ rá»™ng" - lÆ°u táº¡m thÃ´ng tin hay dÃ¹ng
- GiÃºp há»‡ thá»‘ng pháº£n há»“i nhanh hÆ¡n (thay vÃ¬ query database má»—i láº§n)

**Äang lÃ m gÃ¬?**
```redis
# LÆ°u cache cÃ¡c káº¿t quáº£ tÃ¬m kiáº¿m
user:123:last_query = "Quy trÃ¬nh nghá»‰ phÃ©p"
embedding:doc_456 = [0.1, 0.8, 0.3, ...] # Vector embeddings

# Session ngÆ°á»i dÃ¹ng
session:abc123 = {user_id: 456, login_time: "2024-01-01"}
```

**Kiá»ƒm tra Redis:**
```powershell
# VÃ o Redis container
docker exec -it chatbot-redis-test redis-cli

# Test Redis
ping
# Response: PONG

# Xem táº¥t cáº£ keys (hiá»‡n táº¡i cÃ²n trá»‘ng)
keys *

# Táº¡o test data
set test:hello "world"
get test:hello

# ThoÃ¡t
exit
```

### **3. ğŸŸ¢ chatbot-chroma-test - "Kho lÆ°u trá»¯ vector thÃ´ng minh"**

**ÄÃ¢y lÃ  gÃ¬?**
- ChromaDB chuyÃªn lÆ°u trá»¯ "vector embeddings" (sá»‘ hÃ³a vÄƒn báº£n)
- GiÃºp tÃ¬m kiáº¿m theo Ã½ nghÄ©a (semantic search) thay vÃ¬ chá»‰ tá»« khÃ³a

**Äang lÃ m gÃ¬?**
```python
# Chuyá»ƒn Ä‘á»•i vÄƒn báº£n thÃ nh vector
"Quy trÃ¬nh nghá»‰ phÃ©p" â†’ [0.1, 0.8, 0.3, 0.5, 0.2, ...]
"Xin phÃ©p nghá»‰ viá»‡c" â†’ [0.2, 0.7, 0.4, 0.5, 0.1, ...]
# Hai cÃ¢u nÃ y cÃ³ Ã½ nghÄ©a gáº§n nhau â†’ vector gáº§n nhau
```

**Kiá»ƒm tra ChromaDB:**
```powershell
# Test API cá»§a ChromaDB
curl http://localhost:8001/api/v1/heartbeat

# Xem collections (hiá»‡n táº¡i chÆ°a cÃ³)
curl http://localhost:8001/api/v1/collections
```

### **4. ğŸŒ chatbot-adminer - "Giao diá»‡n quáº£n lÃ½ database"**

**ÄÃ¢y lÃ  gÃ¬?**
- Adminer lÃ  cÃ´ng cá»¥ web Ä‘á»ƒ xem/quáº£n lÃ½ database (nhÆ° phpMyAdmin)
- GiÃºp báº¡n duyá»‡t data mÃ  khÃ´ng cáº§n dÃ¹ng command line

**Äang lÃ m gÃ¬?**
- Cung cáº¥p giao diá»‡n web táº¡i http://localhost:8080
- Cho phÃ©p xem/sá»­a/truy váº¥n database PostgreSQL

**CÃ¡ch sá»­ dá»¥ng Adminer:**
1. Má»Ÿ trÃ¬nh duyá»‡t: http://localhost:8080
2. ÄÄƒng nháº­p:
   - System: PostgreSQL
   - Server: postgres-test
   - Username: kb_admin
   - Password: test_password_123
   - Database: knowledge_base_test

### **5. âŒ chatbot-db-setup - "Thá»£ setup database (Ä‘Ã£ hoÃ n thÃ nh)"**

**Táº¡i sao khÃ´ng cháº¡y?**
- Container nÃ y chá»‰ cháº¡y 1 láº§n Ä‘á»ƒ setup database
- Sau khi hoÃ n thÃ nh viá»‡c táº¡o báº£ng vÃ  load data máº«u â†’ tá»± Ä‘á»™ng táº¯t
- ÄÃ¢y lÃ  hÃ nh vi BÃŒNH THÆ¯á»œNG!

**ÄÃ£ lÃ m gÃ¬?**
```python
# 1. Táº¡o cÃ¡c báº£ng (tables)
# 2. Táº¡o cÃ¡c index Ä‘á»ƒ tÄƒng tá»‘c
# 3. Load dá»¯ liá»‡u máº«u (3 tÃ i liá»‡u tiáº¿ng Viá»‡t)
# 4. Táº¡o bÃ¡o cÃ¡o setup
```

**Xem logs Ä‘á»ƒ hiá»ƒu Ä‘Ã£ lÃ m gÃ¬:**
```powershell
docker logs chatbot-db-setup
```

## ğŸ” **HÆ¯á»šNG DáºªN KHÃM PHÃ Há»† THá»NG**

### **BÆ°á»›c 1: KhÃ¡m phÃ¡ Database qua Adminer**

```powershell
# Má»Ÿ Adminer
start http://localhost:8080
```

Trong Adminer:
1. **ÄÄƒng nháº­p** vá»›i thÃ´ng tin á»Ÿ trÃªn
2. **Click vÃ o báº£ng `documents_metadata_v2`** â†’ xem dá»¯ liá»‡u máº«u
3. **Click vÃ o `SQL command`** â†’ cháº¡y cÃ¢u lá»‡nh:

```sql
-- Xem táº¥t cáº£ tÃ i liá»‡u
SELECT title, author, department_owner, status 
FROM documents_metadata_v2;

-- Xem tÃ i liá»‡u tiáº¿ng Viá»‡t
SELECT title, LEFT(content, 100) as preview
FROM documents_metadata_v2 
WHERE language_detected = 'vi';

-- Äáº¿m sá»‘ báº£ng trong database
SELECT COUNT(*) as total_tables 
FROM information_schema.tables 
WHERE table_schema = 'public';
```

### **BÆ°á»›c 2: Táº¡o file test Ä‘á»ƒ hiá»ƒu workflow**

Táº¡o file `understand_system.py`:

```python
# understand_system.py
import asyncio
import asyncpg
import json

async def explore_database():
    """KhÃ¡m phÃ¡ database Ä‘á»ƒ hiá»ƒu há»‡ thá»‘ng"""
    
    # Káº¿t ná»‘i database
    conn = await asyncpg.connect(
        host='localhost',
        port=5433,  # Port cá»§a PostgreSQL test
        database='knowledge_base_test',
        user='kb_admin',
        password='test_password_123'
    )
    
    print("ğŸ”— Connected to Enhanced Database!")
    print("=" * 50)
    
    # 1. Xem táº¥t cáº£ báº£ng
    tables = await conn.fetch("""
        SELECT table_name, 
               (SELECT COUNT(*) FROM information_schema.columns 
                WHERE table_name = t.table_name AND table_schema = 'public') as column_count
        FROM information_schema.tables t
        WHERE table_schema = 'public'
        ORDER BY table_name
    """)
    
    print(f"ğŸ“Š Database cÃ³ {len(tables)} báº£ng:")
    for table in tables:
        print(f"   ğŸ“‹ {table['table_name']} ({table['column_count']} cá»™t)")
    
    # 2. Xem dá»¯ liá»‡u máº«u
    print(f"\nğŸ“„ Dá»¯ liá»‡u máº«u:")
    documents = await conn.fetch("""
        SELECT title, author, department_owner, 
               LENGTH(content) as content_length,
               language_detected, status
        FROM documents_metadata_v2
        ORDER BY title
    """)
    
    for doc in documents:
        print(f"   ğŸ“ '{doc['title']}'")
        print(f"      ğŸ‘¤ TÃ¡c giáº£: {doc['author']}")
        print(f"      ğŸ¢ PhÃ²ng ban: {doc['department_owner']}")
        print(f"      ğŸ“ Ná»™i dung: {doc['content_length']} kÃ½ tá»±")
        print(f"      ğŸŒ NgÃ´n ngá»¯: {doc['language_detected']}")
        print(f"      ğŸ“Š Tráº¡ng thÃ¡i: {doc['status']}")
        print()
    
    # 3. Demo search functionality
    print("ğŸ” Demo tÃ¬m kiáº¿m:")
    
    # TÃ¬m kiáº¿m theo tá»« khÃ³a
    search_results = await conn.fetch("""
        SELECT title, author
        FROM documents_metadata_v2
        WHERE LOWER(title) LIKE '%nghá»‰ phÃ©p%'
           OR LOWER(content) LIKE '%nghá»‰ phÃ©p%'
    """)
    
    print(f"   TÃ¬m 'nghá»‰ phÃ©p': {len(search_results)} káº¿t quáº£")
    for result in search_results:
        print(f"      âœ… {result['title']} - {result['author']}")
    
    # 4. Xem cáº¥u trÃºc enhanced schema
    print(f"\nğŸ—ï¸ Cáº¥u trÃºc Enhanced Schema:")
    enhanced_features = await conn.fetch("""
        SELECT 
            COUNT(*) FILTER (WHERE vietnamese_segmented = true) as vietnamese_processed,
            COUNT(*) FILTER (WHERE search_tokens IS NOT NULL) as search_ready,
            COUNT(*) FILTER (WHERE jsonl_export_ready = true) as flashrag_ready
        FROM documents_metadata_v2
    """)
    
    feature = enhanced_features[0]
    print(f"   ğŸ‡»ğŸ‡³ Vietnamese processed: {feature['vietnamese_processed']}")
    print(f"   ğŸ” Search ready: {feature['search_ready']}")
    print(f"   ğŸ“¤ FlashRAG ready: {feature['flashrag_ready']}")
    
    # 5. Demo táº¡o pipeline session
    print(f"\nâš¡ Demo táº¡o pipeline session:")
    session_id = await conn.fetchval("""
        INSERT INTO rag_pipeline_sessions (
            original_query, processed_query, pipeline_type, pipeline_method,
            chunks_retrieved, processing_time_ms, response_quality_score
        ) VALUES (
            'LÃ m tháº¿ nÃ o Ä‘á»ƒ xin nghá»‰ phÃ©p?',
            'xin nghá»‰ phÃ©p quy trÃ¬nh',
            'standard',
            'hybrid',
            2,
            230,
            0.92
        ) RETURNING session_id
    """)
    
    print(f"   âœ… Táº¡o session thÃ nh cÃ´ng: {session_id}")
    
    # 6. Thá»‘ng kÃª há»‡ thá»‘ng
    print(f"\nğŸ“Š Thá»‘ng kÃª há»‡ thá»‘ng:")
    stats = await conn.fetchrow("""
        SELECT 
            pg_size_pretty(pg_database_size(current_database())) as db_size,
            (SELECT COUNT(*) FROM documents_metadata_v2) as total_docs,
            (SELECT COUNT(*) FROM document_chunks_enhanced) as total_chunks,
            (SELECT COUNT(*) FROM rag_pipeline_sessions) as total_sessions
    """)
    
    print(f"   ğŸ’¾ Dung lÆ°á»£ng database: {stats['db_size']}")
    print(f"   ğŸ“„ Tá»•ng documents: {stats['total_docs']}")
    print(f"   âœ‚ï¸ Tá»•ng chunks: {stats['total_chunks']}")
    print(f"   ğŸ”„ Tá»•ng sessions: {stats['total_sessions']}")
    
    await conn.close()
    print(f"\nğŸ‰ Exploration completed!")

if __name__ == "__main__":
    asyncio.run(explore_database())
```

Cháº¡y script nÃ y:
```powershell
pip install asyncpg
python understand_system.py
```

### **BÆ°á»›c 3: Workflow thá»±c táº¿**

```mermaid
graph TD
    User[ğŸ‘¤ User há»i: 'Xin nghá»‰ phÃ©p nhÆ° tháº¿ nÃ o?']
    
    PostgreSQL[(ğŸ˜ PostgreSQL<br/>TÃ¬m documents liÃªn quan)]
    ChromaDB[(ğŸŸ¢ ChromaDB<br/>Vector search theo Ã½ nghÄ©a)]
    Redis[(ğŸ”´ Redis<br/>Cache káº¿t quáº£)]
    
    Processing[âš¡ RAG Processing<br/>Káº¿t há»£p thÃ´ng tin]
    Response[ğŸ’¬ Tráº£ lá»i: 'Quy trÃ¬nh gá»“m 5 bÆ°á»›c...']
    
    User --> PostgreSQL
    User --> ChromaDB
    PostgreSQL --> Redis
    ChromaDB --> Redis
    Redis --> Processing
    Processing --> Response
    Response --> User
    
    style PostgreSQL fill:#e1f5fe
    style ChromaDB fill:#e8f5e8
    style Redis fill:#ffebee
```

## ğŸ¯ **TÃ“M Táº®T ÄÆ N GIáº¢N**

| Container | Vai trÃ² | Giá»‘ng nhÆ° |
|-----------|---------|-----------|
| **postgres-test** | LÆ°u trá»¯ dá»¯ liá»‡u cÃ³ cáº¥u trÃºc | Tá»§ há»“ sÆ¡ cÃ³ phÃ¢n loáº¡i |
| **chroma-test** | TÃ¬m kiáº¿m theo Ã½ nghÄ©a | ThÆ° viá»‡n thÃ´ng minh hiá»ƒu ngá»¯ cáº£nh |
| **redis-test** | Bá»™ nhá»› Ä‘á»‡m nhanh | BÃ n lÃ m viá»‡c Ä‘á»ƒ Ä‘á»“ hay dÃ¹ng |
| **adminer** | Giao diá»‡n quáº£n lÃ½ | Cá»­a sá»• Ä‘á»ƒ nhÃ¬n vÃ o tá»§ há»“ sÆ¡ |
| **db-setup** | Thá»£ setup (Ä‘Ã£ xong) | NgÆ°á»i sáº¯p xáº¿p tá»§ há»“ sÆ¡ ban Ä‘áº§u |

BÃ¢y giá» báº¡n Ä‘Ã£ hiá»ƒu rÃµ hÆ¡n vá» há»‡ thá»‘ng chÆ°a? CÃ³ muá»‘n tÃ´i giáº£i thÃ­ch thÃªm pháº§n nÃ o khÃ´ng?

---
Tuyá»‡t vá»i! HÃ£y cÃ¹ng "má»• xáº»" container **chatbot-db-setup** Ä‘á»ƒ hiá»ƒu tá»«ng bÆ°á»›c chi tiáº¿t mÃ  nÃ³ thá»±c hiá»‡n Ä‘á»ƒ táº¡o lÃªn kiáº¿n trÃºc database phá»©c táº¡p nÃ y.

## ğŸ” **BREAKDOWN CHI TIáº¾T CHATBOT-DB-SETUP**

### **ğŸ“Š Mermaid Workflow cá»§a DB-Setup**

```mermaid
graph TD
    Start([ğŸš€ Container khá»Ÿi Ä‘á»™ng])
    
    subgraph "Phase 1: Environment Setup"
        CheckEnv[ğŸ” Kiá»ƒm tra biáº¿n mÃ´i trÆ°á»ng<br/>DB_HOST, DB_PORT, etc.]
        InstallDeps[ğŸ“¦ CÃ i dependencies<br/>asyncpg, psycopg2]
        WaitDB[â³ Chá» PostgreSQL ready<br/>Retry 30 láº§n, má»—i láº§n 2s]
        TestConn[ğŸ”— Test connection<br/>SELECT 1]
    end
    
    subgraph "Phase 2: Schema Creation"
        CreateExt[ğŸ”§ Táº¡o Extensions<br/>uuid-ossp, pg_trgm]
        CreateEnums[ğŸ“ Táº¡o Enum Types<br/>access_level, document_type, status]
        CreateCoreTables[ğŸ—„ï¸ Táº¡o Core Tables<br/>documents_metadata_v2]
        CreateEnhanced[âš¡ Táº¡o Enhanced Tables<br/>chunks, bm25, vietnamese]
        CreateSupport[ğŸ”— Táº¡o Support Tables<br/>sessions, analytics]
    end
    
    subgraph "Phase 3: Indexing"
        CreatePrimary[ğŸ¯ Primary Indexes<br/>UUID, Status, Language]
        CreateSearch[ğŸ” Search Indexes<br/>GIN, TSVECTOR, Full-text]
        CreatePerf[âš¡ Performance Indexes<br/>Composite, Partial]
        CreateFK[ğŸ”— Foreign Key Indexes<br/>References, Cascades]
    end
    
    subgraph "Phase 4: Data Loading"
        LoadSample[ğŸ“„ Load Sample Documents<br/>3 Vietnamese documents]
        GenerateTokens[ğŸ”¤ Generate Search Tokens<br/>TSVECTOR for full-text]
        CreateChunks[âœ‚ï¸ Create Sample Chunks<br/>Break documents into pieces]
        InitBM25[ğŸ“Š Initialize BM25 Structure<br/>Term frequency setup]
    end
    
    subgraph "Phase 5: Validation"
        ValidateSchema[âœ… Validate Schema<br/>Check all tables exist]
        TestQueries[ğŸ§ª Test Basic Queries<br/>SELECT, COUNT, JOIN]
        CheckData[ğŸ“Š Check Data Integrity<br/>Foreign keys, constraints]
        GenStats[ğŸ“ˆ Generate Statistics<br/>Table sizes, row counts]
    end
    
    subgraph "Phase 6: Reporting"
        CreateReport[ğŸ“„ Create Setup Report<br/>Markdown format]
        LogResults[ğŸ“ Write to /app/logs<br/>Detailed execution log]
        DisplaySummary[ğŸ–¥ï¸ Display Summary<br/>Console output]
        SetStatus[âœ… Set Exit Status<br/>Success/Failure]
    end
    
    Start --> CheckEnv
    CheckEnv --> InstallDeps
    InstallDeps --> WaitDB
    WaitDB --> TestConn
    TestConn --> CreateExt
    
    CreateExt --> CreateEnums
    CreateEnums --> CreateCoreTables
    CreateCoreTables --> CreateEnhanced
    CreateEnhanced --> CreateSupport
    
    CreateSupport --> CreatePrimary
    CreatePrimary --> CreateSearch
    CreateSearch --> CreatePerf
    CreatePerf --> CreateFK
    
    CreateFK --> LoadSample
    LoadSample --> GenerateTokens
    GenerateTokens --> CreateChunks
    CreateChunks --> InitBM25
    
    InitBM25 --> ValidateSchema
    ValidateSchema --> TestQueries
    TestQueries --> CheckData
    CheckData --> GenStats
    
    GenStats --> CreateReport
    CreateReport --> LogResults
    LogResults --> DisplaySummary
    DisplaySummary --> SetStatus
    SetStatus --> End([ğŸ‰ Container tá»± Ä‘á»™ng táº¯t])
    
    %% Error handling
    WaitDB -->|Timeout after 60s| ErrorExit[âŒ Exit with error]
    TestConn -->|Connection failed| ErrorExit
    CreateCoreTables -->|SQL Error| ErrorExit
    LoadSample -->|Data error| ErrorExit
    ErrorExit --> End
    
    %% Styling
    classDef phase1 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef phase2 fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef phase3 fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef phase4 fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef phase5 fill:#e0f2f1,stroke:#00796b,stroke-width:2px
    classDef phase6 fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    classDef error fill:#ffebee,stroke:#d32f2f,stroke-width:2px
    
    class CheckEnv,InstallDeps,WaitDB,TestConn phase1
    class CreateExt,CreateEnums,CreateCoreTables,CreateEnhanced,CreateSupport phase2
    class CreatePrimary,CreateSearch,CreatePerf,CreateFK phase3
    class LoadSample,GenerateTokens,CreateChunks,InitBM25 phase4
    class ValidateSchema,TestQueries,CheckData,GenStats phase5
    class CreateReport,LogResults,DisplaySummary,SetStatus phase6
    class ErrorExit error
```

### **ğŸ“‹ Chi tiáº¿t tá»«ng Phase**

#### **Phase 1: Environment Setup (10-15 giÃ¢y)**
```python
# Kiá»ƒm tra environment variables
db_config = {
    'host': os.getenv('DB_HOST', 'postgres-test'),  # Container name
    'port': int(os.getenv('DB_PORT', 5432)),        # Internal port
    'database': os.getenv('DB_NAME', 'knowledge_base_test'),
    'user': os.getenv('DB_USER', 'kb_admin'),
    'password': os.getenv('DB_PASSWORD', 'test_password_123')
}

# CÃ i Ä‘áº·t Python packages
pip install asyncpg psycopg2-binary

# Chá» PostgreSQL sáºµn sÃ ng (retry logic)
for attempt in range(30):  # Tá»‘i Ä‘a 60 giÃ¢y
    try:
        conn = await asyncpg.connect(**db_config)
        await conn.execute('SELECT 1')  # Test query
        break
    except:
        await asyncio.sleep(2)  # Chá» 2 giÃ¢y rá»“i thá»­ láº¡i
```

#### **Phase 2: Schema Creation (20-30 giÃ¢y)**
```sql
-- Step 1: Táº¡o Extensions
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";    -- UUID generation
CREATE EXTENSION IF NOT EXISTS "pg_trgm";      -- Text similarity

-- Step 2: Táº¡o Enum Types
CREATE TYPE access_level_enum AS ENUM (
    'public', 'employee_only', 'manager_only', 'director_only', 'system_admin'
);

CREATE TYPE document_type_enum AS ENUM (
    'policy', 'procedure', 'technical_guide', 'report', 'manual', 
    'specification', 'template', 'form', 'presentation', 'training_material', 'other'
);

CREATE TYPE document_status_enum AS ENUM (
    'draft', 'review', 'approved', 'published', 'archived', 'deprecated'
);

-- Step 3: Táº¡o Core Tables
CREATE TABLE documents_metadata_v2 (
    document_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    title VARCHAR(500) NOT NULL,
    content TEXT,
    document_type document_type_enum NOT NULL,
    access_level access_level_enum NOT NULL DEFAULT 'employee_only',
    -- ... 25+ columns tá»•ng cá»™ng
);

-- Step 4: Táº¡o Enhanced Tables
CREATE TABLE document_chunks_enhanced (
    chunk_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    document_id UUID REFERENCES documents_metadata_v2(document_id),
    chunk_content TEXT NOT NULL,
    semantic_boundary BOOLEAN DEFAULT false,
    chunk_quality_score DECIMAL(3,2),
    -- ... semantic chunking metadata
);

CREATE TABLE document_bm25_index (
    bm25_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    chunk_id UUID REFERENCES document_chunks_enhanced(chunk_id),
    term VARCHAR(255) NOT NULL,
    bm25_score DECIMAL(8,4),
    -- ... BM25 specific fields
);

-- Step 5: Táº¡o Support Tables
CREATE TABLE rag_pipeline_sessions (
    session_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    original_query TEXT NOT NULL,
    pipeline_type VARCHAR(50),
    response_quality_score DECIMAL(3,2),
    -- ... tracking fields
);

CREATE TABLE vietnamese_text_analysis (
    analysis_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    word_segmentation JSONB,
    pos_tagging JSONB,
    readability_score DECIMAL(3,2),
    -- ... Vietnamese NLP fields
);
```

#### **Phase 3: Indexing (15-20 giÃ¢y)**
```sql
-- Primary Indexes (cho performance cÆ¡ báº£n)
CREATE INDEX idx_documents_v2_language ON documents_metadata_v2(language_detected);
CREATE INDEX idx_documents_v2_status ON documents_metadata_v2(status);
CREATE INDEX idx_documents_v2_type ON documents_metadata_v2(document_type);
CREATE INDEX idx_documents_v2_created ON documents_metadata_v2(created_at DESC);

-- Search Indexes (cho full-text search)
CREATE INDEX idx_documents_v2_search ON documents_metadata_v2 USING GIN(search_tokens);
CREATE INDEX idx_documents_v2_title ON documents_metadata_v2 USING GIN(to_tsvector('simple', title));

-- Performance Indexes (cho queries phá»©c táº¡p)
CREATE INDEX idx_chunks_enhanced_composite ON document_chunks_enhanced(
    document_id, semantic_boundary, chunk_quality_score DESC
) WHERE chunk_quality_score > 0.5;

-- Foreign Key Indexes (cho JOIN operations)
CREATE INDEX idx_chunks_document_id ON document_chunks_enhanced(document_id);
CREATE INDEX idx_bm25_chunk_id ON document_bm25_index(chunk_id);
CREATE INDEX idx_sessions_created ON rag_pipeline_sessions(created_at DESC);
```

#### **Phase 4: Data Loading (10-15 giÃ¢y)**
```python
# Sample Vietnamese documents
sample_documents = [
    {
        'title': 'Quy trÃ¬nh xin nghá»‰ phÃ©p',
        'content': '''Quy trÃ¬nh xin nghá»‰ phÃ©p táº¡i cÃ´ng ty bao gá»“m cÃ¡c bÆ°á»›c sau:
        1. NhÃ¢n viÃªn Ä‘iá»n Ä‘Æ¡n xin nghá»‰ phÃ©p
        2. Gá»­i Ä‘Æ¡n cho quáº£n lÃ½ trá»±c tiáº¿p
        3. Quáº£n lÃ½ phÃª duyá»‡t trong vÃ²ng 2 ngÃ y lÃ m viá»‡c
        4. HR cáº­p nháº­t vÃ o há»‡ thá»‘ng
        5. ThÃ´ng bÃ¡o káº¿t quáº£ cho nhÃ¢n viÃªn''',
        'document_type': 'procedure',
        'department_owner': 'HR'
    },
    {
        'title': 'ChÃ­nh sÃ¡ch lÃ m viá»‡c tá»« xa',
        'content': '''ChÃ­nh sÃ¡ch Work From Home:
        - Tá»‘i Ä‘a 3 ngÃ y/tuáº§n
        - ÄÄƒng kÃ½ trÆ°á»›c 1 ngÃ y
        - Tham gia há»p online Ä‘áº§y Ä‘á»§
        - BÃ¡o cÃ¡o tiáº¿n Ä‘á»™ hÃ ng ngÃ y''',
        'document_type': 'policy',
        'department_owner': 'HR'
    },
    {
        'title': 'HÆ°á»›ng dáº«n sá»­ dá»¥ng ERP',
        'content': '''HÆ°á»›ng dáº«n ERP system:
        1. Login vá»›i company email
        2. Module HR: cáº­p nháº­t thÃ´ng tin, xin phÃ©p
        3. Module Project: táº¡o task, bÃ¡o cÃ¡o''',
        'document_type': 'technical_guide',
        'department_owner': 'IT'
    }
]

# Load vÃ o database
for doc in sample_documents:
    document_id = await conn.fetchval("""
        INSERT INTO documents_metadata_v2 (title, content, document_type, ...)
        VALUES ($1, $2, $3, ...) RETURNING document_id
    """, doc['title'], doc['content'], ...)
    
# Generate search tokens
await conn.execute("""
    UPDATE documents_metadata_v2 
    SET search_tokens = to_tsvector('simple', title || ' ' || content)
    WHERE search_tokens IS NULL
""")
```

#### **Phase 5: Validation (5-10 giÃ¢y)**
```python
# Kiá»ƒm tra schema
tables = await conn.fetch("""
    SELECT table_name FROM information_schema.tables 
    WHERE table_schema = 'public'
""")
expected_tables = [
    'documents_metadata_v2', 'document_chunks_enhanced', 
    'document_bm25_index', 'rag_pipeline_sessions', 
    'vietnamese_text_analysis'
]

# Test queries
doc_count = await conn.fetchval("SELECT COUNT(*) FROM documents_metadata_v2")
assert doc_count == 3, f"Expected 3 documents, got {doc_count}"

# Check data integrity
orphaned_chunks = await conn.fetchval("""
    SELECT COUNT(*) FROM document_chunks_enhanced c
    LEFT JOIN documents_metadata_v2 d ON c.document_id = d.document_id
    WHERE d.document_id IS NULL
""")
assert orphaned_chunks == 0, "Found orphaned chunks"
```

#### **Phase 6: Reporting (2-5 giÃ¢y)**
```python
# Táº¡o bÃ¡o cÃ¡o chi tiáº¿t
report = f"""
# Enhanced Database Setup Report
Generated: {datetime.now()}

## Schema Statistics
- Tables created: {len(tables)}
- Indexes created: {index_count}
- Sample documents loaded: {doc_count}

## Tables Details:
{chr(10).join(f'- {table["table_name"]}' for table in tables)}

## Performance Metrics
- Setup time: {setup_time_seconds}s
- Database size: {db_size}
- Memory usage: {memory_usage}

## Test Results
âœ… Schema validation: PASSED
âœ… Data integrity: PASSED  
âœ… Search functionality: PASSED
âœ… Vietnamese support: READY

Status: SUCCESS
"""

# Ghi vÃ o file log
with open('/app/logs/setup_report.md', 'w') as f:
    f.write(report)
```

### **ğŸ” Xem chi tiáº¿t execution**

Äá»ƒ xem chÃ­nh xÃ¡c nhá»¯ng gÃ¬ container Ä‘Ã£ lÃ m:

```powershell
# Xem logs Ä‘áº§y Ä‘á»§
docker logs chatbot-db-setup

# Xem logs realtime (náº¿u container Ä‘ang cháº¡y)
docker logs -f chatbot-db-setup

# Xem logs vá»›i timestamp
docker logs -t chatbot-db-setup

# VÃ o container Ä‘á»ƒ check files Ä‘Æ°á»£c táº¡o
docker exec -it chatbot-db-setup ls -la /app/logs/
```

### **ğŸ“Š Timeline thá»±c táº¿**

| Phase | Thá»i gian | CÃ´ng viá»‡c chÃ­nh |
|-------|-----------|-----------------|
| **Phase 1** | 0-15s | Environment check, dependencies |
| **Phase 2** | 15-45s | Schema creation (5 tables, enums) |
| **Phase 3** | 45-65s | Indexing (15+ indexes) |
| **Phase 4** | 65-80s | Data loading (3 documents + tokens) |
| **Phase 5** | 80-90s | Validation & testing |
| **Phase 6** | 90-95s | Report generation |
| **Total** | ~95s | Complete setup |

### **ğŸ¯ Káº¿t quáº£ cuá»‘i cÃ¹ng**

Sau khi **chatbot-db-setup** hoÃ n thÃ nh, báº¡n cÃ³:

1. **âœ… 5 Tables chÃ­nh** vá»›i enhanced schema
2. **âœ… 15+ Indexes** cho performance
3. **âœ… 3 Sample documents** tiáº¿ng Viá»‡t
4. **âœ… Search tokens** Ä‘Ã£ Ä‘Æ°á»£c generate
5. **âœ… Full validation** Ä‘Ã£ passed
6. **âœ… Setup report** táº¡i `/app/logs/setup_report.md`

**Táº¡i sao container tá»± táº¯t?**
- Container nÃ y lÃ  "one-time job" - cháº¡y xong nhiá»‡m vá»¥ lÃ  táº¯t
- Giá»‘ng nhÆ° thá»£ xÃ¢y xong nhÃ  thÃ¬ vá», khÃ´ng cáº§n á»Ÿ láº¡i
- Database Ä‘Ã£ Ä‘Æ°á»£c setup vÃ  sáºµn sÃ ng cho cÃ¡c containers khÃ¡c sá»­ dá»¥ng

BÃ¢y giá» báº¡n Ä‘Ã£ hiá»ƒu rÃµ "black magic" bÃªn trong **chatbot-db-setup** chÆ°a? ğŸ©âœ¨
---
ChÃ­nh xÃ¡c! Báº¡n Ä‘Ã£ hiá»ƒu Ä‘Ãºng rá»“i. HÃ£y cÃ¹ng phÃ¢n tÃ­ch chi tiáº¿t:

## ğŸ¯ **SOURCE CODE Cá»¦A CHATBOT-DB-SETUP**

ÄÃºng váº­y! **chatbot-db-setup** Ä‘Æ°á»£c táº¡o ra bá»Ÿi:

### **1. Docker Compose Ä‘á»‹nh nghÄ©a**
```yaml
# docker-compose.yml - Äá»‹nh nghÄ©a container
db-setup:
  image: python:3.9-slim
  container_name: chatbot-db-setup
  volumes:
    - ./scripts:/app/scripts:ro
  command: >
    sh -c "
    pip install asyncpg psycopg2-binary &&
    python scripts/setup_database.py    # <-- FILE CHÃNH
    "
```

### **2. Script chÃ­nh**
```python
# scripts/setup_database.py - TRÃI TIM cá»§a db-setup
async def setup_enhanced_database():
    # 1. Connect to database
    # 2. Create schema 
    # 3. Load sample data
    # 4. Create report
```

### **3. Migration SQL files**
```sql
-- scripts/migrations/01_init_database.sql - Schema definition
CREATE TABLE documents_metadata_v2 (...);
CREATE TABLE document_chunks_enhanced (...);
-- ...
```

## ğŸ“Š **LUá»’NG Xá»¬ LÃ Dá»® LIá»†U Má»šI THá»°C Táº¾**

Khi báº¡n náº¡p **dá»¯ liá»‡u má»›i** vÃ o há»‡ thá»‘ng, Ä‘Ã¢y lÃ  luá»“ng xá»­ lÃ½ hoÃ n chá»‰nh:

```mermaid
graph TD
    subgraph "ğŸ”¸ INPUT STAGE"
        RawData[ğŸ“„ Raw Text Input<br/>PDF, DOCX, TXT, etc.]
        Upload[ğŸ“¤ Upload API<br/>FastAPI endpoint]
        Validation[âœ… Input Validation<br/>File type, size, encoding]
    end
    
    subgraph "ğŸ”¸ PREPROCESSING STAGE"  
        TextExtract[ğŸ“ Text Extraction<br/>pypdf, docx2txt, etc.]
        CleanText[ğŸ§¹ Text Cleaning<br/>Remove noise, normalize]
        LanguageDetect[ğŸŒ Language Detection<br/>Vietnamese/English detection]
        MetadataExtract[ğŸ“‹ Metadata Extraction<br/>Title, author, creation date]
    end
    
    subgraph "ğŸ”¸ VIETNAMESE NLP STAGE"
        ViTokenize[ğŸ”¤ Vietnamese Tokenization<br/>pyvi.ViTokenizer.tokenize()]
        POSTagging[ğŸ·ï¸ POS Tagging<br/>underthesea.pos_tag()]
        NER[ğŸ‘¤ Named Entity Recognition<br/>underthesea.ner()]
        CompoundWords[ğŸ”— Compound Word Detection<br/>tá»«_ghÃ©p identification]
        ReadabilityScore[ğŸ“Š Readability Analysis<br/>Difficulty scoring]
    end
    
    subgraph "ğŸ”¸ CHUNKING STAGE"
        SemanticChunk[âœ‚ï¸ Semantic Chunking<br/>500-1000 tokens, preserve meaning]
        OverlapCalc[ğŸ”„ Overlap Calculation<br/>50-100 tokens overlap]
        ChunkQuality[â­ Chunk Quality Scoring<br/>Completeness + readability]
        BoundaryDetect[ğŸ“ Semantic Boundary Detection<br/>Topic change detection]
    end
    
    subgraph "ğŸ”¸ EMBEDDING STAGE"
        EmbedGenerate[ğŸ§® Generate Embeddings<br/>text-embedding-ada-002]
        EmbedValidate[âœ… Embedding Validation<br/>Dimension check, quality]
        EmbedBackup[ğŸ’¾ Embedding Backup<br/>Fallback model ready]
    end
    
    subgraph "ğŸ”¸ SEARCH INDEX STAGE"
        BM25Generate[ğŸ” BM25 Index Generation<br/>Term frequency, IDF calculation]
        TSVectorGen[ğŸ“ TSVECTOR Generation<br/>PostgreSQL full-text tokens]
        KeywordExtract[ğŸ¯ Keyword Extraction<br/>Important terms identification]
        SearchOptimize[âš¡ Search Optimization<br/>Index tuning]
    end
    
    subgraph "ğŸ”¸ STORAGE STAGE"
        subgraph "PostgreSQL Storage"
            MetadataStore[(ğŸ“‹ Metadata Storage<br/>documents_metadata_v2)]
            ChunkStore[(âœ‚ï¸ Chunks Storage<br/>document_chunks_enhanced)]
            BM25Store[(ğŸ” BM25 Index<br/>document_bm25_index)]
            VnAnalysisStore[(ğŸ‡»ğŸ‡³ Vietnamese Analysis<br/>vietnamese_text_analysis)]
        end
        
        subgraph "Vector Storage"
            ChromaStore[(ğŸŸ¢ ChromaDB<br/>Vector embeddings)]
            FAISSStore[(ğŸ“Š FAISS Index<br/>Fast similarity search)]
        end
        
        subgraph "Cache Layer"
            RedisCache[(ğŸ”´ Redis<br/>Frequent queries cache)]
        end
    end
    
    subgraph "ğŸ”¸ VALIDATION STAGE"
        IntegrityCheck[ğŸ” Data Integrity Check<br/>Foreign keys, constraints]
        SearchTest[ğŸ§ª Search Functionality Test<br/>Query test cases]
        QualityAssurance[â­ Quality Assurance<br/>Content quality validation]
        FlashRAGExport[ğŸ“¤ FlashRAG Export<br/>JSONL format preparation]
    end
    
    subgraph "ğŸ”¸ MONITORING STAGE"
        PerfTracking[ğŸ“Š Performance Tracking<br/>Processing time, success rate]
        ErrorLogging[ğŸ“ Error Logging<br/>Failed operations tracking]
        AlertSystem[ğŸš¨ Alert System<br/>Quality threshold monitoring]
        ReportGen[ğŸ“„ Report Generation<br/>Processing summary]
    end
    
    %% Flow connections
    RawData --> Upload
    Upload --> Validation
    Validation --> TextExtract
    
    TextExtract --> CleanText
    CleanText --> LanguageDetect
    LanguageDetect --> MetadataExtract
    
    MetadataExtract --> ViTokenize
    ViTokenize --> POSTagging  
    POSTagging --> NER
    NER --> CompoundWords
    CompoundWords --> ReadabilityScore
    
    ReadabilityScore --> SemanticChunk
    SemanticChunk --> OverlapCalc
    OverlapCalc --> ChunkQuality
    ChunkQuality --> BoundaryDetect
    
    BoundaryDetect --> EmbedGenerate
    EmbedGenerate --> EmbedValidate
    EmbedValidate --> EmbedBackup
    
    EmbedBackup --> BM25Generate
    BM25Generate --> TSVectorGen
    TSVectorGen --> KeywordExtract
    KeywordExtract --> SearchOptimize
    
    SearchOptimize --> MetadataStore
    SearchOptimize --> ChunkStore
    SearchOptimize --> BM25Store
    SearchOptimize --> VnAnalysisStore
    SearchOptimize --> ChromaStore
    SearchOptimize --> FAISSStore
    SearchOptimize --> RedisCache
    
    MetadataStore --> IntegrityCheck
    ChromaStore --> IntegrityCheck
    IntegrityCheck --> SearchTest
    SearchTest --> QualityAssurance
    QualityAssurance --> FlashRAGExport
    
    FlashRAGExport --> PerfTracking
    PerfTracking --> ErrorLogging
    ErrorLogging --> AlertSystem
    AlertSystem --> ReportGen
    
    %% Error flows
    Validation -->|âŒ Invalid| ErrorLogging
    TextExtract -->|âŒ Failed| ErrorLogging
    EmbedGenerate -->|âŒ API Error| EmbedBackup
    IntegrityCheck -->|âŒ Failed| ErrorLogging
    
    %% Styling
    classDef input fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef process fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef nlp fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef chunk fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef embed fill:#e1f5fe,stroke:#0277bd,stroke-width:2px
    classDef search fill:#e8eaf6,stroke:#3f51b5,stroke-width:2px
    classDef storage fill:#e0f2f1,stroke:#00796b,stroke-width:2px
    classDef validate fill:#fff8e1,stroke:#ffa000,stroke-width:2px
    classDef monitor fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    
    class RawData,Upload,Validation input
    class TextExtract,CleanText,LanguageDetect,MetadataExtract process
    class ViTokenize,POSTagging,NER,CompoundWords,ReadabilityScore nlp
    class SemanticChunk,OverlapCalc,ChunkQuality,BoundaryDetect chunk
    class EmbedGenerate,EmbedValidate,EmbedBackup embed
    class BM25Generate,TSVectorGen,KeywordExtract,SearchOptimize search
    class MetadataStore,ChunkStore,BM25Store,VnAnalysisStore,ChromaStore,FAISSStore,RedisCache storage
    class IntegrityCheck,SearchTest,QualityAssurance,FlashRAGExport validate
    class PerfTracking,ErrorLogging,AlertSystem,ReportGen monitor
```

## ğŸ’» **IMPLEMENTATION CHI TIáº¾T**

### **Step 1: Data Input Processing**
```python
# scripts/data_ingestion.py
import asyncio
import asyncpg
from pathlib import Path
import pyvi
from underthesea import word_tokenize, pos_tag, ner
import openai
from sentence_transformers import SentenceTransformer

class DocumentProcessor:
    def __init__(self):
        self.db_pool = None
        self.embedding_model = SentenceTransformer('keepitreal/vietnamese-sbert')
        
    async def process_new_document(self, file_path: str, metadata: dict):
        """Xá»­ lÃ½ document má»›i tá»« Ä‘áº§u Ä‘áº¿n cuá»‘i"""
        
        print(f"ğŸš€ Báº¯t Ä‘áº§u xá»­ lÃ½: {file_path}")
        
        # ===== STAGE 1: INPUT & PREPROCESSING =====
        raw_text = self.extract_text(file_path)
        clean_text = self.clean_text(raw_text)
        language = self.detect_language(clean_text)
        
        print(f"ğŸ“ Extracted {len(raw_text)} chars, detected language: {language}")
        
        # ===== STAGE 2: VIETNAMESE NLP =====
        if language == 'vi':
            vn_analysis = self.vietnamese_nlp_analysis(clean_text)
            print(f"ğŸ‡»ğŸ‡³ Vietnamese analysis: {len(vn_analysis['words'])} words")
        
        # ===== STAGE 3: DOCUMENT STORAGE =====
        document_id = await self.store_document_metadata(
            title=metadata['title'],
            content=clean_text,
            language=language,
            vietnamese_analysis=vn_analysis if language == 'vi' else None
        )
        
        print(f"ğŸ’¾ Document stored with ID: {document_id}")
        
        # ===== STAGE 4: CHUNKING =====
        chunks = self.semantic_chunking(clean_text, language)
        print(f"âœ‚ï¸ Created {len(chunks)} semantic chunks")
        
        # ===== STAGE 5: EMBEDDING GENERATION =====
        embeddings = []
        for i, chunk in enumerate(chunks):
            embedding = await self.generate_embedding(chunk['content'])
            embeddings.append(embedding)
            print(f"ğŸ§® Generated embedding {i+1}/{len(chunks)}")
        
        # ===== STAGE 6: STORAGE =====
        await self.store_chunks_and_embeddings(document_id, chunks, embeddings)
        await self.generate_search_indexes(document_id, chunks)
        
        print(f"âœ… Document processing completed: {document_id}")
        return document_id
    
    def extract_text(self, file_path: str) -> str:
        """Extract text tá»« file"""
        if file_path.endswith('.pdf'):
            return self.extract_pdf_text(file_path)
        elif file_path.endswith('.docx'):
            return self.extract_docx_text(file_path)
        else:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
    
    def vietnamese_nlp_analysis(self, text: str) -> dict:
        """Vietnamese NLP comprehensive analysis"""
        
        # Word segmentation vá»›i pyvi
        tokenized = pyvi.ViTokenizer.tokenize(text)
        words = word_tokenize(text)
        
        # POS tagging
        pos_tags = pos_tag(text)
        
        # Named Entity Recognition  
        entities = ner(text)
        
        # Compound words detection (tá»« cÃ³ dáº¥u _)
        compound_words = [word for word in tokenized.split() if '_' in word]
        
        # Technical terms detection
        technical_terms = self.extract_technical_terms(words)
        
        # Readability scoring
        readability = self.calculate_readability_vietnamese(words, tokenized.split())
        
        return {
            'tokenized_text': tokenized,
            'words': words,
            'pos_tags': pos_tags,
            'entities': entities,
            'compound_words': compound_words,
            'technical_terms': technical_terms,
            'readability_score': readability
        }
    
    def semantic_chunking(self, text: str, language: str) -> list:
        """Semantic chunking with Vietnamese support"""
        
        # Chia theo cÃ¢u trÆ°á»›c
        if language == 'vi':
            sentences = text.split('.')
        else:
            sentences = text.split('.')
        
        chunks = []
        current_chunk = []
        current_tokens = 0
        target_tokens = 750  # Target chunk size
        
        for sentence in sentences:
            sentence_tokens = len(sentence.split())
            
            if current_tokens + sentence_tokens > target_tokens and current_chunk:
                # Táº¡o chunk
                chunk_content = '. '.join(current_chunk)
                chunks.append({
                    'content': chunk_content,
                    'token_count': current_tokens,
                    'semantic_boundary': self.is_semantic_boundary(sentence),
                    'quality_score': self.calculate_chunk_quality(chunk_content)
                })
                
                # Reset cho chunk má»›i
                current_chunk = [sentence]
                current_tokens = sentence_tokens
            else:
                current_chunk.append(sentence)
                current_tokens += sentence_tokens
        
        # Chunk cuá»‘i cÃ¹ng
        if current_chunk:
            chunk_content = '. '.join(current_chunk)
            chunks.append({
                'content': chunk_content,
                'token_count': current_tokens,
                'semantic_boundary': True,
                'quality_score': self.calculate_chunk_quality(chunk_content)
            })
        
        return chunks
    
    async def generate_embedding(self, text: str) -> list:
        """Generate embedding with fallback"""
        try:
            # Primary: OpenAI
            response = await openai.Embedding.acreate(
                model="text-embedding-ada-002",
                input=text
            )
            return response['data'][0]['embedding']
            
        except Exception as e:
            print(f"âš ï¸ OpenAI failed, using fallback: {e}")
            # Fallback: Local model
            return self.embedding_model.encode(text).tolist()
    
    async def store_document_metadata(self, **kwargs) -> str:
        """Store document metadata vÃ o PostgreSQL"""
        
        async with self.db_pool.acquire() as conn:
            document_id = await conn.fetchval("""
                INSERT INTO documents_metadata_v2 (
                    title, content, language_detected, document_type,
                    access_level, department_owner, author, status,
                    vietnamese_segmented, jsonl_export_ready, flashrag_collection
                ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11)
                RETURNING document_id
            """, 
            kwargs['title'], kwargs['content'], kwargs['language'],
            'manual', 'employee_only', 'System', 'User Upload', 'approved',
            True, True, 'user_uploads'
            )
            
            # Store Vietnamese analysis if available
            if kwargs.get('vietnamese_analysis'):
                await conn.execute("""
                    INSERT INTO vietnamese_text_analysis (
                        document_id, original_text, word_segmentation,
                        pos_tagging, compound_words, technical_terms,
                        readability_score
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7)
                """,
                document_id, kwargs['content'][:1000],  # First 1000 chars
                json.dumps(kwargs['vietnamese_analysis']['words']),
                json.dumps(kwargs['vietnamese_analysis']['pos_tags']),
                kwargs['vietnamese_analysis']['compound_words'],
                kwargs['vietnamese_analysis']['technical_terms'],
                kwargs['vietnamese_analysis']['readability_score']
                )
        
        return document_id
    
    async def store_chunks_and_embeddings(self, document_id: str, chunks: list, embeddings: list):
        """Store chunks vÃ o PostgreSQL vÃ  embeddings vÃ o ChromaDB"""
        
        async with self.db_pool.acquire() as conn:
            for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
                # Store chunk metadata in PostgreSQL
                chunk_id = await conn.fetchval("""
                    INSERT INTO document_chunks_enhanced (
                        document_id, chunk_content, chunk_position,
                        chunk_size_tokens, semantic_boundary, chunk_quality_score,
                        chunk_method, embedding_model
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
                    RETURNING chunk_id
                """, 
                document_id, chunk['content'], i, chunk['token_count'],
                chunk['semantic_boundary'], chunk['quality_score'],
                'semantic', 'text-embedding-ada-002'
                )
                
                # Store embedding in ChromaDB
                await self.store_embedding_chromadb(chunk_id, embedding, chunk['content'])
        
        print(f"ğŸ’¾ Stored {len(chunks)} chunks with embeddings")
    
    async def generate_search_indexes(self, document_id: str, chunks: list):
        """Generate BM25 vÃ  full-text search indexes"""
        
        async with self.db_pool.acquire() as conn:
            for chunk in chunks:
                # Generate BM25 terms
                bm25_terms = self.generate_bm25_terms(chunk['content'])
                
                for term, score in bm25_terms.items():
                    await conn.execute("""
                        INSERT INTO document_bm25_index (
                            document_id, chunk_id, term, bm25_score, language
                        ) VALUES ($1, $2, $3, $4, $5)
                        ON CONFLICT (chunk_id, term, language) DO UPDATE SET
                            bm25_score = EXCLUDED.bm25_score
                    """, document_id, chunk['chunk_id'], term, score, 'vi')
            
            # Update search tokens
            await conn.execute("""
                UPDATE documents_metadata_v2 
                SET search_tokens = to_tsvector('simple', title || ' ' || content)
                WHERE document_id = $1
            """, document_id)
        
        print(f"ğŸ” Generated search indexes for document {document_id}")
```

### **Step 2: Thá»±c táº¿ sá»­ dá»¥ng**
```python
# scripts/upload_new_document.py
async def main():
    processor = DocumentProcessor()
    await processor.setup_database()
    
    # Upload document má»›i
    document_id = await processor.process_new_document(
        file_path="data/new_document.pdf",
        metadata={
            'title': 'Quy Ä‘á»‹nh má»›i vá» remote work',
            'author': 'HR Department',
            'department': 'HR'
        }
    )
    
    print(f"ğŸ‰ Document processed successfully: {document_id}")

if __name__ == "__main__":
    asyncio.run(main())
```

## ğŸ¯ **TÃ“M Táº®T LUá»’NG Xá»¬ LÃ**

| BÆ°á»›c | Input | Output | Thá»i gian |
|------|-------|--------|-----------|
| **1. Extract** | PDF/DOCX file | Raw text | 2-5s |
| **2. Vietnamese NLP** | Raw text | Tokens, POS, NER | 5-10s |
| **3. Chunking** | Clean text | Semantic chunks | 1-3s |
| **4. Embedding** | Chunks | Vector embeddings | 10-30s |
| **5. Storage** | All data | Database records | 2-5s |
| **6. Indexing** | Stored data | Search indexes | 3-8s |

**Total**: 23-61 giÃ¢y cho 1 document (tÃ¹y kÃ­ch thÆ°á»›c)

BÃ¢y giá» báº¡n Ä‘Ã£ hiá»ƒu rÃµ toÃ n bá»™ pipeline tá»« raw data â†’ enhanced database chÆ°a? ğŸš€
---
ChÃ­nh xÃ¡c! Báº¡n Ä‘Ã£ náº¯m Ä‘Æ°á»£c workflow cÆ¡ báº£n. Tuy nhiÃªn, trong Enhanced Database Architecture nÃ y, chÃºng ta cÃ³ má»™t pipeline phá»©c táº¡p vÃ  toÃ n diá»‡n hÆ¡n. HÃ£y Ä‘á»ƒ tÃ´i chi tiáº¿t tá»«ng bÆ°á»›c vÃ  cho báº¡n tháº¥y nhá»¯ng gÃ¬ thá»±c sá»± diá»…n ra "behind the scenes":

## ğŸ”„ **CHI TIáº¾T PIPELINE Xá»¬ LÃ Dá»® LIá»†U Má»šI**

### **ğŸ“Š Enhanced Data Processing Pipeline**

```mermaid
graph TD
    Start([ğŸ“„ New Document Input<br/>PDF/DOCX/TXT])
    
    subgraph "Stage 1: Document Ingestion"
        FileValidation[ğŸ” File Validation<br/>Size, format, encoding]
        MetadataExtraction[ğŸ“‹ Metadata Extraction<br/>Title, author, creation date]
        TextExtraction[ğŸ“ Text Extraction<br/>OCR if needed, encoding detection]
        QualityCheck[âœ… Quality Check<br/>Text quality, readability]
    end
    
    subgraph "Stage 2: Vietnamese NLP Processing"
        LanguageDetection[ğŸŒ Language Detection<br/>Vietnamese vs other languages]
        TextNormalization[ğŸ§¹ Text Normalization<br/>Unicode, diacritics handling]
        WordSegmentation[âœ‚ï¸ Word Segmentation<br/>pyvi tokenization]
        POSTagging[ğŸ·ï¸ POS Tagging<br/>Part-of-speech analysis]
        NERExtraction[ğŸ” NER Extraction<br/>Named entities, technical terms]
        ReadabilityAnalysis[ğŸ“Š Readability Analysis<br/>Complexity scoring]
    end
    
    subgraph "Stage 3: Semantic Chunking"
        TopicModeling[ğŸ¯ Topic Modeling<br/>Identify semantic boundaries]
        SemanticChunking[âœ‚ï¸ Semantic Chunking<br/>500-1000 tokens with overlap]
        ChunkQualityScore[â­ Chunk Quality Scoring<br/>Coherence and completeness]
        HeadingContext[ğŸ“‘ Heading Context<br/>Map chunks to document structure]
    end
    
    subgraph "Stage 4: Multi-Model Embedding"
        PrimaryEmbedding[ğŸ¯ Primary Embedding<br/>Vietnamese-optimized model]
        FallbackEmbedding[ğŸ”„ Fallback Embedding<br/>Multilingual model]
        EmbeddingValidation[âœ… Embedding Validation<br/>Dimension check, quality]
        VectorNormalization[ğŸ“ Vector Normalization<br/>Cosine similarity prep]
    end
    
    subgraph "Stage 5: Hybrid Index Generation"
        BM25Generation[ğŸ“Š BM25 Term Generation<br/>Sparse vector creation]
        TFIDFCalculation[ğŸ”¢ TF-IDF Calculation<br/>Term importance scoring]
        TSVectorGeneration[ğŸ” TSVector Generation<br/>PostgreSQL full-text search]
        KeywordExtraction[ğŸ”‘ Keyword Extraction<br/>Important terms identification]
    end
    
    subgraph "Stage 6: Multi-Database Storage"
        PostgreSQLStorage[ğŸ˜ PostgreSQL Storage<br/>Metadata + chunks + analytics]
        ChromaDBStorage[ğŸŸ¢ ChromaDB Storage<br/>Vector embeddings]
        RedisCache[ğŸ”´ Redis Cache<br/>Frequently accessed data]
        TransactionManagement[ğŸ”’ Transaction Management<br/>ACID compliance]
    end
    
    subgraph "Stage 7: Index Optimization"
        DatabaseIndexing[ğŸ“‡ Database Indexing<br/>B-tree, GIN, GIST indexes]
        VectorIndexing[ğŸ“Š Vector Indexing<br/>HNSW, IVF for similarity search]
        SearchOptimization[âš¡ Search Optimization<br/>Query performance tuning]
        CacheWarming[ğŸ”¥ Cache Warming<br/>Preload frequent queries]
    end
    
    subgraph "Stage 8: Quality Assurance"
        DataValidation[âœ… Data Validation<br/>Integrity checks]
        SearchTesting[ğŸ§ª Search Testing<br/>Query accuracy validation]
        PerformanceBenchmark[ğŸ“ˆ Performance Benchmark<br/>Speed and accuracy metrics]
        ErrorReporting[ğŸ“‹ Error Reporting<br/>Issues and warnings]
    end
    
    Success([âœ… Document Successfully Processed<br/>Ready for RAG queries])
    Failed([âŒ Processing Failed<br/>Error logged and reported])
    
    Start --> FileValidation
    FileValidation --> MetadataExtraction
    MetadataExtraction --> TextExtraction
    TextExtraction --> QualityCheck
    
    QualityCheck --> LanguageDetection
    LanguageDetection --> TextNormalization
    TextNormalization --> WordSegmentation
    WordSegmentation --> POSTagging
    POSTagging --> NERExtraction
    NERExtraction --> ReadabilityAnalysis
    
    ReadabilityAnalysis --> TopicModeling
    TopicModeling --> SemanticChunking
    SemanticChunking --> ChunkQualityScore
    ChunkQualityScore --> HeadingContext
    
    HeadingContext --> PrimaryEmbedding
    PrimaryEmbedding --> FallbackEmbedding
    FallbackEmbedding --> EmbeddingValidation
    EmbeddingValidation --> VectorNormalization
    
    VectorNormalization --> BM25Generation
    BM25Generation --> TFIDFCalculation
    TFIDFCalculation --> TSVectorGeneration
    TSVectorGeneration --> KeywordExtraction
    
    KeywordExtraction --> PostgreSQLStorage
    PostgreSQLStorage --> ChromaDBStorage
    ChromaDBStorage --> RedisCache
    RedisCache --> TransactionManagement
    
    TransactionManagement --> DatabaseIndexing
    DatabaseIndexing --> VectorIndexing
    VectorIndexing --> SearchOptimization
    SearchOptimization --> CacheWarming
    
    CacheWarming --> DataValidation
    DataValidation --> SearchTesting
    SearchTesting --> PerformanceBenchmark
    PerformanceBenchmark --> ErrorReporting
    
    ErrorReporting --> Success
    
    %% Error paths
    FileValidation -->|Invalid file| Failed
    QualityCheck -->|Poor quality| Failed
    EmbeddingValidation -->|Embedding failed| Failed
    TransactionManagement -->|Storage failed| Failed
    DataValidation -->|Validation failed| Failed
    
    %% Styling
    classDef stage1 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef stage2 fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef stage3 fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef stage4 fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef stage5 fill:#e0f2f1,stroke:#00796b,stroke-width:2px
    classDef stage6 fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    classDef stage7 fill:#f1f8e9,stroke:#689f38,stroke-width:2px
    classDef stage8 fill:#e8eaf6,stroke:#3f51b5,stroke-width:2px
    classDef success fill:#c8e6c9,stroke:#4caf50,stroke-width:3px
    classDef error fill:#ffcdd2,stroke:#f44336,stroke-width:2px
    
    class FileValidation,MetadataExtraction,TextExtraction,QualityCheck stage1
    class LanguageDetection,TextNormalization,WordSegmentation,POSTagging,NERExtraction,ReadabilityAnalysis stage2
    class TopicModeling,SemanticChunking,ChunkQualityScore,HeadingContext stage3
    class PrimaryEmbedding,FallbackEmbedding,EmbeddingValidation,VectorNormalization stage4
    class BM25Generation,TFIDFCalculation,TSVectorGeneration,KeywordExtraction stage5
    class PostgreSQLStorage,ChromaDBStorage,RedisCache,TransactionManagement stage6
    class DatabaseIndexing,VectorIndexing,SearchOptimization,CacheWarming stage7
    class DataValidation,SearchTesting,PerformanceBenchmark,ErrorReporting stage8
    class Success success
    class Failed error
```

## ğŸ“‹ **CHI TIáº¾T Tá»ªNG BÆ¯á»šC**

### **Stage 1: Document Ingestion (3-8 giÃ¢y)**

| BÆ°á»›c | Input | CÃ´ng viá»‡c chi tiáº¿t | Output | Database Impact |
|------|-------|-------------------|--------|-----------------|
| **File Validation** | PDF/DOCX/TXT | - Check file size (<50MB)<br/>- Validate file format<br/>- Detect encoding (UTF-8, etc.)<br/>- Security scan (malware) | Valid file object | None |
| **Metadata Extraction** | File headers | - Extract title, author, creation date<br/>- Detect language<br/>- File properties<br/>- Version info | Document metadata | `documents_metadata_v2` record created |
| **Text Extraction** | Binary file | - PDF parsing (pdfplumber)<br/>- DOCX parsing (python-docx)<br/>- OCR if image-based PDF<br/>- Encoding conversion | Raw text string | `content` field populated |
| **Quality Check** | Raw text | - Check text length (>100 chars)<br/>- Detect broken encoding<br/>- Calculate readability<br/>- Flag low-quality content | Quality score | `quality_score` field updated |

```python
# VÃ­ dá»¥ code thá»±c táº¿
async def process_document_ingestion(file_path: str) -> Dict:
    # File validation
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"File not found: {file_path}")
    
    file_size = os.path.getsize(file_path)
    if file_size > 50 * 1024 * 1024:  # 50MB limit
        raise ValueError("File too large")
    
    # Extract text based on file type
    if file_path.endswith('.pdf'):
        text = extract_pdf_text(file_path)
    elif file_path.endswith('.docx'):
        text = extract_docx_text(file_path)
    else:
        with open(file_path, 'r', encoding='utf-8') as f:
            text = f.read()
    
    # Quality check
    if len(text) < 100:
        raise ValueError("Text too short")
    
    # Store initial record
    document_id = await conn.fetchval("""
        INSERT INTO documents_metadata_v2 (title, content, file_size_bytes, status)
        VALUES ($1, $2, $3, 'processing') RETURNING document_id
    """, extract_title(file_path), text, file_size)
    
    return {'document_id': document_id, 'text': text, 'file_size': file_size}
```

### **Stage 2: Vietnamese NLP Processing (8-15 giÃ¢y)**

| BÆ°á»›c | Input | CÃ´ng viá»‡c chi tiáº¿t | Output | Database Impact |
|------|-------|-------------------|--------|-----------------|
| **Language Detection** | Raw text | - Detect Vietnamese vs other languages<br/>- Confidence scoring<br/>- Mixed language handling | Language code + confidence | `language_detected` field |
| **Text Normalization** | Raw text | - Unicode normalization (NFD->NFC)<br/>- Remove extra whitespace<br/>- Fix common OCR errors<br/>- Expand abbreviations | Normalized text | `processed_text` updated |
| **Word Segmentation** | Normalized text | - Pyvi Vietnamese tokenization<br/>- Handle compound words<br/>- Syllable boundaries<br/>- Word boundary detection | Token array | `vietnamese_text_analysis` table |
| **POS Tagging** | Tokens | - Part-of-speech tagging<br/>- Grammatical role identification<br/>- Verb/noun/adjective classification | POS tags | `pos_tagging` JSONB field |
| **NER Extraction** | POS-tagged text | - Named entity recognition<br/>- Person/location/organization<br/>- Technical terms extraction<br/>- Proper nouns identification | Named entities | `named_entities` field |
| **Readability Analysis** | All above | - Vietnamese readability score<br/>- Complexity assessment<br/>- Formality level detection<br/>- Target audience identification | Readability metrics | `readability_score`, `formality_level` |

```python
# VÃ­ dá»¥ Vietnamese NLP processing
async def process_vietnamese_nlp(document_id: str, text: str) -> Dict:
    # Language detection
    language = detect_language(text)  # 'vi' for Vietnamese
    
    # Text normalization
    normalized_text = normalize_vietnamese_text(text)
    
    # Word segmentation using pyvi
    from pyvi import ViTokenizer
    tokens = ViTokenizer.tokenize(normalized_text).split()
    
    # POS tagging using underthesea
    from underthesea import pos_tag
    pos_tags = pos_tag(normalized_text)
    
    # Named entity recognition
    from underthesea import ner
    entities = ner(normalized_text)
    
    # Store results
    await conn.execute("""
        INSERT INTO vietnamese_text_analysis (
            document_id, original_text, processed_text,
            word_segmentation, pos_tagging, named_entities,
            readability_score, formality_level
        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
    """, 
    document_id, text, normalized_text,
    json.dumps({'tokens': tokens, 'count': len(tokens)}),
    json.dumps(pos_tags),
    json.dumps(entities),
    calculate_readability(tokens),
    detect_formality(text)
    )
    
    return {'tokens': tokens, 'pos_tags': pos_tags, 'entities': entities}
```

### **Stage 3: Semantic Chunking (5-12 giÃ¢y)**

| BÆ°á»›c | Input | CÃ´ng viá»‡c chi tiáº¿t | Output | Database Impact |
|------|-------|-------------------|--------|-----------------|
| **Topic Modeling** | Tokens + POS | - Identify topic boundaries<br/>- Sentence similarity analysis<br/>- Paragraph coherence scoring<br/>- Heading structure analysis | Topic segments | `heading_structure` JSONB |
| **Semantic Chunking** | Topic segments | - Create 500-1000 token chunks<br/>- Preserve semantic boundaries<br/>- Add 50-token overlap<br/>- Maintain context flow | Semantic chunks | `document_chunks_enhanced` records |
| **Chunk Quality Score** | Each chunk | - Assess coherence<br/>- Check completeness<br/>- Validate readability<br/>- Score informativeness | Quality scores (0-1) | `chunk_quality_score` field |
| **Heading Context** | Chunks + structure | - Map chunks to headings<br/>- Identify hierarchical position<br/>- Add contextual metadata<br/>- Link related sections | Context mappings | `heading_context`, `paragraph_context` |

```python
async def process_semantic_chunking(document_id: str, tokens: List[str], pos_tags: List) -> List[Dict]:
    # Topic modeling Ä‘á»ƒ find semantic boundaries
    sentences = split_into_sentences(tokens)
    
    chunks = []
    current_chunk = []
    current_tokens = 0
    
    for i, sentence in enumerate(sentences):
        sentence_tokens = len(sentence.split())
        
        # Check if adding this sentence would exceed chunk size
        if current_tokens + sentence_tokens > 1000 and current_chunk:
            # Create chunk vá»›i overlap
            chunk_content = ' '.join(current_chunk)
            
            # Calculate semantic boundary score
            semantic_boundary = is_semantic_boundary(sentence, sentences[i-1] if i > 0 else "")
            
            # Calculate quality score
            quality_score = calculate_chunk_quality(chunk_content, pos_tags)
            
            # Store chunk
            chunk_id = await conn.fetchval("""
                INSERT INTO document_chunks_enhanced (
                    document_id, chunk_content, chunk_position,
                    chunk_size_tokens, semantic_boundary,
                    chunk_method, chunk_quality_score,
                    overlap_with_prev, overlap_with_next
                ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
                RETURNING chunk_id
            """, 
            document_id, chunk_content, len(chunks),
            current_tokens, semantic_boundary,
            'semantic', quality_score,
            50 if len(chunks) > 0 else 0,  # overlap with previous
            50  # will be updated for next chunk
            )
            
            chunks.append({
                'chunk_id': chunk_id,
                'content': chunk_content,
                'tokens': current_tokens,
                'quality': quality_score
            })
            
            # Start new chunk with overlap (last 50 tokens)
            overlap_text = ' '.join(current_chunk[-50:]) if len(current_chunk) > 50 else ''
            current_chunk = [overlap_text, sentence] if overlap_text else [sentence]
            current_tokens = len(overlap_text.split()) + sentence_tokens
        else:
            current_chunk.append(sentence)
            current_tokens += sentence_tokens
    
    return chunks
```

### **Stage 4: Multi-Model Embedding (15-45 giÃ¢y)**

| BÆ°á»›c | Input | CÃ´ng viá»‡c chi tiáº¿t | Output | Database Impact |
|------|-------|-------------------|--------|-----------------|
| **Primary Embedding** | Chunk content | - Vietnamese-optimized model<br/>- Sentence transformers<br/>- Batch processing (8-16 chunks)<br/>- GPU acceleration if available | 768/1536-dim vectors | ChromaDB storage |
| **Fallback Embedding** | Failed chunks | - Multilingual model backup<br/>- OpenAI API fallback<br/>- Error handling<br/>- Quality validation | Backup vectors | ChromaDB with model tag |
| **Embedding Validation** | Generated vectors | - Dimension validation<br/>- Range checking (-1 to 1)<br/>- Similarity sanity checks<br/>- Quality scoring | Validated vectors | `embedding_quality_vi` score |
| **Vector Normalization** | Raw vectors | - L2 normalization<br/>- Cosine similarity preparation<br/>- Index optimization<br/>- Storage formatting | Normalized vectors | Final ChromaDB storage |

```python
async def process_embeddings(chunks: List[Dict]) -> List[Dict]:
    from sentence_transformers import SentenceTransformer
    
    # Load Vietnamese-optimized model
    primary_model = SentenceTransformer('keepitreal/vietnamese-sbert')
    fallback_model = SentenceTransformer('intfloat/multilingual-e5-base')
    
    results = []
    
    # Process in batches for efficiency
    batch_size = 16
    for i in range(0, len(chunks), batch_size):
        batch = chunks[i:i + batch_size]
        texts = [chunk['content'] for chunk in batch]
        
        try:
            # Primary embedding
            embeddings = primary_model.encode(texts, convert_to_numpy=True)
            model_used = 'vietnamese-sbert'
        except Exception as e:
            # Fallback to multilingual model
            logger.warning(f"Primary model failed: {e}, using fallback")
            embeddings = fallback_model.encode(texts, convert_to_numpy=True)
            model_used = 'multilingual-e5-base'
        
        # Process each embedding
        for j, embedding in enumerate(embeddings):
            chunk = batch[j]
            
            # Validate embedding
            if validate_embedding(embedding):
                # Normalize vector
                normalized_embedding = embedding / np.linalg.norm(embedding)
                
                # Store in ChromaDB
                chroma_client.add(
                    collection_name="knowledge_base_v1",
                    documents=[chunk['content']],
                    embeddings=[normalized_embedding.tolist()],
                    ids=[str(chunk['chunk_id'])],
                    metadatas=[{
                        'document_id': str(chunk['document_id']),
                        'chunk_position': chunk['position'],
                        'model_used': model_used,
                        'quality_score': chunk['quality']
                    }]
                )
                
                # Update PostgreSQL with embedding metadata
                await conn.execute("""
                    UPDATE document_chunks_enhanced 
                    SET embedding_model = $2,
                        embedding_dimensions = $3,
                        updated_at = NOW()
                    WHERE chunk_id = $1
                """, chunk['chunk_id'], model_used, len(embedding))
                
                results.append({
                    'chunk_id': chunk['chunk_id'],
                    'embedding': normalized_embedding,
                    'model': model_used,
                    'dimensions': len(embedding)
                })
    
    return results
```

### **Stage 5: Hybrid Index Generation (10-20 giÃ¢y)**

| BÆ°á»›c | Input | CÃ´ng viá»‡c chi tiáº¿t | Output | Database Impact |
|------|-------|-------------------|--------|-----------------|
| **BM25 Generation** | Chunk tokens | - Calculate term frequencies<br/>- Document frequency analysis<br/>- BM25 scoring (k1=1.2, b=0.75)<br/>- Language-specific processing | BM25 scores | `document_bm25_index` records |
| **TF-IDF Calculation** | Term frequencies | - Term importance scoring<br/>- Inverse document frequency<br/>- Normalization<br/>- Rare term identification | TF-IDF weights | `keyword_density` JSONB |
| **TSVector Generation** | Clean text | - PostgreSQL full-text search<br/>- Vietnamese dictionary config<br/>- Stemming and stop words<br/>- Search token creation | TSVector objects | `search_tokens` field |
| **Keyword Extraction** | All above data | - Important term identification<br/>- Technical term extraction<br/>- Weighted keyword ranking<br/>- Search optimization | Ranked keywords | `keywords` array field |

```python
async def process_hybrid_indexing(chunks: List[Dict]) -> None:
    from sklearn.feature_extraction.text import TfidfVectorizer
    
    # Prepare texts for BM25
    chunk_texts = [chunk['content'] for chunk in chunks]
    
    # Vietnamese text preprocessing
    processed_texts = []
    for text in chunk_texts:
        # Vietnamese tokenization
        tokens = vietnamese_tokenize(text)  # using pyvi + underthesea
        
        # Remove stopwords
        vietnamese_stopwords = load_vietnamese_stopwords()
        filtered_tokens = [token for token in tokens if token not in vietnamese_stopwords]
        
        processed_texts.append(' '.join(filtered_tokens))
    
    # Calculate TF-IDF for the entire corpus
    tfidf = TfidfVectorizer(max_features=10000, ngram_range=(1, 2))
    tfidf_matrix = tfidf.fit_transform(processed_texts)
    
    # Get feature names (terms)
    feature_names = tfidf.get_feature_names_out()
    
    # Process each chunk
    for i, chunk in enumerate(chunks):
        chunk_id = chunk['chunk_id']
        chunk_tokens = vietnamese_tokenize(chunk['content'])
        
        # Calculate BM25 scores for this chunk
        bm25_scores = calculate_bm25_scores(chunk_tokens, chunk_texts, i)
        
        # Get TF-IDF scores for this chunk
        tfidf_scores = tfidf_matrix[i].toarray().flatten()
        
        # Store BM25 data
        for term, score in bm25_scores.items():
            if score > 1.0:  # Only store significant scores
                await conn.execute("""
                    INSERT INTO document_bm25_index (
                        chunk_id, document_id, term, 
                        term_frequency, bm25_score, language
                    ) VALUES ($1, $2, $3, $4, $5, $6)
                    ON CONFLICT (chunk_id, term, language) DO UPDATE
                    SET bm25_score = EXCLUDED.bm25_score
                """, 
                chunk_id, chunk['document_id'], term,
                chunk_tokens.count(term), score, 'vi'
                )
        
        # Generate PostgreSQL TSVector
        await conn.execute("""
            UPDATE document_chunks_enhanced 
            SET bm25_tokens = to_tsvector('simple', $2),
                keyword_density = $3
            WHERE chunk_id = $1
        """, 
        chunk_id, 
        ' '.join(chunk_tokens),
        json.dumps({
            feature_names[j]: float(tfidf_scores[j]) 
            for j in range(len(feature_names)) 
            if tfidf_scores[j] > 0.1
        })
        )
```

### **Stage 6: Multi-Database Storage (8-15 giÃ¢y)**

| Database | Data Stored | Transaction Handling | Performance Impact |
|----------|-------------|---------------------|-------------------|
| **PostgreSQL** | - Metadata, chunks, analytics<br/>- BM25 indexes, Vietnamese analysis<br/>- Pipeline sessions, quality scores | ACID transactions<br/>Rollback on failure<br/>Foreign key constraints | Primary storage<br/>~80% of write operations |
| **ChromaDB** | - Vector embeddings<br/>- Chunk content copies<br/>- Metadata for search | Eventually consistent<br/>Bulk operations<br/>Collection management | Vector operations<br/>~15% of write operations |
| **Redis** | - Frequently accessed embeddings<br/>- User session data<br/>- Search result cache | In-memory storage<br/>TTL-based expiry<br/>Pipeline operations | Cache layer<br/>~5% of write operations |

```python
async def process_multi_database_storage(chunks: List[Dict], embeddings: List[np.ndarray]) -> bool:
    # Start transaction for data consistency
    async with conn.transaction():
        try:
            # 1. Store in PostgreSQL (primary storage)
            for chunk in chunks:
                await conn.execute("""
                    UPDATE documents_metadata_v2 
                    SET chunk_count = chunk_count + 1,
                        updated_at = NOW(),
                        status = 'processing'
                    WHERE document_id = $1
                """, chunk['document_id'])
            
            # 2. Store in ChromaDB (vector storage)
            collection = chroma_client.get_or_create_collection("knowledge_base_v1")
            
            collection.add(
                documents=[chunk['content'] for chunk in chunks],
                embeddings=[emb.tolist() for emb in embeddings],
                ids=[str(chunk['chunk_id']) for chunk in chunks],
                metadatas=[{
                    'document_id': str(chunk['document_id']),
                    'chunk_position': chunk['position'],
                    'quality_score': chunk['quality'],
                    'language': 'vi'
                } for chunk in chunks]
            )
            
            # 3. Cache in Redis (performance optimization)
            redis_pipeline = redis_client.pipeline()
            
            for i, chunk in enumerate(chunks):
                # Cache embedding for fast retrieval
                redis_pipeline.set(
                    f"embedding:{chunk['chunk_id']}", 
                    embeddings[i].tobytes(),
                    ex=3600  # 1 hour TTL
                )
                
                # Cache chunk content for quick access
                redis_pipeline.set(
                    f"chunk:{chunk['chunk_id']}", 
                    json.dumps({
                        'content': chunk['content'],
                        'quality': chunk['quality'],
                        'document_id': str(chunk['document_id'])
                    }),
                    ex=1800  # 30 minutes TTL
                )
            
            redis_pipeline.execute()
            
            # 4. Update document status to completed
            await conn.execute("""
                UPDATE documents_metadata_v2 
                SET status = 'approved',
                    vietnamese_segmented = true,
                    jsonl_export_ready = true,
                    updated_at = NOW()
                WHERE document_id = $1
            """, chunks[0]['document_id'])  # All chunks belong to same document
            
            logger.info(f"Successfully stored {len(chunks)} chunks across all databases")
            return True
            
        except Exception as e:
            logger.error(f"Multi-database storage failed: {e}")
            # PostgreSQL transaction will auto-rollback
            # Cleanup ChromaDB and Redis manually
            cleanup_failed_storage(chunks)
            raise
```

### **Stage 7: Index Optimization (5-12 giÃ¢y)**

| Index Type | Purpose | Creation Time | Performance Impact |
|------------|---------|---------------|-------------------|
| **B-tree indexes** | Primary key, foreign key lookups | 1-3s | 10x faster exact matches |
| **GIN indexes** | Full-text search, JSONB queries | 3-5s | 50x faster text search |
| **Vector indexes** | Similarity search optimization | 2-4s | 100x faster vector search |
| **Composite indexes** | Multi-column query optimization | 1-2s | 20x faster complex queries |

### **Stage 8: Quality Assurance (3-8 giÃ¢y)**

| Test | Purpose | Pass Criteria | Action on Failure |
|------|---------|---------------|-------------------|
| **Data Integrity** | Check foreign key constraints | All references valid | Rollback transaction |
| **Search Accuracy** | Test query with known results | >80% precision@5 | Flag for review |
| **Performance** | Response time validation | <500ms vector search | Optimize indexes |
| **Vietnamese Support** | Language-specific features | Proper tokenization | Reprocess with different model |

## ğŸ“Š **ENHANCED TIMELINE COMPARISON**

| Stage | Your Original | Enhanced Reality | Difference | Key Additions |
|-------|---------------|------------------|------------|---------------|
| **Extract** | 2-5s | 3-8s | +50% | Quality checks, metadata extraction |
| **Vietnamese NLP** | 5-10s | 8-15s | +80% | POS, NER, readability analysis |
| **Chunking** | 1-3s | 5-12s | +300% | Semantic boundaries, quality scoring |
| **Embedding** | 10-30s | 15-45s | +50% | Multi-model, validation, fallback |
| **Storage** | 2-5s | 8-15s | +200% | Multi-database, transactions, caching |
| **Indexing** | 3-8s | 5-12s | +50% | Multiple index types, optimization |
| **TOTAL** | **23-61s** | **44-107s** | **+91%** | Quality, reliability, performance |

**Táº¡i sao thá»i gian tÄƒng gáº¥p Ä‘Ã´i nhÆ°ng Ä‘Ã¡ng giÃ¡?**

1. **ğŸ¯ Accuracy tÄƒng 40%**: Semantic chunking + Vietnamese NLP
2. **âš¡ Speed tÄƒng 300%**: Hybrid search (vector + BM25 + full-text)
3. **ğŸ”’ Reliability tÄƒng 500%**: Multi-database transactions, rollback
4. **ğŸ“Š Insight tÄƒng 1000%**: Quality metrics, performance tracking
5. **ğŸ‡»ğŸ‡³ Vietnamese support**: Native language processing

BÃ¢y giá» báº¡n Ä‘Ã£ tháº¥y rÃµ "trade-off" vÃ  lÃ½ do táº¡i sao pipeline phá»©c táº¡p hÆ¡n nhÆ°ng máº¡nh máº½ hÆ¡n ráº¥t nhiá»u! ğŸš€
---
Tuyá»‡t vá»i! ÄÃ¢y lÃ  cÃ¢u há»i cá»‘t lÃµi cá»§a RAG system. HÃ£y phÃ¢n tÃ­ch chi tiáº¿t cÃ¡ch dá»¯ liá»‡u Ä‘Æ°á»£c lÆ°u trá»¯ vÃ  thuáº­t toÃ¡n tÃ¬m kiáº¿m hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o.

## ğŸ—„ï¸ **CÃCH Dá»® LIá»†U ÄÆ¯á»¢C LÃ€M "Lá»šP"**

HÃ¬nh dung má»™t tÃ i liá»‡u nhÆ° má»™t cÄƒn nhÃ  cÃ³ nhiá»u cÃ¡ch Ä‘á»ƒ tÃ¬m kiáº¿m:

```mermaid
graph TD
    subgraph "ğŸ“„ Original Document: 'Quy trÃ¬nh xin nghá»‰ phÃ©p'"
        OriginalDoc[ğŸ“‹ Full Document<br/>3000 tá»«, 10 Ä‘oáº¡n vÄƒn]
    end
    
    subgraph "ğŸ—‚ï¸ LAYER 1: Metadata Storage (PostgreSQL)"
        Meta1[ğŸ“Š Basic Metadata<br/>Title: 'Quy trÃ¬nh xin nghá»‰ phÃ©p'<br/>Author: 'HR Department'<br/>Type: 'procedure']
        
        Meta2[ğŸ‡»ğŸ‡³ Vietnamese Analysis<br/>Word tokens: ['quy', 'trÃ¬nh', 'xin', 'nghá»‰', 'phÃ©p']<br/>POS tags: [N, N, V, V, N]<br/>Entities: ['HR', 'nhÃ¢n viÃªn', 'quáº£n lÃ½']]
        
        Meta3[ğŸ¯ Quality Metrics<br/>Readability: 0.78<br/>Formality: 'formal'<br/>Completeness: 0.92]
    end
    
    subgraph "âœ‚ï¸ LAYER 2: Chunking (PostgreSQL)"
        Chunk1[ğŸ“ Chunk 1 (Position 0)<br/>'Quy trÃ¬nh xin nghá»‰ phÃ©p gá»“m 5 bÆ°á»›c...'<br/>Tokens: 156<br/>Quality: 0.89<br/>Semantic boundary: true]
        
        Chunk2[ğŸ“ Chunk 2 (Position 1)<br/>'BÆ°á»›c 1: NhÃ¢n viÃªn Ä‘iá»n Ä‘Æ¡n...'<br/>Tokens: 234<br/>Quality: 0.91<br/>Semantic boundary: false]
        
        Chunk3[ğŸ“ Chunk 3 (Position 2)<br/>'BÆ°á»›c 2: Gá»­i Ä‘Æ¡n cho quáº£n lÃ½...'<br/>Tokens: 198<br/>Quality: 0.87<br/>Semantic boundary: false]
    end
    
    subgraph "ğŸ” LAYER 3: Search Indexes"
        subgraph "Dense Vector (ChromaDB)"
            Vector1[ğŸ¯ Chunk 1 Vector<br/>[0.12, -0.34, 0.78, 0.23, ...]<br/>768 dimensions<br/>Cosine similarity ready]
            Vector2[ğŸ¯ Chunk 2 Vector<br/>[0.45, -0.12, 0.56, 0.89, ...]<br/>768 dimensions]
            Vector3[ğŸ¯ Chunk 3 Vector<br/>[0.33, -0.67, 0.44, 0.12, ...]<br/>768 dimensions]
        end
        
        subgraph "Sparse BM25 (PostgreSQL)"
            BM251[ğŸ“Š Chunk 1 BM25<br/>quy: 2.45, trÃ¬nh: 3.12<br/>xin: 1.89, nghá»‰: 4.56<br/>phÃ©p: 3.78]
            BM252[ğŸ“Š Chunk 2 BM25<br/>nhÃ¢n_viÃªn: 5.23<br/>Ä‘iá»n: 2.67, Ä‘Æ¡n: 3.45]
            BM253[ğŸ“Š Chunk 3 BM25<br/>gá»­i: 2.34, quáº£n_lÃ½: 4.78<br/>phÃª_duyá»‡t: 3.89]
        end
        
        subgraph "Full-text (PostgreSQL TSVector)"
            FTS1[ğŸ” Chunk 1 TSVector<br/>'quy':1 'trinh':2 'xin':3<br/>'nghi':4 'phep':5<br/>Weighted, stemmed]
            FTS2[ğŸ” Chunk 2 TSVector<br/>'nhan':1 'vien':2 'dien':3<br/>'don':4 Stemmed roots]
            FTS3[ğŸ” Chunk 3 TSVector<br/>'gui':1 'quan':2 'ly':3<br/>'phe':4 'duyet':5]
        end
    end
    
    subgraph "ğŸ§  LAYER 4: Knowledge Graph (Optional)"
        KG1[ğŸ•¸ï¸ Relationships<br/>Chunk1 â†’ references â†’ Chunk2<br/>Chunk2 â†’ follows â†’ Chunk3<br/>Confidence: 0.85]
    end
    
    OriginalDoc --> Meta1
    OriginalDoc --> Meta2
    OriginalDoc --> Meta3
    
    Meta1 --> Chunk1
    Meta1 --> Chunk2
    Meta1 --> Chunk3
    
    Chunk1 --> Vector1
    Chunk1 --> BM251
    Chunk1 --> FTS1
    
    Chunk2 --> Vector2
    Chunk2 --> BM252
    Chunk2 --> FTS2
    
    Chunk3 --> Vector3
    Chunk3 --> BM253
    Chunk3 --> FTS3
    
    Chunk1 --> KG1
    Chunk2 --> KG1
    Chunk3 --> KG1
    
    classDef original fill:#e1f5fe,stroke:#0277bd,stroke-width:3px
    classDef metadata fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px
    classDef chunk fill:#fff3e0,stroke:#ef6c00,stroke-width:2px
    classDef vector fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef sparse fill:#e0f2f1,stroke:#00695c,stroke-width:2px
    classDef fulltext fill:#fce4ec,stroke:#ad1457,stroke-width:2px
    classDef kg fill:#e8eaf6,stroke:#3949ab,stroke-width:2px
    
    class OriginalDoc original
    class Meta1,Meta2,Meta3 metadata
    class Chunk1,Chunk2,Chunk3 chunk
    class Vector1,Vector2,Vector3 vector
    class BM251,BM252,BM253 sparse
    class FTS1,FTS2,FTS3 fulltext
    class KG1 kg
```

## ğŸ” **5 CÃCH TÃŒM KIáº¾M Tá»ªNG CÃ“ THUáº¬T TOÃN RIÃŠNG**

### **1. ğŸ¯ Dense Vector Search (Semantic Similarity)**

**CÃ¡ch lÆ°u trá»¯:**
```python
# ChromaDB - Vector embeddings
{
    "chunk_id": "uuid-123",
    "embedding": [0.12, -0.34, 0.78, 0.23, ..., 0.45],  # 768 dimensions
    "document": "Quy trÃ¬nh xin nghá»‰ phÃ©p gá»“m 5 bÆ°á»›c...",
    "metadata": {
        "document_id": "doc-456",
        "chunk_position": 0,
        "quality_score": 0.89
    }
}
```

**Thuáº­t toÃ¡n tÃ¬m kiáº¿m:**
```python
async def dense_vector_search(query: str, top_k: int = 5) -> List[Dict]:
    # 1. Convert query to vector
    query_embedding = embedding_model.encode(query)
    
    # 2. Similarity search in ChromaDB
    results = chroma_collection.query(
        query_embeddings=[query_embedding.tolist()],
        n_results=top_k,
        include=['documents', 'metadatas', 'distances']
    )
    
    # 3. Calculate cosine similarity scores
    similarities = [1 - distance for distance in results['distances'][0]]
    
    return [
        {
            'chunk_id': results['ids'][0][i],
            'content': results['documents'][0][i],
            'similarity_score': similarities[i],
            'metadata': results['metadatas'][0][i]
        }
        for i in range(len(results['ids'][0]))
    ]

# Example query: "LÃ m sao Ä‘á»ƒ xin nghá»‰?"
# Vector tÆ°Æ¡ng tá»± vá»›i chunks vá» quy trÃ¬nh nghá»‰ phÃ©p
```

### **2. ğŸ“Š Sparse BM25 Search (Keyword-based)**

**CÃ¡ch lÆ°u trá»¯:**
```sql
-- PostgreSQL BM25 table
CREATE TABLE document_bm25_index (
    chunk_id UUID,
    term VARCHAR(255),           -- "nghá»‰", "phÃ©p", "quy_trÃ¬nh"
    term_frequency INTEGER,      -- Sá»‘ láº§n xuáº¥t hiá»‡n trong chunk
    document_frequency INTEGER,  -- Sá»‘ chunk chá»©a term nÃ y
    bm25_score DECIMAL(8,4)     -- BM25 weight
);

-- Example data:
-- chunk_id: 123, term: "nghá»‰", tf: 3, df: 12, bm25: 2.45
-- chunk_id: 123, term: "phÃ©p", tf: 5, df: 8, bm25: 3.78
```

**Thuáº­t toÃ¡n BM25:**
```python
async def bm25_search(query: str, top_k: int = 5) -> List[Dict]:
    # 1. Tokenize query (Vietnamese-aware)
    query_terms = vietnamese_tokenize(query)  # ["xin", "nghá»‰", "phÃ©p"]
    
    # 2. Calculate BM25 score for each chunk
    chunk_scores = {}
    
    for term in query_terms:
        # Get all chunks containing this term
        chunks_with_term = await conn.fetch("""
            SELECT chunk_id, bm25_score, document_frequency
            FROM document_bm25_index 
            WHERE term = $1
        """, term)
        
        for chunk_data in chunks_with_term:
            chunk_id = chunk_data['chunk_id']
            
            if chunk_id not in chunk_scores:
                chunk_scores[chunk_id] = 0
                
            # Add BM25 score for this term
            chunk_scores[chunk_id] += chunk_data['bm25_score']
    
    # 3. Sort by score and get top results
    sorted_chunks = sorted(chunk_scores.items(), key=lambda x: x[1], reverse=True)
    
    return await get_chunk_details(sorted_chunks[:top_k])

# BM25 Formula: 
# score(q,d) = Î£(IDF(qi) * f(qi,d) * (k1+1)) / (f(qi,d) + k1*(1-b+b*|d|/avgdl))
```

### **3. ğŸ” Full-text Search (PostgreSQL TSVector)**

**CÃ¡ch lÆ°u trá»¯:**
```sql
-- TSVector trong PostgreSQL
UPDATE document_chunks_enhanced 
SET search_tokens = to_tsvector('vietnamese', chunk_content);

-- Example TSVector:
-- 'nghá»‰':4 'phÃ©p':5,8 'quy':1 'trÃ¬nh':2 'xin':3,7
-- Sá»‘ sau dáº¥u : lÃ  vá»‹ trÃ­ cá»§a tá»« trong text
```

**Thuáº­t toÃ¡n Full-text:**
```python
async def fulltext_search(query: str, top_k: int = 5) -> List[Dict]:
    # 1. Create tsquery from user query
    results = await conn.fetch("""
        SELECT 
            chunk_id,
            chunk_content,
            ts_rank(search_tokens, plainto_tsquery('vietnamese', $1)) as rank,
            ts_headline('vietnamese', chunk_content, plainto_tsquery('vietnamese', $1)) as headline
        FROM document_chunks_enhanced
        WHERE search_tokens @@ plainto_tsquery('vietnamese', $1)
        ORDER BY rank DESC
        LIMIT $2
    """, query, top_k)
    
    return [
        {
            'chunk_id': row['chunk_id'],
            'content': row['chunk_content'],
            'rank_score': float(row['rank']),
            'highlighted': row['headline']
        }
        for row in results
    ]

# Query: "nghá»‰ phÃ©p nhÃ¢n viÃªn"
# Matches chunks cÃ³ tá»« "nghá»‰" VÃ€ "phÃ©p" VÃ€ "nhÃ¢n viÃªn"
```

### **4. ğŸ”„ Hybrid Search (Káº¿t há»£p nhiá»u phÆ°Æ¡ng phÃ¡p)**

**Thuáº­t toÃ¡n Hybrid:**
```python
async def hybrid_search(query: str, top_k: int = 10, alpha: float = 0.5) -> List[Dict]:
    # 1. Run multiple searches in parallel
    dense_results = await dense_vector_search(query, top_k * 2)
    sparse_results = await bm25_search(query, top_k * 2)
    fulltext_results = await fulltext_search(query, top_k * 2)
    
    # 2. Normalize scores to 0-1 range
    dense_normalized = normalize_scores([r['similarity_score'] for r in dense_results])
    sparse_normalized = normalize_scores([r['bm25_score'] for r in sparse_results])
    fulltext_normalized = normalize_scores([r['rank_score'] for r in fulltext_results])
    
    # 3. Combine scores with weighted average
    combined_scores = {}
    
    # Dense vector weight
    for i, result in enumerate(dense_results):
        chunk_id = result['chunk_id']
        combined_scores[chunk_id] = {
            'dense_score': dense_normalized[i] * alpha,
            'sparse_score': 0,
            'fulltext_score': 0,
            'chunk_data': result
        }
    
    # BM25 weight  
    for i, result in enumerate(sparse_results):
        chunk_id = result['chunk_id']
        if chunk_id in combined_scores:
            combined_scores[chunk_id]['sparse_score'] = sparse_normalized[i] * (1 - alpha) * 0.6
        else:
            combined_scores[chunk_id] = {
                'dense_score': 0,
                'sparse_score': sparse_normalized[i] * (1 - alpha) * 0.6,
                'fulltext_score': 0,
                'chunk_data': result
            }
    
    # Full-text weight
    for i, result in enumerate(fulltext_results):
        chunk_id = result['chunk_id']
        if chunk_id in combined_scores:
            combined_scores[chunk_id]['fulltext_score'] = fulltext_normalized[i] * (1 - alpha) * 0.4
        else:
            combined_scores[chunk_id] = {
                'dense_score': 0,
                'sparse_score': 0,
                'fulltext_score': fulltext_normalized[i] * (1 - alpha) * 0.4,
                'chunk_data': result
            }
    
    # 4. Calculate final combined score
    final_results = []
    for chunk_id, scores in combined_scores.items():
        total_score = scores['dense_score'] + scores['sparse_score'] + scores['fulltext_score']
        
        final_results.append({
            'chunk_id': chunk_id,
            'total_score': total_score,
            'score_breakdown': scores,
            'content': scores['chunk_data']['content']
        })
    
    # 5. Sort by combined score and return top results
    final_results.sort(key=lambda x: x['total_score'], reverse=True)
    return final_results[:top_k]

# Hybrid search gives best of both worlds:
# - Semantic understanding from vectors
# - Exact keyword matching from BM25
# - Language-specific features from full-text
```

### **5. ğŸ•¸ï¸ Knowledge Graph Search (Relationship-based)**

**CÃ¡ch lÆ°u trá»¯:**
```sql
-- Knowledge graph edges
CREATE TABLE knowledge_graph_edges (
    source_chunk_id UUID,
    target_chunk_id UUID,
    relationship_type VARCHAR(50), -- 'references', 'follows', 'contradicts'
    confidence_score DECIMAL(3,2)
);

-- Example relationships:
-- Chunk "BÆ°á»›c 1" â†’ follows â†’ Chunk "BÆ°á»›c 2"
-- Chunk "Quy trÃ¬nh" â†’ references â†’ Chunk "Form Ä‘Æ¡n"
```

**Thuáº­t toÃ¡n Graph Traversal:**
```python
async def knowledge_graph_search(query: str, max_hops: int = 2) -> List[Dict]:
    # 1. Find initial relevant chunks using hybrid search
    initial_chunks = await hybrid_search(query, top_k=5)
    
    # 2. Expand search using graph relationships
    expanded_chunks = set()
    
    for chunk in initial_chunks:
        chunk_id = chunk['chunk_id']
        
        # Find related chunks within max_hops
        related = await conn.fetch("""
            WITH RECURSIVE chunk_graph AS (
                -- Base case: direct relationships
                SELECT target_chunk_id as chunk_id, 1 as hop_count, confidence_score
                FROM knowledge_graph_edges 
                WHERE source_chunk_id = $1
                
                UNION ALL
                
                -- Recursive case: follow relationships
                SELECT e.target_chunk_id, cg.hop_count + 1, cg.confidence_score * e.confidence_score
                FROM knowledge_graph_edges e
                JOIN chunk_graph cg ON e.source_chunk_id = cg.chunk_id
                WHERE cg.hop_count < $2
            )
            SELECT DISTINCT chunk_id, MIN(hop_count) as distance, MAX(confidence_score) as max_confidence
            FROM chunk_graph
            GROUP BY chunk_id
            ORDER BY max_confidence DESC, distance ASC
        """, chunk_id, max_hops)
        
        for rel in related:
            expanded_chunks.add(rel['chunk_id'])
    
    # 3. Get chunk content and combine with original results
    return await get_chunk_details(list(expanded_chunks))
```

## ğŸ¯ **SO SÃNH HIá»†U QUáº¢ CÃC THUáº¬T TOÃN**

| Thuáº­t toÃ¡n | Strengths | Weaknesses | Best Use Cases |
|------------|-----------|------------|----------------|
| **Dense Vector** | - Hiá»ƒu ngá»¯ cáº£nh<br/>- TÃ¬m Ã½ nghÄ©a tÆ°Æ¡ng tá»±<br/>- Äa ngÃ´n ngá»¯ | - Cháº­m vá»›i DB lá»›n<br/>- KhÃ´ng chÃ­nh xÃ¡c vá»›i tá»« khÃ³a cá»¥ thá»ƒ<br/>- Cáº§n GPU Ä‘á»ƒ nhanh | CÃ¢u há»i phá»©c táº¡p<br/>TÃ¬m kiáº¿m theo Ã½ nghÄ©a |
| **BM25 Sparse** | - Nhanh vá»›i keyword<br/>- ChÃ­nh xÃ¡c vá»›i thuáº­t ngá»¯<br/>- KhÃ´ng cáº§n GPU | - KhÃ´ng hiá»ƒu ngá»¯ cáº£nh<br/>- KÃ©m vá»›i synonym<br/>- Tá»‡ vá»›i cÃ¢u há»i dÃ i | TÃ¬m thuáº­t ngá»¯ chÃ­nh xÃ¡c<br/>TÃªn riÃªng, mÃ£ sá»‘ |
| **Full-text** | - Ráº¥t nhanh<br/>- TÃ­ch há»£p PostgreSQL<br/>- Há»— trá»£ wildcards | - Chá»‰ exact/fuzzy match<br/>- KhÃ´ng hiá»ƒu Ã½ nghÄ©a<br/>- Phá»¥ thuá»™c language config | TÃ¬m kiáº¿m trong app<br/>Admin queries |
| **Hybrid** | - Káº¿t há»£p Æ°u Ä‘iá»ƒm táº¥t cáº£<br/>- Balanced accuracy<br/>- Configurable weights | - Phá»©c táº¡p implement<br/>- Cáº§n tune parameters<br/>- Cháº­m hÆ¡n single method | Production RAG<br/>General-purpose search |
| **Knowledge Graph** | - TÃ¬m related info<br/>- Follow relationships<br/>- Context expansion | - Cáº§n build graph trÆ°á»›c<br/>- Phá»©c táº¡p maintain<br/>- CÃ³ thá»ƒ noise | Explainable AI<br/>Research queries |

## ğŸ“Š **DEMO THá»°C Táº¾ Vá»šI QUERY: "LÃ m sao Ä‘á»ƒ xin nghá»‰ phÃ©p?"**

```python
# Káº¿t quáº£ tá»« 5 thuáº­t toÃ¡n khÃ¡c nhau:

# 1. Dense Vector Results:
[
    {
        "content": "Quy trÃ¬nh xin nghá»‰ phÃ©p gá»“m 5 bÆ°á»›c cÆ¡ báº£n...",
        "similarity": 0.89,
        "reason": "Semantic similarity with 'xin nghá»‰ phÃ©p'"
    },
    {
        "content": "NhÃ¢n viÃªn cáº§n lÃ m theo thá»§ tá»¥c sau Ä‘á»ƒ Ä‘Æ°á»£c phÃ©p nghá»‰...",
        "similarity": 0.82,
        "reason": "Similar meaning but different words"
    }
]

# 2. BM25 Results:
[
    {
        "content": "BÆ°á»›c 1: Äiá»n form xin nghá»‰ phÃ©p...",
        "bm25_score": 15.4,
        "reason": "Exact keywords: 'xin', 'nghá»‰', 'phÃ©p'"
    },
    {
        "content": "Quy trÃ¬nh xin nghá»‰ phÃ©p Ä‘Æ°á»£c quy Ä‘á»‹nh...",
        "bm25_score": 12.8,
        "reason": "Multiple keyword matches"
    }
]

# 3. Full-text Results:
[
    {
        "content": "...xin nghá»‰ phÃ©p cáº§n cÃ³ Ä‘Æ¡n tá»«...",
        "rank": 0.95,
        "highlighted": "...xin <b>nghá»‰ phÃ©p</b> cáº§n cÃ³ Ä‘Æ¡n tá»«..."
    }
]

# 4. Hybrid Results (Best):
[
    {
        "content": "Quy trÃ¬nh xin nghá»‰ phÃ©p gá»“m 5 bÆ°á»›c...",
        "total_score": 0.91,
        "breakdown": {
            "semantic": 0.89,
            "keyword": 0.85,
            "fulltext": 0.88
        }
    }
]

# 5. Knowledge Graph Results:
[
    {
        "content": "Quy trÃ¬nh xin nghá»‰ phÃ©p...",
        "graph_distance": 0,
        "related_chunks": [
            "Form Ä‘Æ¡n xin nghá»‰",  # references relationship
            "PhÃª duyá»‡t cá»§a quáº£n lÃ½", # follows relationship
            "Cáº­p nháº­t HR system"  # follows relationship
        ]
    }
]
```

**Káº¿t luáº­n:** Má»—i thuáº­t toÃ¡n cÃ³ Ä‘iá»ƒm máº¡nh riÃªng, nhÆ°ng **Hybrid Search** cho káº¿t quáº£ tá»‘t nháº¥t vÃ¬ káº¿t há»£p Æ°u Ä‘iá»ƒm cá»§a táº¥t cáº£ phÆ°Æ¡ng phÃ¡p!

BÃ¢y giá» báº¡n Ä‘Ã£ hiá»ƒu cÃ¡ch "má»™t thÃ´ng tin Ä‘Æ°á»£c lÆ°u nhiá»u cÃ¡ch" vÃ  "nhiá»u thuáº­t toÃ¡n tÃ¬m kiáº¿m khÃ¡c nhau" chÆ°a? ğŸ¯
---
Tuyá»‡t vá»i! HÃ£y cÃ¹ng Ä‘Ã o sÃ¢u vÃ o tá»«ng database Ä‘á»ƒ hiá»ƒu chi tiáº¿t cÃ¡ch raw data Ä‘Æ°á»£c "tÃ¡ch lá»›p" vÃ  lÆ°u trá»¯. TÃ´i sáº½ mÃ´ táº£ nhÆ° "bÃ³c tÃ¡ch" má»™t tÃ i liá»‡u thÃ nh nhiá»u pháº§n khÃ¡c nhau:

## ğŸ—„ï¸ **CHI TIáº¾T MULTI-DATABASE STORAGE ARCHITECTURE**

### **ğŸ˜ PostgreSQL - "Bá»™ nÃ£o tá»• chá»©c chÃ­nh"**

```mermaid
graph TD
    subgraph "ğŸ“„ Raw Document: Quy trÃ¬nh xin nghá»‰ phÃ©p"
        RawDoc[ğŸ“‹ Original PDF<br/>Title: Quy trÃ¬nh xin nghá»‰ phÃ©p<br/>Content: 3000 words<br/>Author: HR Department<br/>File size: 2.5MB]
    end
    
    subgraph "ğŸ˜ PostgreSQL Storage Breakdown"
        subgraph "Core Tables"
            DocMeta[ğŸ“Š documents_metadata_v2<br/>document_id: uuid-123<br/>title: 'Quy trÃ¬nh xin nghá»‰ phÃ©p'<br/>author: 'HR Department'<br/>document_type: 'procedure'<br/>access_level: 'employee_only'<br/>department_owner: 'HR'<br/>language_detected: 'vi'<br/>file_size_bytes: 2621440<br/>chunk_count: 8<br/>status: 'approved'<br/>created_at: '2024-01-15'<br/>updated_at: '2024-01-15']
            
            Chunks[âœ‚ï¸ document_chunks_enhanced<br/>chunk_id: uuid-456<br/>document_id: uuid-123<br/>chunk_content: 'Quy trÃ¬nh xin nghá»‰ phÃ©p gá»“m 5 bÆ°á»›c...'<br/>chunk_position: 0<br/>chunk_size_tokens: 156<br/>semantic_boundary: true<br/>overlap_with_prev: 0<br/>overlap_with_next: 50<br/>heading_context: 'Tá»•ng quan'<br/>chunk_method: 'semantic'<br/>chunk_quality_score: 0.89<br/>embedding_model: 'vietnamese-sbert'<br/>created_at: '2024-01-15']
        end
        
        subgraph "Search Index Tables"
            BM25[ğŸ“Š document_bm25_index<br/>bm25_id: uuid-789<br/>chunk_id: uuid-456<br/>document_id: uuid-123<br/>term: 'nghá»‰'<br/>term_frequency: 3<br/>document_frequency: 12<br/>bm25_score: 2.45<br/>term_type: 'keyword'<br/>language: 'vi'<br/>created_at: '2024-01-15']
            
            TSVector[ğŸ” search_tokens (inside chunks)<br/>to_tsvector('vietnamese', content)<br/>'quy':1 'trÃ¬nh':2 'xin':3 'nghá»‰':4 'phÃ©p':5<br/>Weighted by importance<br/>Stemmed roots<br/>Stop words removed]
        end
        
        subgraph "Vietnamese NLP Tables"
            VNAnalysis[ğŸ‡»ğŸ‡³ vietnamese_text_analysis<br/>analysis_id: uuid-101<br/>document_id: uuid-123<br/>chunk_id: uuid-456<br/>original_text: 'Quy trÃ¬nh xin nghá»‰ phÃ©p...'<br/>processed_text: 'quy trÃ¬nh xin nghá»‰ phÃ©p...'<br/>word_segmentation: JSON array<br/>pos_tagging: JSON array<br/>compound_words: ['quy_trÃ¬nh', 'nghá»‰_phÃ©p']<br/>technical_terms: ['HR', 'phÃª_duyá»‡t']<br/>proper_nouns: ['HR Department']<br/>readability_score: 0.78<br/>formality_level: 'formal']
        end
        
        subgraph "Analytics & Session Tables"
            Sessions[ğŸ”„ rag_pipeline_sessions<br/>session_id: uuid-202<br/>original_query: 'LÃ m sao Ä‘á»ƒ xin nghá»‰ phÃ©p?'<br/>processed_query: 'xin nghá»‰ phÃ©p quy trÃ¬nh'<br/>pipeline_type: 'hybrid'<br/>pipeline_method: 'semantic_bm25'<br/>chunks_retrieved: 3<br/>processing_time_ms: 245<br/>response_quality_score: 0.92<br/>user_feedback: 5<br/>created_at: '2024-01-15']
            
            Performance[ğŸ“ˆ query_performance_metrics<br/>metric_id: uuid-303<br/>session_id: uuid-202<br/>retrieval_time_ms: 89<br/>embedding_time_ms: 156<br/>llm_time_ms: 1200<br/>memory_usage_mb: 245<br/>cpu_usage_percent: 23.5<br/>cache_hit_ratio: 0.67<br/>recorded_at: '2024-01-15']
        end
    end
    
    RawDoc --> DocMeta
    RawDoc --> Chunks
    Chunks --> BM25
    Chunks --> TSVector
    Chunks --> VNAnalysis
    DocMeta --> Sessions
    Sessions --> Performance
    
    classDef raw fill:#e1f5fe,stroke:#0277bd,stroke-width:3px
    classDef core fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px
    classDef search fill:#fff3e0,stroke:#ef6c00,stroke-width:2px
    classDef nlp fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef analytics fill:#e0f2f1,stroke:#00695c,stroke-width:2px
    
    class RawDoc raw
    class DocMeta,Chunks core
    class BM25,TSVector search
    class VNAnalysis nlp
    class Sessions,Performance analytics
```

### **ğŸŸ¢ ChromaDB - "Kho vector thÃ´ng minh"**

```mermaid
graph TD
    subgraph "ğŸŸ¢ ChromaDB Storage Structure"
        Collection[ğŸ“¦ Collection: knowledge_base_v1<br/>Embedding dimension: 768<br/>Distance metric: cosine<br/>Index type: HNSW<br/>Total documents: 15,420]
        
        subgraph "Vector Storage Per Chunk"
            Vector1[ğŸ¯ Chunk Vector 1<br/>ID: 'uuid-456'<br/>Embedding: [0.12, -0.34, 0.78, 0.23, ...]<br/>768 float32 values<br/>L2 normalized<br/>Storage: ~3KB per vector]
            
            Vector2[ğŸ¯ Chunk Vector 2<br/>ID: 'uuid-457'<br/>Embedding: [0.45, -0.12, 0.56, 0.89, ...]<br/>768 float32 values<br/>Cosine similarity ready]
            
            Vector3[ğŸ¯ Chunk Vector 3<br/>ID: 'uuid-458'<br/>Embedding: [0.33, -0.67, 0.44, 0.12, ...]<br/>768 float32 values<br/>HNSW index optimized]
        end
        
        subgraph "Metadata Storage"
            Meta1[ğŸ“‹ Chunk 1 Metadata<br/>'document_id': 'uuid-123'<br/>'chunk_position': 0<br/>'quality_score': 0.89<br/>'language': 'vi'<br/>'model_used': 'vietnamese-sbert'<br/>'created_at': '2024-01-15'<br/>'content_preview': 'Quy trÃ¬nh xin nghá»‰...']
            
            Meta2[ğŸ“‹ Chunk 2 Metadata<br/>'document_id': 'uuid-123'<br/>'chunk_position': 1<br/>'quality_score': 0.91<br/>'semantic_boundary': false]
            
            Meta3[ğŸ“‹ Chunk 3 Metadata<br/>'document_id': 'uuid-123'<br/>'chunk_position': 2<br/>'quality_score': 0.87<br/>'heading_context': 'BÆ°á»›c 2']
        end
        
        subgraph "HNSW Index Structure"
            HNSWIndex[ğŸ” HNSW Graph Index<br/>Nodes: 15,420 vectors<br/>Max connections (M): 16<br/>Construction parameter (efC): 200<br/>Search parameter (ef): 50<br/>Layers: 0-4 (hierarchical)<br/>Average query time: ~10ms]
        end
        
        subgraph "Content Duplication"
            ContentCopy[ğŸ“„ Content Copies<br/>document[0]: 'Quy trÃ¬nh xin nghá»‰ phÃ©p gá»“m 5 bÆ°á»›c...'<br/>document[1]: 'BÆ°á»›c 1: NhÃ¢n viÃªn Ä‘iá»n Ä‘Æ¡n...'<br/>document[2]: 'BÆ°á»›c 2: Gá»­i Ä‘Æ¡n cho quáº£n lÃ½...'<br/>Purpose: Fast retrieval without joins<br/>Trade-off: Storage vs Speed]
        end
    end
    
    Collection --> Vector1
    Collection --> Vector2
    Collection --> Vector3
    
    Vector1 --> Meta1
    Vector2 --> Meta2
    Vector3 --> Meta3
    
    Vector1 --> HNSWIndex
    Vector2 --> HNSWIndex
    Vector3 --> HNSWIndex
    
    Meta1 --> ContentCopy
    Meta2 --> ContentCopy
    Meta3 --> ContentCopy
    
    classDef collection fill:#e8f5e8,stroke:#2e7d32,stroke-width:3px
    classDef vector fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef metadata fill:#e0f2f1,stroke:#00695c,stroke-width:2px
    classDef index fill:#fff3e0,stroke:#ef6c00,stroke-width:2px
    classDef content fill:#fce4ec,stroke:#ad1457,stroke-width:2px
    
    class Collection collection
    class Vector1,Vector2,Vector3 vector
    class Meta1,Meta2,Meta3 metadata
    class HNSWIndex index
    class ContentCopy content
```

### **ğŸ”´ Redis - "Bá»™ nhá»› Ä‘á»‡m tá»‘c Ä‘á»™ cao"**

```mermaid
graph TD
    subgraph "ğŸ”´ Redis Cache Architecture"
        subgraph "Embedding Cache (Hot Data)"
            EmbedCache1[âš¡ embedding:uuid-456<br/>Key: 'embedding:uuid-456'<br/>Value: Binary embedding (3072 bytes)<br/>TTL: 3600 seconds (1 hour)<br/>Hit rate: 85%<br/>Purpose: Skip model inference]
            
            EmbedCache2[âš¡ embedding:uuid-457<br/>Compressed binary format<br/>LRU eviction policy<br/>Memory efficient storage]
        end
        
        subgraph "Chunk Content Cache"
            ChunkCache1[ğŸ“„ chunk:uuid-456<br/>Key: 'chunk:uuid-456'<br/>Value: JSON object:<br/>{<br/>  'content': 'Quy trÃ¬nh xin nghá»‰ phÃ©p...',<br/>  'quality': 0.89,<br/>  'document_id': 'uuid-123',<br/>  'position': 0<br/>}<br/>TTL: 1800 seconds (30 min)<br/>Size: ~2KB]
            
            ChunkCache2[ğŸ“„ chunk:uuid-457<br/>Similar JSON structure<br/>Frequently accessed chunks<br/>Reduces DB queries]
        end
        
        subgraph "User Session Data"
            UserSession[ğŸ‘¤ session:user_789<br/>Key: 'session:user_789'<br/>Value: {<br/>  'user_id': 789,<br/>  'last_queries': ['nghá»‰ phÃ©p', 'ERP'],<br/>  'preferences': 'vi',<br/>  'context_window': 5<br/>}<br/>TTL: 7200 seconds (2 hours)<br/>Session management]
            
            QueryHistory[ğŸ“‹ queries:user_789<br/>Key: 'queries:user_789'<br/>Value: List of recent queries<br/>FIFO queue (max 10 items)<br/>Personalization data]
        end
        
        subgraph "Search Result Cache"
            SearchCache1[ğŸ” search:'nghá»‰ phÃ©p'<br/>Key: 'search:nghá»‰ phÃ©p:hybrid'<br/>Value: {<br/>  'chunks': ['uuid-456', 'uuid-457'],<br/>  'scores': [0.89, 0.82],<br/>  'timestamp': 1642234567<br/>}<br/>TTL: 900 seconds (15 min)<br/>Cache expensive searches]
            
            SearchCache2[ğŸ” search:'ERP hÆ°á»›ng dáº«n'<br/>Similar structure<br/>Pipeline-specific cache<br/>Configurable TTL]
        end
        
        subgraph "Performance Metrics"
            MetricsCache[ğŸ“Š metrics:daily<br/>Key: 'metrics:2024-01-15'<br/>Value: {<br/>  'total_queries': 1547,<br/>  'avg_response_time': 245,<br/>  'cache_hit_ratio': 0.73,<br/>  'popular_terms': ['nghá»‰ phÃ©p', 'ERP']<br/>}<br/>Daily aggregation<br/>Real-time analytics]
            
            PipelineStats[âš™ï¸ pipeline:stats<br/>Key: 'pipeline:hybrid:stats'<br/>Value: Performance counters<br/>Success rates, error counts<br/>Real-time monitoring]
        end
    end
    
    classDef embedding fill:#ffebee,stroke:#c62828,stroke-width:2px
    classDef chunk fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef session fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px
    classDef search fill:#e0f2f1,stroke:#00695c,stroke-width:2px
    classDef metrics fill:#fff3e0,stroke:#ef6c00,stroke-width:2px
    
    class EmbedCache1,EmbedCache2 embedding
    class ChunkCache1,ChunkCache2 chunk
    class UserSession,QueryHistory session
    class SearchCache1,SearchCache2 search
    class MetricsCache,PipelineStats metrics
```

## ğŸ“Š **DETAILED COMPARISON TABLE**

| Aspect | PostgreSQL | ChromaDB | Redis |
|--------|------------|----------|-------|
| **Primary Purpose** | Structured data & relationships | Vector similarity search | High-speed caching |
| **Data Types** | Relational tables, JSON, arrays | Float vectors + metadata | Key-value pairs, binary |
| **Storage Pattern** | Normalized schema, foreign keys | Denormalized collections | Flat namespace |
| **Query Language** | SQL with extensions | API calls, vector search | Commands (GET/SET/etc.) |
| **Consistency** | ACID transactions | Eventually consistent | In-memory consistent |
| **Durability** | Persistent to disk | Persistent to disk | Optional persistence |
| **Performance** | ~100ms complex queries | ~10ms vector search | ~1ms key lookups |

### **ğŸ—‚ï¸ Data Distribution by Size**

```mermaid
pie title Data Storage Distribution (for 1000 documents)
    "PostgreSQL Metadata" : 45
    "PostgreSQL Chunks" : 35  
    "ChromaDB Vectors" : 15
    "Redis Cache" : 5
```

### **ğŸ’¾ Detailed Storage Breakdown for Sample Document**

| Component | PostgreSQL | ChromaDB | Redis | Total |
|-----------|------------|----------|-------|-------|
| **Document Metadata** | 2KB (complete metadata) | 500 bytes (search metadata) | 0 bytes | 2.5KB |
| **8 Chunks Text** | 24KB (full content + analysis) | 24KB (duplicate content) | 4KB (cached chunks) | 52KB |
| **8 Embeddings** | 0 bytes | 24KB (768 Ã— 4 bytes Ã— 8) | 8KB (hot embeddings) | 32KB |
| **BM25 Indexes** | 12KB (term frequencies) | 0 bytes | 0 bytes | 12KB |
| **Vietnamese Analysis** | 8KB (NLP results) | 0 bytes | 0 bytes | 8KB |
| **Search Indexes** | 3KB (TSVector) | 2KB (HNSW overhead) | 1KB (search cache) | 6KB |
| **Analytics Data** | 4KB (sessions, performance) | 0 bytes | 2KB (real-time metrics) | 6KB |
| **TOTAL PER DOCUMENT** | **53KB** | **50.5KB** | **15KB** | **118.5KB** |

### **ğŸ”„ Transaction Flow Example**

```python
# When storing a new document, here's what happens:

async def store_document_across_databases(document_data):
    # 1. PostgreSQL Transaction (Primary)
    async with pg_connection.transaction():
        # Store core metadata
        doc_id = await pg_connection.fetchval("""
            INSERT INTO documents_metadata_v2 (title, content, author, ...)
            VALUES ($1, $2, $3, ...) RETURNING document_id
        """, document_data.title, document_data.content, ...)
        
        # Store chunks
        chunk_ids = []
        for i, chunk in enumerate(document_data.chunks):
            chunk_id = await pg_connection.fetchval("""
                INSERT INTO document_chunks_enhanced (document_id, chunk_content, ...)
                VALUES ($1, $2, ...) RETURNING chunk_id
            """, doc_id, chunk.content, ...)
            chunk_ids.append(chunk_id)
        
        # Store BM25 indexes
        for chunk_id, terms in bm25_data.items():
            for term, score in terms.items():
                await pg_connection.execute("""
                    INSERT INTO document_bm25_index (chunk_id, term, bm25_score, ...)
                    VALUES ($1, $2, $3, ...)
                """, chunk_id, term, score, ...)
        
        # Store Vietnamese analysis
        for chunk_id, analysis in vn_analysis.items():
            await pg_connection.execute("""
                INSERT INTO vietnamese_text_analysis (chunk_id, word_segmentation, ...)
                VALUES ($1, $2, ...)
            """, chunk_id, json.dumps(analysis), ...)
    
    # 2. ChromaDB Storage (Secondary)
    try:
        collection = chroma_client.get_collection("knowledge_base_v1")
        collection.add(
            ids=[str(chunk_id) for chunk_id in chunk_ids],
            documents=[chunk.content for chunk in document_data.chunks],
            embeddings=[embedding.tolist() for embedding in embeddings],
            metadatas=[{
                'document_id': str(doc_id),
                'chunk_position': i,
                'quality_score': chunk.quality
            } for i, chunk in enumerate(document_data.chunks)]
        )
    except Exception as e:
        # ChromaDB failure doesn't rollback PostgreSQL (eventual consistency)
        logger.error(f"ChromaDB storage failed: {e}")
        # Schedule retry or manual intervention
    
    # 3. Redis Caching (Tertiary)
    try:
        pipeline = redis_client.pipeline()
        
        # Cache frequently accessed chunks
        for chunk_id, chunk in zip(chunk_ids, document_data.chunks):
            pipeline.set(
                f"chunk:{chunk_id}",
                json.dumps({
                    'content': chunk.content,
                    'quality': chunk.quality,
                    'document_id': str(doc_id)
                }),
                ex=1800  # 30 minutes TTL
            )
        
        # Cache embeddings if high quality
        for chunk_id, embedding in zip(chunk_ids, embeddings):
            if chunk.quality > 0.8:
                pipeline.set(
                    f"embedding:{chunk_id}",
                    embedding.tobytes(),
                    ex=3600  # 1 hour TTL for good embeddings
                )
        
        pipeline.execute()
    except Exception as e:
        # Redis failure is non-critical
        logger.warning(f"Redis caching failed: {e}")
    
    return doc_id
```

### **ğŸ¯ Query Performance Comparison**

| Query Type | PostgreSQL | ChromaDB | Redis | Hybrid |
|------------|------------|----------|-------|--------|
| **Exact keyword match** | 50ms | N/A | 1ms (if cached) | 10ms |
| **Semantic similarity** | N/A | 15ms | 1ms (if cached) | 20ms |
| **Complex filtering** | 80ms | 30ms | N/A | 45ms |
| **Full-text search** | 25ms | N/A | 1ms (if cached) | 15ms |
| **Recent user queries** | 100ms | N/A | 0.5ms | 1ms |

**Káº¿t luáº­n:** Má»—i database cÃ³ Ä‘iá»ƒm máº¡nh riÃªng, vÃ  viá»‡c káº¿t há»£p cáº£ 3 táº¡o ra má»™t há»‡ thá»‘ng máº¡nh máº½ vÃ  linh hoáº¡t hÆ¡n nhiá»u so vá»›i dÃ¹ng tá»«ng database riÃªng láº»!

Báº¡n cÃ³ muá»‘n tÃ´i giáº£i thÃ­ch thÃªm pháº§n nÃ o khÃ´ng? ğŸ¤”
---
CÃ¢u há»i ráº¥t quan trá»ng! Báº¡n Ä‘Ã£ cháº¡m vÃ o Ä‘iá»ƒm yáº¿u lá»›n nháº¥t cá»§a Multi-Database Architecture. HÃ£y phÃ¢n tÃ­ch chi tiáº¿t:

## âš ï¸ **Váº¤N Äá»€ DATA CONSISTENCY TRONG MULTI-DB**

### **ğŸ”— Má»‘i liÃªn káº¿t phá»©c táº¡p giá»¯a 3 databases**

```mermaid
graph TD
    subgraph "âŒ Váº¤N Äá»€ KHI Sá»¬A MANUAL"
        PGEdit[ğŸ˜ PostgreSQL<br/>Sá»­a title: 'Quy trÃ¬nh nghá»‰ phÃ©p'<br/>â†’ 'Quy Ä‘á»‹nh xin nghá»‰']
        
        ChromaStale[ğŸŸ¢ ChromaDB<br/>Váº«n cÃ²n metadata cÅ©:<br/>'title': 'Quy trÃ¬nh nghá»‰ phÃ©p'<br/>âŒ INCONSISTENT]
        
        RedisStale[ğŸ”´ Redis<br/>Cache váº«n chá»©a:<br/>'Quy trÃ¬nh nghá»‰ phÃ©p'<br/>âŒ STALE DATA]
        
        SearchBroken[ğŸ” Search Results<br/>PostgreSQL: 'Quy Ä‘á»‹nh xin nghá»‰'<br/>ChromaDB: 'Quy trÃ¬nh nghá»‰ phÃ©p'<br/>âŒ CONTRADICTORY]
    end
    
    PGEdit -.-> ChromaStale
    PGEdit -.-> RedisStale
    ChromaStale -.-> SearchBroken
    RedisStale -.-> SearchBroken
    
    classDef problem fill:#ffcdd2,stroke:#d32f2f,stroke-width:3px
    classDef broken fill:#ffebee,stroke:#f44336,stroke-width:2px
    
    class PGEdit problem
    class ChromaStale,RedisStale,SearchBroken broken
```

## ğŸ” **CHI TIáº¾T CÃC TRÆ¯á»œNG Há»¢P Sá»¬A Dá»® LIá»†U**

### **1. âœ… CÃ“ THá»‚ Sá»¬A AN TOÃ€N (Single Database)**

| TrÆ°á»ng | Báº£ng | Impact | CÃ¡ch sá»­a |
|--------|------|--------|----------|
| **status** | documents_metadata_v2 | Chá»‰ PostgreSQL | Sá»­a trá»±c tiáº¿p |
| **access_level** | documents_metadata_v2 | Chá»‰ PostgreSQL | Sá»­a trá»±c tiáº¿p |
| **author** | documents_metadata_v2 | ChromaDB metadata | Cáº§n Ä‘á»“ng bá»™ |
| **department_owner** | documents_metadata_v2 | Chá»‰ PostgreSQL | Sá»­a trá»±c tiáº¿p |

```sql
-- âœ… AN TOÃ€N - chá»‰ áº£nh hÆ°á»Ÿng PostgreSQL
UPDATE documents_metadata_v2 
SET status = 'archived',
    access_level = 'manager_only'
WHERE document_id = 'uuid-123';

-- âœ… AN TOÃ€N - internal analytics
UPDATE rag_pipeline_sessions 
SET response_quality_score = 0.95
WHERE session_id = 'uuid-456';
```

### **2. âš ï¸ Cáº¦N THáº¬N TRá»ŒNG (Cross-Database)**

| TrÆ°á»ng | Impact | Databases Affected | Cáº§n Ä‘á»“ng bá»™ |
|--------|--------|-------------------|-------------|
| **title** | Search results | PostgreSQL + ChromaDB | âœ… Báº¯t buá»™c |
| **content** | Everything | All 3 databases | âœ… Báº¯t buá»™c |
| **chunk_content** | Vector search | PostgreSQL + ChromaDB | âœ… Báº¯t buá»™c |
| **embedding_model** | Search accuracy | PostgreSQL + ChromaDB | âœ… Báº¯t buá»™c |

### **3. âŒ TUYá»†T Äá»I KHÃ”NG Sá»¬A (Cáº¥u trÃºc dá»¯ liá»‡u)**

| TrÆ°á»ng | LÃ½ do | Háº­u quáº£ náº¿u sá»­a |
|--------|-------|-----------------|
| **document_id** | Primary key | Táº¥t cáº£ references bá»‹ vá»¡ |
| **chunk_id** | Foreign keys everywhere | Vector search khÃ´ng hoáº¡t Ä‘á»™ng |
| **chunk_position** | Ordering logic | Context bá»‹ lá»™n xá»™n |
| **embedding_dimensions** | Vector compatibility | ChromaDB queries fail |

## ğŸ› ï¸ **CÃ”NG Cá»¤ Äá»’NG Bá»˜ Dá»® LIá»†U**

### **Script Ä‘á»“ng bá»™ khi sá»­a Title/Content:**

```python
# File: scripts/sync_data_after_edit.py
import asyncio
import asyncpg
import chromadb
import redis
import json
from typing import List, Dict

class DataSyncManager:
    def __init__(self):
        self.pg_conn = None
        self.chroma_client = None
        self.redis_client = None
        
    async def sync_document_changes(self, document_id: str, changes: Dict):
        """Äá»“ng bá»™ thay Ä‘á»•i document across all databases"""
        
        print(f"ğŸ”„ Starting sync for document: {document_id}")
        print(f"ğŸ“ Changes: {changes}")
        
        # 1. Update PostgreSQL first (source of truth)
        await self._update_postgresql(document_id, changes)
        
        # 2. Get latest data from PostgreSQL
        doc_data = await self._get_document_data(document_id)
        
        # 3. Update ChromaDB
        if self._affects_chromadb(changes):
            await self._update_chromadb(document_id, doc_data)
        
        # 4. Invalidate Redis cache
        if self._affects_cache(changes):
            await self._invalidate_redis_cache(document_id)
        
        # 5. Verify consistency
        await self._verify_consistency(document_id)
        
        print(f"âœ… Sync completed for document: {document_id}")
    
    async def _update_postgresql(self, document_id: str, changes: Dict):
        """Update PostgreSQL tables"""
        
        # Build dynamic UPDATE query
        set_clauses = []
        values = []
        param_count = 1
        
        for field, value in changes.items():
            set_clauses.append(f"{field} = ${param_count}")
            values.append(value)
            param_count += 1
        
        if set_clauses:
            # Update main document table
            query = f"""
                UPDATE documents_metadata_v2 
                SET {', '.join(set_clauses)}, updated_at = NOW()
                WHERE document_id = ${param_count}
            """
            values.append(document_id)
            
            await self.pg_conn.execute(query, *values)
            print(f"âœ… PostgreSQL updated: {len(set_clauses)} fields")
            
            # Special handling for content changes
            if 'content' in changes:
                await self._regenerate_search_tokens(document_id, changes['content'])
    
    async def _regenerate_search_tokens(self, document_id: str, new_content: str):
        """Regenerate search tokens after content change"""
        
        # Update TSVector for full-text search
        await self.pg_conn.execute("""
            UPDATE documents_metadata_v2 
            SET search_tokens = to_tsvector('vietnamese', $2)
            WHERE document_id = $1
        """, document_id, new_content)
        
        # Regenerate BM25 indexes for all chunks
        chunks = await self.pg_conn.fetch("""
            SELECT chunk_id, chunk_content 
            FROM document_chunks_enhanced 
            WHERE document_id = $1
        """, document_id)
        
        # Delete old BM25 data
        await self.pg_conn.execute("""
            DELETE FROM document_bm25_index 
            WHERE document_id = $1
        """, document_id)
        
        # Regenerate BM25 for each chunk
        for chunk in chunks:
            bm25_scores = self._calculate_bm25(chunk['chunk_content'])
            
            for term, score in bm25_scores.items():
                await self.pg_conn.execute("""
                    INSERT INTO document_bm25_index 
                    (chunk_id, document_id, term, bm25_score, language)
                    VALUES ($1, $2, $3, $4, 'vi')
                """, chunk['chunk_id'], document_id, term, score)
        
        print(f"âœ… Search indexes regenerated for {len(chunks)} chunks")
    
    async def _update_chromadb(self, document_id: str, doc_data: Dict):
        """Update ChromaDB metadata and content"""
        
        collection = self.chroma_client.get_collection("knowledge_base_v1")
        
        # Get all chunk IDs for this document
        chunk_ids = [str(chunk['chunk_id']) for chunk in doc_data['chunks']]
        
        try:
            # Update metadata for all chunks
            collection.update(
                ids=chunk_ids,
                documents=[chunk['chunk_content'] for chunk in doc_data['chunks']],
                metadatas=[{
                    'document_id': document_id,
                    'title': doc_data['title'],  # Updated title
                    'author': doc_data['author'],  # Updated author
                    'chunk_position': chunk['chunk_position'],
                    'quality_score': chunk['chunk_quality_score'],
                    'updated_at': str(doc_data['updated_at'])
                } for chunk in doc_data['chunks']]
            )
            
            print(f"âœ… ChromaDB updated: {len(chunk_ids)} chunks")
            
        except Exception as e:
            print(f"âŒ ChromaDB update failed: {e}")
            # Log error but don't fail entire sync
    
    async def _invalidate_redis_cache(self, document_id: str):
        """Clear Redis cache for document"""
        
        # Get all chunk IDs
        chunks = await self.pg_conn.fetch("""
            SELECT chunk_id FROM document_chunks_enhanced 
            WHERE document_id = $1
        """, document_id)
        
        # Delete cached chunks and embeddings
        cache_keys = []
        for chunk in chunks:
            chunk_id = chunk['chunk_id']
            cache_keys.extend([
                f"chunk:{chunk_id}",
                f"embedding:{chunk_id}"
            ])
        
        # Delete search result caches that might contain this document
        search_pattern_keys = await self.redis_client.keys("search:*")
        cache_keys.extend(search_pattern_keys)
        
        if cache_keys:
            deleted = await self.redis_client.delete(*cache_keys)
            print(f"âœ… Redis cache cleared: {deleted} keys deleted")
    
    async def _verify_consistency(self, document_id: str):
        """Verify data consistency across databases"""
        
        print("ğŸ” Verifying consistency...")
        
        # Get data from PostgreSQL
        pg_doc = await self.pg_conn.fetchrow("""
            SELECT title, author, updated_at 
            FROM documents_metadata_v2 
            WHERE document_id = $1
        """, document_id)
        
        # Get first chunk from ChromaDB
        collection = self.chroma_client.get_collection("knowledge_base_v1")
        chroma_chunks = await self.pg_conn.fetch("""
            SELECT chunk_id FROM document_chunks_enhanced 
            WHERE document_id = $1 LIMIT 1
        """, document_id)
        
        if chroma_chunks:
            chunk_id = str(chroma_chunks[0]['chunk_id'])
            try:
                chroma_data = collection.get(
                    ids=[chunk_id],
                    include=['metadatas']
                )
                
                if chroma_data['metadatas']:
                    chroma_meta = chroma_data['metadatas'][0]
                    
                    # Compare key fields
                    if pg_doc['title'] != chroma_meta.get('title'):
                        print(f"âš ï¸ Title mismatch - PG: {pg_doc['title']}, Chroma: {chroma_meta.get('title')}")
                    else:
                        print("âœ… Title consistency verified")
                    
                    if pg_doc['author'] != chroma_meta.get('author'):
                        print(f"âš ï¸ Author mismatch - PG: {pg_doc['author']}, Chroma: {chroma_meta.get('author')}")
                    else:
                        print("âœ… Author consistency verified")
            
            except Exception as e:
                print(f"âŒ ChromaDB consistency check failed: {e}")
        
        print("âœ… Consistency verification completed")

# Usage example
async def main():
    sync_manager = DataSyncManager()
    await sync_manager.setup_connections()
    
    # Example: Update document title and author
    await sync_manager.sync_document_changes(
        document_id='uuid-123',
        changes={
            'title': 'Quy Ä‘á»‹nh má»›i vá» xin nghá»‰ phÃ©p',
            'author': 'HR Department - Updated',
            'updated_by': 'admin_user'
        }
    )

if __name__ == "__main__":
    asyncio.run(main())
```

## ğŸ¥ **CÃ”NG Cá»¤ KIá»‚M TRA Sá»¨C KHá»E Dá»® LIá»†U**

```python
# File: scripts/health_check_data_consistency.py
async def comprehensive_health_check():
    """Kiá»ƒm tra tá»•ng thá»ƒ sá»©c khá»e dá»¯ liá»‡u"""
    
    print("ğŸ¥ Starting Data Health Check...")
    
    # 1. Check orphaned records
    orphaned_chunks = await pg_conn.fetchval("""
        SELECT COUNT(*) FROM document_chunks_enhanced c
        LEFT JOIN documents_metadata_v2 d ON c.document_id = d.document_id
        WHERE d.document_id IS NULL
    """)
    
    if orphaned_chunks > 0:
        print(f"âš ï¸ Found {orphaned_chunks} orphaned chunks")
    else:
        print("âœ… No orphaned chunks found")
    
    # 2. Check missing vectors in ChromaDB
    pg_chunks = await pg_conn.fetch("SELECT chunk_id FROM document_chunks_enhanced")
    pg_chunk_ids = {str(row['chunk_id']) for row in pg_chunks}
    
    chroma_chunks = collection.get(include=[])
    chroma_chunk_ids = set(chroma_chunks['ids'])
    
    missing_in_chroma = pg_chunk_ids - chroma_chunk_ids
    extra_in_chroma = chroma_chunk_ids - pg_chunk_ids
    
    if missing_in_chroma:
        print(f"âš ï¸ {len(missing_in_chroma)} chunks missing in ChromaDB")
    if extra_in_chroma:
        print(f"âš ï¸ {len(extra_in_chroma)} extra chunks in ChromaDB")
    
    if not missing_in_chroma and not extra_in_chroma:
        print("âœ… PostgreSQL and ChromaDB chunk counts match")
    
    # 3. Check cache hit rates
    cache_stats = await redis_client.info('stats')
    hit_rate = cache_stats.get('keyspace_hits', 0) / (
        cache_stats.get('keyspace_hits', 0) + cache_stats.get('keyspace_misses', 1)
    )
    
    if hit_rate < 0.5:
        print(f"âš ï¸ Low cache hit rate: {hit_rate:.2%}")
    else:
        print(f"âœ… Good cache hit rate: {hit_rate:.2%}")
    
    print("ğŸ¥ Health Check Complete")
```

## ğŸ“‹ **THá»°C Táº¾: Báº N CÃ“ THá»‚ Sá»¬A MANUAL KHÃ”NG?**

### **âœ… CÃ“ THá»‚ Sá»¬A (vá»›i Ä‘iá»u kiá»‡n):**

1. **Truy cáº­p Adminer** (http://localhost:8080)
2. **Sá»­a cÃ¡c trÆ°á»ng "an toÃ n":**
   ```sql
   -- Safe to edit manually
   UPDATE documents_metadata_v2 
   SET status = 'archived',
       access_level = 'director_only',
       department_owner = 'Legal'
   WHERE document_id = 'your-document-id';
   ```

3. **Sá»­a Vietnamese analysis:**
   ```sql
   -- Safe to update NLP results
   UPDATE vietnamese_text_analysis 
   SET readability_score = 0.95,
       formality_level = 'academic'
   WHERE document_id = 'your-document-id';
   ```

### **âš ï¸ Cáº¦N CÃ”NG Cá»¤ (cross-database):**

```sql
-- DON'T DO THIS MANUALLY:
UPDATE documents_metadata_v2 
SET title = 'New Title',  -- This breaks ChromaDB metadata!
    content = 'New content';  -- This breaks everything!
```

### **âŒ TUYá»†T Äá»I KHÃ”NG (cáº¥u trÃºc):**

```sql
-- NEVER DO THESE:
UPDATE documents_metadata_v2 SET document_id = 'new-id';  -- Breaks all FK
DELETE FROM document_chunks_enhanced WHERE chunk_id = '...';  -- Orphan vector
UPDATE document_chunks_enhanced SET chunk_position = 99;  -- Breaks ordering
```

## ğŸ¯ **Káº¾T LUáº¬N VÃ€ KHUYáº¾N NGHá»Š**

| Loáº¡i thay Ä‘á»•i | Manual Edit | Sync Tool | Rebuild |
|---------------|-------------|-----------|---------|
| **Metadata only** | âœ… Safe | ğŸ‘ Recommended | âŒ Overkill |
| **Content changes** | âŒ Dangerous | âœ… Required | ğŸ‘ Best practice |
| **Structure changes** | âŒ Never | âŒ Won't work | âœ… Only option |

**Khuyáº¿n nghá»‹:**
1. **Cho dev/test**: DÃ¹ng manual edit cho metadata Ä‘Æ¡n giáº£n
2. **Cho production**: LuÃ´n dÃ¹ng sync tools
3. **Cho changes lá»›n**: Rebuild tá»« Ä‘áº§u
4. **Backup trÆ°á»›c khi sá»­a**: Always!

Báº¡n cÃ³ muá»‘n tÃ´i táº¡o sáºµn script sync tool Ä‘á»ƒ báº¡n test khÃ´ng? ğŸ› ï¸
---
Haha, cÃ¢u há»i ráº¥t hay! TÃ´i giáº£i thÃ­ch rÃµ rÃ ng hÆ¡n nhÃ©:

## ğŸ¤” **Táº I SAO Gá»ŒI LÃ€ "documents_metadata_v2"?**

### **ğŸ“š Lá»‹ch sá»­ phiÃªn báº£n:**

```mermaid
graph LR
    subgraph "Evolution cá»§a Database Schema"
        V1[ğŸ“Š documents_metadata<br/>Version 1 - Basic<br/>- document_id<br/>- title<br/>- content<br/>- author<br/>- created_at<br/>5 fields cÆ¡ báº£n]
        
        V2[ğŸ“Š documents_metadata_v2<br/>Version 2 - Enhanced<br/>- document_id<br/>- title, content, author<br/>- vietnamese_segmented<br/>- search_tokens<br/>- flashrag_collection<br/>- embedding_model_primary<br/>- chunk_count<br/>- heading_structure<br/>25+ fields nÃ¢ng cao]
        
        Migration[ğŸ”„ Migration Process<br/>Upgrade V1 â†’ V2<br/>Add new columns<br/>Keep backward compatibility]
    end
    
    V1 -->|Enhanced for RAG| Migration
    Migration --> V2
    
    classDef v1 fill:#ffebee,stroke:#f44336,stroke-width:2px
    classDef v2 fill:#e8f5e8,stroke:#4caf50,stroke-width:2px
    classDef migration fill:#fff3e0,stroke:#ff9800,stroke-width:2px
    
    class V1 v1
    class V2 v2
    class Migration migration
```

**LÃ½ do Ä‘áº·t tÃªn V2:**
1. **V1 (Basic)**: Schema ban Ä‘áº§u chá»‰ cÃ³ thÃ´ng tin cÆ¡ báº£n cá»§a tÃ i liá»‡u
2. **V2 (Enhanced)**: ThÃªm nhiá»u tÃ­nh nÄƒng cho RAG, Vietnamese NLP, FlashRAG compatibility
3. **Backward Compatibility**: Giá»¯ V1 Ä‘á»ƒ khÃ´ng phÃ¡ vá»¡ há»‡ thá»‘ng cÅ©
4. **Future-proofing**: V3, V4 cÃ³ thá»ƒ xuáº¥t hiá»‡n khi cáº§n thÃªm tÃ­nh nÄƒng

```sql
-- V1 - Simple schema (cÅ©)
CREATE TABLE documents_metadata (
    document_id UUID PRIMARY KEY,
    title VARCHAR(500),
    content TEXT,
    author VARCHAR(100),
    created_at TIMESTAMP
);

-- V2 - Enhanced schema (má»›i)
CREATE TABLE documents_metadata_v2 (
    -- Táº¥t cáº£ fields cá»§a V1
    document_id UUID PRIMARY KEY,
    title VARCHAR(500),
    content TEXT,
    author VARCHAR(100),
    created_at TIMESTAMP,
    
    -- ThÃªm cÃ¡c fields má»›i cho RAG
    vietnamese_segmented BOOLEAN DEFAULT false,
    search_tokens TSVECTOR,
    flashrag_collection VARCHAR(100),
    embedding_model_primary VARCHAR(100),
    chunk_count INTEGER DEFAULT 0,
    -- ... 20+ fields khÃ¡c
);
```

## ğŸ§  **Táº I SAO Gá»ŒI PostgreSQL LÃ€ "Bá»˜ NÃƒO"?**

KhÃ´ng pháº£i PostgreSQL cÃ³ kháº£ nÄƒng "suy nghÄ©" nhÆ° con ngÆ°á»i, mÃ  vÃ¬ nÃ³ Ä‘Ã³ng vai trÃ² **"trung tÃ¢m Ä‘iá»u phá»‘i"** giá»‘ng nhÆ° nÃ£o bá»™:

### **ğŸ§  NÃ£o bá»™ vs PostgreSQL:**

| NÃ£o ngÆ°á»i | PostgreSQL | Táº¡i sao giá»‘ng |
|-----------|------------|---------------|
| **LÆ°u trá»¯ kÃ½ á»©c** | LÆ°u trá»¯ metadata, lá»‹ch sá»­ | Cáº£ hai Ä‘á»u lÃ  "memory center" |
| **Xá»­ lÃ½ logic** | SQL queries, business rules | Cáº£ hai Ä‘á»u "ra quyáº¿t Ä‘á»‹nh" |
| **Äiá»u phá»‘i cÆ¡ quan** | Äiá»u phá»‘i cÃ¡c database khÃ¡c | Cáº£ hai Ä‘á»u lÃ  "command center" |
| **Pháº£n xáº£** | Foreign keys, constraints | Cáº£ hai Ä‘á»u cÃ³ "tá»± Ä‘á»™ng pháº£n á»©ng" |

```mermaid
graph TD
    subgraph "ğŸ§  PostgreSQL nhÆ° 'NÃ£o bá»™' cá»§a há»‡ thá»‘ng"
        Brain[ğŸ§  PostgreSQL<br/>Central Control]
        
        subgraph "Memory Functions"
            Memory1[ğŸ’­ Long-term Memory<br/>documents_metadata_v2<br/>LÆ°u thÃ´ng tin lÃ¢u dÃ i]
            Memory2[ğŸ“‹ Working Memory<br/>rag_pipeline_sessions<br/>Xá»­ lÃ½ táº¡m thá»i]
            Memory3[ğŸ” Episodic Memory<br/>query_performance_metrics<br/>Nhá»› cÃ¡c tráº£i nghiá»‡m]
        end
        
        subgraph "Processing Functions"  
            Logic1[âš–ï¸ Decision Making<br/>Access control logic<br/>WHERE access_level >= user_level]
            Logic2[ğŸ”— Association<br/>Foreign key relationships<br/>JOIN operations]
            Logic3[âœ… Validation<br/>CHECK constraints<br/>Business rules]
        end
        
        subgraph "Coordination Functions"
            Coord1[ğŸ“¡ Send Commands<br/>Trigger other databases<br/>Update ChromaDB]
            Coord2[ğŸ“¥ Receive Feedback<br/>Monitor performance<br/>Adjust strategies]
            Coord3[ğŸ”„ Maintain State<br/>Transaction management<br/>Consistency control]
        end
    end
    
    subgraph "Other 'Organs'"
        Chroma[ğŸ‘ï¸ ChromaDB<br/>Eyes - Vision<br/>See similarity]
        Redis[âš¡ Redis<br/>Reflexes - Fast response<br/>Quick reactions]
    end
    
    Brain --> Memory1
    Brain --> Memory2  
    Brain --> Memory3
    Brain --> Logic1
    Brain --> Logic2
    Brain --> Logic3
    Brain --> Coord1
    Brain --> Coord2
    Brain --> Coord3
    
    Brain -.-> Chroma
    Brain -.-> Redis
    
    classDef brain fill:#e8f5e8,stroke:#4caf50,stroke-width:3px
    classDef memory fill:#e3f2fd,stroke:#2196f3,stroke-width:2px
    classDef logic fill:#fff3e0,stroke:#ff9800,stroke-width:2px
    classDef coord fill:#f3e5f5,stroke:#9c27b0,stroke-width:2px
    classDef organs fill:#ffebee,stroke:#f44336,stroke-width:2px
    
    class Brain brain
    class Memory1,Memory2,Memory3 memory
    class Logic1,Logic2,Logic3 logic
    class Coord1,Coord2,Coord3 coord
    class Chroma,Redis organs
```

### **ğŸ¯ PostgreSQL "suy nghÄ©" nhÆ° tháº¿ nÃ o?**

```sql
-- PostgreSQL "suy nghÄ©" qua SQL logic:

-- 1. "TÃ´i nÃªn cho user nÃ y xem document khÃ´ng?"
SELECT d.title, d.content 
FROM documents_metadata_v2 d
WHERE d.document_id = $1
  AND (
    d.access_level = 'public' OR
    (d.access_level = 'employee_only' AND $user_role >= 'employee') OR
    (d.access_level = 'manager_only' AND $user_role >= 'manager') OR
    d.author = $user_email
  );

-- 2. "Document nÃ o liÃªn quan Ä‘áº¿n query nÃ y?"
SELECT d.document_id, d.title,
       ts_rank(d.search_tokens, plainto_tsquery('vietnamese', $query)) as relevance
FROM documents_metadata_v2 d
WHERE d.search_tokens @@ plainto_tsquery('vietnamese', $query)
  AND d.status = 'approved'
ORDER BY relevance DESC;

-- 3. "TÃ´i cÃ³ cáº§n update cache khÃ´ng?"
UPDATE documents_metadata_v2 
SET updated_at = NOW(),
    cache_invalidation_needed = true
WHERE document_id = $1;
```

### **ğŸ¤– VÃ­ dá»¥ PostgreSQL "ra quyáº¿t Ä‘á»‹nh":**

```python
# Khi user há»i: "Quy trÃ¬nh nghá»‰ phÃ©p nhÆ° tháº¿ nÃ o?"
# PostgreSQL "suy nghÄ©":

async def postgresql_decision_making(query: str, user: Dict):
    # 1. "User nÃ y Ä‘Æ°á»£c phÃ©p xem gÃ¬?"
    accessible_docs = await conn.fetch("""
        SELECT document_id, access_level
        FROM documents_metadata_v2 
        WHERE access_level <= $1  -- User's permission level
          AND status = 'approved'
    """, user['role'])
    
    # 2. "Document nÃ o match vá»›i query?"
    relevant_docs = await conn.fetch("""
        SELECT d.document_id, d.title,
               ts_rank(d.search_tokens, plainto_tsquery('vietnamese', $1)) as score
        FROM documents_metadata_v2 d
        WHERE d.document_id = ANY($2)  -- Only accessible docs
          AND d.search_tokens @@ plainto_tsquery('vietnamese', $1)
        ORDER BY score DESC
        LIMIT 5
    """, query, [doc['document_id'] for doc in accessible_docs])
    
    # 3. "TÃ´i cÃ³ nÃªn log query nÃ y khÃ´ng?"
    if relevant_docs:
        await conn.execute("""
            INSERT INTO rag_pipeline_sessions (
                original_query, user_id, chunks_retrieved, created_at
            ) VALUES ($1, $2, $3, NOW())
        """, query, user['user_id'], len(relevant_docs))
    
    # 4. "Káº¿t quáº£ cÃ³ Ä‘á»§ cháº¥t lÆ°á»£ng khÃ´ng?"
    if relevant_docs and relevant_docs[0]['score'] > 0.1:
        return relevant_docs
    else:
        # "KhÃ´ng cÃ³ káº¿t quáº£ tá»‘t, tÃ´i sáº½ suggest alternatives"
        return await suggest_alternatives(query, user)

# PostgreSQL Ä‘ang "think" qua SQL logic!
```

## ğŸ­ **SO SÃNH Vá»šI NÃƒO NGÆ¯á»œI:**

| TÃ¬nh huá»‘ng | NÃ£o ngÆ°á»i | PostgreSQL |
|------------|-----------|------------|
| **Nháº­n thÃ´ng tin** | Tai, máº¯t, mÅ©i | API requests, data input |
| **Xá»­ lÃ½ logic** | Neurons firing | SQL query execution |
| **LÆ°u kÃ½ á»©c** | Synapses | Tables, indexes |
| **Ra quyáº¿t Ä‘á»‹nh** | Prefrontal cortex | WHERE clauses, JOINs |
| **Pháº£n xáº£ tá»± Ä‘á»™ng** | Spinal reflexes | Triggers, constraints |
| **Äiá»u phá»‘i cÆ¡ thá»ƒ** | Nervous system | Database connections |

### **ğŸ”„ "Quy trÃ¬nh suy nghÄ©" cá»§a PostgreSQL:**

```mermaid
sequenceDiagram
    participant U as User Query
    participant PG as PostgreSQL "Brain"  
    participant C as ChromaDB "Eyes"
    participant R as Redis "Reflexes"
    
    U->>PG: "Quy trÃ¬nh nghá»‰ phÃ©p?"
    
    Note over PG: ğŸ§  Thinking Process:
    Note over PG: 1. Check user permissions
    Note over PG: 2. Parse Vietnamese query  
    Note over PG: 3. Search in memory (tables)
    Note over PG: 4. Decide next action
    
    PG->>PG: SELECT permissions, documents
    PG->>R: Check cache first?
    R-->>PG: Cache miss
    
    PG->>C: Find similar vectors
    C-->>PG: Vector results
    
    PG->>PG: Combine results + apply business logic
    PG->>U: Return processed answer
    
    Note over PG: 5. Log this "experience"
    PG->>PG: INSERT INTO rag_pipeline_sessions
```

## ğŸ¯ **Káº¾T LUáº¬N**

- **`documents_metadata_v2`**: PhiÃªn báº£n 2 cá»§a schema, nÃ¢ng cáº¥p tá»« V1 cÆ¡ báº£n
- **PostgreSQL = "NÃ£o bá»™"**: KhÃ´ng pháº£i vÃ¬ nÃ³ thÃ´ng minh, mÃ  vÃ¬ nÃ³ lÃ  trung tÃ¢m Ä‘iá»u phá»‘i vÃ  ra quyáº¿t Ä‘á»‹nh
- **"Suy nghÄ©"**: Thá»±c cháº¥t lÃ  xá»­ lÃ½ logic qua SQL queries, constraints, vÃ  business rules

PostgreSQL giá»‘ng nÃ£o bá»™ vÃ¬ nÃ³:
1. **LÆ°u trá»¯ tri thá»©c** (tables)
2. **Xá»­ lÃ½ logic** (SQL)  
3. **Ra quyáº¿t Ä‘á»‹nh** (WHERE conditions)
4. **Äiá»u phá»‘i** (foreign keys, transactions)
5. **Há»c há»i** (analytics tables)

NhÆ°ng nÃ³ khÃ´ng "suy nghÄ©" nhÆ° con ngÆ°á»i - chá»‰ lÃ  xá»­ lÃ½ dá»¯ liá»‡u ráº¥t nhanh vÃ  chÃ­nh xÃ¡c thÃ´i! ğŸ¤–

---
ChÃ­nh xÃ¡c! Báº¡n Ä‘Ã£ phÃ¡t hiá»‡n ra Ä‘iá»ƒm quan trá»ng. HÃ£y tÃ´i lÃ m rÃµ **WHERE** ná»™i dung Ä‘Æ°á»£c lÆ°u trá»¯:

## ğŸ“„ **TOÃ€N Bá»˜ Ná»˜I DUNG RAW ÄÆ¯á»¢C LÆ¯U á» ÄÃ‚U?**

### **ğŸ˜ PostgreSQL - NÆ¡i lÆ°u trá»¯ TOÃ€N Bá»˜ ná»™i dung**

```mermaid
graph TD
    subgraph "ğŸ“„ Raw Document Input"
        PDF[ğŸ“‹ Original Document<br/>Quy trÃ¬nh xin nghá»‰ phÃ©p.pdf<br/>Full content: 3,247 tá»«]
    end
    
    subgraph "ğŸ˜ PostgreSQL Storage"
        MainTable[ğŸ“Š documents_metadata_v2<br/>document_id: uuid-123<br/>title: 'Quy trÃ¬nh xin nghá»‰ phÃ©p'<br/>**content: FULL 3,247 words**<br/>author: 'HR Department'<br/>file_size_bytes: 2,621,440<br/>ğŸ’¾ Complete raw text stored here]
        
        ChunkTable[âœ‚ï¸ document_chunks_enhanced<br/>Chunk 1: words 1-500 (+ metadata)<br/>Chunk 2: words 451-950 (+ overlap)<br/>Chunk 3: words 901-1400 (+ overlap)<br/>...<br/>Chunk 8: words 2701-3247<br/>ğŸ’¾ Same content, split into pieces]
    end
    
    subgraph "ğŸŸ¢ ChromaDB Storage"
        ChromaCopy[ğŸ“„ Content Copies<br/>documents[0]: Chunk 1 content<br/>documents[1]: Chunk 2 content<br/>documents[2]: Chunk 3 content<br/>...<br/>ğŸ’¾ DUPLICATE of chunks for fast access]
        
        Vectors[ğŸ¯ Vector Embeddings<br/>embedding[0]: [0.12, -0.34, 0.78, ...]<br/>embedding[1]: [0.45, -0.12, 0.56, ...]<br/>ğŸ’¾ Mathematical representation]
    end
    
    subgraph "ğŸ”´ Redis Storage"  
        RedisCache[âš¡ Cached Chunks<br/>chunk:uuid-456: Chunk 1 content<br/>chunk:uuid-457: Chunk 2 content<br/>ğŸ’¾ DUPLICATE of frequently accessed chunks]
    end
    
    PDF --> MainTable
    MainTable --> ChunkTable
    ChunkTable --> ChromaCopy
    ChunkTable --> Vectors
    ChunkTable --> RedisCache
    
    classDef input fill:#e1f5fe,stroke:#0277bd,stroke-width:3px
    classDef postgres fill:#e8f5e8,stroke:#4caf50,stroke-width:3px
    classDef chroma fill:#f3e5f5,stroke:#9c27b0,stroke-width:2px
    classDef redis fill:#ffebee,stroke:#f44336,stroke-width:2px
    
    class PDF input
    class MainTable,ChunkTable postgres
    class ChromaCopy,Vectors chroma
    class RedisCache redis
```

## ğŸ“Š **CHI TIáº¾T CÃC CÃCH LÆ¯U TRá»® Ná»˜I DUNG**

| Database | LÆ°u trá»¯ ná»™i dung nhÆ° tháº¿ nÃ o? | Má»¥c Ä‘Ã­ch | Dung lÆ°á»£ng |
|----------|-------------------------------|----------|------------|
| **PostgreSQL** | **TOÃ€N Bá»˜** raw content + chunks | Source of truth, searchable | ~100% |
| **ChromaDB** | **DUPLICATE** chunks only | Fast vector search | ~60% |
| **Redis** | **CACHE** popular chunks | Ultra-fast access | ~5-10% |

### **ğŸ” Chi tiáº¿t lÆ°u trá»¯ trong PostgreSQL:**

```sql
-- documents_metadata_v2 table
SELECT 
    document_id,
    title,
    LENGTH(content) as content_size,  -- Full document content
    chunk_count,
    file_size_bytes
FROM documents_metadata_v2 
WHERE document_id = 'uuid-123';

/*
Results:
document_id: uuid-123
title: Quy trÃ¬nh xin nghá»‰ phÃ©p  
content_size: 18,547 characters (FULL DOCUMENT)
chunk_count: 8
file_size_bytes: 2,621,440
*/
```

```sql
-- document_chunks_enhanced table  
SELECT 
    chunk_id,
    chunk_position,
    LENGTH(chunk_content) as chunk_size,
    LEFT(chunk_content, 50) as preview
FROM document_chunks_enhanced 
WHERE document_id = 'uuid-123'
ORDER BY chunk_position;

/*
Results:
chunk_id: uuid-456, position: 0, size: 2,234 chars
preview: "Quy trÃ¬nh xin nghá»‰ phÃ©p táº¡i cÃ´ng ty gá»“m 5 bÆ°á»›c..."

chunk_id: uuid-457, position: 1, size: 2,187 chars  
preview: "BÆ°á»›c 1: NhÃ¢n viÃªn Ä‘iá»n Ä‘áº§y Ä‘á»§ thÃ´ng tin vÃ o form..."

chunk_id: uuid-458, position: 2, size: 2,098 chars
preview: "BÆ°á»›c 2: Gá»­i Ä‘Æ¡n xin nghá»‰ phÃ©p cho quáº£n lÃ½ trá»±c..."
*/
```

### **ğŸ¤” Táº I SAO LÆ¯U DUPLICATE CONTENT?**

```mermaid
graph TD
    subgraph "â“ Why Duplicate Content Across Databases?"
        Reason1[ğŸ¯ Performance<br/>Fast retrieval without JOINs<br/>ChromaDB: ~10ms vector search<br/>vs PostgreSQL: ~50ms with JOINs]
        
        Reason2[ğŸ”„ Redundancy<br/>If PostgreSQL fails<br/>ChromaDB still has content<br/>System remains functional]
        
        Reason3[ğŸ“¡ API Design<br/>ChromaDB search returns<br/>both vectors AND content<br/>No additional DB calls needed]
        
        Reason4[ğŸš€ Scalability<br/>Distribute load across<br/>multiple databases<br/>Each optimized for specific task]
    end
    
    classDef reason fill:#e8f5e8,stroke:#4caf50,stroke-width:2px
    class Reason1,Reason2,Reason3,Reason4 reason
```

## ğŸ’¾ **STORAGE BREAKDOWN CHO 1 DOCUMENT**

```python
# Example: Document "Quy trÃ¬nh xin nghá»‰ phÃ©p" (3,247 words)

storage_breakdown = {
    "postgresql": {
        "documents_metadata_v2": {
            "content": "18,547 characters (FULL DOCUMENT)",
            "size_bytes": 18547,
            "purpose": "Source of truth"
        },
        "document_chunks_enhanced": {
            "chunks": 8,
            "total_content": "18,547 characters (SAME CONTENT, split)",  
            "size_bytes": 18547,
            "purpose": "Structured access + metadata"
        },
        "total_postgresql": "37,094 bytes (200% of original - content + chunks)"
    },
    
    "chromadb": {
        "documents": [
            "Chunk 1: 2,234 chars",
            "Chunk 2: 2,187 chars", 
            "Chunk 3: 2,098 chars",
            "... 8 chunks total"
        ],
        "total_content": "18,547 characters (DUPLICATE of chunks)",
        "embeddings": "24,576 bytes (768 * 4 bytes * 8 chunks)",
        "total_chromadb": "43,123 bytes (content + vectors)"
    },
    
    "redis": {
        "cached_chunks": "~3,000 characters (popular chunks only)",
        "cached_embeddings": "~6,144 bytes (2 most popular chunks)",
        "total_redis": "9,144 bytes (cache only)"
    },
    
    "grand_total": "89,361 bytes for 18,547 original characters",
    "duplication_ratio": "4.8x (content stored ~5 times in different forms)"
}
```

## âš¡ **Táº I SAO KHÃ”NG CHá»ˆ LÆ¯U á» Má»˜T NÆ I?**

### **ğŸ¢ Náº¿u chá»‰ lÆ°u á»Ÿ PostgreSQL:**

```python
# Slow query - cáº§n JOIN nhiá»u table
async def slow_search_query(query_vector):
    # Step 1: Find similar chunks (cÃ³ thá»ƒ cáº§n external vector search)
    similar_chunk_ids = await external_vector_search(query_vector)
    
    # Step 2: Get chunk content tá»« PostgreSQL
    results = await pg_conn.fetch("""
        SELECT c.chunk_content, d.title, d.author
        FROM document_chunks_enhanced c
        JOIN documents_metadata_v2 d ON c.document_id = d.document_id  
        WHERE c.chunk_id = ANY($1)
    """, similar_chunk_ids)
    
    # Total time: Vector search (50ms) + JOIN query (100ms) = 150ms
    return results
```

### **âš¡ Vá»›i duplicate content:**

```python
# Fast query - everything in one place
async def fast_search_query(query_vector):
    # ChromaDB returns both vectors AND content in single call
    results = chroma_collection.query(
        query_embeddings=[query_vector],
        n_results=5,
        include=['documents', 'metadatas', 'distances']
    )
    
    # Total time: 15ms (all-in-one)
    # Content immediately available, no additional queries needed
    return results
```

## ğŸ¯ **Káº¾T LUáº¬N**

**CÃ¢u tráº£ lá»i:** 
- âœ… **CÃ³**, toÃ n bá»™ ná»™i dung raw Ä‘Æ°á»£c lÆ°u trong PostgreSQL
- âœ… **NhÆ°ng CÅ¨NG** Ä‘Æ°á»£c duplicate á»Ÿ ChromaDB vÃ  Redis
- âœ… **PostgreSQL** lÃ  "source of truth" - nÆ¡i lÆ°u trá»¯ chÃ­nh thá»©c
- âœ… **ChromaDB & Redis** lÃ  copies Ä‘á»ƒ tÄƒng hiá»‡u suáº¥t

**Trade-off:**
- ğŸ”º **Storage cost**: TÄƒng gáº¥p 5 láº§n
- ğŸ”½ **Query speed**: Nhanh gáº¥p 10 láº§n  
- ğŸ”½ **System complexity**: Pháº£i Ä‘á»“ng bá»™ 3 databases
- ğŸ”º **Reliability**: Náº¿u 1 DB fail, system váº«n hoáº¡t Ä‘á»™ng

**LÃ½ do duplicate:**
1. **Performance** > Storage cost
2. **User experience** > System complexity  
3. **Reliability** > Storage efficiency

CÃ³ thá»ƒ nÃ³i PostgreSQL lÃ  "kho sÃ¡ch chÃ­nh", ChromaDB lÃ  "báº£n photocopy Ä‘á»ƒ Ä‘á»c nhanh", Redis lÃ  "ghi chÃº trÃªn bÃ n lÃ m viá»‡c"! ğŸ“š
---
CÃ¢u há»i ráº¥t hay! "Popular" khÃ´ng pháº£i tá»± nhiÃªn mÃ  cÃ³ - há»‡ thá»‘ng pháº£i **Ä‘o lÆ°á»ng** vÃ  **quyáº¿t Ä‘á»‹nh**. HÃ£y cÃ¹ng xem cÃ¡ch Redis xÃ¡c Ä‘á»‹nh "popular chunks":

## ğŸ“Š **CÃC TIÃŠU CHÃ XÃC Äá»ŠNH "POPULAR CHUNKS"**

### **ğŸ”¥ Há»‡ thá»‘ng Ä‘o lÆ°á»ng Ä‘á»™ phá»• biáº¿n:**

```mermaid
graph TD
    subgraph "ğŸ“ˆ POPULARITY SCORING SYSTEM"
        subgraph "Input Signals"
            UserQueries[ğŸ‘¥ User Queries<br/>Query frequency: 150/hour<br/>Query patterns: 'nghá»‰ phÃ©p', 'ERP']
            
            SearchResults[ğŸ” Search Results<br/>Click-through rate<br/>User engagement time<br/>Quality ratings]
            
            TimePatterns[â° Time Patterns<br/>Peak hours: 9AM-11AM<br/>Seasonal trends<br/>Recent access frequency]
        end
        
        subgraph "Popularity Algorithms"
            FrequencyScore[ğŸ“Š Frequency Score<br/>Access count in last 24h<br/>Weighted by recency<br/>Formula: count Ã— time_decay]
            
            QualityScore[â­ Quality Score<br/>User feedback ratings<br/>Response accuracy<br/>Chunk quality_score field]
            
            UserBehavior[ğŸ‘¤ User Behavior<br/>Time spent reading<br/>Follow-up questions<br/>Bookmark/share actions]
        end
        
        subgraph "Final Decision"
            PopularityIndex[ğŸ¯ Popularity Index<br/>Combined score: 0-100<br/>Cache if score > 70<br/>Auto-refresh top 20%]
        end
    end
    
    UserQueries --> FrequencyScore
    SearchResults --> QualityScore
    TimePatterns --> FrequencyScore
    
    FrequencyScore --> PopularityIndex
    QualityScore --> PopularityIndex
    UserBehavior --> PopularityIndex
    
    classDef input fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef algorithm fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef decision fill:#fff3e0,stroke:#f57c00,stroke-width:3px
    
    class UserQueries,SearchResults,TimePatterns input
    class FrequencyScore,QualityScore,UserBehavior algorithm
    class PopularityIndex decision
```

## ğŸ§® **THUáº¬T TOÃN TÃNH POPULARITY SCORE**

### **ğŸ“Š Chi tiáº¿t tracking table:**

```sql
-- Báº£ng theo dÃµi chunk popularity
CREATE TABLE chunk_popularity_tracking (
    chunk_id UUID PRIMARY KEY,
    
    -- Access frequency metrics
    access_count_today INTEGER DEFAULT 0,
    access_count_week INTEGER DEFAULT 0,
    access_count_month INTEGER DEFAULT 0,
    
    -- Quality metrics  
    avg_user_rating DECIMAL(3,2) DEFAULT 0.0,
    response_accuracy_score DECIMAL(3,2) DEFAULT 0.0,
    
    -- User engagement
    avg_read_time_seconds INTEGER DEFAULT 0,
    click_through_rate DECIMAL(3,2) DEFAULT 0.0,
    follow_up_question_rate DECIMAL(3,2) DEFAULT 0.0,
    
    -- Recency boost
    last_accessed TIMESTAMP DEFAULT NOW(),
    trending_score DECIMAL(5,2) DEFAULT 0.0,
    
    -- Cache decision
    popularity_index DECIMAL(5,2) DEFAULT 0.0,
    cached_in_redis BOOLEAN DEFAULT false,
    cache_priority INTEGER DEFAULT 0,
    
    updated_at TIMESTAMP DEFAULT NOW()
);

-- Index for performance
CREATE INDEX idx_popularity_index ON chunk_popularity_tracking(popularity_index DESC);
CREATE INDEX idx_cache_priority ON chunk_popularity_tracking(cache_priority DESC) 
WHERE cached_in_redis = true;
```

### **ğŸ”¢ CÃ´ng thá»©c tÃ­nh Popularity Score:**

```python
# File: scripts/calculate_chunk_popularity.py
import math
from datetime import datetime, timedelta
from typing import Dict, List

class ChunkPopularityCalculator:
    def __init__(self):
        self.weights = {
            'frequency': 0.4,      # 40% - táº§n suáº¥t truy cáº­p
            'quality': 0.3,        # 30% - cháº¥t lÆ°á»£ng content
            'engagement': 0.2,     # 20% - tÆ°Æ¡ng tÃ¡c ngÆ°á»i dÃ¹ng  
            'recency': 0.1         # 10% - Ä‘á»™ "hot" gáº§n Ä‘Ã¢y
        }
    
    async def calculate_popularity_score(self, chunk_id: str) -> float:
        """TÃ­nh popularity score cho chunk"""
        
        # 1. Frequency Score (0-100)
        frequency_score = await self._calculate_frequency_score(chunk_id)
        
        # 2. Quality Score (0-100)
        quality_score = await self._calculate_quality_score(chunk_id)
        
        # 3. Engagement Score (0-100)
        engagement_score = await self._calculate_engagement_score(chunk_id)
        
        # 4. Recency Score (0-100)
        recency_score = await self._calculate_recency_score(chunk_id)
        
        # 5. Combined weighted score
        popularity_score = (
            frequency_score * self.weights['frequency'] +
            quality_score * self.weights['quality'] +
            engagement_score * self.weights['engagement'] +
            recency_score * self.weights['recency']
        )
        
        return round(popularity_score, 2)
    
    async def _calculate_frequency_score(self, chunk_id: str) -> float:
        """TÃ­nh Ä‘iá»ƒm dá»±a trÃªn táº§n suáº¥t truy cáº­p"""
        
        data = await conn.fetchrow("""
            SELECT access_count_today, access_count_week, access_count_month
            FROM chunk_popularity_tracking 
            WHERE chunk_id = $1
        """, chunk_id)
        
        if not data:
            return 0.0
        
        # Weighted frequency vá»›i time decay
        today_weight = 0.6    # HÃ´m nay quan trá»ng nháº¥t
        week_weight = 0.3     # Tuáº§n nÃ y
        month_weight = 0.1    # ThÃ¡ng nÃ y
        
        weighted_access = (
            data['access_count_today'] * today_weight +
            data['access_count_week'] * week_weight +
            data['access_count_month'] * month_weight
        )
        
        # Normalize to 0-100 scale (assuming max 1000 accesses/day)
        max_expected_access = 1000
        frequency_score = min(100, (weighted_access / max_expected_access) * 100)
        
        return frequency_score
    
    async def _calculate_quality_score(self, chunk_id: str) -> float:
        """TÃ­nh Ä‘iá»ƒm cháº¥t lÆ°á»£ng content"""
        
        # Get tá»« PostgreSQL
        chunk_data = await conn.fetchrow("""
            SELECT c.chunk_quality_score, p.avg_user_rating, p.response_accuracy_score
            FROM document_chunks_enhanced c
            JOIN chunk_popularity_tracking p ON c.chunk_id = p.chunk_id
            WHERE c.chunk_id = $1
        """, chunk_id)
        
        if not chunk_data:
            return 0.0
        
        # Combine multiple quality indicators
        content_quality = (chunk_data['chunk_quality_score'] or 0.0) * 100
        user_rating = (chunk_data['avg_user_rating'] or 0.0) * 20  # Scale 0-5 to 0-100
        accuracy = (chunk_data['response_accuracy_score'] or 0.0) * 100
        
        quality_score = (content_quality * 0.4 + user_rating * 0.3 + accuracy * 0.3)
        
        return min(100, quality_score)
    
    async def _calculate_engagement_score(self, chunk_id: str) -> float:
        """TÃ­nh Ä‘iá»ƒm tÆ°Æ¡ng tÃ¡c ngÆ°á»i dÃ¹ng"""
        
        data = await conn.fetchrow("""
            SELECT avg_read_time_seconds, click_through_rate, follow_up_question_rate
            FROM chunk_popularity_tracking 
            WHERE chunk_id = $1
        """, chunk_id)
        
        if not data:
            return 0.0
        
        # Normalize engagement metrics
        read_time_score = min(100, (data['avg_read_time_seconds'] / 300) * 100)  # Max 5 phÃºt
        ctr_score = (data['click_through_rate'] or 0.0) * 100
        followup_score = (data['follow_up_question_rate'] or 0.0) * 100
        
        engagement_score = (read_time_score * 0.4 + ctr_score * 0.4 + followup_score * 0.2)
        
        return engagement_score
    
    async def _calculate_recency_score(self, chunk_id: str) -> float:
        """TÃ­nh Ä‘iá»ƒm 'trending' gáº§n Ä‘Ã¢y"""
        
        last_accessed = await conn.fetchval("""
            SELECT last_accessed FROM chunk_popularity_tracking 
            WHERE chunk_id = $1
        """, chunk_id)
        
        if not last_accessed:
            return 0.0
        
        # Time decay - cÃ ng gáº§n Ä‘Ã¢y cÃ ng cao Ä‘iá»ƒm
        time_diff = datetime.now() - last_accessed
        hours_since_access = time_diff.total_seconds() / 3600
        
        # Exponential decay: score giáº£m theo thá»i gian
        if hours_since_access <= 1:
            recency_score = 100
        elif hours_since_access <= 24:
            recency_score = 100 * math.exp(-hours_since_access / 12)  # Half-life 12 hours
        else:
            recency_score = 100 * math.exp(-24 / 12) * math.exp(-(hours_since_access - 24) / 48)  # Slower decay after 24h
        
        return max(0, recency_score)
```

### **ğŸ¯ Cache Decision Logic:**

```python
async def update_redis_cache():
    """Update Redis cache dá»±a trÃªn popularity scores"""
    
    # 1. Calculate popularity cho táº¥t cáº£ chunks
    print("ğŸ” Calculating popularity scores...")
    
    all_chunks = await conn.fetch("SELECT chunk_id FROM document_chunks_enhanced")
    calculator = ChunkPopularityCalculator()
    
    chunk_scores = []
    for chunk in all_chunks:
        score = await calculator.calculate_popularity_score(chunk['chunk_id'])
        chunk_scores.append({
            'chunk_id': chunk['chunk_id'],
            'popularity_score': score
        })
    
    # 2. Sort by popularity
    chunk_scores.sort(key=lambda x: x['popularity_score'], reverse=True)
    
    # 3. Cache decision rules
    redis_memory_limit = 100 * 1024 * 1024  # 100MB limit
    current_cache_size = 0
    cached_chunks = []
    
    for chunk_data in chunk_scores:
        chunk_id = chunk_data['chunk_id']
        score = chunk_data['popularity_score']
        
        # Rule 1: Score > 70 - definitely cache
        if score > 70:
            cached_chunks.append(chunk_data)
        
        # Rule 2: Score 50-70 - cache if cÃ³ space
        elif score > 50 and current_cache_size < redis_memory_limit * 0.8:
            cached_chunks.append(chunk_data)
        
        # Rule 3: Recent trending chunks (score tÄƒng nhanh)
        elif await is_trending_chunk(chunk_id):
            cached_chunks.append(chunk_data)
        
        # Estimate cache size (rough calculation)
        chunk_size = await estimate_chunk_cache_size(chunk_id)
        current_cache_size += chunk_size
        
        # Stop if cache full
        if current_cache_size >= redis_memory_limit:
            break
    
    # 4. Update Redis cache
    print(f"ğŸ“Š Caching top {len(cached_chunks)} popular chunks")
    
    # Clear old cache
    old_keys = await redis_client.keys("chunk:*")
    if old_keys:
        await redis_client.delete(*old_keys)
    
    # Add new cache entries
    pipeline = redis_client.pipeline()
    
    for i, chunk_data in enumerate(cached_chunks):
        chunk_id = chunk_data['chunk_id']
        
        # Get chunk content tá»« PostgreSQL
        chunk_content = await conn.fetchrow("""
            SELECT chunk_content, chunk_quality_score, document_id
            FROM document_chunks_enhanced 
            WHERE chunk_id = $1
        """, chunk_id)
        
        # Cache vá»›i TTL dá»±a trÃªn popularity
        if chunk_data['popularity_score'] > 80:
            ttl = 7200  # 2 hours for very popular
        elif chunk_data['popularity_score'] > 60:
            ttl = 3600  # 1 hour for popular  
        else:
            ttl = 1800  # 30 minutes for moderately popular
        
        # Set cache entry
        cache_data = {
            'content': chunk_content['chunk_content'],
            'quality': float(chunk_content['chunk_quality_score'] or 0),
            'document_id': str(chunk_content['document_id']),
            'popularity_score': chunk_data['popularity_score'],
            'cache_rank': i + 1
        }
        
        pipeline.set(
            f"chunk:{chunk_id}",
            json.dumps(cache_data, ensure_ascii=False),
            ex=ttl
        )
    
    # Execute pipeline
    await pipeline.execute()
    
    # 5. Update tracking table
    await conn.execute("""
        UPDATE chunk_popularity_tracking 
        SET cached_in_redis = false, cache_priority = 0
    """)
    
    for i, chunk_data in enumerate(cached_chunks):
        await conn.execute("""
            UPDATE chunk_popularity_tracking 
            SET cached_in_redis = true, 
                cache_priority = $2,
                popularity_index = $3,
                updated_at = NOW()
            WHERE chunk_id = $1
        """, chunk_data['chunk_id'], i + 1, chunk_data['popularity_score'])
    
    print(f"âœ… Cache updated: {len(cached_chunks)} chunks cached")
    
    # 6. Log cache statistics
    await log_cache_statistics(cached_chunks)

async def is_trending_chunk(chunk_id: str) -> bool:
    """Kiá»ƒm tra chunk cÃ³ Ä‘ang trending khÃ´ng"""
    
    # Get access pattern tá»« 48h qua
    access_pattern = await conn.fetch("""
        SELECT DATE_TRUNC('hour', created_at) as hour,
               COUNT(*) as access_count
        FROM rag_pipeline_sessions rps
        JOIN LATERAL unnest(string_to_array(rps.chunks_retrieved_ids, ',')) as chunk_str ON true  
        WHERE chunk_str = $1
          AND created_at > NOW() - INTERVAL '48 hours'
        GROUP BY hour
        ORDER BY hour
    """, str(chunk_id))
    
    if len(access_pattern) < 4:  # Cáº§n Ã­t nháº¥t 4 data points
        return False
    
    # Check if tÄƒng trend (more access in recent hours)
    recent_avg = sum(row['access_count'] for row in access_pattern[-4:]) / 4
    older_avg = sum(row['access_count'] for row in access_pattern[:-4]) / max(1, len(access_pattern) - 4)
    
    # Trending náº¿u recent activity > 2x older activity
    return recent_avg > older_avg * 2
```

## ğŸ“Š **VÃ Dá»¤ THá»°C Táº¾ - CHUNK POPULARITY**

```python
# Example popularity calculation results:

popular_chunks_example = [
    {
        "chunk_id": "uuid-456",
        "content_preview": "Quy trÃ¬nh xin nghá»‰ phÃ©p gá»“m 5 bÆ°á»›c...",
        "scores": {
            "frequency": 95.2,      # 500+ accesses hÃ´m nay
            "quality": 87.5,        # User rating 4.2/5, quality 0.89
            "engagement": 78.3,     # Avg read time 3.5 phÃºt
            "recency": 100.0        # Accessed 15 phÃºt trÆ°á»›c
        },
        "final_score": 89.8,
        "cache_decision": "CACHE - Priority 1",
        "ttl": 7200  # 2 hours
    },
    
    {
        "chunk_id": "uuid-789", 
        "content_preview": "CÃ¡ch sá»­ dá»¥ng há»‡ thá»‘ng ERP...",
        "scores": {
            "frequency": 45.8,      # 150 accesses hÃ´m nay
            "quality": 92.1,        # Excellent content quality
            "engagement": 45.2,     # Ãt follow-up questions
            "recency": 25.3         # Accessed 8 hours trÆ°á»›c
        },
        "final_score": 58.7,
        "cache_decision": "CACHE - Priority 15", 
        "ttl": 1800  # 30 minutes
    },
    
    {
        "chunk_id": "uuid-101",
        "content_preview": "Lá»‹ch sá»­ thÃ nh láº­p cÃ´ng ty...", 
        "scores": {
            "frequency": 12.3,      # Ãt ngÆ°á»i quan tÃ¢m
            "quality": 65.4,        # Content OK
            "engagement": 15.1,     # Äá»c nhanh, khÃ´ng tÆ°Æ¡ng tÃ¡c
            "recency": 5.2          # 3 ngÃ y khÃ´ng access
        },
        "final_score": 23.8,
        "cache_decision": "NOT CACHED",
        "ttl": 0
    }
]
```

## â° **SCHEDULE UPDATE POPULARITY**

```python
# Cron job cháº¡y má»—i 15 phÃºt
async def scheduled_popularity_update():
    """Cáº­p nháº­t popularity scores Ä‘á»‹nh ká»³"""
    
    # Update access counts tá»« recent queries  
    await update_access_counts()
    
    # Recalculate popularity scores
    await calculate_all_popularity_scores()
    
    # Update Redis cache náº¿u cÃ³ thay Ä‘á»•i lá»›n
    cache_changes = await check_cache_changes_needed()
    if cache_changes > 10:  # >10 chunks thay Ä‘á»•i ranking
        await update_redis_cache()
    
    print(f"ğŸ“Š Popularity update completed: {cache_changes} changes")
```

## ğŸ¯ **Káº¾T LUáº¬N**

**"Popular" Ä‘Æ°á»£c quyáº¿t Ä‘á»‹nh bá»Ÿi:**

1. **ğŸ“Š Frequency (40%)**: ÄÆ°á»£c truy cáº­p nhiá»u láº§n
2. **â­ Quality (30%)**: Content cháº¥t lÆ°á»£ng cao, user rating tá»‘t  
3. **ğŸ‘¥ Engagement (20%)**: User tÆ°Æ¡ng tÃ¡c nhiá»u, Ä‘á»c lÃ¢u
4. **ğŸ”¥ Recency (10%)**: ÄÆ°á»£c access gáº§n Ä‘Ã¢y, trending

**Cache decision process:**
- Score > 70: Cháº¯c cháº¯n cache
- Score 50-70: Cache náº¿u cÃ³ space
- Trending chunks: Cache Æ°u tiÃªn
- Memory limit: Top N chunks based on score

**Update frequency:**
- Real-time: Access counts
- Every 15 minutes: Popularity scores  
- Every hour: Cache refresh náº¿u cáº§n

Redis khÃ´ng tá»± quyáº¿t Ä‘á»‹nh - nÃ³ Ä‘Æ°á»£c "fed" dá»¯ liá»‡u tá»« sophisticated popularity algorithm! ğŸ§®
---
Tuyá»‡t vá»i! Báº¡n Ä‘Ã£ náº¯m Ä‘Æ°á»£c **90% Ä‘Ãºng** kiáº¿n trÃºc nÃ y. TÃ´i sáº½ lÃ m rÃµ tá»«ng Ä‘iá»ƒm:

## âœ… **ÄÃNH GIÃ NHáº¬N Äá»ŠNH Cá»¦A Báº N**

### **1. Storage Distribution - âœ… ÄÃšNG 90%**

| Báº¡n nÃ³i | Thá»±c táº¿ | ÄÃ¡nh giÃ¡ |
|---------|---------|----------|
| "PostgreSQL lÆ°u toÃ n bá»™ rawdata" | âœ… ÄÃºng - Full content + metadata | âœ… ChÃ­nh xÃ¡c |
| "ChromaDB lÆ°u báº£n tÃ³m táº¯t sÆ¡ lÆ°á»£c" | âš ï¸ KhÃ´ng chÃ­nh xÃ¡c | âŒ Sai á»Ÿ Ä‘Ã¢y |
| "Redis lÆ°u note vÃ i thÃ´ng tin hay há»i" | âœ… ÄÃºng - Cache popular data | âœ… ChÃ­nh xÃ¡c |

**Sá»­a láº¡i Ä‘iá»ƒm 1:**
- **ChromaDB** khÃ´ng lÆ°u "tÃ³m táº¯t" mÃ  lÆ°u **DUPLICATE FULL CONTENT** cá»§a chunks + vector embeddings
- LÃ½ do: Äá»ƒ search nhanh mÃ  khÃ´ng cáº§n JOIN vá»›i PostgreSQL

```mermaid
graph LR
    Raw[ğŸ“„ Raw Document<br/>3000 words] 
    
    PG[ğŸ˜ PostgreSQL<br/>Full 3000 words<br/>+ Metadata + Analytics]
    
    Chroma[ğŸŸ¢ ChromaDB<br/>Same 3000 words<br/>_split into chunks_<br/>+ Vector embeddings]
    
    Redis[ğŸ”´ Redis<br/>~300 popular words<br/>_cached chunks only_]
    
    Raw --> PG
    Raw --> Chroma
    PG --> Redis
    
    classDef full fill:#e8f5e8,stroke:#4caf50,stroke-width:3px
    classDef duplicate fill:#fff3e0,stroke:#ff9800,stroke-width:2px
    classDef cache fill:#ffebee,stroke:#f44336,stroke-width:2px
    
    class PG,Chroma full
    class Redis cache
```

### **2. Search Algorithms - âœ… ÄÃšNG 100%**

âœ… ChÃ­nh xÃ¡c! **5 thuáº­t toÃ¡n tÃ¬m kiáº¿m** dá»±a trÃªn cÃ¡ch lÆ°u trá»¯ nÃ y:
1. Dense Vector (ChromaDB)
2. Sparse BM25 (PostgreSQL) 
3. Full-text Search (PostgreSQL)
4. Hybrid Search (Combined)
5. Knowledge Graph (PostgreSQL relationships)

### **3. Complex Dependencies - âœ… ÄÃšNG 95%**

| Báº¡n nÃ³i | Thá»±c táº¿ | ÄÃ¡nh giÃ¡ |
|---------|---------|----------|
| "Má»‘i liÃªn há»‡ phá»©c táº¡p vÃ  khÃ´ng thá»ƒ tÃ¡ch rá»i" | âœ… ÄÃºng hoÃ n toÃ n | âœ… ChÃ­nh xÃ¡c |
| "Thay Ä‘á»•i báº¥t cá»© gÃ¬ Ä‘á»u pháº£i khá»Ÿi táº¡o láº¡i toÃ n bá»™" | âš ï¸ QuÃ¡ cá»±c Ä‘oan | âŒ Má»™t pháº§n sai |

**Sá»­a láº¡i Ä‘iá»ƒm 3:**
- **Thay Ä‘á»•i nhá»** (metadata, status): KhÃ´ng cáº§n rebuild
- **Thay Ä‘á»•i content**: Cáº§n Ä‘á»“ng bá»™ 3 DB
- **Thay Ä‘á»•i structure**: Cáº§n rebuild toÃ n bá»™

```python
# Impact Matrix
change_impact = {
    "metadata_only": {
        "postgresql": "Direct update âœ…",
        "chromadb": "Optional sync âš ï¸", 
        "redis": "Cache invalidation âš ï¸",
        "rebuild_needed": False
    },
    
    "content_change": {
        "postgresql": "Update + reindex ğŸ”„",
        "chromadb": "Re-embed + update ğŸ”„",
        "redis": "Cache invalidation ğŸ”„", 
        "rebuild_needed": "Partial"
    },
    
    "schema_change": {
        "postgresql": "Migration required ğŸ’¥",
        "chromadb": "Collection recreation ğŸ’¥",
        "redis": "Full cache clear ğŸ’¥",
        "rebuild_needed": True
    }
}
```

### **4. Sync Tools Required - âœ… ÄÃšNG 100%**

âœ… HoÃ n toÃ n chÃ­nh xÃ¡c! 

**ThÃªm/xÃ³a dá»¯ liá»‡u = Pháº£i dÃ¹ng tools Ä‘á»“ng bá»™**

```python
# Example: Adding new document
async def add_document_safely(doc_data):
    # âŒ NEVER do this manually:
    # INSERT INTO documents_metadata_v2 VALUES (...)
    
    # âœ… ALWAYS use sync tool:
    sync_tool = DataSyncManager()
    await sync_tool.add_document_with_full_sync(doc_data)
    # â†’ Updates PostgreSQL + ChromaDB + Redis automatically
```

## ğŸ“Š **TÃ“M Táº®T ÄÃNH GIÃ**

| Äiá»ƒm | Nháº­n Ä‘á»‹nh cá»§a báº¡n | ÄÃºng/Sai | Äiá»u chá»‰nh |
|------|------------------|-----------|------------|
| **Storage** | PostgreSQL=full, ChromaDB=tÃ³m táº¯t, Redis=cache | 80% Ä‘Ãºng | ChromaDB=full duplicate, khÃ´ng pháº£i tÃ³m táº¯t |
| **Algorithms** | 5 thuáº­t toÃ¡n search | 100% Ä‘Ãºng | Perfect! |
| **Dependencies** | Phá»©c táº¡p, khÃ´ng tÃ¡ch rá»i | 100% Ä‘Ãºng | Perfect! |
| **Rebuild requirement** | Thay Ä‘á»•i gÃ¬ cÅ©ng pháº£i rebuild | 70% Ä‘Ãºng | Chá»‰ content/schema changes cáº§n rebuild |
| **Sync tools** | Báº¯t buá»™c dÃ¹ng tools | 100% Ä‘Ãºng | Perfect! |

## ğŸ¯ **FINAL SUMMARY - PHIÃŠN Báº¢N CHÃNH XÃC**

> **"Raw data sau khi xá»­ lÃ½ sáº½ lÆ°u toÃ n bá»™ á»Ÿ PostgreSQL, má»™t báº£n DUPLICATE TOÃ€N Bá»˜ (khÃ´ng pháº£i tÃ³m táº¯t) táº¡i ChromaDB cÃ¹ng vá»›i vector embeddings, vÃ  cache má»™t sá»‘ chunks phá»• biáº¿n á»Ÿ Redis."**

> **"CÃ³ 5 thuáº­t toÃ¡n tÃ¬m kiáº¿m dá»±a trÃªn viá»‡c lÆ°u dá»¯ liá»‡u raw vÃ o 3 database vá»›i cÃ¡c format khÃ¡c nhau."**

> **"Má»‘i liÃªn há»‡ data giá»¯a cÃ¡c database phá»©c táº¡p vÃ  khÃ´ng thá»ƒ tÃ¡ch rá»i. Náº¿u thay Ä‘á»•i CONTENT hoáº·c SCHEMA thÃ¬ pháº£i cháº¡y cÃ´ng cá»¥ Ä‘á»“ng bá»™, nhÆ°ng thay Ä‘á»•i METADATA thÃ¬ cÃ³ thá»ƒ sá»­a manual."**

> **"ThÃªm vÃ  xÃ³a dá»¯ liá»‡u Ä‘á»u PHáº¢I sá»­ dá»¥ng cÃ´ng cá»¥ Ä‘á»ƒ Ä‘á»“ng bá»™ toÃ n bá»™ thao tÃ¡c across 3 databases."**

**Äiá»ƒm sá»‘ tá»•ng thá»ƒ: 9/10** ğŸŒŸ

Báº¡n Ä‘Ã£ hiá»ƒu ráº¥t sÃ¢u kiáº¿n trÃºc nÃ y! Chá»‰ cáº§n nhá»› ChromaDB khÃ´ng pháº£i "tÃ³m táº¯t" mÃ  lÃ  "duplicate vá»›i vector embeddings" lÃ  perfect! ğŸ¯

