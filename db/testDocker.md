Tuyá»‡t vá»i! TÃ´i sáº½ hÆ°á»›ng dáº«n báº¡n tá»«ng bÆ°á»›c Ä‘á»ƒ test kiáº¿n trÃºc database enhanced trÃªn Windows vá»›i Docker.

## ğŸš€ **HÆ¯á»šNG DáºªN TRIá»‚N KHAI Tá»ªNG BÆ¯á»šC TRÃŠN WINDOWS**

### **BÆ°á»›c 1: Táº¡o cáº¥u trÃºc thÆ° má»¥c**

Má»Ÿ **Command Prompt** hoáº·c **PowerShell** vá»›i quyá»n Administrator vÃ  cháº¡y:

```cmd
mkdir chatbot-enhanced-db
cd chatbot-enhanced-db

mkdir scripts
mkdir scripts\migrations
mkdir logs
mkdir test_results
mkdir data
mkdir data\sample_documents
mkdir config
mkdir config\grafana
mkdir config\grafana\dashboards
mkdir config\grafana\datasources
mkdir docker
mkdir tests

echo. > docker-compose.yml
```

### **BÆ°á»›c 2: Táº¡o Docker Compose file**

Táº¡o file `docker-compose.yml` trong thÆ° má»¥c gá»‘c:

```yaml
# docker-compose.yml
version: '3.8'

services:
  # PostgreSQL with enhanced schema
  postgres-test:
    image: postgres:15-alpine
    container_name: chatbot-postgres-test
    environment:
      POSTGRES_DB: knowledge_base_test
      POSTGRES_USER: kb_admin
      POSTGRES_PASSWORD: test_password_123
      PGDATA: /var/lib/postgresql/data/pgdata
    volumes:
      - postgres_test_data:/var/lib/postgresql/data
      - ./scripts/migrations:/docker-entrypoint-initdb.d:ro
    ports:
      - "5433:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U kb_admin -d knowledge_base_test"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - chatbot-test-network

  # Redis for caching
  redis-test:
    image: redis:7-alpine
    container_name: chatbot-redis-test
    ports:
      - "6380:6379"
    volumes:
      - redis_test_data:/data
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5
    networks:
      - chatbot-test-network

  # ChromaDB for vector storage
  chromadb-test:
    image: chromadb/chroma:latest
    container_name: chatbot-chroma-test
    environment:
      CHROMA_SERVER_HOST: 0.0.0.0
      CHROMA_SERVER_HTTP_PORT: 8000
    volumes:
      - chromadb_test_data:/chroma/chroma
    ports:
      - "8001:8000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/heartbeat"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - chatbot-test-network

  # Database setup service
  db-setup:
    image: python:3.9-slim
    container_name: chatbot-db-setup
    environment:
      DB_HOST: postgres-test
      DB_PORT: 5432
      DB_NAME: knowledge_base_test
      DB_USER: kb_admin
      DB_PASSWORD: test_password_123
    volumes:
      - ./scripts:/app/scripts:ro
      - ./logs:/app/logs
    working_dir: /app
    depends_on:
      postgres-test:
        condition: service_healthy
      redis-test:
        condition: service_healthy
    command: >
      sh -c "
      pip install asyncpg psycopg2-binary &&
      python scripts/setup_database.py
      "
    networks:
      - chatbot-test-network

  # Monitoring dashboard
  adminer:
    image: adminer
    container_name: chatbot-adminer
    ports:
      - "8080:8080"
    environment:
      ADMINER_DEFAULT_SERVER: postgres-test
    depends_on:
      postgres-test:
        condition: service_healthy
    networks:
      - chatbot-test-network

volumes:
  postgres_test_data:
  redis_test_data:
  chromadb_test_data:

networks:
  chatbot-test-network:
    driver: bridge
```

### **BÆ°á»›c 3: Táº¡o Migration Scripts**

Táº¡o file `scripts/migrations/01_init_database.sql`:

```sql
-- scripts/migrations/01_init_database.sql

-- Enable required extensions
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pg_trgm";

-- Create enhanced enum types
DO $$ BEGIN
    CREATE TYPE access_level_enum AS ENUM (
        'public', 'employee_only', 'manager_only', 'director_only', 'system_admin'
    );
EXCEPTION
    WHEN duplicate_object THEN null;
END $$;

DO $$ BEGIN
    CREATE TYPE document_type_enum AS ENUM (
        'policy', 'procedure', 'technical_guide', 'report', 
        'manual', 'specification', 'template', 'form', 
        'presentation', 'training_material', 'other'
    );
EXCEPTION
    WHEN duplicate_object THEN null;
END $$;

DO $$ BEGIN
    CREATE TYPE document_status_enum AS ENUM (
        'draft', 'review', 'approved', 'published', 'archived', 'deprecated'
    );
EXCEPTION
    WHEN duplicate_object THEN null;
END $$;

-- Enhanced documents metadata table
CREATE TABLE IF NOT EXISTS documents_metadata_v2 (
    document_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    
    -- Basic information
    title VARCHAR(500) NOT NULL,
    content TEXT,
    document_type document_type_enum NOT NULL,
    access_level access_level_enum NOT NULL DEFAULT 'employee_only',
    department_owner VARCHAR(100) NOT NULL,
    author VARCHAR(255) NOT NULL,
    status document_status_enum DEFAULT 'draft',
    
    -- Vietnamese language support
    language_detected VARCHAR(10) DEFAULT 'vi',
    vietnamese_segmented BOOLEAN DEFAULT false,
    diacritics_normalized BOOLEAN DEFAULT false,
    tone_marks_preserved BOOLEAN DEFAULT true,
    
    -- FlashRAG support
    flashrag_collection VARCHAR(100) DEFAULT 'default_collection',
    jsonl_export_ready BOOLEAN DEFAULT false,
    
    -- Search support
    search_tokens TSVECTOR,
    keyword_density JSONB,
    heading_structure JSONB,
    
    -- Metadata
    embedding_model_primary VARCHAR(100),
    chunk_count INTEGER DEFAULT 0,
    file_size_bytes BIGINT,
    
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Enhanced document chunks table
CREATE TABLE IF NOT EXISTS document_chunks_enhanced (
    chunk_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    document_id UUID REFERENCES documents_metadata_v2(document_id) ON DELETE CASCADE,
    
    -- Content data
    chunk_content TEXT NOT NULL,
    chunk_position INTEGER NOT NULL,
    chunk_size_tokens INTEGER,
    
    -- Semantic chunking metadata
    semantic_boundary BOOLEAN DEFAULT false,
    overlap_with_prev INTEGER DEFAULT 0,
    overlap_with_next INTEGER DEFAULT 0,
    heading_context TEXT,
    
    -- Quality and method
    chunk_method VARCHAR(20) DEFAULT 'semantic',
    chunk_quality_score DECIMAL(3,2) CHECK (chunk_quality_score BETWEEN 0.00 AND 1.00),
    
    -- Vector storage references
    embedding_model VARCHAR(100),
    embedding_dimensions INTEGER,
    
    -- BM25 support
    bm25_tokens TSVECTOR,
    
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- BM25 support table
CREATE TABLE IF NOT EXISTS document_bm25_index (
    bm25_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    document_id UUID REFERENCES documents_metadata_v2(document_id) ON DELETE CASCADE,
    chunk_id UUID REFERENCES document_chunks_enhanced(chunk_id) ON DELETE CASCADE,
    
    term VARCHAR(255) NOT NULL,
    term_frequency INTEGER NOT NULL,
    document_frequency INTEGER NOT NULL,
    bm25_score DECIMAL(8,4),
    
    language VARCHAR(10) DEFAULT 'vi',
    
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    UNIQUE(chunk_id, term, language)
);

-- Pipeline tracking table
CREATE TABLE IF NOT EXISTS rag_pipeline_sessions (
    session_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    
    -- Query information
    original_query TEXT NOT NULL,
    processed_query TEXT,
    query_language VARCHAR(10) DEFAULT 'vi',
    
    -- Pipeline metadata
    pipeline_type VARCHAR(50) NOT NULL DEFAULT 'standard',
    pipeline_method VARCHAR(50) NOT NULL DEFAULT 'hybrid',
    
    -- Performance metrics
    chunks_retrieved INTEGER,
    processing_time_ms INTEGER,
    response_quality_score DECIMAL(3,2),
    
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Vietnamese text analysis table
CREATE TABLE IF NOT EXISTS vietnamese_text_analysis (
    analysis_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    document_id UUID REFERENCES documents_metadata_v2(document_id) ON DELETE CASCADE,
    chunk_id UUID REFERENCES document_chunks_enhanced(chunk_id) ON DELETE CASCADE,
    
    original_text TEXT NOT NULL,
    processed_text TEXT,
    
    word_segmentation JSONB,
    pos_tagging JSONB,
    
    compound_words TEXT[],
    technical_terms TEXT[],
    proper_nouns TEXT[],
    
    readability_score DECIMAL(3,2),
    formality_level VARCHAR(20),
    
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Create performance indexes
CREATE INDEX IF NOT EXISTS idx_documents_v2_language ON documents_metadata_v2(language_detected);
CREATE INDEX IF NOT EXISTS idx_documents_v2_status ON documents_metadata_v2(status);
CREATE INDEX IF NOT EXISTS idx_documents_v2_collection ON documents_metadata_v2(flashrag_collection);
CREATE INDEX IF NOT EXISTS idx_documents_v2_search ON documents_metadata_v2 USING GIN(search_tokens);

CREATE INDEX IF NOT EXISTS idx_chunks_enhanced_document ON document_chunks_enhanced(document_id);
CREATE INDEX IF NOT EXISTS idx_chunks_enhanced_position ON document_chunks_enhanced(chunk_position);
CREATE INDEX IF NOT EXISTS idx_chunks_enhanced_semantic ON document_chunks_enhanced(semantic_boundary) WHERE semantic_boundary = true;

CREATE INDEX IF NOT EXISTS idx_bm25_term ON document_bm25_index(term);
CREATE INDEX IF NOT EXISTS idx_bm25_chunk ON document_bm25_index(chunk_id);
CREATE INDEX IF NOT EXISTS idx_bm25_score ON document_bm25_index(bm25_score DESC);

CREATE INDEX IF NOT EXISTS idx_pipeline_sessions_created ON rag_pipeline_sessions(created_at DESC);
CREATE INDEX IF NOT EXISTS idx_pipeline_sessions_type ON rag_pipeline_sessions(pipeline_type, pipeline_method);

-- Insert sample data
INSERT INTO documents_metadata_v2 (
    title, content, document_type, access_level, department_owner, author, status, jsonl_export_ready
) VALUES 
(
    'Quy trÃ¬nh xin nghá»‰ phÃ©p',
    'Quy trÃ¬nh xin nghá»‰ phÃ©p táº¡i cÃ´ng ty bao gá»“m cÃ¡c bÆ°á»›c sau: 1. NhÃ¢n viÃªn Ä‘iá»n Ä‘Æ¡n xin nghá»‰ phÃ©p 2. Gá»­i Ä‘Æ¡n cho quáº£n lÃ½ trá»±c tiáº¿p 3. Quáº£n lÃ½ phÃª duyá»‡t trong vÃ²ng 2 ngÃ y lÃ m viá»‡c 4. HR cáº­p nháº­t vÃ o há»‡ thá»‘ng 5. ThÃ´ng bÃ¡o káº¿t quáº£ cho nhÃ¢n viÃªn',
    'procedure',
    'employee_only',
    'HR',
    'HR Department',
    'approved',
    true
),
(
    'ChÃ­nh sÃ¡ch lÃ m viá»‡c tá»« xa',
    'ChÃ­nh sÃ¡ch lÃ m viá»‡c tá»« xa (Work From Home) Ä‘Æ°á»£c Ã¡p dá»¥ng nhÆ° sau: - NhÃ¢n viÃªn cÃ³ thá»ƒ lÃ m viá»‡c tá»« xa tá»‘i Ä‘a 3 ngÃ y/tuáº§n - Cáº§n Ä‘Äƒng kÃ½ trÆ°á»›c Ã­t nháº¥t 1 ngÃ y - Äáº£m báº£o mÃ´i trÆ°á»ng lÃ m viá»‡c á»•n Ä‘á»‹nh - Tham gia Ä‘áº§y Ä‘á»§ cÃ¡c cuá»™c há»p online - BÃ¡o cÃ¡o tiáº¿n Ä‘á»™ cÃ´ng viá»‡c hÃ ng ngÃ y',
    'policy',
    'employee_only',
    'HR',
    'Management Team',
    'approved',
    true
),
(
    'HÆ°á»›ng dáº«n sá»­ dá»¥ng há»‡ thá»‘ng ERP',
    'HÆ°á»›ng dáº«n chi tiáº¿t sá»­ dá»¥ng há»‡ thá»‘ng ERP cÃ´ng ty: 1. ÄÄƒng nháº­p há»‡ thá»‘ng - Sá»­ dá»¥ng tÃ i khoáº£n company email - Máº­t kháº©u Ä‘Æ°á»£c cáº¥p ban Ä‘áº§u cáº§n Ä‘á»•i ngay láº§n Ä‘áº§u Ä‘Äƒng nháº­p 2. Module quáº£n lÃ½ nhÃ¢n sá»± - Cáº­p nháº­t thÃ´ng tin cÃ¡ nhÃ¢n - ÄÄƒng kÃ½ nghá»‰ phÃ©p - Xem báº£ng lÆ°Æ¡ng 3. Module quáº£n lÃ½ dá»± Ã¡n - Táº¡o task má»›i - Cáº­p nháº­t tiáº¿n Ä‘á»™ - BÃ¡o cÃ¡o hÃ ng tuáº§n',
    'technical_guide',
    'employee_only',
    'IT',
    'IT Support Team',
    'approved',
    true
)
ON CONFLICT DO NOTHING;

-- Update search tokens for sample documents
UPDATE documents_metadata_v2 
SET search_tokens = to_tsvector('simple', title || ' ' || COALESCE(content, ''))
WHERE search_tokens IS NULL;

-- Success message
DO $$ BEGIN
    RAISE NOTICE 'Enhanced Database Architecture initialized successfully!';
    RAISE NOTICE 'Sample Vietnamese documents loaded.';
    RAISE NOTICE 'Database is ready for testing.';
END $$;
```

### **BÆ°á»›c 4: Táº¡o Database Setup Script**

Táº¡o file `scripts/setup_database.py`:

```python
# scripts/setup_database.py
import asyncio
import asyncpg
import logging
import time
import os
import sys

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

async def setup_enhanced_database():
    """Setup and verify enhanced database architecture"""
    
    db_config = {
        'host': os.getenv('DB_HOST', 'localhost'),
        'port': int(os.getenv('DB_PORT', 5432)),
        'database': os.getenv('DB_NAME', 'knowledge_base_test'),
        'user': os.getenv('DB_USER', 'kb_admin'),
        'password': os.getenv('DB_PASSWORD', 'test_password_123')
    }
    
    logger.info("ğŸš€ Starting Enhanced Database Setup")
    
    # Wait for database to be ready
    max_retries = 30
    for attempt in range(max_retries):
        try:
            conn = await asyncpg.connect(**db_config)
            await conn.execute('SELECT 1')
            await conn.close()
            logger.info("âœ… Database connection successful!")
            break
        except Exception as e:
            logger.info(f"â³ Waiting for database... (attempt {attempt + 1}/{max_retries})")
            if attempt == max_retries - 1:
                logger.error("âŒ Database connection failed after maximum retries")
                return False
            await asyncio.sleep(2)
    
    # Connect to database
    try:
        conn = await asyncpg.connect(**db_config)
        logger.info("ğŸ”— Connected to database")
        
        # Verify table creation
        tables = await conn.fetch("""
            SELECT table_name FROM information_schema.tables 
            WHERE table_schema = 'public' 
            ORDER BY table_name
        """)
        
        logger.info(f"ğŸ“Š Database created with {len(tables)} tables:")
        for table in tables:
            logger.info(f"  âœ… {table['table_name']}")
        
        # Verify sample data
        doc_count = await conn.fetchval("SELECT COUNT(*) FROM documents_metadata_v2")
        logger.info(f"ğŸ“„ Sample documents loaded: {doc_count}")
        
        if doc_count > 0:
            # Show sample documents
            docs = await conn.fetch("SELECT title, author, status FROM documents_metadata_v2 LIMIT 3")
            logger.info("ğŸ“‹ Sample documents:")
            for doc in docs:
                logger.info(f"  ğŸ“„ '{doc['title']}' by {doc['author']} ({doc['status']})")
        
        # Test basic queries
        logger.info("ğŸ” Testing basic queries...")
        
        # Test Vietnamese search
        vn_docs = await conn.fetchval("""
            SELECT COUNT(*) FROM documents_metadata_v2 
            WHERE language_detected = 'vi'
        """)
        logger.info(f"  ğŸ‡»ğŸ‡³ Vietnamese documents: {vn_docs}")
        
        # Test full-text search capability
        search_ready = await conn.fetchval("""
            SELECT COUNT(*) FROM documents_metadata_v2 
            WHERE search_tokens IS NOT NULL
        """)
        logger.info(f"  ğŸ” Documents with search tokens: {search_ready}")
        
        # Test enum types
        enum_test = await conn.fetchval("""
            SELECT COUNT(DISTINCT document_type) FROM documents_metadata_v2
        """)
        logger.info(f"  ğŸ“ Document types in use: {enum_test}")
        
        # Create a sample pipeline session for testing
        session_id = await conn.fetchval("""
            INSERT INTO rag_pipeline_sessions (
                original_query, processed_query, pipeline_type, pipeline_method,
                chunks_retrieved, processing_time_ms, response_quality_score
            ) VALUES (
                'Quy trÃ¬nh xin nghá»‰ phÃ©p nhÆ° tháº¿ nÃ o?',
                'quy trÃ¬nh xin nghá»‰ phÃ©p',
                'standard',
                'hybrid',
                3,
                150,
                0.85
            ) RETURNING session_id
        """)
        
        logger.info(f"  âœ… Sample pipeline session created: {session_id}")
        
        # Generate database statistics
        db_size = await conn.fetchval("SELECT pg_size_pretty(pg_database_size(current_database()))")
        logger.info(f"ğŸ’¾ Database size: {db_size}")
        
        # Create comprehensive test report
        report = f"""
# Enhanced Database Architecture Test Report
Generated: {time.strftime('%Y-%m-%d %H:%M:%S')}

## Database Information
- **Host**: {db_config['host']}:{db_config['port']}
- **Database**: {db_config['database']}
- **Size**: {db_size}

## Tables Created: {len(tables)}
{chr(10).join(f'- {table["table_name"]}' for table in tables)}

## Sample Data
- **Documents**: {doc_count}
- **Vietnamese Documents**: {vn_docs}
- **Search-Ready Documents**: {search_ready}
- **Document Types**: {enum_test}

## Features Tested
âœ… Enhanced schema with Vietnamese support
âœ… Multi-type enum support
âœ… Full-text search capability
âœ… Pipeline session tracking
âœ… BM25 index structure
âœ… FlashRAG compatibility structure

## Connection Information
- **PostgreSQL**: localhost:5433
- **Redis**: localhost:6380
- **ChromaDB**: localhost:8001
- **Adminer**: http://localhost:8080

## Next Steps
1. Access Adminer at http://localhost:8080 to browse database
2. Connect using: Server: postgres-test, Username: kb_admin, Password: test_password_123
3. Test queries using the sample Vietnamese documents
4. Explore enhanced schema features

## Test Status: âœ… SUCCESS
All components initialized and tested successfully!
        """
        
        # Write report to log file
        with open('/app/logs/setup_report.md', 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info("ğŸ“„ Setup report saved to: /app/logs/setup_report.md")
        print(report)
        
        await conn.close()
        logger.info("ğŸ‰ Enhanced Database Architecture setup completed successfully!")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Database setup failed: {e}")
        return False

if __name__ == "__main__":
    success = asyncio.run(setup_enhanced_database())
    sys.exit(0 if success else 1)
```

### **BÆ°á»›c 5: Cháº¡y Docker Test**

Má»Ÿ PowerShell trong thÆ° má»¥c `chatbot-enhanced-db` vÃ  cháº¡y:

```powershell
# Khá»Ÿi Ä‘á»™ng cÃ¡c services
docker-compose up -d

# Kiá»ƒm tra tráº¡ng thÃ¡i
docker-compose ps

# Xem logs cá»§a database setup
docker logs chatbot-db-setup

# Xem logs cá»§a PostgreSQL
docker logs chatbot-postgres-test
```

### **BÆ°á»›c 6: Kiá»ƒm tra káº¿t quáº£**

1. **Má»Ÿ Adminer (Database Browser)**:
   - Truy cáº­p: http://localhost:8080
   - Server: `postgres-test`
   - Username: `kb_admin`
   - Password: `test_password_123`
   - Database: `knowledge_base_test`

2. **Kiá»ƒm tra ChromaDB**:
   ```powershell
   # Test ChromaDB API
   curl http://localhost:8001/api/v1/heartbeat
   curl.exe http://localhost:8001/api/v2/heartbeat
   ```

3. **Kiá»ƒm tra Redis**:
   ```powershell
   # Connect to Redis container
   docker exec -it chatbot-redis-test redis-cli ping
   ```

### **BÆ°á»›c 7: Test Database vá»›i PowerShell**

Táº¡o file `test_database.ps1`:

```powershell
# test_database.ps1

Write-Host "ğŸ” Testing Enhanced Database Architecture" -ForegroundColor Green

# Test PostgreSQL connection
Write-Host "`nğŸ“Š Testing PostgreSQL..." -ForegroundColor Yellow
try {
    $response = Invoke-RestMethod -Uri "http://localhost:8080" -Method Get
    Write-Host "âœ… Adminer accessible at http://localhost:8080" -ForegroundColor Green
} catch {
    Write-Host "âŒ Adminer not accessible" -ForegroundColor Red
}

# Test ChromaDB
Write-Host "`nğŸ”¢ Testing ChromaDB..." -ForegroundColor Yellow
try {
    $response = Invoke-RestMethod -Uri "http://localhost:8001/api/v1/heartbeat" -Method Get
    Write-Host "âœ… ChromaDB accessible: $($response.nanosecond_heartbeat)" -ForegroundColor Green
} catch {
    Write-Host "âŒ ChromaDB not accessible" -ForegroundColor Red
}

# Test Redis
Write-Host "`nğŸ”´ Testing Redis..." -ForegroundColor Yellow
try {
    $redisTest = docker exec chatbot-redis-test redis-cli ping
    if ($redisTest -eq "PONG") {
        Write-Host "âœ… Redis accessible: $redisTest" -ForegroundColor Green
    }
} catch {
    Write-Host "âŒ Redis not accessible" -ForegroundColor Red
}

# Check logs
Write-Host "`nğŸ“‹ Recent setup logs:" -ForegroundColor Yellow
docker logs --tail 10 chatbot-db-setup

Write-Host "`nğŸ‰ Database test completed!" -ForegroundColor Green
Write-Host "Access Adminer: http://localhost:8080" -ForegroundColor Cyan
Write-Host "ChromaDB API: http://localhost:8001" -ForegroundColor Cyan

# Show connection info
Write-Host "`nğŸ“ Connection Information:" -ForegroundColor Yellow
Write-Host "PostgreSQL: localhost:5433" -ForegroundColor White
Write-Host "Redis: localhost:6380" -ForegroundColor White
Write-Host "ChromaDB: localhost:8001" -ForegroundColor White
Write-Host "Adminer: http://localhost:8080" -ForegroundColor White
```

Cháº¡y test:
```powershell
powershell -ExecutionPolicy Bypass -File test_database.ps1
```

### **BÆ°á»›c 8: Cleanup khi hoÃ n thÃ nh**

```powershell
# Dá»«ng vÃ  xÃ³a containers
docker-compose down

# XÃ³a volumes (náº¿u muá»‘n xÃ³a háº¿t data)
docker-compose down -v

# XÃ³a images (náº¿u muá»‘n cleanup hoÃ n toÃ n)
docker system prune -a
```

## ğŸ“Š **Káº¿t quáº£ mong Ä‘á»£i**

Sau khi cháº¡y thÃ nh cÃ´ng, báº¡n sáº½ cÃ³:

1. **âœ… PostgreSQL** vá»›i enhanced schema running trÃªn port 5433
2. **âœ… Redis** cache running trÃªn port 6380  
3. **âœ… ChromaDB** vector database trÃªn port 8001
4. **âœ… Adminer** database browser táº¡i http://localhost:8080
5. **âœ… Sample Vietnamese documents** Ä‘Ã£ Ä‘Æ°á»£c load
6. **âœ… Test report** trong `logs/setup_report.md`

**TÃ­nh nÄƒng Ä‘Ã£ test:**
- ğŸ‡»ğŸ‡³ Vietnamese language support
- ğŸ” Hybrid search structure (BM25 + Vector)
- ğŸ“Š Enhanced metadata schema
- âš¡ Performance indexes
- ğŸ“„ FlashRAG compatibility
- ğŸ”„ Pipeline tracking

Báº¡n cÃ³ gáº·p lá»—i gÃ¬ trong quÃ¡ trÃ¬nh setup khÃ´ng? TÃ´i sáº½ giÃºp troubleshoot!
---
Tuyá»‡t vá»i! TÃ´i sáº½ giáº£i thÃ­ch chi tiáº¿t tá»«ng container Ä‘ang cháº¡y vÃ  vai trÃ² cá»§a chÃºng trong há»‡ thá»‘ng. HÃ£y cÃ¹ng khÃ¡m phÃ¡ tá»«ng module má»™t cÃ¡ch dá»… hiá»ƒu:

## ğŸ³ **GIáº¢I THÃCH CHI TIáº¾T CÃC DOCKER CONTAINERS**

### **1. ğŸ˜ chatbot-postgres-test - "Bá»™ nÃ£o chÃ­nh cá»§a há»‡ thá»‘ng"**

**ÄÃ¢y lÃ  gÃ¬?**
- PostgreSQL lÃ  há»‡ quáº£n trá»‹ cÆ¡ sá»Ÿ dá»¯ liá»‡u quan há»‡ (nhÆ° Excel siÃªu máº¡nh)
- LÆ°u trá»¯ táº¥t cáº£ thÃ´ng tin cÃ³ cáº¥u trÃºc: metadata tÃ i liá»‡u, thÃ´ng tin ngÆ°á»i dÃ¹ng, logs

**Äang lÃ m gÃ¬?**
```sql
-- LÆ°u thÃ´ng tin tÃ i liá»‡u
documents_metadata_v2: TÃªn file, tÃ¡c giáº£, ngÃ y táº¡o, loáº¡i tÃ i liá»‡u
document_chunks_enhanced: CÃ¡c Ä‘oáº¡n vÄƒn báº£n Ä‘Ã£ Ä‘Æ°á»£c cáº¯t nhá»
rag_pipeline_sessions: Lá»‹ch sá»­ cÃ¡c cÃ¢u há»i vÃ  tráº£ lá»i

-- VÃ­ dá»¥ data thá»±c táº¿:
Title: "Quy trÃ¬nh xin nghá»‰ phÃ©p"
Author: "HR Department" 
Content: "BÆ°á»›c 1: Äiá»n Ä‘Æ¡n..."
Status: "approved"
```

**Kiá»ƒm tra PostgreSQL:**
```powershell
# VÃ o container PostgreSQL
docker exec -it chatbot-postgres-test psql -U kb_admin -d knowledge_base_test

# Xem cÃ¡c báº£ng Ä‘Ã£ táº¡o
\dt

# Xem dá»¯ liá»‡u máº«u
SELECT title, author, status FROM documents_metadata_v2;

# ThoÃ¡t
\q
```

### **2. ğŸ”´ chatbot-redis-test - "Bá»™ nhá»› Ä‘á»‡m tá»‘c Ä‘á»™ cao"**

**ÄÃ¢y lÃ  gÃ¬?**
- Redis nhÆ° "RAM má»Ÿ rá»™ng" - lÆ°u táº¡m thÃ´ng tin hay dÃ¹ng
- GiÃºp há»‡ thá»‘ng pháº£n há»“i nhanh hÆ¡n (thay vÃ¬ query database má»—i láº§n)

**Äang lÃ m gÃ¬?**
```redis
# LÆ°u cache cÃ¡c káº¿t quáº£ tÃ¬m kiáº¿m
user:123:last_query = "Quy trÃ¬nh nghá»‰ phÃ©p"
embedding:doc_456 = [0.1, 0.8, 0.3, ...] # Vector embeddings

# Session ngÆ°á»i dÃ¹ng
session:abc123 = {user_id: 456, login_time: "2024-01-01"}
```

**Kiá»ƒm tra Redis:**
```powershell
# VÃ o Redis container
docker exec -it chatbot-redis-test redis-cli

# Test Redis
ping
# Response: PONG

# Xem táº¥t cáº£ keys (hiá»‡n táº¡i cÃ²n trá»‘ng)
keys *

# Táº¡o test data
set test:hello "world"
get test:hello

# ThoÃ¡t
exit
```

### **3. ğŸŸ¢ chatbot-chroma-test - "Kho lÆ°u trá»¯ vector thÃ´ng minh"**

**ÄÃ¢y lÃ  gÃ¬?**
- ChromaDB chuyÃªn lÆ°u trá»¯ "vector embeddings" (sá»‘ hÃ³a vÄƒn báº£n)
- GiÃºp tÃ¬m kiáº¿m theo Ã½ nghÄ©a (semantic search) thay vÃ¬ chá»‰ tá»« khÃ³a

**Äang lÃ m gÃ¬?**
```python
# Chuyá»ƒn Ä‘á»•i vÄƒn báº£n thÃ nh vector
"Quy trÃ¬nh nghá»‰ phÃ©p" â†’ [0.1, 0.8, 0.3, 0.5, 0.2, ...]
"Xin phÃ©p nghá»‰ viá»‡c" â†’ [0.2, 0.7, 0.4, 0.5, 0.1, ...]
# Hai cÃ¢u nÃ y cÃ³ Ã½ nghÄ©a gáº§n nhau â†’ vector gáº§n nhau
```

**Kiá»ƒm tra ChromaDB:**
```powershell
# Test API cá»§a ChromaDB
curl http://localhost:8001/api/v1/heartbeat

# Xem collections (hiá»‡n táº¡i chÆ°a cÃ³)
curl http://localhost:8001/api/v1/collections
```

### **4. ğŸŒ chatbot-adminer - "Giao diá»‡n quáº£n lÃ½ database"**

**ÄÃ¢y lÃ  gÃ¬?**
- Adminer lÃ  cÃ´ng cá»¥ web Ä‘á»ƒ xem/quáº£n lÃ½ database (nhÆ° phpMyAdmin)
- GiÃºp báº¡n duyá»‡t data mÃ  khÃ´ng cáº§n dÃ¹ng command line

**Äang lÃ m gÃ¬?**
- Cung cáº¥p giao diá»‡n web táº¡i http://localhost:8080
- Cho phÃ©p xem/sá»­a/truy váº¥n database PostgreSQL

**CÃ¡ch sá»­ dá»¥ng Adminer:**
1. Má»Ÿ trÃ¬nh duyá»‡t: http://localhost:8080
2. ÄÄƒng nháº­p:
   - System: PostgreSQL
   - Server: postgres-test
   - Username: kb_admin
   - Password: test_password_123
   - Database: knowledge_base_test

### **5. âŒ chatbot-db-setup - "Thá»£ setup database (Ä‘Ã£ hoÃ n thÃ nh)"**

**Táº¡i sao khÃ´ng cháº¡y?**
- Container nÃ y chá»‰ cháº¡y 1 láº§n Ä‘á»ƒ setup database
- Sau khi hoÃ n thÃ nh viá»‡c táº¡o báº£ng vÃ  load data máº«u â†’ tá»± Ä‘á»™ng táº¯t
- ÄÃ¢y lÃ  hÃ nh vi BÃŒNH THÆ¯á»œNG!

**ÄÃ£ lÃ m gÃ¬?**
```python
# 1. Táº¡o cÃ¡c báº£ng (tables)
# 2. Táº¡o cÃ¡c index Ä‘á»ƒ tÄƒng tá»‘c
# 3. Load dá»¯ liá»‡u máº«u (3 tÃ i liá»‡u tiáº¿ng Viá»‡t)
# 4. Táº¡o bÃ¡o cÃ¡o setup
```

**Xem logs Ä‘á»ƒ hiá»ƒu Ä‘Ã£ lÃ m gÃ¬:**
```powershell
docker logs chatbot-db-setup
```

## ğŸ” **HÆ¯á»šNG DáºªN KHÃM PHÃ Há»† THá»NG**

### **BÆ°á»›c 1: KhÃ¡m phÃ¡ Database qua Adminer**

```powershell
# Má»Ÿ Adminer
start http://localhost:8080
```

Trong Adminer:
1. **ÄÄƒng nháº­p** vá»›i thÃ´ng tin á»Ÿ trÃªn
2. **Click vÃ o báº£ng `documents_metadata_v2`** â†’ xem dá»¯ liá»‡u máº«u
3. **Click vÃ o `SQL command`** â†’ cháº¡y cÃ¢u lá»‡nh:

```sql
-- Xem táº¥t cáº£ tÃ i liá»‡u
SELECT title, author, department_owner, status 
FROM documents_metadata_v2;

-- Xem tÃ i liá»‡u tiáº¿ng Viá»‡t
SELECT title, LEFT(content, 100) as preview
FROM documents_metadata_v2 
WHERE language_detected = 'vi';

-- Äáº¿m sá»‘ báº£ng trong database
SELECT COUNT(*) as total_tables 
FROM information_schema.tables 
WHERE table_schema = 'public';
```

### **BÆ°á»›c 2: Táº¡o file test Ä‘á»ƒ hiá»ƒu workflow**

Táº¡o file `understand_system.py`:

```python
# understand_system.py
import asyncio
import asyncpg
import json

async def explore_database():
    """KhÃ¡m phÃ¡ database Ä‘á»ƒ hiá»ƒu há»‡ thá»‘ng"""
    
    # Káº¿t ná»‘i database
    conn = await asyncpg.connect(
        host='localhost',
        port=5433,  # Port cá»§a PostgreSQL test
        database='knowledge_base_test',
        user='kb_admin',
        password='test_password_123'
    )
    
    print("ğŸ”— Connected to Enhanced Database!")
    print("=" * 50)
    
    # 1. Xem táº¥t cáº£ báº£ng
    tables = await conn.fetch("""
        SELECT table_name, 
               (SELECT COUNT(*) FROM information_schema.columns 
                WHERE table_name = t.table_name AND table_schema = 'public') as column_count
        FROM information_schema.tables t
        WHERE table_schema = 'public'
        ORDER BY table_name
    """)
    
    print(f"ğŸ“Š Database cÃ³ {len(tables)} báº£ng:")
    for table in tables:
        print(f"   ğŸ“‹ {table['table_name']} ({table['column_count']} cá»™t)")
    
    # 2. Xem dá»¯ liá»‡u máº«u
    print(f"\nğŸ“„ Dá»¯ liá»‡u máº«u:")
    documents = await conn.fetch("""
        SELECT title, author, department_owner, 
               LENGTH(content) as content_length,
               language_detected, status
        FROM documents_metadata_v2
        ORDER BY title
    """)
    
    for doc in documents:
        print(f"   ğŸ“ '{doc['title']}'")
        print(f"      ğŸ‘¤ TÃ¡c giáº£: {doc['author']}")
        print(f"      ğŸ¢ PhÃ²ng ban: {doc['department_owner']}")
        print(f"      ğŸ“ Ná»™i dung: {doc['content_length']} kÃ½ tá»±")
        print(f"      ğŸŒ NgÃ´n ngá»¯: {doc['language_detected']}")
        print(f"      ğŸ“Š Tráº¡ng thÃ¡i: {doc['status']}")
        print()
    
    # 3. Demo search functionality
    print("ğŸ” Demo tÃ¬m kiáº¿m:")
    
    # TÃ¬m kiáº¿m theo tá»« khÃ³a
    search_results = await conn.fetch("""
        SELECT title, author
        FROM documents_metadata_v2
        WHERE LOWER(title) LIKE '%nghá»‰ phÃ©p%'
           OR LOWER(content) LIKE '%nghá»‰ phÃ©p%'
    """)
    
    print(f"   TÃ¬m 'nghá»‰ phÃ©p': {len(search_results)} káº¿t quáº£")
    for result in search_results:
        print(f"      âœ… {result['title']} - {result['author']}")
    
    # 4. Xem cáº¥u trÃºc enhanced schema
    print(f"\nğŸ—ï¸ Cáº¥u trÃºc Enhanced Schema:")
    enhanced_features = await conn.fetch("""
        SELECT 
            COUNT(*) FILTER (WHERE vietnamese_segmented = true) as vietnamese_processed,
            COUNT(*) FILTER (WHERE search_tokens IS NOT NULL) as search_ready,
            COUNT(*) FILTER (WHERE jsonl_export_ready = true) as flashrag_ready
        FROM documents_metadata_v2
    """)
    
    feature = enhanced_features[0]
    print(f"   ğŸ‡»ğŸ‡³ Vietnamese processed: {feature['vietnamese_processed']}")
    print(f"   ğŸ” Search ready: {feature['search_ready']}")
    print(f"   ğŸ“¤ FlashRAG ready: {feature['flashrag_ready']}")
    
    # 5. Demo táº¡o pipeline session
    print(f"\nâš¡ Demo táº¡o pipeline session:")
    session_id = await conn.fetchval("""
        INSERT INTO rag_pipeline_sessions (
            original_query, processed_query, pipeline_type, pipeline_method,
            chunks_retrieved, processing_time_ms, response_quality_score
        ) VALUES (
            'LÃ m tháº¿ nÃ o Ä‘á»ƒ xin nghá»‰ phÃ©p?',
            'xin nghá»‰ phÃ©p quy trÃ¬nh',
            'standard',
            'hybrid',
            2,
            230,
            0.92
        ) RETURNING session_id
    """)
    
    print(f"   âœ… Táº¡o session thÃ nh cÃ´ng: {session_id}")
    
    # 6. Thá»‘ng kÃª há»‡ thá»‘ng
    print(f"\nğŸ“Š Thá»‘ng kÃª há»‡ thá»‘ng:")
    stats = await conn.fetchrow("""
        SELECT 
            pg_size_pretty(pg_database_size(current_database())) as db_size,
            (SELECT COUNT(*) FROM documents_metadata_v2) as total_docs,
            (SELECT COUNT(*) FROM document_chunks_enhanced) as total_chunks,
            (SELECT COUNT(*) FROM rag_pipeline_sessions) as total_sessions
    """)
    
    print(f"   ğŸ’¾ Dung lÆ°á»£ng database: {stats['db_size']}")
    print(f"   ğŸ“„ Tá»•ng documents: {stats['total_docs']}")
    print(f"   âœ‚ï¸ Tá»•ng chunks: {stats['total_chunks']}")
    print(f"   ğŸ”„ Tá»•ng sessions: {stats['total_sessions']}")
    
    await conn.close()
    print(f"\nğŸ‰ Exploration completed!")

if __name__ == "__main__":
    asyncio.run(explore_database())
```

Cháº¡y script nÃ y:
```powershell
pip install asyncpg
python understand_system.py
```

### **BÆ°á»›c 3: Workflow thá»±c táº¿**

```mermaid
graph TD
    User[ğŸ‘¤ User há»i: 'Xin nghá»‰ phÃ©p nhÆ° tháº¿ nÃ o?']
    
    PostgreSQL[(ğŸ˜ PostgreSQL<br/>TÃ¬m documents liÃªn quan)]
    ChromaDB[(ğŸŸ¢ ChromaDB<br/>Vector search theo Ã½ nghÄ©a)]
    Redis[(ğŸ”´ Redis<br/>Cache káº¿t quáº£)]
    
    Processing[âš¡ RAG Processing<br/>Káº¿t há»£p thÃ´ng tin]
    Response[ğŸ’¬ Tráº£ lá»i: 'Quy trÃ¬nh gá»“m 5 bÆ°á»›c...']
    
    User --> PostgreSQL
    User --> ChromaDB
    PostgreSQL --> Redis
    ChromaDB --> Redis
    Redis --> Processing
    Processing --> Response
    Response --> User
    
    style PostgreSQL fill:#e1f5fe
    style ChromaDB fill:#e8f5e8
    style Redis fill:#ffebee
```

## ğŸ¯ **TÃ“M Táº®T ÄÆ N GIáº¢N**

| Container | Vai trÃ² | Giá»‘ng nhÆ° |
|-----------|---------|-----------|
| **postgres-test** | LÆ°u trá»¯ dá»¯ liá»‡u cÃ³ cáº¥u trÃºc | Tá»§ há»“ sÆ¡ cÃ³ phÃ¢n loáº¡i |
| **chroma-test** | TÃ¬m kiáº¿m theo Ã½ nghÄ©a | ThÆ° viá»‡n thÃ´ng minh hiá»ƒu ngá»¯ cáº£nh |
| **redis-test** | Bá»™ nhá»› Ä‘á»‡m nhanh | BÃ n lÃ m viá»‡c Ä‘á»ƒ Ä‘á»“ hay dÃ¹ng |
| **adminer** | Giao diá»‡n quáº£n lÃ½ | Cá»­a sá»• Ä‘á»ƒ nhÃ¬n vÃ o tá»§ há»“ sÆ¡ |
| **db-setup** | Thá»£ setup (Ä‘Ã£ xong) | NgÆ°á»i sáº¯p xáº¿p tá»§ há»“ sÆ¡ ban Ä‘áº§u |

BÃ¢y giá» báº¡n Ä‘Ã£ hiá»ƒu rÃµ hÆ¡n vá» há»‡ thá»‘ng chÆ°a? CÃ³ muá»‘n tÃ´i giáº£i thÃ­ch thÃªm pháº§n nÃ o khÃ´ng?


---
Lá»—i nÃ y lÃ  do version conflict cá»§a `huggingface_hub`. TÃ´i sáº½ fix nhanh:

## ğŸ”§ **FIX NHANH - Cáº¬P NHáº¬T REQUIREMENTS**

Cáº­p nháº­t file `requirements/tool_requirements.txt`:

```txt
# requirements/tool_requirements.txt - VERSION TÆ¯Æ NG THÃCH
asyncpg==0.29.0
python-multipart==0.0.6
fastapi==0.104.1
uvicorn==0.24.0
streamlit==1.28.2
python-docx==1.1.0
PyPDF2==3.0.1
openpyxl==3.1.2
pandas==2.0.3

# FIXED VERSIONS - tÆ°Æ¡ng thÃ­ch vá»›i nhau
huggingface_hub==0.17.3
transformers==4.33.2
sentence-transformers==2.2.2
torch==2.0.1
tokenizers==0.13.3

# Other dependencies
chromadb==0.4.15
redis==5.0.1
pyvi==0.1.1
underthesea==6.7.0
numpy==1.24.3
scikit-learn==1.3.0
```

## ğŸ”„ **REBUILD CONTAINER**

```cmd
# Dá»«ng containers
docker-compose -f docker-compose.tool.yml down

# XÃ³a container cÅ© (force rebuild)
docker rmi chatbot-enhanced-db-document-tool

# Build láº¡i vá»›i dependencies má»›i
docker-compose -f docker-compose.tool.yml up --build -d

# Xem logs
docker logs -f chatbot-document-tool
```

## âš¡ **CÃCH NHANH HÆ N - DÃ™NG PRE-BUILT IMAGE**

Náº¿u váº«n lá»—i, sá»­a `docker/Dockerfile.tool` thÃ nh:

```dockerfile
# docker/Dockerfile.tool - VERSION ÄÆ N GIáº¢N
FROM python:3.9-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    postgresql-client \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Install dependencies tá»«ng bÆ°á»›c Ä‘á»ƒ trÃ¡nh conflict
RUN pip install --upgrade pip setuptools wheel

# Install core dependencies trÆ°á»›c
RUN pip install torch==2.0.1 --index-url https://download.pytorch.org/whl/cpu
RUN pip install transformers==4.33.2
RUN pip install sentence-transformers==2.2.2

# Install other dependencies  
RUN pip install \
    asyncpg==0.29.0 \
    streamlit==1.28.2 \
    python-docx==1.1.0 \
    PyPDF2==3.0.1 \
    pandas==2.0.3 \
    pyvi==0.1.1 \
    underthesea==6.7.0 \
    chromadb==0.4.15

# Copy code
COPY tools/ /app/tools/
RUN mkdir -p /app/logs

EXPOSE 8501
CMD ["streamlit", "run", "tools/document_ingestion/streamlit_app.py", "--server.port=8501", "--server.address=0.0.0.0"]
```

## ğŸ§ª **TEST BUILD**

```cmd
# Test build container
docker build -f docker/Dockerfile.tool -t test-tool .

# Náº¿u build OK, cháº¡y full stack
docker-compose -f docker-compose.tool.yml up --build
```

## ğŸ“Š **KIá»‚M TRA Káº¾T QUáº¢**

Sau khi cháº¡y xong:
- Streamlit tool: http://localhost:8501
- Database: http://localhost:8080

**Náº¿u váº«n lá»—i**, báº¡n copy toÃ n bá»™ error message cho tÃ´i. TÃ´i sáº½ táº¡o version Ä‘Æ¡n giáº£n hÆ¡n khÃ´ng dÃ¹ng sentence-transformers phá»©c táº¡p.

Thá»­ xem sao nhÃ©! ğŸš€
