Tuy·ªát v·ªùi! T√¥i s·∫Ω h∆∞·ªõng d·∫´n b·∫°n t·ª´ng b∆∞·ªõc ƒë·ªÉ test ki·∫øn tr√∫c database enhanced tr√™n Windows v·ªõi Docker.

## üöÄ **H∆Ø·ªöNG D·∫™N TRI·ªÇN KHAI T·ª™NG B∆Ø·ªöC TR√äN WINDOWS**

### **B∆∞·ªõc 1: T·∫°o c·∫•u tr√∫c th∆∞ m·ª•c**

M·ªü **Command Prompt** ho·∫∑c **PowerShell** v·ªõi quy·ªÅn Administrator v√† ch·∫°y:

```cmd
mkdir chatbot-enhanced-db
cd chatbot-enhanced-db

mkdir scripts
mkdir scripts\migrations
mkdir logs
mkdir test_results
mkdir data
mkdir data\sample_documents
mkdir config
mkdir config\grafana
mkdir config\grafana\dashboards
mkdir config\grafana\datasources
mkdir docker
mkdir tests

echo. > docker-compose.yml
```

### **B∆∞·ªõc 2: T·∫°o Docker Compose file**

T·∫°o file `docker-compose.yml` trong th∆∞ m·ª•c g·ªëc:

```yaml
# docker-compose.yml
version: '3.8'

services:
  # PostgreSQL with enhanced schema
  postgres-test:
    image: postgres:15-alpine
    container_name: chatbot-postgres-test
    environment:
      POSTGRES_DB: knowledge_base_test
      POSTGRES_USER: kb_admin
      POSTGRES_PASSWORD: test_password_123
      PGDATA: /var/lib/postgresql/data/pgdata
    volumes:
      - postgres_test_data:/var/lib/postgresql/data
      - ./scripts/migrations:/docker-entrypoint-initdb.d:ro
    ports:
      - "5433:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U kb_admin -d knowledge_base_test"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - chatbot-test-network

  # Redis for caching
  redis-test:
    image: redis:7-alpine
    container_name: chatbot-redis-test
    ports:
      - "6380:6379"
    volumes:
      - redis_test_data:/data
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5
    networks:
      - chatbot-test-network

  # ChromaDB for vector storage
  chromadb-test:
    image: chromadb/chroma:latest
    container_name: chatbot-chroma-test
    environment:
      CHROMA_SERVER_HOST: 0.0.0.0
      CHROMA_SERVER_HTTP_PORT: 8000
    volumes:
      - chromadb_test_data:/chroma/chroma
    ports:
      - "8001:8000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/heartbeat"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - chatbot-test-network

  # Database setup service
  db-setup:
    image: python:3.9-slim
    container_name: chatbot-db-setup
    environment:
      DB_HOST: postgres-test
      DB_PORT: 5432
      DB_NAME: knowledge_base_test
      DB_USER: kb_admin
      DB_PASSWORD: test_password_123
    volumes:
      - ./scripts:/app/scripts:ro
      - ./logs:/app/logs
    working_dir: /app
    depends_on:
      postgres-test:
        condition: service_healthy
      redis-test:
        condition: service_healthy
    command: >
      sh -c "
      pip install asyncpg psycopg2-binary &&
      python scripts/setup_database.py
      "
    networks:
      - chatbot-test-network

  # Monitoring dashboard
  adminer:
    image: adminer
    container_name: chatbot-adminer
    ports:
      - "8080:8080"
    environment:
      ADMINER_DEFAULT_SERVER: postgres-test
    depends_on:
      postgres-test:
        condition: service_healthy
    networks:
      - chatbot-test-network

volumes:
  postgres_test_data:
  redis_test_data:
  chromadb_test_data:

networks:
  chatbot-test-network:
    driver: bridge
```

### **B∆∞·ªõc 3: T·∫°o Migration Scripts**

T·∫°o file `scripts/migrations/01_init_database.sql`:

```sql
-- scripts/migrations/01_init_database.sql

-- Enable required extensions
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pg_trgm";

-- Create enhanced enum types
DO $$ BEGIN
    CREATE TYPE access_level_enum AS ENUM (
        'public', 'employee_only', 'manager_only', 'director_only', 'system_admin'
    );
EXCEPTION
    WHEN duplicate_object THEN null;
END $$;

DO $$ BEGIN
    CREATE TYPE document_type_enum AS ENUM (
        'policy', 'procedure', 'technical_guide', 'report', 
        'manual', 'specification', 'template', 'form', 
        'presentation', 'training_material', 'other'
    );
EXCEPTION
    WHEN duplicate_object THEN null;
END $$;

DO $$ BEGIN
    CREATE TYPE document_status_enum AS ENUM (
        'draft', 'review', 'approved', 'published', 'archived', 'deprecated'
    );
EXCEPTION
    WHEN duplicate_object THEN null;
END $$;

-- Enhanced documents metadata table
CREATE TABLE IF NOT EXISTS documents_metadata_v2 (
    document_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    
    -- Basic information
    title VARCHAR(500) NOT NULL,
    content TEXT,
    document_type document_type_enum NOT NULL,
    access_level access_level_enum NOT NULL DEFAULT 'employee_only',
    department_owner VARCHAR(100) NOT NULL,
    author VARCHAR(255) NOT NULL,
    status document_status_enum DEFAULT 'draft',
    
    -- Vietnamese language support
    language_detected VARCHAR(10) DEFAULT 'vi',
    vietnamese_segmented BOOLEAN DEFAULT false,
    diacritics_normalized BOOLEAN DEFAULT false,
    tone_marks_preserved BOOLEAN DEFAULT true,
    
    -- FlashRAG support
    flashrag_collection VARCHAR(100) DEFAULT 'default_collection',
    jsonl_export_ready BOOLEAN DEFAULT false,
    
    -- Search support
    search_tokens TSVECTOR,
    keyword_density JSONB,
    heading_structure JSONB,
    
    -- Metadata
    embedding_model_primary VARCHAR(100),
    chunk_count INTEGER DEFAULT 0,
    file_size_bytes BIGINT,
    
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Enhanced document chunks table
CREATE TABLE IF NOT EXISTS document_chunks_enhanced (
    chunk_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    document_id UUID REFERENCES documents_metadata_v2(document_id) ON DELETE CASCADE,
    
    -- Content data
    chunk_content TEXT NOT NULL,
    chunk_position INTEGER NOT NULL,
    chunk_size_tokens INTEGER,
    
    -- Semantic chunking metadata
    semantic_boundary BOOLEAN DEFAULT false,
    overlap_with_prev INTEGER DEFAULT 0,
    overlap_with_next INTEGER DEFAULT 0,
    heading_context TEXT,
    
    -- Quality and method
    chunk_method VARCHAR(20) DEFAULT 'semantic',
    chunk_quality_score DECIMAL(3,2) CHECK (chunk_quality_score BETWEEN 0.00 AND 1.00),
    
    -- Vector storage references
    embedding_model VARCHAR(100),
    embedding_dimensions INTEGER,
    
    -- BM25 support
    bm25_tokens TSVECTOR,
    
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- BM25 support table
CREATE TABLE IF NOT EXISTS document_bm25_index (
    bm25_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    document_id UUID REFERENCES documents_metadata_v2(document_id) ON DELETE CASCADE,
    chunk_id UUID REFERENCES document_chunks_enhanced(chunk_id) ON DELETE CASCADE,
    
    term VARCHAR(255) NOT NULL,
    term_frequency INTEGER NOT NULL,
    document_frequency INTEGER NOT NULL,
    bm25_score DECIMAL(8,4),
    
    language VARCHAR(10) DEFAULT 'vi',
    
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    UNIQUE(chunk_id, term, language)
);

-- Pipeline tracking table
CREATE TABLE IF NOT EXISTS rag_pipeline_sessions (
    session_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    
    -- Query information
    original_query TEXT NOT NULL,
    processed_query TEXT,
    query_language VARCHAR(10) DEFAULT 'vi',
    
    -- Pipeline metadata
    pipeline_type VARCHAR(50) NOT NULL DEFAULT 'standard',
    pipeline_method VARCHAR(50) NOT NULL DEFAULT 'hybrid',
    
    -- Performance metrics
    chunks_retrieved INTEGER,
    processing_time_ms INTEGER,
    response_quality_score DECIMAL(3,2),
    
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Vietnamese text analysis table
CREATE TABLE IF NOT EXISTS vietnamese_text_analysis (
    analysis_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    document_id UUID REFERENCES documents_metadata_v2(document_id) ON DELETE CASCADE,
    chunk_id UUID REFERENCES document_chunks_enhanced(chunk_id) ON DELETE CASCADE,
    
    original_text TEXT NOT NULL,
    processed_text TEXT,
    
    word_segmentation JSONB,
    pos_tagging JSONB,
    
    compound_words TEXT[],
    technical_terms TEXT[],
    proper_nouns TEXT[],
    
    readability_score DECIMAL(3,2),
    formality_level VARCHAR(20),
    
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Create performance indexes
CREATE INDEX IF NOT EXISTS idx_documents_v2_language ON documents_metadata_v2(language_detected);
CREATE INDEX IF NOT EXISTS idx_documents_v2_status ON documents_metadata_v2(status);
CREATE INDEX IF NOT EXISTS idx_documents_v2_collection ON documents_metadata_v2(flashrag_collection);
CREATE INDEX IF NOT EXISTS idx_documents_v2_search ON documents_metadata_v2 USING GIN(search_tokens);

CREATE INDEX IF NOT EXISTS idx_chunks_enhanced_document ON document_chunks_enhanced(document_id);
CREATE INDEX IF NOT EXISTS idx_chunks_enhanced_position ON document_chunks_enhanced(chunk_position);
CREATE INDEX IF NOT EXISTS idx_chunks_enhanced_semantic ON document_chunks_enhanced(semantic_boundary) WHERE semantic_boundary = true;

CREATE INDEX IF NOT EXISTS idx_bm25_term ON document_bm25_index(term);
CREATE INDEX IF NOT EXISTS idx_bm25_chunk ON document_bm25_index(chunk_id);
CREATE INDEX IF NOT EXISTS idx_bm25_score ON document_bm25_index(bm25_score DESC);

CREATE INDEX IF NOT EXISTS idx_pipeline_sessions_created ON rag_pipeline_sessions(created_at DESC);
CREATE INDEX IF NOT EXISTS idx_pipeline_sessions_type ON rag_pipeline_sessions(pipeline_type, pipeline_method);

-- Insert sample data
INSERT INTO documents_metadata_v2 (
    title, content, document_type, access_level, department_owner, author, status, jsonl_export_ready
) VALUES 
(
    'Quy tr√¨nh xin ngh·ªâ ph√©p',
    'Quy tr√¨nh xin ngh·ªâ ph√©p t·∫°i c√¥ng ty bao g·ªìm c√°c b∆∞·ªõc sau: 1. Nh√¢n vi√™n ƒëi·ªÅn ƒë∆°n xin ngh·ªâ ph√©p 2. G·ª≠i ƒë∆°n cho qu·∫£n l√Ω tr·ª±c ti·∫øp 3. Qu·∫£n l√Ω ph√™ duy·ªát trong v√≤ng 2 ng√†y l√†m vi·ªác 4. HR c·∫≠p nh·∫≠t v√†o h·ªá th·ªëng 5. Th√¥ng b√°o k·∫øt qu·∫£ cho nh√¢n vi√™n',
    'procedure',
    'employee_only',
    'HR',
    'HR Department',
    'approved',
    true
),
(
    'Ch√≠nh s√°ch l√†m vi·ªác t·ª´ xa',
    'Ch√≠nh s√°ch l√†m vi·ªác t·ª´ xa (Work From Home) ƒë∆∞·ª£c √°p d·ª•ng nh∆∞ sau: - Nh√¢n vi√™n c√≥ th·ªÉ l√†m vi·ªác t·ª´ xa t·ªëi ƒëa 3 ng√†y/tu·∫ßn - C·∫ßn ƒëƒÉng k√Ω tr∆∞·ªõc √≠t nh·∫•t 1 ng√†y - ƒê·∫£m b·∫£o m√¥i tr∆∞·ªùng l√†m vi·ªác ·ªïn ƒë·ªãnh - Tham gia ƒë·∫ßy ƒë·ªß c√°c cu·ªôc h·ªçp online - B√°o c√°o ti·∫øn ƒë·ªô c√¥ng vi·ªác h√†ng ng√†y',
    'policy',
    'employee_only',
    'HR',
    'Management Team',
    'approved',
    true
),
(
    'H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng h·ªá th·ªëng ERP',
    'H∆∞·ªõng d·∫´n chi ti·∫øt s·ª≠ d·ª•ng h·ªá th·ªëng ERP c√¥ng ty: 1. ƒêƒÉng nh·∫≠p h·ªá th·ªëng - S·ª≠ d·ª•ng t√†i kho·∫£n company email - M·∫≠t kh·∫©u ƒë∆∞·ª£c c·∫•p ban ƒë·∫ßu c·∫ßn ƒë·ªïi ngay l·∫ßn ƒë·∫ßu ƒëƒÉng nh·∫≠p 2. Module qu·∫£n l√Ω nh√¢n s·ª± - C·∫≠p nh·∫≠t th√¥ng tin c√° nh√¢n - ƒêƒÉng k√Ω ngh·ªâ ph√©p - Xem b·∫£ng l∆∞∆°ng 3. Module qu·∫£n l√Ω d·ª± √°n - T·∫°o task m·ªõi - C·∫≠p nh·∫≠t ti·∫øn ƒë·ªô - B√°o c√°o h√†ng tu·∫ßn',
    'technical_guide',
    'employee_only',
    'IT',
    'IT Support Team',
    'approved',
    true
)
ON CONFLICT DO NOTHING;

-- Update search tokens for sample documents
UPDATE documents_metadata_v2 
SET search_tokens = to_tsvector('simple', title || ' ' || COALESCE(content, ''))
WHERE search_tokens IS NULL;

-- Success message
DO $$ BEGIN
    RAISE NOTICE 'Enhanced Database Architecture initialized successfully!';
    RAISE NOTICE 'Sample Vietnamese documents loaded.';
    RAISE NOTICE 'Database is ready for testing.';
END $$;
```

### **B∆∞·ªõc 4: T·∫°o Database Setup Script**

T·∫°o file `scripts/setup_database.py`:

```python
# scripts/setup_database.py
import asyncio
import asyncpg
import logging
import time
import os
import sys

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

async def setup_enhanced_database():
    """Setup and verify enhanced database architecture"""
    
    db_config = {
        'host': os.getenv('DB_HOST', 'localhost'),
        'port': int(os.getenv('DB_PORT', 5432)),
        'database': os.getenv('DB_NAME', 'knowledge_base_test'),
        'user': os.getenv('DB_USER', 'kb_admin'),
        'password': os.getenv('DB_PASSWORD', 'test_password_123')
    }
    
    logger.info("üöÄ Starting Enhanced Database Setup")
    
    # Wait for database to be ready
    max_retries = 30
    for attempt in range(max_retries):
        try:
            conn = await asyncpg.connect(**db_config)
            await conn.execute('SELECT 1')
            await conn.close()
            logger.info("‚úÖ Database connection successful!")
            break
        except Exception as e:
            logger.info(f"‚è≥ Waiting for database... (attempt {attempt + 1}/{max_retries})")
            if attempt == max_retries - 1:
                logger.error("‚ùå Database connection failed after maximum retries")
                return False
            await asyncio.sleep(2)
    
    # Connect to database
    try:
        conn = await asyncpg.connect(**db_config)
        logger.info("üîó Connected to database")
        
        # Verify table creation
        tables = await conn.fetch("""
            SELECT table_name FROM information_schema.tables 
            WHERE table_schema = 'public' 
            ORDER BY table_name
        """)
        
        logger.info(f"üìä Database created with {len(tables)} tables:")
        for table in tables:
            logger.info(f"  ‚úÖ {table['table_name']}")
        
        # Verify sample data
        doc_count = await conn.fetchval("SELECT COUNT(*) FROM documents_metadata_v2")
        logger.info(f"üìÑ Sample documents loaded: {doc_count}")
        
        if doc_count > 0:
            # Show sample documents
            docs = await conn.fetch("SELECT title, author, status FROM documents_metadata_v2 LIMIT 3")
            logger.info("üìã Sample documents:")
            for doc in docs:
                logger.info(f"  üìÑ '{doc['title']}' by {doc['author']} ({doc['status']})")
        
        # Test basic queries
        logger.info("üîç Testing basic queries...")
        
        # Test Vietnamese search
        vn_docs = await conn.fetchval("""
            SELECT COUNT(*) FROM documents_metadata_v2 
            WHERE language_detected = 'vi'
        """)
        logger.info(f"  üáªüá≥ Vietnamese documents: {vn_docs}")
        
        # Test full-text search capability
        search_ready = await conn.fetchval("""
            SELECT COUNT(*) FROM documents_metadata_v2 
            WHERE search_tokens IS NOT NULL
        """)
        logger.info(f"  üîç Documents with search tokens: {search_ready}")
        
        # Test enum types
        enum_test = await conn.fetchval("""
            SELECT COUNT(DISTINCT document_type) FROM documents_metadata_v2
        """)
        logger.info(f"  üìù Document types in use: {enum_test}")
        
        # Create a sample pipeline session for testing
        session_id = await conn.fetchval("""
            INSERT INTO rag_pipeline_sessions (
                original_query, processed_query, pipeline_type, pipeline_method,
                chunks_retrieved, processing_time_ms, response_quality_score
            ) VALUES (
                'Quy tr√¨nh xin ngh·ªâ ph√©p nh∆∞ th·∫ø n√†o?',
                'quy tr√¨nh xin ngh·ªâ ph√©p',
                'standard',
                'hybrid',
                3,
                150,
                0.85
            ) RETURNING session_id
        """)
        
        logger.info(f"  ‚úÖ Sample pipeline session created: {session_id}")
        
        # Generate database statistics
        db_size = await conn.fetchval("SELECT pg_size_pretty(pg_database_size(current_database()))")
        logger.info(f"üíæ Database size: {db_size}")
        
        # Create comprehensive test report
        report = f"""
# Enhanced Database Architecture Test Report
Generated: {time.strftime('%Y-%m-%d %H:%M:%S')}

## Database Information
- **Host**: {db_config['host']}:{db_config['port']}
- **Database**: {db_config['database']}
- **Size**: {db_size}

## Tables Created: {len(tables)}
{chr(10).join(f'- {table["table_name"]}' for table in tables)}

## Sample Data
- **Documents**: {doc_count}
- **Vietnamese Documents**: {vn_docs}
- **Search-Ready Documents**: {search_ready}
- **Document Types**: {enum_test}

## Features Tested
‚úÖ Enhanced schema with Vietnamese support
‚úÖ Multi-type enum support
‚úÖ Full-text search capability
‚úÖ Pipeline session tracking
‚úÖ BM25 index structure
‚úÖ FlashRAG compatibility structure

## Connection Information
- **PostgreSQL**: localhost:5433
- **Redis**: localhost:6380
- **ChromaDB**: localhost:8001
- **Adminer**: http://localhost:8080

## Next Steps
1. Access Adminer at http://localhost:8080 to browse database
2. Connect using: Server: postgres-test, Username: kb_admin, Password: test_password_123
3. Test queries using the sample Vietnamese documents
4. Explore enhanced schema features

## Test Status: ‚úÖ SUCCESS
All components initialized and tested successfully!
        """
        
        # Write report to log file
        with open('/app/logs/setup_report.md', 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info("üìÑ Setup report saved to: /app/logs/setup_report.md")
        print(report)
        
        await conn.close()
        logger.info("üéâ Enhanced Database Architecture setup completed successfully!")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Database setup failed: {e}")
        return False

if __name__ == "__main__":
    success = asyncio.run(setup_enhanced_database())
    sys.exit(0 if success else 1)
```

### **B∆∞·ªõc 5: Ch·∫°y Docker Test**

M·ªü PowerShell trong th∆∞ m·ª•c `chatbot-enhanced-db` v√† ch·∫°y:

```powershell
# Kh·ªüi ƒë·ªông c√°c services
docker-compose up -d

# Ki·ªÉm tra tr·∫°ng th√°i
docker-compose ps

# Xem logs c·ªßa database setup
docker logs chatbot-db-setup

# Xem logs c·ªßa PostgreSQL
docker logs chatbot-postgres-test
```

### **B∆∞·ªõc 6: Ki·ªÉm tra k·∫øt qu·∫£**

1. **M·ªü Adminer (Database Browser)**:
   - Truy c·∫≠p: http://localhost:8080
   - Server: `postgres-test`
   - Username: `kb_admin`
   - Password: `test_password_123`
   - Database: `knowledge_base_test`

2. **Ki·ªÉm tra ChromaDB**:
   ```powershell
   # Test ChromaDB API
   curl http://localhost:8001/api/v1/heartbeat
   curl.exe http://localhost:8001/api/v2/heartbeat
   ```

3. **Ki·ªÉm tra Redis**:
   ```powershell
   # Connect to Redis container
   docker exec -it chatbot-redis-test redis-cli ping
   ```

### **B∆∞·ªõc 7: Test Database v·ªõi PowerShell**

T·∫°o file `test_database.ps1`:

```powershell
# test_database.ps1

Write-Host "üîç Testing Enhanced Database Architecture" -ForegroundColor Green

# Test PostgreSQL connection
Write-Host "`nüìä Testing PostgreSQL..." -ForegroundColor Yellow
try {
    $response = Invoke-RestMethod -Uri "http://localhost:8080" -Method Get
    Write-Host "‚úÖ Adminer accessible at http://localhost:8080" -ForegroundColor Green
} catch {
    Write-Host "‚ùå Adminer not accessible" -ForegroundColor Red
}

# Test ChromaDB
Write-Host "`nüî¢ Testing ChromaDB..." -ForegroundColor Yellow
try {
    $response = Invoke-RestMethod -Uri "http://localhost:8001/api/v1/heartbeat" -Method Get
    Write-Host "‚úÖ ChromaDB accessible: $($response.nanosecond_heartbeat)" -ForegroundColor Green
} catch {
    Write-Host "‚ùå ChromaDB not accessible" -ForegroundColor Red
}

# Test Redis
Write-Host "`nüî¥ Testing Redis..." -ForegroundColor Yellow
try {
    $redisTest = docker exec chatbot-redis-test redis-cli ping
    if ($redisTest -eq "PONG") {
        Write-Host "‚úÖ Redis accessible: $redisTest" -ForegroundColor Green
    }
} catch {
    Write-Host "‚ùå Redis not accessible" -ForegroundColor Red
}

# Check logs
Write-Host "`nüìã Recent setup logs:" -ForegroundColor Yellow
docker logs --tail 10 chatbot-db-setup

Write-Host "`nüéâ Database test completed!" -ForegroundColor Green
Write-Host "Access Adminer: http://localhost:8080" -ForegroundColor Cyan
Write-Host "ChromaDB API: http://localhost:8001" -ForegroundColor Cyan

# Show connection info
Write-Host "`nüìù Connection Information:" -ForegroundColor Yellow
Write-Host "PostgreSQL: localhost:5433" -ForegroundColor White
Write-Host "Redis: localhost:6380" -ForegroundColor White
Write-Host "ChromaDB: localhost:8001" -ForegroundColor White
Write-Host "Adminer: http://localhost:8080" -ForegroundColor White
```

Ch·∫°y test:
```powershell
powershell -ExecutionPolicy Bypass -File test_database.ps1
```

### **B∆∞·ªõc 8: Cleanup khi ho√†n th√†nh**

```powershell
# D·ª´ng v√† x√≥a containers
docker-compose down

# X√≥a volumes (n·∫øu mu·ªën x√≥a h·∫øt data)
docker-compose down -v

# X√≥a images (n·∫øu mu·ªën cleanup ho√†n to√†n)
docker system prune -a
```

## üìä **K·∫øt qu·∫£ mong ƒë·ª£i**

Sau khi ch·∫°y th√†nh c√¥ng, b·∫°n s·∫Ω c√≥:

1. **‚úÖ PostgreSQL** v·ªõi enhanced schema running tr√™n port 5433
2. **‚úÖ Redis** cache running tr√™n port 6380  
3. **‚úÖ ChromaDB** vector database tr√™n port 8001
4. **‚úÖ Adminer** database browser t·∫°i http://localhost:8080
5. **‚úÖ Sample Vietnamese documents** ƒë√£ ƒë∆∞·ª£c load
6. **‚úÖ Test report** trong `logs/setup_report.md`

**T√≠nh nƒÉng ƒë√£ test:**
- üáªüá≥ Vietnamese language support
- üîç Hybrid search structure (BM25 + Vector)
- üìä Enhanced metadata schema
- ‚ö° Performance indexes
- üìÑ FlashRAG compatibility
- üîÑ Pipeline tracking

B·∫°n c√≥ g·∫∑p l·ªói g√¨ trong qu√° tr√¨nh setup kh√¥ng? T√¥i s·∫Ω gi√∫p troubleshoot!
---

D·ª±a tr√™n ƒëo·∫°n m√£ b·∫°n cung c·∫•p, c√°c file `docker-compose.yml` v√† `setup_database.py` **kh√¥ng c√≥ v·∫•n ƒë·ªÅ nghi√™m tr·ªçng** v√† ƒë∆∞·ª£c vi·∫øt kh√° t·ªët ƒë·ªÉ th·ª±c hi·ªán m·ª•c ƒë√≠ch ƒë√£ ƒë·ªÅ ra. Tuy nhi√™n, c√≥ m·ªôt v√†i ƒëi·ªÉm nh·ªè c·∫ßn l∆∞u √Ω ƒë·ªÉ t·ªëi ∆∞u v√† ph√≤ng tr√°nh l·ªói trong c√°c k·ªãch b·∫£n kh√°c nhau.

---

Ch√†o b·∫°n, sau khi ch·∫°y t·∫•t c·∫£ c√°c l·ªánh trong h∆∞·ªõng d·∫´n, c∆° s·ªü d·ªØ li·ªáu PostgreSQL c·ªßa b·∫°n (t√™n l√† `knowledge_base_test`) s·∫Ω c√≥ **5 b·∫£ng ch√≠nh**.

C√°c b·∫£ng n√†y ƒë∆∞·ª£c t·∫°o ra b·ªüi file script `scripts/migrations/01_init_database.sql`. D∆∞·ªõi ƒë√¢y l√† danh s√°ch v√† ch·ª©c nƒÉng c·ªßa t·ª´ng b·∫£ng:

---

### ## 1. `documents_metadata_v2`
ƒê√¢y l√† b·∫£ng quan tr·ªçng nh·∫•t, d√πng ƒë·ªÉ l∆∞u tr·ªØ th√¥ng tin t·ªïng quan (si√™u d·ªØ li·ªáu) v·ªÅ m·ªói t√†i li·ªáu g·ªëc.
* **Ch·ª©c nƒÉng ch√≠nh**: Qu·∫£n l√Ω c√°c t√†i li·ªáu nh∆∞ ch√≠nh s√°ch, quy tr√¨nh, h∆∞·ªõng d·∫´n...
* **C√°c c·ªôt ƒë√°ng ch√∫ √Ω**:
    * `document_id`: Kh√≥a ch√≠nh ƒë·ªãnh danh cho m·ªói t√†i li·ªáu.
    * `title`, `content`: Ti√™u ƒë·ªÅ v√† n·ªôi dung ch√≠nh c·ªßa t√†i li·ªáu.
    * `document_type`, `access_level`, `status`: C√°c tr∆∞·ªùng d√πng ki·ªÉu d·ªØ li·ªáu `ENUM` ƒë·ªÉ ph√¢n lo·∫°i t√†i li·ªáu (v√≠ d·ª•: 'policy', 'procedure'), quy ƒë·ªãnh quy·ªÅn truy c·∫≠p v√† tr·∫°ng th√°i ('draft', 'approved').
    * `search_tokens`: Tr∆∞·ªùng `TSVECTOR` ƒë·ªÉ h·ªó tr·ª£ t√¨m ki·∫øm to√†n vƒÉn (full-text search).

---

### ## 2. `document_chunks_enhanced`
B·∫£ng n√†y ch·ª©a c√°c "m·∫©u" ho·∫∑c "ƒëo·∫°n" vƒÉn b·∫£n (chunks) ƒë∆∞·ª£c chia nh·ªè ra t·ª´ t√†i li·ªáu g·ªëc trong b·∫£ng `documents_metadata_v2`.
* **Ch·ª©c nƒÉng ch√≠nh**: Chia nh·ªè t√†i li·ªáu ƒë·ªÉ d·ªÖ d√†ng x·ª≠ l√Ω, nh√∫ng (embedding) v√† t√¨m ki·∫øm ng·ªØ nghƒ©a (semantic search).
* **C√°c c·ªôt ƒë√°ng ch√∫ √Ω**:
    * `chunk_id`: Kh√≥a ch√≠nh cho m·ªói ƒëo·∫°n vƒÉn b·∫£n.
    * `document_id`: Kh√≥a ngo·∫°i, li√™n k·∫øt ƒëo·∫°n vƒÉn b·∫£n n√†y v·ªõi t√†i li·ªáu g·ªëc c·ªßa n√≥.
    * `chunk_content`: N·ªôi dung c·ªßa ƒëo·∫°n vƒÉn b·∫£n.
    * `chunk_position`: V·ªã tr√≠ c·ªßa ƒëo·∫°n vƒÉn b·∫£n trong t√†i li·ªáu g·ªëc.

---

### ## 3. `document_bm25_index`
B·∫£ng n√†y h·ªó tr·ª£ cho vi·ªác t√¨m ki·∫øm t·ª´ kh√≥a truy·ªÅn th·ªëng b·∫±ng thu·∫≠t to√°n BM25.
* **Ch·ª©c nƒÉng ch√≠nh**: L∆∞u tr·ªØ ch·ªâ m·ª•c (index) c√°c t·ª´ kh√≥a ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô v√† ƒë·ªô ch√≠nh x√°c c·ªßa t√¨m ki·∫øm lai (hybrid search), k·∫øt h·ª£p c·∫£ t√¨m ki·∫øm t·ª´ kh√≥a v√† t√¨m ki·∫øm ng·ªØ nghƒ©a.
* **C√°c c·ªôt ƒë√°ng ch√∫ √Ω**:
    * `chunk_id`: Li√™n k·∫øt ƒë·∫øn ƒëo·∫°n vƒÉn b·∫£n ch·ª©a t·ª´ kh√≥a.
    * `term`: T·ª´ kh√≥a ƒë∆∞·ª£c t√¨m th·∫•y.
    * `term_frequency`: T·∫ßn su·∫•t xu·∫•t hi·ªán c·ªßa t·ª´ kh√≥a.

---

### ## 4. `rag_pipeline_sessions`
B·∫£ng n√†y d√πng ƒë·ªÉ theo d√µi v√† ghi l·∫°i l·ªãch s·ª≠ c√°c phi√™n truy v·∫•n c·ªßa ng∆∞·ªùi d√πng ho·∫∑c h·ªá th·ªëng.
* **Ch·ª©c nƒÉng ch√≠nh**: Gi√°m s√°t hi·ªáu su·∫•t c·ªßa h·ªá th·ªëng RAG (Retrieval-Augmented Generation), ph√¢n t√≠ch c√°c c√¢u h·ªèi v√† ch·∫•t l∆∞·ª£ng c√¢u tr·∫£ l·ªùi.
* **C√°c c·ªôt ƒë√°ng ch√∫ √Ω**:
    * `session_id`: ƒê·ªãnh danh cho m·ªói phi√™n l√†m vi·ªác.
    * `original_query`: C√¢u h·ªèi g·ªëc c·ªßa ng∆∞·ªùi d√πng.
    * `processing_time_ms`: Th·ªùi gian x·ª≠ l√Ω ƒë·ªÉ ƒë∆∞a ra c√¢u tr·∫£ l·ªùi.
    * `response_quality_score`: ƒêi·ªÉm ch·∫•t l∆∞·ª£ng c·ªßa c√¢u tr·∫£ l·ªùi.

---

### ## 5. `vietnamese_text_analysis`
B·∫£ng n√†y ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·∫∑c bi·ªát ƒë·ªÉ l∆∞u k·∫øt qu·∫£ ph√¢n t√≠ch s√¢u v·ªÅ vƒÉn b·∫£n ti·∫øng Vi·ªát.
* **Ch·ª©c nƒÉng ch√≠nh**: H·ªó tr·ª£ x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n (NLP) cho ti·∫øng Vi·ªát, nh∆∞ t√°ch t·ª´, g·∫Øn th·∫ª t·ª´ lo·∫°i, nh·∫≠n di·ªán danh t·ª´ ri√™ng...
* **C√°c c·ªôt ƒë√°ng ch√∫ √Ω**:
    * `document_id`, `chunk_id`: Li√™n k·∫øt ƒë·∫øn t√†i li·ªáu/ƒëo·∫°n vƒÉn b·∫£n ƒë∆∞·ª£c ph√¢n t√≠ch.
    * `word_segmentation`, `pos_tagging`: L∆∞u k·∫øt qu·∫£ t√°ch t·ª´ v√† g·∫Øn th·∫ª t·ª´ lo·∫°i d∆∞·ªõi d·∫°ng `JSONB`.
    * `technical_terms`, `proper_nouns`: L∆∞u c√°c m·∫£ng (array) ch·ª©a thu·∫≠t ng·ªØ k·ªπ thu·∫≠t ho·∫∑c danh t·ª´ ri√™ng.

T√≥m l·∫°i, 5 b·∫£ng n√†y c√πng nhau t·∫°o n√™n m·ªôt ki·∫øn tr√∫c c∆° s·ªü d·ªØ li·ªáu ho√†n ch·ªânh ƒë·ªÉ x√¢y d·ª±ng v√† qu·∫£n l√Ω m·ªôt h·ªá th·ªëng h·ªèi ƒë√°p th√¥ng minh (chatbot/RAG) c√≥ h·ªó tr·ª£ chuy√™n s√¢u cho ti·∫øng Vi·ªát.
---
### ## üìÇ File `docker-compose.yml`

Nh√¨n chung, file n√†y ƒë∆∞·ª£c c·∫•u h√¨nh t·ªët, r√µ r√†ng v√† tu√¢n th·ªß c√°c th·ª±c ti·ªÖn t·ªët nh·∫•t.

**‚úÖ ∆Øu ƒëi·ªÉm:**
* **T√°ch bi·ªát d·ªãch v·ª•:** C√°c d·ªãch v·ª• `postgres`, `redis`, `chromadb` ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a r√µ r√†ng, d·ªÖ qu·∫£n l√Ω.
* **S·ª≠ d·ª•ng `healthcheck`:** ƒê√¢y l√† m·ªôt ƒëi·ªÉm r·∫•t t·ªët, gi√∫p ƒë·∫£m b·∫£o c√°c d·ªãch v·ª• ph·ª• thu·ªôc (nh∆∞ `db-setup` v√† `adminer`) ch·ªâ kh·ªüi ƒë·ªông khi d·ªãch v·ª• ch√≠nh (nh∆∞ `postgres-test`) ƒë√£ th·ª±c s·ª± s·∫µn s√†ng, gi·∫£m thi·ªÉu l·ªói k·∫øt n·ªëi.
* **Qu·∫£n l√Ω volumes v√† networks:** S·ª≠ d·ª•ng volumes ƒë∆∞·ª£c ƒë·∫∑t t√™n (`postgres_test_data`) v√† m·ªôt m·∫°ng chung (`chatbot-test-network`) l√† c√°ch l√†m chu·∫©n m·ª±c, gi√∫p d·ªØ li·ªáu b·ªÅn v·ªØng v√† c√°c container giao ti·∫øp an to√†n.
* **G·∫Øn script migrations:** Vi·ªác mount th∆∞ m·ª•c `./scripts/migrations` v√†o `/docker-entrypoint-initdb.d` l√† c√°ch ch√≠nh x√°c ƒë·ªÉ t·ª± ƒë·ªông ch·∫°y c√°c file `.sql` khi PostgreSQL kh·ªüi t·∫°o l·∫ßn ƒë·∫ßu.

**‚ö†Ô∏è ƒêi·ªÉm c·∫ßn l∆∞u √Ω:**
1.  **M·∫≠t kh·∫©u trong file:** M·∫≠t kh·∫©u `test_password_123` ƒë∆∞·ª£c ghi tr·ª±c ti·∫øp trong file. ƒêi·ªÅu n√†y ch·∫•p nh·∫≠n ƒë∆∞·ª£c cho m√¥i tr∆∞·ªùng test, nh∆∞ng trong m√¥i tr∆∞·ªùng th·ª±c t·∫ø (production), b·∫°n n√™n s·ª≠ d·ª•ng Docker secrets ho·∫∑c file bi·∫øn m√¥i tr∆∞·ªùng (`.env`) ƒë·ªÉ qu·∫£n l√Ω th√¥ng tin nh·∫°y c·∫£m.
2.  **Phi√™n b·∫£n `latest`:** Image `chromadb/chroma:latest` c√≥ th·ªÉ g√¢y ra l·ªói kh√¥ng mong mu·ªën trong t∆∞∆°ng lai n·∫øu c√≥ m·ªôt b·∫£n c·∫≠p nh·∫≠t l·ªõn kh√¥ng t∆∞∆°ng th√≠ch. ƒê·ªÉ ƒë·∫£m b·∫£o t√≠nh ·ªïn ƒë·ªãnh, b·∫°n n√™n ghim v√†o m·ªôt phi√™n b·∫£n c·ª• th·ªÉ (v√≠ d·ª•: `chromadb/chroma:0.4.24`).

---

### ## üêç File `scripts/setup_database.py`

ƒê√¢y l√† m·ªôt script Python ch·∫Øc ch·∫Øn, s·ª≠ d·ª•ng `asyncio` v√† `asyncpg` hi·ªáu qu·∫£ ƒë·ªÉ ki·ªÉm tra v√† x√°c th·ª±c c∆° s·ªü d·ªØ li·ªáu.

**‚úÖ ∆Øu ƒëi·ªÉm:**
* **C∆° ch·∫ø ch·ªù (Wait Mechanism):** V√≤ng l·∫∑p `for attempt in range(max_retries)` ƒë·ªÉ ch·ªù database s·∫µn s√†ng l√† r·∫•t quan tr·ªçng v√† ƒë∆∞·ª£c tri·ªÉn khai ƒë√∫ng c√°ch. N√≥ gi√∫p script kh√¥ng b·ªã l·ªói ngay l·∫≠p t·ª©c n·∫øu container `db-setup` kh·ªüi ƒë·ªông nhanh h∆°n `postgres-test`.
* **X√°c th·ª±c to√†n di·ªán:** Script kh√¥ng ch·ªâ k·∫øt n·ªëi m√† c√≤n ki·ªÉm tra s·ª± t·ªìn t·∫°i c·ªßa b·∫£ng, s·ªë l∆∞·ª£ng d·ªØ li·ªáu m·∫´u v√† c√°c t√≠nh nƒÉng nh∆∞ full-text search, ƒë·∫£m b·∫£o database ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o ƒë√∫ng nh∆∞ mong ƒë·ª£i.
* **B√°o c√°o chi ti·∫øt:** Vi·ªác t·∫°o ra m·ªôt file b√°o c√°o (`setup_report.md`) r·∫•t h·ªØu √≠ch cho vi·ªác g·ª° l·ªói v√† ki·ªÉm tra nhanh tr·∫°ng th√°i h·ªá th·ªëng.
* **S·ª≠ d·ª•ng bi·∫øn m√¥i tr∆∞·ªùng:** L·∫•y th√¥ng tin c·∫•u h√¨nh t·ª´ bi·∫øn m√¥i tr∆∞·ªùng (`os.getenv`) l√† m·ªôt th·ª±c ti·ªÖn t·ªët, gi√∫p script linh ho·∫°t v√† kh√¥ng c·∫ßn s·ª≠a code khi thay ƒë·ªïi c·∫•u h√¨nh.

**‚ö†Ô∏è ƒêi·ªÉm c·∫ßn l∆∞u √Ω:**
1.  **Thi·∫øu th∆∞ vi·ªán trong image:** Service `db-setup` s·ª≠ d·ª•ng image `python:3.9-slim` v√† sau ƒë√≥ c√†i ƒë·∫∑t th∆∞ vi·ªán b·∫±ng `pip install`. ƒêi·ªÅu n√†y ho·∫°t ƒë·ªông t·ªët nh∆∞ng m·ªói l·∫ßn container ch·∫°y l·∫°i, n√≥ s·∫Ω th·ª±c hi·ªán c√†i ƒë·∫∑t l·∫°i. ƒê·ªÉ t·ªëi ∆∞u, b·∫°n c√≥ th·ªÉ t·∫°o m·ªôt `Dockerfile` ri√™ng cho `db-setup`, c√†i s·∫µn c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt, v√† build th√†nh m·ªôt image t√πy ch·ªânh. ƒêi·ªÅu n√†y gi√∫p container kh·ªüi ƒë·ªông nhanh h∆°n v√† ƒë·∫£m b·∫£o m√¥i tr∆∞·ªùng nh·∫•t qu√°n.
2.  **Tho√°t v·ªõi m√£ l·ªói:** D√≤ng `sys.exit(0 if success else 1)` l√† m·ªôt c√°ch l√†m ƒë√∫ng ƒë·∫Øn, gi√∫p Docker Compose bi·∫øt ƒë∆∞·ª£c li·ªáu service `db-setup` ƒë√£ ho√†n th√†nh th√†nh c√¥ng hay th·∫•t b·∫°i.

### ## üéØ K·∫øt lu·∫≠n

C·∫£ hai file ƒë·ªÅu **an to√†n ƒë·ªÉ ch·∫°y** cho m·ª•c ƒë√≠ch th·ª≠ nghi·ªám v√† h·ªçc t·∫≠p. C√°c "v·∫•n ƒë·ªÅ" ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p ·ªü tr√™n ch·ªâ l√† nh·ªØng g·ª£i √Ω ƒë·ªÉ t·ªëi ∆∞u v√† tƒÉng c∆∞·ªùng b·∫£o m·∫≠t khi b·∫°n chuy·ªÉn sang m·ªôt m√¥i tr∆∞·ªùng ph·ª©c t·∫°p h∆°n ho·∫∑c tri·ªÉn khai th·ª±c t·∫ø. V·ªõi h∆∞·ªõng d·∫´n hi·ªán t·∫°i, b·∫°n c√≥ th·ªÉ t·ª± tin ch·∫°y c√°c l·ªánh m√† kh√¥ng g·∫∑p l·ªói do c·∫•u h√¨nh sai.

---
Tuy·ªát v·ªùi! T√¥i s·∫Ω gi·∫£i th√≠ch chi ti·∫øt t·ª´ng container ƒëang ch·∫°y v√† vai tr√≤ c·ªßa ch√∫ng trong h·ªá th·ªëng. H√£y c√πng kh√°m ph√° t·ª´ng module m·ªôt c√°ch d·ªÖ hi·ªÉu:

## üê≥ **GI·∫¢I TH√çCH CHI TI·∫æT C√ÅC DOCKER CONTAINERS**

### **1. üêò chatbot-postgres-test - "B·ªô n√£o ch√≠nh c·ªßa h·ªá th·ªëng"**

**ƒê√¢y l√† g√¨?**
- PostgreSQL l√† h·ªá qu·∫£n tr·ªã c∆° s·ªü d·ªØ li·ªáu quan h·ªá (nh∆∞ Excel si√™u m·∫°nh)
- L∆∞u tr·ªØ t·∫•t c·∫£ th√¥ng tin c√≥ c·∫•u tr√∫c: metadata t√†i li·ªáu, th√¥ng tin ng∆∞·ªùi d√πng, logs

**ƒêang l√†m g√¨?**
```sql
-- L∆∞u th√¥ng tin t√†i li·ªáu
documents_metadata_v2: T√™n file, t√°c gi·∫£, ng√†y t·∫°o, lo·∫°i t√†i li·ªáu
document_chunks_enhanced: C√°c ƒëo·∫°n vƒÉn b·∫£n ƒë√£ ƒë∆∞·ª£c c·∫Øt nh·ªè
rag_pipeline_sessions: L·ªãch s·ª≠ c√°c c√¢u h·ªèi v√† tr·∫£ l·ªùi

-- V√≠ d·ª• data th·ª±c t·∫ø:
Title: "Quy tr√¨nh xin ngh·ªâ ph√©p"
Author: "HR Department" 
Content: "B∆∞·ªõc 1: ƒêi·ªÅn ƒë∆°n..."
Status: "approved"
```

**Ki·ªÉm tra PostgreSQL:**
```powershell
# V√†o container PostgreSQL
docker exec -it chatbot-postgres-test psql -U kb_admin -d knowledge_base_test

# Xem c√°c b·∫£ng ƒë√£ t·∫°o
\dt

# Xem d·ªØ li·ªáu m·∫´u
SELECT title, author, status FROM documents_metadata_v2;

# Tho√°t
\q
```

### **2. üî¥ chatbot-redis-test - "B·ªô nh·ªõ ƒë·ªám t·ªëc ƒë·ªô cao"**

**ƒê√¢y l√† g√¨?**
- Redis nh∆∞ "RAM m·ªü r·ªông" - l∆∞u t·∫°m th√¥ng tin hay d√πng
- Gi√∫p h·ªá th·ªëng ph·∫£n h·ªìi nhanh h∆°n (thay v√¨ query database m·ªói l·∫ßn)

**ƒêang l√†m g√¨?**
```redis
# L∆∞u cache c√°c k·∫øt qu·∫£ t√¨m ki·∫øm
user:123:last_query = "Quy tr√¨nh ngh·ªâ ph√©p"
embedding:doc_456 = [0.1, 0.8, 0.3, ...] # Vector embeddings

# Session ng∆∞·ªùi d√πng
session:abc123 = {user_id: 456, login_time: "2024-01-01"}
```

**Ki·ªÉm tra Redis:**
```powershell
# V√†o Redis container
docker exec -it chatbot-redis-test redis-cli

# Test Redis
ping
# Response: PONG

# Xem t·∫•t c·∫£ keys (hi·ªán t·∫°i c√≤n tr·ªëng)
keys *

# T·∫°o test data
set test:hello "world"
get test:hello

# Tho√°t
exit
```

### **3. üü¢ chatbot-chroma-test - "Kho l∆∞u tr·ªØ vector th√¥ng minh"**

**ƒê√¢y l√† g√¨?**
- ChromaDB chuy√™n l∆∞u tr·ªØ "vector embeddings" (s·ªë h√≥a vƒÉn b·∫£n)
- Gi√∫p t√¨m ki·∫øm theo √Ω nghƒ©a (semantic search) thay v√¨ ch·ªâ t·ª´ kh√≥a

**ƒêang l√†m g√¨?**
```python
# Chuy·ªÉn ƒë·ªïi vƒÉn b·∫£n th√†nh vector
"Quy tr√¨nh ngh·ªâ ph√©p" ‚Üí [0.1, 0.8, 0.3, 0.5, 0.2, ...]
"Xin ph√©p ngh·ªâ vi·ªác" ‚Üí [0.2, 0.7, 0.4, 0.5, 0.1, ...]
# Hai c√¢u n√†y c√≥ √Ω nghƒ©a g·∫ßn nhau ‚Üí vector g·∫ßn nhau
```

**Ki·ªÉm tra ChromaDB:**
```powershell
# Test API c·ªßa ChromaDB
curl.exe http://localhost:8001/api/v1/heartbeat
curl.exe http://localhost:8001/api/v2/version
# Xem collections (hi·ªán t·∫°i ch∆∞a c√≥)
curl.exe http://localhost:8001/api/v1/collections
```

### **4. üåê chatbot-adminer - "Giao di·ªán qu·∫£n l√Ω database"**

**ƒê√¢y l√† g√¨?**
- Adminer l√† c√¥ng c·ª• web ƒë·ªÉ xem/qu·∫£n l√Ω database (nh∆∞ phpMyAdmin)
- Gi√∫p b·∫°n duy·ªát data m√† kh√¥ng c·∫ßn d√πng command line

**ƒêang l√†m g√¨?**
- Cung c·∫•p giao di·ªán web t·∫°i http://localhost:8080
- Cho ph√©p xem/s·ª≠a/truy v·∫•n database PostgreSQL

**C√°ch s·ª≠ d·ª•ng Adminer:**
1. M·ªü tr√¨nh duy·ªát: http://localhost:8080
2. ƒêƒÉng nh·∫≠p:
   - System: PostgreSQL
   - Server: postgres-test
   - Username: kb_admin
   - Password: test_password_123
   - Database: knowledge_base_test

### **5. ‚ùå chatbot-db-setup - "Th·ª£ setup database (ƒë√£ ho√†n th√†nh)"**

**T·∫°i sao kh√¥ng ch·∫°y?**
- Container n√†y ch·ªâ ch·∫°y 1 l·∫ßn ƒë·ªÉ setup database
- Sau khi ho√†n th√†nh vi·ªác t·∫°o b·∫£ng v√† load data m·∫´u ‚Üí t·ª± ƒë·ªông t·∫Øt
- ƒê√¢y l√† h√†nh vi B√åNH TH∆Ø·ªúNG!

**ƒê√£ l√†m g√¨?**
```python
# 1. T·∫°o c√°c b·∫£ng (tables)
# 2. T·∫°o c√°c index ƒë·ªÉ tƒÉng t·ªëc
# 3. Load d·ªØ li·ªáu m·∫´u (3 t√†i li·ªáu ti·∫øng Vi·ªát)
# 4. T·∫°o b√°o c√°o setup
```

**Xem logs ƒë·ªÉ hi·ªÉu ƒë√£ l√†m g√¨:**
```powershell
docker logs chatbot-db-setup
```

## üîç **H∆Ø·ªöNG D·∫™N KH√ÅM PH√Å H·ªÜ TH·ªêNG**

### **B∆∞·ªõc 1: Kh√°m ph√° Database qua Adminer**

```powershell
# M·ªü Adminer
start http://localhost:8080
```

Trong Adminer:
1. **ƒêƒÉng nh·∫≠p** v·ªõi th√¥ng tin ·ªü tr√™n
2. **Click v√†o b·∫£ng `documents_metadata_v2`** ‚Üí xem d·ªØ li·ªáu m·∫´u
3. **Click v√†o `SQL command`** ‚Üí ch·∫°y c√¢u l·ªánh:

```sql
-- Xem t·∫•t c·∫£ t√†i li·ªáu
SELECT title, author, department_owner, status 
FROM documents_metadata_v2;

-- Xem t√†i li·ªáu ti·∫øng Vi·ªát
SELECT title, LEFT(content, 100) as preview
FROM documents_metadata_v2 
WHERE language_detected = 'vi';

-- ƒê·∫øm s·ªë b·∫£ng trong database
SELECT COUNT(*) as total_tables 
FROM information_schema.tables 
WHERE table_schema = 'public';
```

### **B∆∞·ªõc 2: T·∫°o file test ƒë·ªÉ hi·ªÉu workflow**

T·∫°o file `understand_system.py`:

```python
# understand_system.py
import asyncio
import asyncpg
import json

async def explore_database():
    """Kh√°m ph√° database ƒë·ªÉ hi·ªÉu h·ªá th·ªëng"""
    
    # K·∫øt n·ªëi database
    conn = await asyncpg.connect(
        host='localhost',
        port=5433,  # Port c·ªßa PostgreSQL test
        database='knowledge_base_test',
        user='kb_admin',
        password='test_password_123'
    )
    
    print("üîó Connected to Enhanced Database!")
    print("=" * 50)
    
    # 1. Xem t·∫•t c·∫£ b·∫£ng
    tables = await conn.fetch("""
        SELECT table_name, 
               (SELECT COUNT(*) FROM information_schema.columns 
                WHERE table_name = t.table_name AND table_schema = 'public') as column_count
        FROM information_schema.tables t
        WHERE table_schema = 'public'
        ORDER BY table_name
    """)
    
    print(f"üìä Database c√≥ {len(tables)} b·∫£ng:")
    for table in tables:
        print(f"   üìã {table['table_name']} ({table['column_count']} c·ªôt)")
    
    # 2. Xem d·ªØ li·ªáu m·∫´u
    print(f"\nüìÑ D·ªØ li·ªáu m·∫´u:")
    documents = await conn.fetch("""
        SELECT title, author, department_owner, 
               LENGTH(content) as content_length,
               language_detected, status
        FROM documents_metadata_v2
        ORDER BY title
    """)
    
    for doc in documents:
        print(f"   üìù '{doc['title']}'")
        print(f"      üë§ T√°c gi·∫£: {doc['author']}")
        print(f"      üè¢ Ph√≤ng ban: {doc['department_owner']}")
        print(f"      üìè N·ªôi dung: {doc['content_length']} k√Ω t·ª±")
        print(f"      üåê Ng√¥n ng·ªØ: {doc['language_detected']}")
        print(f"      üìä Tr·∫°ng th√°i: {doc['status']}")
        print()
    
    # 3. Demo search functionality
    print("üîç Demo t√¨m ki·∫øm:")
    
    # T√¨m ki·∫øm theo t·ª´ kh√≥a
    search_results = await conn.fetch("""
        SELECT title, author
        FROM documents_metadata_v2
        WHERE LOWER(title) LIKE '%ngh·ªâ ph√©p%'
           OR LOWER(content) LIKE '%ngh·ªâ ph√©p%'
    """)
    
    print(f"   T√¨m 'ngh·ªâ ph√©p': {len(search_results)} k·∫øt qu·∫£")
    for result in search_results:
        print(f"      ‚úÖ {result['title']} - {result['author']}")
    
    # 4. Xem c·∫•u tr√∫c enhanced schema
    print(f"\nüèóÔ∏è C·∫•u tr√∫c Enhanced Schema:")
    enhanced_features = await conn.fetch("""
        SELECT 
            COUNT(*) FILTER (WHERE vietnamese_segmented = true) as vietnamese_processed,
            COUNT(*) FILTER (WHERE search_tokens IS NOT NULL) as search_ready,
            COUNT(*) FILTER (WHERE jsonl_export_ready = true) as flashrag_ready
        FROM documents_metadata_v2
    """)
    
    feature = enhanced_features[0]
    print(f"   üáªüá≥ Vietnamese processed: {feature['vietnamese_processed']}")
    print(f"   üîç Search ready: {feature['search_ready']}")
    print(f"   üì§ FlashRAG ready: {feature['flashrag_ready']}")
    
    # 5. Demo t·∫°o pipeline session
    print(f"\n‚ö° Demo t·∫°o pipeline session:")
    session_id = await conn.fetchval("""
        INSERT INTO rag_pipeline_sessions (
            original_query, processed_query, pipeline_type, pipeline_method,
            chunks_retrieved, processing_time_ms, response_quality_score
        ) VALUES (
            'L√†m th·∫ø n√†o ƒë·ªÉ xin ngh·ªâ ph√©p?',
            'xin ngh·ªâ ph√©p quy tr√¨nh',
            'standard',
            'hybrid',
            2,
            230,
            0.92
        ) RETURNING session_id
    """)
    
    print(f"   ‚úÖ T·∫°o session th√†nh c√¥ng: {session_id}")
    
    # 6. Th·ªëng k√™ h·ªá th·ªëng
    print(f"\nüìä Th·ªëng k√™ h·ªá th·ªëng:")
    stats = await conn.fetchrow("""
        SELECT 
            pg_size_pretty(pg_database_size(current_database())) as db_size,
            (SELECT COUNT(*) FROM documents_metadata_v2) as total_docs,
            (SELECT COUNT(*) FROM document_chunks_enhanced) as total_chunks,
            (SELECT COUNT(*) FROM rag_pipeline_sessions) as total_sessions
    """)
    
    print(f"   üíæ Dung l∆∞·ª£ng database: {stats['db_size']}")
    print(f"   üìÑ T·ªïng documents: {stats['total_docs']}")
    print(f"   ‚úÇÔ∏è T·ªïng chunks: {stats['total_chunks']}")
    print(f"   üîÑ T·ªïng sessions: {stats['total_sessions']}")
    
    await conn.close()
    print(f"\nüéâ Exploration completed!")

if __name__ == "__main__":
    asyncio.run(explore_database())
```

Ch·∫°y script n√†y:
```powershell
pip install asyncpg
python understand_system.py
```

### **B∆∞·ªõc 3: Workflow th·ª±c t·∫ø**

```mermaid
graph TD
    User[üë§ User h·ªèi: 'Xin ngh·ªâ ph√©p nh∆∞ th·∫ø n√†o?']
    
    PostgreSQL[(üêò PostgreSQL<br/>T√¨m documents li√™n quan)]
    ChromaDB[(üü¢ ChromaDB<br/>Vector search theo √Ω nghƒ©a)]
    Redis[(üî¥ Redis<br/>Cache k·∫øt qu·∫£)]
    
    Processing[‚ö° RAG Processing<br/>K·∫øt h·ª£p th√¥ng tin]
    Response[üí¨ Tr·∫£ l·ªùi: 'Quy tr√¨nh g·ªìm 5 b∆∞·ªõc...']
    
    User --> PostgreSQL
    User --> ChromaDB
    PostgreSQL --> Redis
    ChromaDB --> Redis
    Redis --> Processing
    Processing --> Response
    Response --> User
    
    style PostgreSQL fill:#e1f5fe
    style ChromaDB fill:#e8f5e8
    style Redis fill:#ffebee
```

## üéØ **T√ìM T·∫ÆT ƒê∆†N GI·∫¢N**

| Container | Vai tr√≤ | Gi·ªëng nh∆∞ |
|-----------|---------|-----------|
| **postgres-test** | L∆∞u tr·ªØ d·ªØ li·ªáu c√≥ c·∫•u tr√∫c | T·ªß h·ªì s∆° c√≥ ph√¢n lo·∫°i |
| **chroma-test** | T√¨m ki·∫øm theo √Ω nghƒ©a | Th∆∞ vi·ªán th√¥ng minh hi·ªÉu ng·ªØ c·∫£nh |
| **redis-test** | B·ªô nh·ªõ ƒë·ªám nhanh | B√†n l√†m vi·ªác ƒë·ªÉ ƒë·ªì hay d√πng |
| **adminer** | Giao di·ªán qu·∫£n l√Ω | C·ª≠a s·ªï ƒë·ªÉ nh√¨n v√†o t·ªß h·ªì s∆° |
| **db-setup** | Th·ª£ setup (ƒë√£ xong) | Ng∆∞·ªùi s·∫Øp x·∫øp t·ªß h·ªì s∆° ban ƒë·∫ßu |

B√¢y gi·ªù b·∫°n ƒë√£ hi·ªÉu r√µ h∆°n v·ªÅ h·ªá th·ªëng ch∆∞a? C√≥ mu·ªën t√¥i gi·∫£i th√≠ch th√™m ph·∫ßn n√†o kh√¥ng?


---
L·ªói n√†y l√† do version conflict c·ªßa `huggingface_hub`. T√¥i s·∫Ω fix nhanh:

## üîß **FIX NHANH - C·∫¨P NH·∫¨T REQUIREMENTS**

C·∫≠p nh·∫≠t file `requirements/tool_requirements.txt`:

```txt
# requirements/tool_requirements.txt 
# pip install -r tool_requirements.txt --extra-index-url https://download.pytorch.org/whl/cu126

asyncpg==0.29.0
python-multipart==0.0.6
fastapi==0.115.9
uvicorn==0.24.0
streamlit==1.28.2
python-docx==1.1.0
PyPDF2==3.0.1
openpyxl==3.1.2
pandas==2.0.3

# FIXED VERSIONS - t∆∞∆°ng th√≠ch v·ªõi nhau
huggingface_hub==0.17.3
transformers==4.33.2
sentence-transformers==2.2.2
torch==2.8.0+cu126
torchvision==0.23.0+cu126
torchaudio==2.8.0+cu126

tokenizers==0.13.3

# Other dependencies
chromadb==1.0.0
redis==5.0.1
pyvi==0.1.1
underthesea==6.7.0
numpy==1.24.3
scikit-learn==1.3.0

```

## üîÑ **REBUILD CONTAINER**

```cmd
# D·ª´ng containers
docker-compose -f docker-compose.tool.yml down

# X√≥a container c≈© (force rebuild)
docker rmi chatbot-enhanced-db-document-tool

# Build l·∫°i v·ªõi dependencies m·ªõi
docker-compose -f docker-compose.tool.yml up --build -d

# Xem logs
docker logs -f chatbot-document-tool
```

## ‚ö° **C√ÅCH NHANH H∆†N - D√ôNG PRE-BUILT IMAGE**

N·∫øu v·∫´n l·ªói, s·ª≠a `docker/Dockerfile.tool` th√†nh:

```dockerfile
# docker/Dockerfile.tool - VERSION ƒê∆†N GI·∫¢N
FROM python:3.9-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    postgresql-client \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Install dependencies t·ª´ng b∆∞·ªõc ƒë·ªÉ tr√°nh conflict
RUN pip install --upgrade pip setuptools wheel

# Install core dependencies tr∆∞·ªõc
RUN pip install torch==2.0.1 --index-url https://download.pytorch.org/whl/cpu
RUN pip install transformers==4.33.2
RUN pip install sentence-transformers==2.2.2

# Install other dependencies  
RUN pip install \
    asyncpg==0.29.0 \
    streamlit==1.28.2 \
    python-docx==1.1.0 \
    PyPDF2==3.0.1 \
    pandas==2.0.3 \
    pyvi==0.1.1 \
    underthesea==6.7.0 \
    chromadb==0.4.15

# Copy code
COPY tools/ /app/tools/
RUN mkdir -p /app/logs

EXPOSE 8501
CMD ["streamlit", "run", "tools/document_ingestion/streamlit_app.py", "--server.port=8501", "--server.address=0.0.0.0"]
```

## üß™ **TEST BUILD**

```cmd
# Test build container
docker build -f docker/Dockerfile.tool -t test-tool .

# N·∫øu build OK, ch·∫°y full stack
docker-compose -f docker-compose.tool.yml up --build
```

## üìä **KI·ªÇM TRA K·∫æT QU·∫¢**

Sau khi ch·∫°y xong:
- Streamlit tool: http://localhost:8501
- Database: http://localhost:8080

**N·∫øu v·∫´n l·ªói**, b·∫°n copy to√†n b·ªô error message cho t√¥i. T√¥i s·∫Ω t·∫°o version ƒë∆°n gi·∫£n h∆°n kh√¥ng d√πng sentence-transformers ph·ª©c t·∫°p.

Th·ª≠ xem sao nh√©! üöÄ
