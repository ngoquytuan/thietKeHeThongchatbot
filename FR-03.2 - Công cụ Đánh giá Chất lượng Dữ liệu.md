Nếu đã đủ thông tin hãy giúp tôi viết tài liệu thiết kế module dưới đây.
Tài liệu này cung cấp roadmap đầy đủ để implement FR-03.2 mà không cần code cụ thể nhưng 
có các bước làm cụ thể như chuẩn bị môi trường PC thế nào, phần mềm gì?
 để Team kỹ thuật có thể follow step-by-step guide này để thiết kế tối ưu cho hệ thống chatbot
 và có mermaidchart luồng công việc, điểm quay đầu khi bị fail.

3.3 Module Xử lý Dữ liệu (FR-03)
**FR-03.2 - Công cụ Đánh giá Chất lượng Dữ liệu**
- Phát hiện tài liệu trùng lặp (ngữ nghĩa và từ khóa)
- Xác định nội dung mơ hồ, mâu thuẫn
- Đánh giá độ hoàn chỉnh của metadata
- Báo cáo chất lượng trực quan
---
# TÀI LIỆU THIẾT KẾ MODULE - CÔNG CỤ ĐÁNH GIÁ CHẤT LƯỢNG DỮ LIỆU
## DATA QUALITY ASSESSMENT TOOL (FR-03.2)

---

**Module ID:** FR-03.2  
**Tên Module:** Công cụ Đánh giá Chất lượng Dữ liệu  
**Phiên bản:** 1.0  
**Ngày:** 31/08/2025  
**Người soạn thảo:** Chuyên gia Thiết kế Hệ thống  
**Trạng thái:** Ready for Implementation  

---

## 1. TỔNG QUAN MODULE

### 1.1 Mục đích
Module này đảm bảo chất lượng dữ liệu đầu vào cho hệ thống RAG bằng cách:
- Phát hiện và loại bỏ tài liệu trùng lặp (semantic & keyword-based)
- Xác định nội dung mơ hồ, mâu thuẫn trong tài liệu
- Đánh giá độ hoàn chỉnh của metadata
- Tạo báo cáo chất lượng trực quan cho stakeholders

### 1.2 Vị trí trong hệ thống
```
Raw Documents → [FR-03.1 Raw-to-Clean] → [FR-03.2 Quality Assessment] → [FR-03.3 Data Ingestion] → Vector DB
```

### 1.3 Input/Output
- **Input**: Clean documents từ FR-03.1 với metadata
- **Output**: Quality-validated documents + Quality Report + Rejected items log

---

## 2. CHUẨN BỊ MÔI TRƯỜNG PHÁT TRIỂN

### 2.1 Yêu cầu Hardware tối thiểu

| Component | Specification | Lý do |
|-----------|---------------|-------|
| **CPU** | 8 cores, 2.4GHz+ | Xử lý song song nhiều documents |
| **RAM** | 32GB+ | Load embedding models + vector operations |
| **Storage** | 1TB SSD | Fast I/O cho document processing |
| **GPU** | 8GB VRAM (optional) | Tăng tốc embedding generation |

### 2.2 Môi trường phần mềm

#### **Operating System**
```bash
# Recommended: Ubuntu 22.04 LTS hoặc CentOS 8+
# Windows 11 Pro (với WSL2) cũng được hỗ trợ
```

#### **Core Development Tools**

| Tool | Version | Installation Command | Purpose |
|------|---------|---------------------|---------|
| **Python** | 3.9+ | `sudo apt install python3.9 python3.9-venv` | Core language |
| **Node.js** | 18+ | `curl -fsSL https://deb.nodesource.com/setup_18.x \| sudo -E bash -` | Frontend dashboard |
| **Docker** | 24+ | `curl -fsSL https://get.docker.com -o get-docker.sh && sh get-docker.sh` | Containerization |
| **PostgreSQL** | 15+ | `sudo apt install postgresql-15 postgresql-client-15` | Metadata storage |
| **Redis** | 7+ | `sudo apt install redis-server` | Caching layer |

#### **Python Dependencies**
```bash
# Tạo virtual environment
python3 -m venv data_quality_env
source data_quality_env/bin/activate

# Core ML/NLP libraries
pip install torch>=2.0.0 transformers>=4.30.0
pip install sentence-transformers>=2.2.0
pip install scikit-learn>=1.3.0 numpy>=1.24.0 pandas>=2.0.0

# Text processing
pip install spacy>=3.6.0 nltk>=3.8.0
pip install textdistance>=4.5.0 fuzzywuzzy>=0.18.0

# Database & API
pip install psycopg2-binary>=2.9.0 redis>=4.5.0
pip install fastapi>=0.100.0 uvicorn>=0.22.0
pip install sqlalchemy>=2.0.0 alembic>=1.11.0

# Visualization & Reporting  
pip install plotly>=5.15.0 dash>=2.11.0
pip install matplotlib>=3.7.0 seaborn>=0.12.0
pip install reportlab>=4.0.0 jinja2>=3.1.0

# Quality assessment specific
pip install dedupe>=2.0.0 recordlinkage>=0.15.0
pip install textstat>=0.7.0 language-tool-python>=2.7.0
```

#### **Development Environment Setup**
```bash
# IDE Setup (chọn 1 trong các options)
# Option 1: VS Code
sudo snap install --classic code
code --install-extension ms-python.python
code --install-extension ms-toolsai.jupyter

# Option 2: PyCharm Professional
# Download từ JetBrains website

# Option 3: Jupyter Lab
pip install jupyterlab>=4.0.0
jupyter lab --generate-config
```

### 2.3 Database Setup

#### **PostgreSQL Configuration**
```sql
-- Tạo database và user
CREATE DATABASE data_quality_db;
CREATE USER dq_user WITH PASSWORD 'secure_password_here';
GRANT ALL PRIVILEGES ON DATABASE data_quality_db TO dq_user;

-- Tạo extensions cần thiết
\c data_quality_db;
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pg_trgm"; -- Cho fuzzy text matching
CREATE EXTENSION IF NOT EXISTS "btree_gin"; -- Tối ưu indexing
```

#### **Redis Configuration**
```bash
# Chỉnh sửa /etc/redis/redis.conf
maxmemory 8gb
maxmemory-policy allkeys-lru
save 900 1
save 300 10
```

---

## 3. KIẾN TRÚC MODULE VÀ LUỒNG CÔNG VIỆC

### 3.1 Module Architecture Overview

```mermaid
graph TB
    %% Input
    Input[📄 Clean Documents<br/>+ Metadata]
    
    %% Main Processing Engine
    subgraph "🔍 DATA QUALITY ASSESSMENT ENGINE"
        subgraph "1️⃣ Duplicate Detection"
            HashDup[🔢 Hash-based<br/>Duplicate Detection]
            SemDup[🧠 Semantic<br/>Duplicate Detection]
            KeywordDup[🔤 Keyword<br/>Duplicate Detection]
        end
        
        subgraph "2️⃣ Content Quality Analysis"
            AmbiguityDet[❓ Ambiguity<br/>Detector]
            ContradictDet[⚡ Contradiction<br/>Detector]
            ReadabilityCheck[📚 Readability<br/>Checker]
        end
        
        subgraph "3️⃣ Metadata Completeness"
            MetadataVal[✅ Metadata<br/>Validator]
            RequiredFields[📋 Required Fields<br/>Checker]
            DataTypes[🏷️ Data Type<br/>Validator]
        end
        
        subgraph "4️⃣ Quality Scoring"
            ScoreCalc[📊 Quality Score<br/>Calculator]
            RuleEngine[⚙️ Business Rules<br/>Engine]
            ThresholdCheck[🎯 Threshold<br/>Checker]
        end
    end
    
    %% Decision Point
    Decision{📋 Quality<br/>Passed?}
    
    %% Outputs
    Approved[✅ Approved<br/>Documents]
    Rejected[❌ Rejected<br/>Documents]
    Report[📊 Quality<br/>Report]
    
    %% Storage
    QualityDB[(🗄️ Quality<br/>Database)]
    Cache[(⚡ Redis<br/>Cache)]
    
    %% Flow
    Input --> HashDup
    Input --> SemDup
    Input --> KeywordDup
    Input --> AmbiguityDet
    Input --> ContradictDet
    Input --> ReadabilityCheck
    Input --> MetadataVal
    Input --> RequiredFields
    Input --> DataTypes
    
    HashDup --> ScoreCalc
    SemDup --> ScoreCalc
    KeywordDup --> ScoreCalc
    AmbiguityDet --> ScoreCalc
    ContradictDet --> ScoreCalc
    ReadabilityCheck --> ScoreCalc
    MetadataVal --> ScoreCalc
    RequiredFields --> ScoreCalc
    DataTypes --> ScoreCalc
    
    ScoreCalc --> RuleEngine
    RuleEngine --> ThresholdCheck
    ThresholdCheck --> Decision
    
    Decision -->|Yes| Approved
    Decision -->|No| Rejected
    
    Approved --> QualityDB
    Rejected --> QualityDB
    ScoreCalc --> Report
    
    %% Caching
    SemDup -.-> Cache
    ScoreCalc -.-> Cache
    
    %% Styling
    classDef input fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef duplicate fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef content fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef metadata fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef scoring fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    classDef decision fill:#ffebee,stroke:#c62828,stroke-width:3px
    classDef output fill:#e0f2f1,stroke:#00796b,stroke-width:2px
    classDef storage fill:#f1f8e9,stroke:#689f38,stroke-width:2px
    
    class Input input
    class HashDup,SemDup,KeywordDup duplicate
    class AmbiguityDet,ContradictDet,ReadabilityCheck content
    class MetadataVal,RequiredFields,DataTypes metadata
    class ScoreCalc,RuleEngine,ThresholdCheck scoring
    class Decision decision
    class Approved,Rejected,Report output
    class QualityDB,Cache storage
```

### 3.2 Detailed Workflow với Retry Logic

```mermaid
flowchart TD
    Start([🚀 Start Process])
    
    %% Input Validation
    InputCheck{📋 Input<br/>Valid?}
    InputFail[❌ Invalid Input<br/>Log Error]
    
    %% Duplicate Detection Phase
    DupPhase[🔍 Phase 1:<br/>Duplicate Detection]
    HashCheck[🔢 Hash Duplicate<br/>Check]
    HashFail{⚠️ Hash Check<br/>Failed?}
    HashRetry[🔄 Retry Hash<br/>Check _3x_]
    
    SemCheck[🧠 Semantic Duplicate<br/>Check]
    SemFail{⚠️ Semantic Check<br/>Failed?}
    SemRetry[🔄 Retry Semantic<br/>Check _3x_]
    
    KeywordCheck[🔤 Keyword Duplicate<br/>Check]
    KeywordFail{⚠️ Keyword Check<br/>Failed?}
    KeywordRetry[🔄 Retry Keyword<br/>Check _3x_]
    
    %% Content Quality Phase
    ContentPhase[📚 Phase 2:<br/>Content Quality]
    AmbiguityCheck[❓ Ambiguity<br/>Detection]
    AmbFail{⚠️ Ambiguity Check<br/>Failed?}
    AmbRetry[🔄 Retry Ambiguity<br/>Check _3x_]
    
    ContradictCheck[⚡ Contradiction<br/>Detection]
    ConFail{⚠️ Contradiction Check<br/>Failed?}
    ConRetry[🔄 Retry Contradiction<br/>Check _3x_]
    
    ReadabilityCheck[📖 Readability<br/>Assessment]
    ReadFail{⚠️ Readability Check<br/>Failed?}
    ReadRetry[🔄 Retry Readability<br/>Check 3x]
    
    %% Metadata Phase
    MetadataPhase[🏷️ Phase 3:<br/>Metadata Validation]
    MetaCheck[✅ Metadata<br/>Completeness]
    MetaFail{⚠️ Metadata Check<br/>Failed?}
    MetaRetry[🔄 Retry Metadata<br/>Check 3x]
    
    %% Scoring Phase
    ScoringPhase[📊 Phase 4:<br/>Quality Scoring]
    ScoreCalc[🧮 Calculate<br/>Quality Score]
    ScoreFail{⚠️ Scoring<br/>Failed?}
    ScoreRetry[🔄 Retry Scoring<br/>3x]
    
    %% Decision Phase
    Decision{🎯 Quality Score<br/>>= Threshold?}
    
    %% Output Handling
    Approve[✅ Approve<br/>Document]
    Reject[❌ Reject<br/>Document]
    
    %% Database Operations
    SaveApproved[💾 Save to<br/>Approved DB]
    SaveRejected[💾 Save to<br/>Rejected DB]
    
    SaveFail{⚠️ DB Save<br/>Failed?}
    SaveRetry[🔄 Retry DB Save<br/>5x]
    
    %% Report Generation
    GenReport[📊 Generate<br/>Quality Report]
    ReportFail{⚠️ Report Gen<br/>Failed?}
    ReportRetry[🔄 Retry Report<br/>Gen 3x]
    
    %% Final States
    Success([✅ Process<br/>Completed])
    CriticalFail([💀 Critical<br/>Failure])
    
    %% Main Flow
    Start --> InputCheck
    InputCheck -->|Valid| DupPhase
    InputCheck -->|Invalid| InputFail
    InputFail --> CriticalFail
    
    %% Phase 1: Duplicate Detection
    DupPhase --> HashCheck
    HashCheck --> HashFail
    HashFail -->|Yes| HashRetry
    HashFail -->|No| SemCheck
    HashRetry -->|Max Retry| CriticalFail
    HashRetry -->|Retry| HashCheck
    
    SemCheck --> SemFail
    SemFail -->|Yes| SemRetry
    SemFail -->|No| KeywordCheck
    SemRetry -->|Max Retry| CriticalFail
    SemRetry -->|Retry| SemCheck
    
    KeywordCheck --> KeywordFail
    KeywordFail -->|Yes| KeywordRetry
    KeywordFail -->|No| ContentPhase
    KeywordRetry -->|Max Retry| CriticalFail
    KeywordRetry -->|Retry| KeywordCheck
    
    %% Phase 2: Content Quality
    ContentPhase --> AmbiguityCheck
    AmbiguityCheck --> AmbFail
    AmbFail -->|Yes| AmbRetry
    AmbFail -->|No| ContradictCheck
    AmbRetry -->|Max Retry| CriticalFail
    AmbRetry -->|Retry| AmbiguityCheck
    
    ContradictCheck --> ConFail
    ConFail -->|Yes| ConRetry
    ConFail -->|No| ReadabilityCheck
    ConRetry -->|Max Retry| CriticalFail
    ConRetry -->|Retry| ContradictCheck
    
    ReadabilityCheck --> ReadFail
    ReadFail -->|Yes| ReadRetry
    ReadFail -->|No| MetadataPhase
    ReadRetry -->|Max Retry| CriticalFail
    ReadRetry -->|Retry| ReadabilityCheck
    
    %% Phase 3: Metadata
    MetadataPhase --> MetaCheck
    MetaCheck --> MetaFail
    MetaFail -->|Yes| MetaRetry
    MetaFail -->|No| ScoringPhase
    MetaRetry -->|Max Retry| CriticalFail
    MetaRetry -->|Retry| MetaCheck
    
    %% Phase 4: Scoring
    ScoringPhase --> ScoreCalc
    ScoreCalc --> ScoreFail
    ScoreFail -->|Yes| ScoreRetry
    ScoreFail -->|No| Decision
    ScoreRetry -->|Max Retry| CriticalFail
    ScoreRetry -->|Retry| ScoreCalc
    
    %% Decision & Output
    Decision -->|Pass| Approve
    Decision -->|Fail| Reject
    
    Approve --> SaveApproved
    Reject --> SaveRejected
    
    SaveApproved --> SaveFail
    SaveRejected --> SaveFail
    
    SaveFail -->|Yes| SaveRetry
    SaveFail -->|No| GenReport
    SaveRetry -->|Max Retry| CriticalFail
    SaveRetry -->|Retry| SaveApproved
    
    GenReport --> ReportFail
    ReportFail -->|Yes| ReportRetry
    ReportFail -->|No| Success
    ReportRetry -->|Max Retry| CriticalFail
    ReportRetry -->|Retry| GenReport
    
    %% Styling
    classDef startEnd fill:#e3f2fd,stroke:#1976d2,stroke-width:3px
    classDef phase fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef process fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef decision fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef retry fill:#ffecb3,stroke:#f57f17,stroke-width:2px
    classDef fail fill:#ffebee,stroke:#c62828,stroke-width:2px
    classDef success fill:#e0f2f1,stroke:#00796b,stroke-width:2px
    
    class Start,Success,CriticalFail startEnd
    class DupPhase,ContentPhase,MetadataPhase,ScoringPhase phase
    class HashCheck,SemCheck,KeywordCheck,AmbiguityCheck,ContradictCheck,ReadabilityCheck,MetaCheck,ScoreCalc,SaveApproved,SaveRejected,GenReport process
    class InputCheck,HashFail,SemFail,KeywordFail,AmbFail,ConFail,ReadFail,MetaFail,ScoreFail,SaveFail,ReportFail,Decision decision
    class HashRetry,SemRetry,KeywordRetry,AmbRetry,ConRetry,ReadRetry,MetaRetry,ScoreRetry,SaveRetry,ReportRetry retry
    class InputFail fail
    class Approve,Reject success
```

---

## 4. IMPLEMENTATION ROADMAP (4 TUẦN)

### 4.1 Week 1: Core Infrastructure & Duplicate Detection

#### **Day 1-2: Environment Setup**
```bash
# Step 1: Clone repository structure
mkdir data_quality_module
cd data_quality_module
mkdir -p {src,tests,config,docs,scripts,data/{input,output,temp}}

# Step 2: Setup Python environment
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# Step 3: Database setup
sudo -u postgres createdb data_quality_db
python scripts/init_db.py

# Step 4: Configuration
cp config/config.example.yaml config/config.yaml
# Edit config.yaml với database credentials
```

#### **Day 3-5: Hash-based Duplicate Detection**
```python
# File: src/duplicate_detection/hash_detector.py

"""
Implementation tasks:
1. MD5/SHA256 hash generation cho documents
2. Hash comparison algorithm
3. Near-duplicate detection using simhash
4. Database storage cho hash values
5. Batch processing optimization
"""

# Key algorithms to implement:
# - Exact hash matching: MD5, SHA256
# - Fuzzy hash matching: SimHash, TLSH
# - Content fingerprinting: MinHash, LSH
```

#### **Day 6-7: Semantic Duplicate Detection**
```python
# File: src/duplicate_detection/semantic_detector.py

"""
Implementation tasks:
1. Load pre-trained embedding model (sentence-transformers)
2. Generate document embeddings
3. Cosine similarity calculation
4. Clustering similar documents
5. Threshold tuning for semantic similarity
"""

# Key components:
# - Embedding model: all-MiniLM-L6-v2 or multilingual variants
# - Similarity metrics: Cosine, Euclidean, Jaccard
# - Clustering: KMeans, DBSCAN
# - Optimization: Approximate nearest neighbor (Annoy, FAISS)
```

### 4.2 Week 2: Content Quality Analysis

#### **Day 8-10: Ambiguity Detection**
```python
# File: src/content_analysis/ambiguity_detector.py

"""
Implementation approach:
1. Sentence-level ambiguity detection
   - Multiple interpretations
   - Unclear pronouns/references
   - Missing context

2. Word-level ambiguity
   - Polysemic words
   - Context-dependent meanings
   
3. Structural ambiguity
   - Grammar parsing issues
   - Sentence structure problems
"""

# Algorithms to implement:
# - Named Entity Recognition (NER) for unclear references
# - Part-of-speech tagging for grammatical ambiguity
# - Dependency parsing for structural issues
# - Custom rule-based patterns for domain-specific ambiguity
```

#### **Day 11-12: Contradiction Detection**
```python
# File: src/content_analysis/contradiction_detector.py

"""
Implementation strategy:
1. Within-document contradictions
   - Fact extraction and comparison
   - Negation detection
   - Temporal contradictions

2. Cross-document contradictions
   - Compare statements across documents
   - Version control conflicts
   - Policy contradictions
"""

# Technical approach:
# - Natural Language Inference (NLI) models
# - Fact extraction using Open Information Extraction
# - Negation scope detection
# - Temporal relation extraction
```

#### **Day 13-14: Readability Assessment**
```python
# File: src/content_analysis/readability_checker.py

"""
Metrics to implement:
1. Traditional readability scores
   - Flesch Reading Ease
   - Flesch-Kincaid Grade Level
   - SMOG Index
   - Coleman-Liau Index

2. Modern NLP-based metrics
   - Sentence complexity analysis
   - Vocabulary difficulty assessment
   - Technical jargon density
"""

# Implementation:
# - Use textstat library for traditional metrics
# - Custom analysis for technical content
# - Domain-specific vocabulary lists
# - Readability recommendations generator
```

### 4.3 Week 3: Metadata Validation & Scoring

#### **Day 15-17: Metadata Completeness Checker**
```python
# File: src/metadata_validation/completeness_checker.py

"""
Validation rules:
1. Required fields validation
   - document_id, title, author, department
   - creation_date, last_updated
   - access_level, document_type

2. Field format validation
   - Date format consistency
   - Email format for authors
   - Enum values for categories

3. Business logic validation
   - Department-author relationship
   - Access level permissions
   - Version numbering consistency
"""

# Schema definition:
metadata_schema = {
    "required_fields": ["document_id", "title", "author"],
    "optional_fields": ["tags", "description", "version"],
    "field_types": {
        "document_id": "uuid",
        "creation_date": "datetime",
        "access_level": "enum"
    },
    "business_rules": {
        "author_department_match": True,
        "version_increment": True
    }
}
```

#### **Day 18-19: Quality Scoring Engine**
```python
# File: src/scoring/quality_calculator.py

"""
Scoring algorithm:
Total Score = Σ(Component Score × Weight)

Components:
1. Duplicate Score (30%)
   - No duplicates: 100 points
   - Semantic similarity < 0.8: 80 points
   - High similarity: 0-50 points

2. Content Quality Score (40%)
   - Readability: 25%
   - Ambiguity level: 25%
   - Contradiction: 50%

3. Metadata Completeness (30%)
   - Required fields: 60%
   - Optional fields: 20%
   - Format compliance: 20%
"""

# Configurable thresholds:
quality_thresholds = {
    "excellent": 90,
    "good": 75,
    "acceptable": 60,
    "needs_improvement": 40,
    "reject": 0
}
```

#### **Day 20-21: Business Rules Engine**
```python
# File: src/scoring/rules_engine.py

"""
Rule categories:
1. Mandatory rules (auto-reject if failed)
   - Critical metadata missing
   - Exact duplicate found
   - Contains forbidden content

2. Scoring rules (affect quality score)
   - Readability below threshold
   - Ambiguous content detected
   - Incomplete metadata

3. Warning rules (flag for review)
   - Potential outdated information
   - Cross-references missing
   - Inconsistent formatting
"""

# Rule definition format:
rule_example = {
    "id": "META_001",
    "name": "Required Author Field",
    "type": "mandatory",
    "condition": "metadata.author is not null",
    "action": "reject",
    "message": "Author field is mandatory"
}
```

### 4.4 Week 4: Reporting & Integration

#### **Day 22-24: Quality Report Generation**
```python
# File: src/reporting/report_generator.py

"""
Report types:
1. Executive Summary
   - Overall quality metrics
   - Pass/fail statistics
   - Trending analysis

2. Technical Report
   - Detailed scoring breakdown
   - Failed validation rules
   - Recommended actions

3. Visual Dashboard
   - Interactive charts
   - Quality trends over time
   - Department-wise analysis
"""

# Report formats:
# - HTML dashboard (Plotly Dash)
# - PDF report (ReportLab)
# - JSON API response
# - Excel export
```

#### **Day 25-26: Web Dashboard Development**
```python
# File: src/dashboard/app.py

"""
Dashboard features:
1. Real-time quality monitoring
2. Document processing status
3. Quality trends visualization
4. Drill-down capability for failed documents
5. Bulk action interface
"""

# Tech stack:
# - Frontend: Plotly Dash
# - Backend: FastAPI
# - Database: PostgreSQL
# - Caching: Redis
```

#### **Day 27-28: Integration & Testing**
```python
# Integration checklist:
# 1. API endpoints cho external integration
# 2. Database migration scripts
# 3. Configuration management
# 4. Error handling và logging
# 5. Performance optimization
# 6. Unit tests và integration tests
# 7. Documentation update
```

---

## 5. CHI TIẾT TECHNICAL IMPLEMENTATION

### 5.1 Database Schema

```sql
-- Quality Assessment Tables
CREATE TABLE quality_assessments (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    document_id UUID NOT NULL,
    assessment_date TIMESTAMP DEFAULT NOW(),
    overall_score DECIMAL(5,2),
    status VARCHAR(20) CHECK (status IN ('approved', 'rejected', 'needs_review')),
    
    -- Duplicate detection results
    hash_duplicate_found BOOLEAN DEFAULT FALSE,
    semantic_similarity_score DECIMAL(5,4),
    keyword_similarity_score DECIMAL(5,4),
    
    -- Content quality results
    readability_score DECIMAL(5,2),
    ambiguity_score DECIMAL(5,2),
    contradiction_found BOOLEAN DEFAULT FALSE,
    
    -- Metadata quality
    metadata_completeness_score DECIMAL(5,2),
    required_fields_missing TEXT[],
    
    -- Additional metadata
    processing_time_ms INTEGER,
    quality_rules_applied JSONB,
    recommendations TEXT[],
    
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

CREATE TABLE duplicate_pairs (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    document_id_1 UUID NOT NULL,
    document_id_2 UUID NOT NULL,
    duplicate_type VARCHAR(20) CHECK (duplicate_type IN ('exact', 'semantic', 'keyword')),
    similarity_score DECIMAL(5,4),
    detected_at TIMESTAMP DEFAULT NOW()
);

CREATE TABLE quality_rules (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    rule_name VARCHAR(100) NOT NULL,
    rule_type VARCHAR(20) CHECK (rule_type IN ('mandatory', 'scoring', 'warning')),
    rule_condition TEXT NOT NULL,
    rule_action VARCHAR(50),
    rule_weight DECIMAL(3,2),
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT NOW()
);

-- Indexes for performance
CREATE INDEX idx_quality_assessments_document_id ON quality_assessments(document_id);
CREATE INDEX idx_quality_assessments_status ON quality_assessments(status);
CREATE INDEX idx_quality_assessments_date ON quality_assessments(assessment_date);
CREATE INDEX idx_duplicate_pairs_doc1 ON duplicate_pairs(document_id_1);
CREATE INDEX idx_duplicate_pairs_doc2 ON duplicate_pairs(document_id_2);
```

### 5.2 API Specification

```python
# File: src/api/endpoints.py

from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel
from typing import List, Optional

app = FastAPI(title="Data Quality Assessment API", version="1.0.0")

class QualityAssessmentRequest(BaseModel):
    document_id: str
    document_content: str
    metadata: dict
    assessment_options: Optional[dict] = {}

class QualityAssessmentResponse(BaseModel):
    document_id: str
    overall_score: float
    status: str
    detailed_results: dict
    recommendations: List[str]
    processing_time_ms: int

@app.post("/api/v1/assess-quality", response_model=QualityAssessmentResponse)
async def assess_document_quality(request: QualityAssessmentRequest):
    """
    Assess quality of a single document
    """
    # Implementation here
    pass

@app.post("/api/v1/batch-assess")
async def batch_assess_quality(
    documents: List[QualityAssessmentRequest],
    background_tasks: BackgroundTasks
):
    """
    Batch process multiple documents
    """
    # Implementation here
    pass

@app.get("/api/v1/quality-report/{document_id}")
async def get_quality_report(document_id: str):
    """
    Retrieve quality assessment report for specific document
    """
    # Implementation here
    pass

@app.get("/api/v1/quality-dashboard")
async def get_quality_dashboard():
    """
    Get dashboard data for quality overview
    """
    # Implementation here
    pass
```

### 5.3 Configuration Management

```yaml
# File: config/config.yaml

# Database Configuration
database:
  host: localhost
  port: 5432
  name: data_quality_db
  user: dq_user
  password: ${DB_PASSWORD}
  pool_size: 20
  max_overflow: 0

# Redis Configuration  
redis:
  host: localhost
  port: 6379
  db: 0
```
### 5.3 Configuration Management (tiếp theo)

```yaml
# File: config/config.yaml (tiếp)

# Quality Assessment Settings
quality_assessment:
  # Duplicate Detection
  duplicate_detection:
    hash_algorithm: "sha256"
    semantic_similarity_threshold: 0.85
    keyword_similarity_threshold: 0.75
    embedding_model: "sentence-transformers/all-MiniLM-L6-v2"
    batch_size: 32
    
  # Content Quality
  content_quality:
    readability:
      target_grade_level: 12
      min_acceptable_score: 30
    ambiguity:
      max_ambiguous_sentences_ratio: 0.1
      confidence_threshold: 0.7
    contradiction:
      nli_model: "microsoft/DialoGPT-medium"
      confidence_threshold: 0.8
      
  # Metadata Validation
  metadata_validation:
    required_fields:
      - document_id
      - title
      - author
      - department_owner
      - access_level
      - document_type
    optional_fields:
      - description
      - tags
      - version
      - keywords
    field_validation:
      email_pattern: "^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$"
      date_format: "%Y-%m-%d"
      version_pattern: "^\\d+\\.\\d+\\.\\d+$"

# Scoring Configuration
scoring:
  weights:
    duplicate_score: 0.30
    content_quality: 0.40
    metadata_completeness: 0.30
  
  thresholds:
    excellent: 90.0
    good: 75.0
    acceptable: 60.0
    needs_improvement: 40.0
    reject: 30.0
    
  penalties:
    exact_duplicate: -100  # Auto-reject
    high_semantic_similarity: -50
    missing_required_field: -20
    contradiction_found: -30

# Processing Configuration
processing:
  max_concurrent_tasks: 8
  timeout_seconds: 300
  retry_attempts: 3
  retry_delay_seconds: 5
  batch_processing_size: 100

# Reporting Configuration
reporting:
  formats: ["html", "pdf", "json"]
  dashboard_refresh_interval: 30  # seconds
  report_retention_days: 90
  export_formats: ["xlsx", "csv"]

# Logging Configuration
logging:
  level: INFO
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "logs/data_quality.log"
  max_file_size: "10MB"
  backup_count: 5
```

### 5.4 Error Handling Strategy

```python
# File: src/utils/error_handler.py

import logging
import traceback
from enum import Enum
from typing import Optional, Dict, Any
from datetime import datetime

class ErrorSeverity(Enum):
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"

class QualityAssessmentError(Exception):
    """Base exception cho Data Quality Assessment module"""
    
    def __init__(
        self, 
        message: str, 
        error_code: str,
        severity: ErrorSeverity = ErrorSeverity.MEDIUM,
        details: Optional[Dict[str, Any]] = None
    ):
        self.message = message
        self.error_code = error_code
        self.severity = severity
        self.details = details or {}
        self.timestamp = datetime.now()
        super().__init__(self.message)

class DuplicateDetectionError(QualityAssessmentError):
    """Errors trong quá trình duplicate detection"""
    pass

class ContentAnalysisError(QualityAssessmentError):
    """Errors trong content quality analysis"""
    pass

class MetadataValidationError(QualityAssessmentError):
    """Errors trong metadata validation"""
    pass

class ErrorHandler:
    def __init__(self, logger: logging.Logger):
        self.logger = logger
        self.error_counts = {}
        
    def handle_error(
        self, 
        error: Exception, 
        context: Dict[str, Any] = None,
        retry_count: int = 0
    ) -> Dict[str, Any]:
        """
        Centralized error handling với retry logic
        """
        error_info = {
            "error_type": type(error).__name__,
            "message": str(error),
            "timestamp": datetime.now().isoformat(),
            "context": context or {},
            "retry_count": retry_count,
            "traceback": traceback.format_exc()
        }
        
        if isinstance(error, QualityAssessmentError):
            error_info.update({
                "error_code": error.error_code,
                "severity": error.severity.value,
                "details": error.details
            })
            
            # Log based on severity
            if error.severity == ErrorSeverity.CRITICAL:
                self.logger.critical(f"CRITICAL ERROR: {error.message}", extra=error_info)
            elif error.severity == ErrorSeverity.HIGH:
                self.logger.error(f"HIGH SEVERITY: {error.message}", extra=error_info)
            elif error.severity == ErrorSeverity.MEDIUM:
                self.logger.warning(f"MEDIUM SEVERITY: {error.message}", extra=error_info)
            else:
                self.logger.info(f"LOW SEVERITY: {error.message}", extra=error_info)
        else:
            self.logger.error(f"Unexpected error: {error.message}", extra=error_info)
            
        # Track error frequency
        error_key = f"{type(error).__name__}:{getattr(error, 'error_code', 'unknown')}"
        self.error_counts[error_key] = self.error_counts.get(error_key, 0) + 1
        
        return error_info
        
    def should_retry(self, error: Exception, retry_count: int, max_retries: int = 3) -> bool:
        """
        Determine if operation should be retried
        """
        if retry_count >= max_retries:
            return False
            
        # Don't retry critical errors or validation errors
        if isinstance(error, QualityAssessmentError):
            if error.severity == ErrorSeverity.CRITICAL:
                return False
            if isinstance(error, MetadataValidationError):
                return False
                
        # Retry for transient errors
        transient_errors = [
            ConnectionError,
            TimeoutError,
            "rate_limit_exceeded",
            "temporary_service_unavailable"
        ]
        
        return any(
            isinstance(error, err_type) or 
            (hasattr(error, 'error_code') and error.error_code == err_type)
            for err_type in transient_errors
        )
```

### 5.5 Performance Optimization

```python
# File: src/optimization/performance_optimizer.py

import asyncio
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache, wraps
from typing import List, Dict, Any, Callable
import redis
import hashlib
import pickle

class PerformanceOptimizer:
    def __init__(self, redis_client: redis.Redis):
        self.redis_client = redis_client
        self.executor = ThreadPoolExecutor(max_workers=8)
        self.cache_ttl = 3600  # 1 hour
        
    def async_cache(self, ttl: int = 3600):
        """
        Decorator for caching async function results
        """
        def decorator(func: Callable):
            @wraps(func)
            async def wrapper(*args, **kwargs):
                # Create cache key
                cache_key = f"{func.__name__}:{hashlib.md5(str(args + tuple(kwargs.items())).encode()).hexdigest()}"
                
                # Try to get from cache
                cached_result = self.redis_client.get(cache_key)
                if cached_result:
                    return pickle.loads(cached_result)
                
                # Execute function and cache result
                result = await func(*args, **kwargs)
                self.redis_client.setex(
                    cache_key, 
                    ttl, 
                    pickle.dumps(result)
                )
                return result
            return wrapper
        return decorator
    
    async def batch_process_documents(
        self, 
        documents: List[Dict[str, Any]], 
        processing_func: Callable,
        batch_size: int = 32
    ) -> List[Dict[str, Any]]:
        """
        Process documents in batches for optimal performance
        """
        results = []
        
        for i in range(0, len(documents), batch_size):
            batch = documents[i:i + batch_size]
            
            # Process batch concurrently
            tasks = []
            for doc in batch:
                task = asyncio.create_task(processing_func(doc))
                tasks.append(task)
            
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)
            results.extend(batch_results)
            
            # Small delay to prevent overwhelming resources
            await asyncio.sleep(0.1)
            
        return results
    
    def preload_models(self):
        """
        Preload ML models to memory for faster inference
        """
        # Preload embedding model
        self._load_embedding_model()
        
        # Preload NLP models
        self._load_nlp_models()
        
        # Warm up cache
        self._warmup_cache()
    
    @lru_cache(maxsize=1)
    def _load_embedding_model(self):
        from sentence_transformers import SentenceTransformer
        model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
        return model
    
    @lru_cache(maxsize=128)
    def compute_text_hash(self, text: str) -> str:
        """Cached hash computation"""
        return hashlib.sha256(text.encode()).hexdigest()
    
    async def parallel_similarity_computation(
        self, 
        target_embedding: List[float], 
        candidate_embeddings: List[List[float]]
    ) -> List[float]:
        """
        Compute similarities in parallel
        """
        from sklearn.metrics.pairwise import cosine_similarity
        import numpy as np
        
        def compute_batch_similarity(batch_embeddings):
            return cosine_similarity([target_embedding], batch_embeddings)[0]
        
        # Process in smaller batches for memory efficiency
        batch_size = 1000
        similarity_scores = []
        
        for i in range(0, len(candidate_embeddings), batch_size):
            batch = candidate_embeddings[i:i + batch_size]
            batch_similarities = await asyncio.get_event_loop().run_in_executor(
                self.executor, 
                compute_batch_similarity, 
                batch
            )
            similarity_scores.extend(batch_similarities)
            
        return similarity_scores
```

---

## 6. TESTING STRATEGY

### 6.1 Unit Testing Framework

```python
# File: tests/test_duplicate_detection.py

import pytest
import asyncio
from unittest.mock import Mock, patch, AsyncMock
from src.duplicate_detection.hash_detector import HashDuplicateDetector
from src.duplicate_detection.semantic_detector import SemanticDuplicateDetector

class TestHashDuplicateDetector:
    
    @pytest.fixture
    def hash_detector(self):
        return HashDuplicateDetector()
    
    def test_exact_duplicate_detection(self, hash_detector):
        """Test exact duplicate detection using hash"""
        doc1 = "This is a test document with specific content."
        doc2 = "This is a test document with specific content."
        doc3 = "This is a different document with other content."
        
        hash1 = hash_detector.compute_hash(doc1)
        hash2 = hash_detector.compute_hash(doc2)
        hash3 = hash_detector.compute_hash(doc3)
        
        assert hash1 == hash2  # Exact duplicates
        assert hash1 != hash3  # Different content
    
    def test_near_duplicate_detection(self, hash_detector):
        """Test near-duplicate detection using SimHash"""
        doc1 = "The quick brown fox jumps over the lazy dog."
        doc2 = "The quick brown fox jumped over the lazy dog."  # Minor change
        doc3 = "A completely different sentence about cats and mice."
        
        similarity_1_2 = hash_detector.compute_simhash_similarity(doc1, doc2)
        similarity_1_3 = hash_detector.compute_simhash_similarity(doc1, doc3)
        
        assert similarity_1_2 > 0.8  # High similarity
        assert similarity_1_3 < 0.3  # Low similarity
    
    @pytest.mark.asyncio
    async def test_batch_duplicate_detection(self, hash_detector):
        """Test batch processing of duplicate detection"""
        documents = [
            {"id": "doc1", "content": "Content A"},
            {"id": "doc2", "content": "Content A"},  # Duplicate
            {"id": "doc3", "content": "Content B"},
            {"id": "doc4", "content": "Content C"}
        ]
        
        duplicates = await hash_detector.find_duplicates_batch(documents)
        
        assert len(duplicates) == 1
        assert duplicates[0]["documents"] == ["doc1", "doc2"]

class TestSemanticDuplicateDetector:
    
    @pytest.fixture
    def semantic_detector(self):
        return SemanticDuplicateDetector()
    
    @pytest.mark.asyncio
    async def test_semantic_similarity(self, semantic_detector):
        """Test semantic similarity detection"""
        doc1 = "The company's revenue increased significantly last quarter."
        doc2 = "Last quarter, the firm saw a substantial rise in income."  # Semantic duplicate
        doc3 = "Today is a sunny day with clear blue skies."  # Unrelated
        
        similarity_1_2 = await semantic_detector.compute_similarity(doc1, doc2)
        similarity_1_3 = await semantic_detector.compute_similarity(doc1, doc3)
        
        assert similarity_1_2 > 0.7  # High semantic similarity
        assert similarity_1_3 < 0.2  # Low semantic similarity
    
    @pytest.mark.asyncio
    async def test_clustering_similar_documents(self, semantic_detector):
        """Test clustering of semantically similar documents"""
        documents = [
            {"id": "doc1", "content": "Machine learning algorithms"},
            {"id": "doc2", "content": "AI and ML techniques"},  
            {"id": "doc3", "content": "Cooking recipes for dinner"},
            {"id": "doc4", "content": "Food preparation methods"}
        ]
        
        clusters = await semantic_detector.cluster_documents(documents)
        
        assert len(clusters) == 2  # Two clusters expected
        # Verify AI/ML documents are clustered together
        # Verify cooking documents are clustered together
```

### 6.2 Integration Testing

```python
# File: tests/test_integration.py

import pytest
import asyncio
from src.quality_assessment_engine import QualityAssessmentEngine
from src.database.models import QualityAssessment
from tests.fixtures import sample_documents

class TestQualityAssessmentIntegration:
    
    @pytest.fixture
    async def assessment_engine(self):
        """Setup quality assessment engine with test database"""
        engine = QualityAssessmentEngine(config_path="config/test_config.yaml")
        await engine.initialize()
        return engine
    
    @pytest.mark.asyncio
    async def test_end_to_end_quality_assessment(self, assessment_engine):
        """Test complete quality assessment pipeline"""
        test_document = {
            "document_id": "test_doc_001",
            "content": "This is a well-written document with clear content and proper structure.",
            "metadata": {
                "title": "Test Document",
                "author": "test@company.com",
                "department_owner": "IT",
                "access_level": "employee_only",
                "document_type": "policy"
            }
        }
        
        result = await assessment_engine.assess_document(test_document)
        
        # Verify all components were executed
        assert "duplicate_detection" in result
        assert "content_quality" in result
        assert "metadata_validation" in result
        assert "overall_score" in result
        assert "status" in result
        
        # Verify score is calculated correctly
        assert 0 <= result["overall_score"] <= 100
        
        # Verify status is assigned
        assert result["status"] in ["approved", "rejected", "needs_review"]
    
    @pytest.mark.asyncio
    async def test_batch_processing(self, assessment_engine):
        """Test batch processing of multiple documents"""
        test_documents = sample_documents.get_test_batch(50)  # 50 test documents
        
        start_time = time.time()
        results = await assessment_engine.assess_documents_batch(test_documents)
        processing_time = time.time() - start_time
        
        # Verify all documents were processed
        assert len(results) == len(test_documents)
        
        # Verify performance requirement (should process 50 docs within reasonable time)
        assert processing_time < 300  # 5 minutes max for 50 documents
        
        # Verify results structure
        for result in results:
            assert "document_id" in result
            assert "overall_score" in result
            assert "processing_time_ms" in result
    
    @pytest.mark.asyncio
    async def test_database_persistence(self, assessment_engine):
        """Test database storage of assessment results"""
        test_document = sample_documents.get_single_document()
        
        result = await assessment_engine.assess_document(test_document)
        
        # Verify result was saved to database
        saved_assessment = await QualityAssessment.get_by_document_id(
            test_document["document_id"]
        )
        
        assert saved_assessment is not None
        assert saved_assessment.overall_score == result["overall_score"]
        assert saved_assessment.status == result["status"]
    
    @pytest.mark.asyncio
    async def test_error_recovery(self, assessment_engine):
        """Test error handling and recovery mechanisms"""
        # Test with malformed document
        malformed_document = {
            "document_id": "malformed_doc",
            "content": "",  # Empty content
            "metadata": {}  # Missing required fields
        }
        
        with pytest.raises(MetadataValidationError):
            await assessment_engine.assess_document(malformed_document)
        
        # Test with corrupted content
        corrupted_document = {
            "document_id": "corrupted_doc", 
            "content": "\x00\x01\x02",  # Binary content
            "metadata": sample_documents.get_valid_metadata()
        }
        
        result = await assessment_engine.assess_document(corrupted_document)
        # Should not crash, but should have low quality score
        assert result["status"] in ["rejected", "needs_review"]
```

### 6.3 Performance Testing

```python
# File: tests/test_performance.py

import pytest
import asyncio
import time
import psutil
from concurrent.futures import ThreadPoolExecutor
from src.quality_assessment_engine import QualityAssessmentEngine

class TestPerformance:
    
    @pytest.fixture
    async def assessment_engine(self):
        engine = QualityAssessmentEngine()
        await engine.initialize()
        return engine
    
    @pytest.mark.performance
    async def test_throughput_performance(self, assessment_engine):
        """Test system throughput with concurrent requests"""
        num_documents = 100
        test_documents = [
            sample_documents.generate_random_document(i) 
            for i in range(num_documents)
        ]
        
        start_time = time.time()
        start_memory = psutil.virtual_memory().used
        
        # Process documents concurrently
        tasks = []
        for doc in test_documents:
            task = asyncio.create_task(assessment_engine.assess_document(doc))
            tasks.append(task)
        
        results = await asyncio.gather(*tasks)
        
        end_time = time.time()
        end_memory = psutil.virtual_memory().used
        
        processing_time = end_time - start_time
        memory_used = (end_memory - start_memory) / 1024 / 1024  # MB
        
        # Performance assertions
        throughput = num_documents / processing_time
        assert throughput >= 2.0  # At least 2 documents per second
        assert processing_time < 60  # Should complete within 1 minute
        assert memory_used < 1000  # Should use less than 1GB additional memory
        
        print(f"Throughput: {throughput:.2f} docs/sec")
        print(f"Memory usage: {memory_used:.2f} MB")
        print(f"Total time: {processing_time:.2f} seconds")
    
    @pytest.mark.performance
    async def test_memory_efficiency(self, assessment_engine):
        """Test memory usage with large documents"""
        # Generate large document (1MB text)
        large_content = "A" * (1024 * 1024)
        large_document = {
            "document_id": "large_doc",
            "content": large_content,
            "metadata": sample_documents.get_valid_metadata()
        }
        
        memory_before = psutil.virtual_memory().used
        
        result = await assessment_engine.assess_document(large_document)
        
        memory_after = psutil.virtual_memory().used
        memory_increase = (memory_after - memory_before) / 1024 / 1024
        
        # Should not use excessive memory for large documents
        assert memory_increase < 500  # Less than 500MB increase
        assert result is not None
    
    @pytest.mark.performance  
    async def test_cache_effectiveness(self, assessment_engine):
        """Test caching improves performance for repeated operations"""
        test_document = sample_documents.get_single_document()
        
        # First assessment (cold cache)
        start_time = time.time()
        result1 = await assessment_engine.assess_document(test_document)
        first_time = time.time() - start_time
        
        # Second assessment (warm cache)
        start_time = time.time()
        result2 = await assessment_engine.assess_document(test_document)
        second_time = time.time() - start_time
        
        # Cache should improve performance significantly
        assert second_time < first_time * 0.5  # At least 50% faster
        assert result1 == result2  # Results should be identical
```

---

## 7. DEPLOYMENT GUIDE

### 7.1 Production Environment Setup

```bash
#!/bin/bash
# File: scripts/production_setup.sh

# Production deployment script
set -e

echo "🚀 Starting Data Quality Assessment Module Deployment"

# 1. System requirements check
echo "📋 Checking system requirements..."
python3 --version | grep -E "3\.[9-1][0-9]\." || {
    echo "❌ Python 3.9+ required"
    exit 1
}

docker --version || {
    echo "❌ Docker required"
    exit 1
}

# 2. Create production directories
echo "📁 Creating directory structure..."
sudo mkdir -p /opt/data_quality/{logs,data,config,backups}
sudo chown -R $USER:$USER /opt/data_quality

# 3. Setup Python environment
echo "🐍 Setting up Python environment..."
cd /opt/data_quality
python3 -m venv venv_prod
source venv_prod/bin/activate
pip install --upgrade pip
pip install -r requirements.txt

# 4. Database setup
echo "🗄️ Setting up PostgreSQL database..."
sudo -u postgres createdb data_quality_prod
sudo -u postgres createuser dq_prod_user
sudo -u postgres psql -c "ALTER USER dq_prod_user WITH PASSWORD 'prod_password_here';"
sudo -u postgres psql -c "GRANT ALL PRIVILEGES ON DATABASE data_quality_prod TO dq_prod_user;"

# 5. Redis setup
echo "🔴 Configuring Redis..."
sudo systemctl enable redis-server
sudo systemctl start redis-server

# 6. Environment variables
echo "⚙️ Setting up environment variables..."
cat > /opt/data_quality/.env << EOF
ENVIRONMENT=production
DATABASE_URL=postgresql://dq_prod_user:prod_password_here@localhost:5432/data_quality_prod
REDIS_URL=redis://localhost:6379/0
SECRET_KEY=$(openssl rand -base64 32)
LOG_LEVEL=INFO
MAX_WORKERS=8
CACHE_TTL=3600
EOF

# 7. System service setup
echo "🔧 Creating systemd service..."
sudo tee /etc/systemd/system/data-quality-api.service > /dev/null << EOF
[Unit]
Description=Data Quality Assessment API
After=network.target postgresql.service redis.service

[Service]
Type=simple
User=$USER
WorkingDirectory=/opt/data_quality
Environment=PATH=/opt/data_quality/venv_prod/bin
EnvironmentFile=/opt/data_quality/.env
ExecStart=/opt/data_quality/venv_prod/bin/uvicorn src.api.main:app --host 0.0.0.0 --port 8000 --workers 4
Restart=always
RestartSec=3

[Install]
WantedBy=multi-user.target
EOF

# 8. Start services
echo "🚀 Starting services..."
sudo systemctl daemon-reload
sudo systemctl enable data-quality-api
sudo systemctl start data-quality-api

# 9. Health check
echo "🏥 Running health check..."
sleep 10
curl -f http://localhost:8000/health || {
    echo "❌ Health check failed"
    sudo systemctl status data-quality-api
    exit 1
}

echo "✅ Deployment completed successfully!"
echo "📊 Dashboard available at: http://localhost:8000/dashboard"
echo "🔍 API documentation at: http://localhost:8000/docs"
```

### 7.2 Docker Deployment

```dockerfile
# File: Dockerfile
FROM python:3.11-slim

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    curl \
    postgresql-client \
    && rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /app

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Download NLTK data and spaCy models
RUN python -m nltk.downloader punkt stopwords averaged_perceptron_tagger
RUN python -m spacy download en_core_web_sm

# Copy application code
COPY src/ ./src/
COPY config/ ./config/
COPY scripts/ ./scripts/

# Create non-root user
RUN useradd --create-home --shell /bin/bash appuser
RUN chown -R appuser:appuser /app
USER appuser

# Expose port
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Start application
CMD ["uvicorn", "src.api.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

```yaml
# File: docker-compose.yml
version: '3.8'

services:
  data-quality-api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://dq_user:password@postgres:5432/data_quality_db
      - REDIS_URL=redis://redis:6379/0
      - ENVIRONMENT=production
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_started
    volumes:
      - ./logs:/app/logs
      - ./data:/app/data
    restart: unless-stopped

  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: data_quality_db
      POSTGRES_USER: dq_user
      POSTGRES_PASSWORD: password
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init_db.sql:/docker-entrypoint-initdb.d/init_db.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U dq_user -d data_quality_db"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data
    restart: unless-stopped

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - data-quality-api
    restart: unless-stopped

volumes:
  postgres_data:
  redis_data:
```

### 7.3 Monitoring và Alerting

```python
# File: src/monitoring/metrics.py

import time
import psutil
from prometheus_client import Counter, Histogram, Gauge, start_http_server
from functools import wraps

# Prometheus metrics
REQUEST_COUNT = Counter('data_quality_requests_total', 'Total requests', ['method', 'endpoint'])
REQUEST_DURATION = Histogram('data_quality_request_duration_seconds', 'Request duration')
PROCESSING_TIME = Histogram('document_processing_seconds', 'Document processing time')
ACTIVE_ASSESSMENTS = Gauge('active_assessments', 'Number of active quality assessments')
MEMORY_USAGE = Gauge('memory_usage_bytes', 'Memory usage in bytes')
CPU_USAGE = Gauge('cpu_usage_percent', 'CPU usage percentage')

# Error metrics
ERROR_COUNT = Counter('data_quality_errors_total', 'Total errors', ['error_type'])
QUALITY_SCORES = Histogram('quality_scores', 'Distribution of quality scores')

class MetricsCollector:
    def __init__(self):
        self.start_metrics_server()
        
    def start_metrics_server(self, port=9090):
        """Start Prometheus metrics server"""
        start_http_server(port)
        
    def track_request(self, method: str, endpoint: str):
        """Decorator to track API requests"""
        def decorator(func):
            @wraps(func)
            async def wrapper(*args, **kwargs):
                REQUEST_COUNT.labels(method=method, endpoint=endpoint).inc()
                
                with REQUEST_DURATION.time():
                    result = await func(*args, **kwargs)
                    
                return result
            return wrapper
        return decorator
        
    def track_processing_time(self, func):
        """Decorator to track document processing time"""
        @wraps(func)
        async def wrapper(*args, **kwargs):
            start_time = time.time()
            try:
                result = await func(*args, **kwargs)
                PROCESSING_TIME.observe(time.time() - start_time)
                return result
            except Exception as e:
                ERROR_COUNT.labels(error_type=type(e).__name__).inc()
                raise
        return wrapper
        
    def update_system_metrics(self):
        """Update system resource metrics"""
        MEMORY_USAGE.set(psutil.virtual_memory().used)
        CPU_USAGE.set(psutil.cpu_percent())
        
    def record_quality_score(self, score: float):
        """Record quality assessment score"""
        QUALITY_SCORES.observe(score)
        
    def increment_active_assessments(self):
        """Increment active assessment counter"""
        ACTIVE_ASSESSMENTS.inc()
        
    def decrement_active_assessments(self):
        """Decrement active assessment counter"""
        ACTIVE_ASSESSMENTS.dec()

# Usage example
metrics = MetricsCollector()

@metrics.track_request('POST', '/assess-quality')
@metrics.track_processing_time
async def assess_document_quality(document):
    metrics.increment_active_assessments()
    try:
        # Processing logic here
        result = await process_document(document)
        metrics.record_quality_score(result['overall_score'])
        return result
    finally:
        metrics.decrement_active_assessments()
```

```yaml
# File: monitoring/alerting_rules.yml

groups:
  - name: data_quality_alerts
    rules:
      # High error rate
      - alert: HighErrorRate
        expr: rate(data_quality_errors_total[5m]) > 0.1
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "High error rate in data quality assessment"
          description: "Error rate is {{ $value }} errors per second"
          
      # High response time
      - alert: HighResponseTime
        expr: histogram_quantile(0.95, rate(data_quality_request_duration_seconds_bucket[5m])) > 30
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High response time detected"
          description: "95th percentile response time is {{ $value }} seconds"
          
      # High memory usage
      - alert: HighMemoryUsage
        expr: memory_usage_bytes / (1024^3) > 8
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage"
          description: "Memory usage is {{ $value }}GB"
          
      # Service down
      - alert: ServiceDown
        expr: up{job="data-quality-api"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Data Quality API is down"
          description: "Service has been down for more than 1 minute"
          
      # Database connection issues
      - alert: DatabaseConnectionFail
        expr: increase(data_quality_errors_total{error_type="DatabaseError"}[5m]) > 5
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Database connection issues"
          description: "Multiple database connection failures detected"
```

---

## 8. TROUBLESHOOTING GUIDE

### 8.1 Common Issues và Solutions

#### **Issue 1: Memory Usage quá cao**
```bash
# Symptoms
- Process bị kill bởi OOM killer
- Slow response times
- High swap usage

# Diagnosis
htop
free -h
ps aux --sort=-%mem | head -10

# Solutions
# 1. Tăng batch size để giảm concurrent processing
# File: config/config.yaml
processing:
  batch_processing_size: 50  # Giảm từ 100
  max_concurrent_tasks: 4    # Giảm từ 8

# 2. Implement memory cleanup
import gc
import psutil

def cleanup_memory():
    gc.collect()
    if psutil.virtual_memory().percent > 85:
        # Force cleanup embedding cache
        embedding_cache.clear()
        
# 3. Monitor memory usage
def memory_monitor(func):
    @wraps(func)
    async def wrapper(*args, **kwargs):
        before = psutil.virtual_memory().used
        result = await func(*args, **kwargs)
        after = psutil.virtual_memory().used
        if (after - before) > 500 * 1024 * 1024:  # 500MB
            logging.warning(f"High memory usage: {(after-before)/1024/1024:.2f}MB")
        return result
    return wrapper
```

#### **Issue 2: Embedding Model Loading Lỗi**
```python
# Symptoms
- ImportError cho sentence-transformers
- CUDA out of memory errors
- Model loading timeout

# Diagnosis
try:
    from sentence_transformers import SentenceTransformer
    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
    print("✅ Model loaded successfully")
except Exception as e:
    print(f"❌ Model loading failed: {e}")

# Solutions
# 1. Fallback model loading
def load_embedding_model_with_fallback():
    models_to_try = [
        'sentence-transformers/all-MiniLM-L6-v2',
        'sentence-transformers/all-mpnet-base-v2',
        'sentence-transformers/paraphrase-MiniLM-L6-v2'
    ]
    
    for model_name in models_to_try:
        try:
            model = SentenceTransformer(model_name)
            logging.info(f"✅ Loaded model: {model_name}")
            return model
        except Exception as e:
            logging.warning(f"❌ Failed to load {model_name}: {e}")
            continue
    
    raise RuntimeError("Failed to load any embedding model")

# 2. CPU-only mode for memory constraints
model = SentenceTransformer(
    'sentence-transformers/all-MiniLM-L6-v2',
    device='cpu'  # Force CPU mode
)
```

#### **Issue 3: Database Connection Pool Exhausted**
```python
# Symptoms
- "pool limit exceeded" errors
- Slow database queries
- Connection timeout errors

# Diagnosis
# Monitor active connections
SELECT count(*) as active_connections 
FROM pg_stat_activity 
WHERE state = 'active';

SELECT pid, state, query_start, query 
FROM pg_stat_activity 
WHERE state != 'idle' 
ORDER BY query_start;

# Solutions
# 1. Connection pool tuning
# File: src/database/connection.py
from sqlalchemy.pool import QueuePool

engine = create_engine(
    DATABASE_URL,
    poolclass=QueuePool,
    pool_size=20,          # Increase pool size
    max_overflow=30,       # Allow overflow
    pool_recycle=3600,     # Recycle connections every hour
    pool_pre_ping=True,    # Verify connections before use
    pool_timeout=30        # Timeout for getting connection
)

# 2. Proper connection management
async def get_db_session():
    async with AsyncSession() as session:
        try:
            yield session
            await session.commit()
        except Exception:
            await session.rollback()
            raise
        finally:
            await session.close()

# 3. Connection monitoring
async def monitor_db_connections():
    while True:
        active = await get_active_connection_count()
        if active > 15:  # 75% of pool size
            logging.warning(f"High DB connection usage: {active}")
        await asyncio.sleep(30)
```

#### **Issue 4: Redis Cache Performance Issues**
```python
# Symptoms
- Slow cache lookups
- Memory errors in Redis
- Cache miss rate cao

# Diagnosis
redis-cli info memory
redis-cli info stats
redis-cli monitor  # Watch real-time commands

# Solutions
# 1. Memory optimization
# redis.conf
maxmemory 4gb
maxmemory-policy allkeys-lru
maxmemory-samples 10

# 2. Connection pooling
import aioredis

redis_pool = aioredis.ConnectionPool.from_url(
    "redis://localhost:6379",
    max_connections=20,
    retry_on_timeout=True,
    health_check_interval=30
)

# 3. Async Redis operations với timeout
async def safe_redis_get(key: str, timeout: int = 5):
    try:
        return await asyncio.wait_for(
            redis_client.get(key),
            timeout=timeout
        )
    except asyncio.TimeoutError:
        logging.warning(f"Redis timeout for key: {key}")
        return None
```

### 8.2 Health Check System

```python
# File: src/monitoring/health_check.py

import asyncio
import time
from typing import Dict, Any
from enum import Enum

class HealthStatus(Enum):
    HEALTHY = "healthy"
    DEGRADED = "degraded"
    UNHEALTHY = "unhealthy"

class HealthChecker:
    def __init__(self):
        self.checks = {}
        
    def register_check(self, name: str, check_func, timeout: int = 10):
        """Register a health check function"""
        self.checks[name] = {
            'func': check_func,
            'timeout': timeout,
            'last_result': None,
            'last_check': None
        }
    
    async def run_all_checks(self) -> Dict[str, Any]:
        """Run all registered health checks"""
        results = {
            'status': HealthStatus.HEALTHY.value,
            'timestamp': time.time(),
            'checks': {}
        }
        
        overall_healthy = True
        
        for name, check_info in self.checks.items():
            try:
                result = await asyncio.wait_for(
                    check_info['func'](),
                    timeout=check_info['timeout']
                )
                
                results['checks'][name] = {
                    'status': HealthStatus.HEALTHY.value,
                    'response_time_ms': result.get('response_time_ms', 0),
                    'details': result.get('details', {})
                }
                
            except asyncio.TimeoutError:
                results['checks'][name] = {
                    'status': HealthStatus.UNHEALTHY.value,
                    'error': 'Timeout',
                    'response_time_ms': check_info['timeout'] * 1000
                }
                overall_healthy = False
                
            except Exception as e:
                results['checks'][name] = {
                    'status': HealthStatus.UNHEALTHY.value,
                    'error': str(e),
                    'response_time_ms': 0
                }
                overall_healthy = False
        
        # Determine overall status
        unhealthy_count = sum(
            1 for check in results['checks'].values()
            if check['status'] == HealthStatus.UNHEALTHY.value
        )
        
        if not overall_healthy:
            if unhealthy_count >= len(self.checks) / 2:
                results['status'] = HealthStatus.UNHEALTHY.value
            else:
                results['status'] = HealthStatus.DEGRADED.value
        
        return results

# Health check functions
async def check_database():
    """Check database connectivity and performance"""
    start_time = time.time()
    try:
        # Simple query to test connection
        result = await database.fetch_one("SELECT 1 as test")
        response_time = (time.time() - start_time) * 1000
        
        return {
            'response_time_ms': response_time,
            'details': {'connection': 'ok', 'query_result': result['test']}
        }
    except Exception as e:
        raise Exception(f"Database check failed: {e}")

async def check_redis():
    """Check Redis connectivity and performance"""
    start_time = time.time()
    try:
        test_key = "health_check_test"
        await redis_client.set(test_key, "test_value", ex=60)
        value = await redis_client.get(test_key)
        await redis_client.delete(test_key)
        
        response_time = (time.time() - start_time) * 1000
        
        return {
            'response_time_ms': response_time,
            'details': {'connection': 'ok', 'read_write': 'ok'}
        }
    except Exception as e:
        raise Exception(f"Redis check failed: {e}")

async def check_embedding_model():
    """Check embedding model availability"""
    start_time = time.time()
    try:
        test_text = "This is a test sentence for health check"
        embedding = await embedding_service.get_embedding(test_text)
        response_time = (time.time() - start_time) * 1000
        
        return {
            'response_time_ms': response_time,
            'details': {
                'model_loaded': True,
                'embedding_size': len(embedding)
            }
        }
    except Exception as e:
        raise Exception(f"Embedding model check failed: {e}")

async def check_disk_space():
    """Check available disk space"""
    import shutil
    try:
        total, used, free = shutil.disk_usage('/opt/data_quality')
        free_percent = (free / total) * 100
        
        if free_percent < 10:
            raise Exception(f"Low disk space: {free_percent:.1f}% free")
        
        return {
            'response_time_ms': 1,
            'details': {
                'total_gb': round(total / (1024**3), 2),
                'free_gb': round(free / (1024**3), 2),
                'free_percent': round(free_percent, 1)
            }
        }
    except Exception as e:
        raise Exception(f"Disk space check failed: {e}")

# Initialize health checker
health_checker = HealthChecker()
health_checker.register_check('database', check_database, timeout=10)
health_checker.register_check('redis', check_redis, timeout=5)
health_checker.register_check('embedding_model', check_embedding_model, timeout=15)
health_checker.register_check('disk_space', check_disk_space, timeout=5)
```

### 8.3 Log Analysis Tools

```bash
#!/bin/bash
# File: scripts/analyze_logs.sh

# Log analysis script for troubleshooting

LOG_FILE="/opt/data_quality/logs/data_quality.log"
DATE_PATTERN=$(date +"%Y-%m-%d")

echo "📊 Data Quality Module Log Analysis - $DATE_PATTERN"
echo "=================================================="

# Error summary
echo ""
echo "🔥 ERROR SUMMARY:"
echo "----------------"
grep "ERROR" $LOG_FILE | grep "$DATE_PATTERN" | \
    sed 's/.*ERROR - //' | sort | uniq -c | sort -nr

# Performance issues
echo ""
echo "⏱️ SLOW OPERATIONS (>30 seconds):"
echo "---------------------------------"
grep "processing_time" $LOG_FILE | grep "$DATE_PATTERN" | \
    awk '$NF > 30000 {print $0}' | tail -10

# Memory warnings
echo ""
echo "💾 MEMORY WARNINGS:"
echo "------------------"
grep -i "memory" $LOG_FILE | grep "$DATE_PATTERN" | tail -10

# Database connection issues
echo ""
echo "🗄️ DATABASE ISSUES:"
echo "------------------"
grep -i "database\|connection\|pool" $LOG_FILE | grep "ERROR\|WARNING" | \
    grep "$DATE_PATTERN" | tail -10

# Quality assessment statistics
echo ""
echo "📈 QUALITY ASSESSMENT STATS:"
echo "----------------------------"
echo "Total assessments today: $(grep "assessment completed" $LOG_FILE | grep "$DATE_PATTERN" | wc -l)"
echo "Approved documents: $(grep "status.*approved" $LOG_FILE | grep "$DATE_PATTERN" | wc -l)"
echo "Rejected documents: $(grep "status.*rejected" $LOG_FILE | grep "$DATE_PATTERN" | wc -l)"

# Top error types
echo ""
echo "🚨 TOP ERROR TYPES:"
echo "------------------"
grep "ERROR" $LOG_FILE | grep "$DATE_PATTERN" | \
    sed -n 's/.*\(Error\|Exception\): \([^:]*\).*/\2/p' | \
    sort | uniq -c | sort -nr | head -5
```

---

## 9. MAINTENANCE VÀ OPTIMIZATION

### 9.1 Database Maintenance

```sql
-- File: scripts/db_maintenance.sql

-- Daily maintenance tasks
-- 1. Update table statistics
ANALYZE quality_assessments;
ANALYZE duplicate_pairs;
ANALYZE quality_rules;

-- 2. Clean old assessment data (older than 90 days)
DELETE FROM quality_assessments 
WHERE created_at < NOW() - INTERVAL '90 days';

-- 3. Rebuild indexes if fragmented
REINDEX INDEX CONCURRENTLY idx_quality_assessments_document_id;
REINDEX INDEX CONCURRENTLY idx_quality_assessments_date;

-- 4. Check for unused indexes
SELECT 
    schemaname, 
    tablename, 
    attname, 
    n_distinct, 
    correlation 
FROM pg_stats 
WHERE schemaname = 'public' 
    AND n_distinct < 10;

-- 5. Monitor query performance
SELECT 
    query,
    calls,
    total_time,
    mean_time,
    rows
FROM pg_stat_statements 
WHERE query LIKE '%quality_assessments%'
ORDER BY total_time DESC
LIMIT 10;
```

### 9.2 Performance Optimization

```python
# File: src/optimization/batch_optimizer.py

import asyncio
from typing import List, Dict, Any
from concurrent.futures import ThreadPoolExecutor

class BatchOptimizer:
    def __init__(self, max_workers: int = 8):
        self.max_workers = max_workers
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        
    async def optimize_duplicate_detection(self, documents: List[Dict[str, Any]]):
        """
        Optimize duplicate detection using clustering và parallel processing
        """
        # Step 1: Quick hash-based filtering
        hash_groups = self._group_by_hash(documents)
        
        # Step 2: Semantic clustering for remaining documents
        semantic_candidates = []
        for group in hash_groups.values():
            if len(group) == 1:  # No hash duplicates
                semantic_candidates.extend(group)
        
        # Step 3: Parallel semantic analysis
        if semantic_candidates:
            semantic_duplicates = await self._parallel_semantic_analysis(
                semantic_candidates
            )
        
        return {
            'hash_duplicates': [g for g in hash_groups.values() if len(g) > 1],
            'semantic_duplicates': semantic_duplicates
        }
    
    def _group_by_hash(self, documents: List[Dict[str, Any]]) -> Dict[str, List]:
        """Group documents by content hash"""
        hash_groups = {}
        for doc in documents:
            content_hash = hashlib.sha256(doc['content'].encode()).hexdigest()
            if content_hash not in hash_groups:
                hash_groups[content_hash] = []
            hash_groups[content_hash].append(doc)
        return hash_groups
    
    async def _parallel_semantic_analysis(self, documents: List[Dict[str, Any]]):
        """Run semantic analysis in parallel batches"""
        batch_size = 50
        batches = [
            documents[i:i + batch_size] 
            for i in range(0, len(documents), batch_size)
        ]
        
        tasks = []
        for batch in batches:
            task = asyncio.create_task(self._process_semantic_batch(batch))
            tasks.append(task)
        
        results = await asyncio.gather(*tasks)
        
        # Flatten results
        all_duplicates = []
        for batch_result in results:
            all_duplicates.extend(batch_result)
        
        return all_duplicates
    
    async def _process_semantic_batch(self, batch: List[Dict[str, Any]]):
        """Process a batch of documents for semantic similarity"""
        # Implementation would use clustering algorithms
        # like DBSCAN or hierarchical clustering
        pass

# Usage
optimizer = BatchOptimizer(max_workers=8)
```

### 9.3 Caching Strategy

```python
# File: src/caching/cache_manager.py

import json
import hashlib
from typing import Any, Optional, Dict
from datetime import datetime, timedelta

class CacheManager:
    def __init__(self, redis_client):
        self.redis = redis_client
        self.default_ttl = 3600  # 1 hour
        
    async def get_cached_result(
        self, 
        key: str, 
        compute_func: callable,
        ttl: Optional[int] = None
    ) -> Any:
        """
        Get cached result or compute và cache new result
        """
        ttl = ttl or self.default_ttl
        
        # Try to get from cache
        cached = await self.redis.get(key)
        if cached:
            try:
                return json.loads(cached)
            except json.JSONDecodeError:
                await self.redis.delete(key)  # Invalid cache entry
        
        # Compute new result
        result = await compute_func()
        
        # Cache result
        await self.redis.setex(
            key, 
            ttl, 
            json.dumps(result, default=str)  # Handle datetime serialization
        )
        
        return result
    
    def generate_cache_key(self, prefix: str, **kwargs) -> str:
        """Generate consistent cache key từ parameters"""
        key_parts = [prefix]
        
        # Sort kwargs for consistent ordering
        for k, v in sorted(kwargs.items()):
            if isinstance(v, (dict, list)):
                v = json.dumps(v, sort_keys=True)
            key_parts.append(f"{k}:{v}")
        
        key_string = "|".join(key_parts)
        
        # Hash long keys
        if len(key_string) > 200:
            key_hash = hashlib.md5(key_string.encode()).hexdigest()
            return f"{prefix}:hash:{key_hash}"
        
        return key_string
    
    async def invalidate_pattern(self, pattern: str):
        """Invalidate cache keys matching pattern"""
        keys = await self.redis.keys(pattern)
        if keys:
            await self.redis.delete(*keys)
    
    async def warm_up_cache(self, document_ids: List[str]):
        """Pre-warm cache với commonly accessed documents"""
        for doc_id in document_ids:
            cache_key = self.generate_cache_key("quality_assessment", document_id=doc_id)
            
            # Check if already cached
            exists = await self.redis.exists(cache_key)
            if not exists:
                # Compute và cache result
                await self.get_cached_result(
                    cache_key,
                    lambda: self._compute_quality_assessment(doc_id),
                    ttl=7200  # 2 hours for warm-up cache
                )

# Cache decorators
def cached_method(ttl: int = 3600, key_prefix: str = None):
    """Decorator for caching method results"""
    def decorator(func):
        @wraps(func)
        async def wrapper(self, *args, **kwargs):
            cache_key = cache_manager.generate_cache_key(
                key_prefix or func.__name__,
                args=args,
                kwargs=kwargs
            )
            
            return await cache_manager.get_cached_result(
                cache_key,
                lambda: func(self, *args, **kwargs),
                ttl=ttl
            )
        return wrapper
    return decorator
```

---

## 10. DOCUMENTATION VÀ HANDOVER

### 10.1 API Documentation

```python
# File: src/api/main.py - Enhanced với detailed documentation

from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks
from fastapi.openapi.docs import get_swagger_ui_html
from fastapi.openapi.utils import get_openapi

app = FastAPI(
    title="Data Quality Assessment API",
    description="""
    ## Data Quality Assessment Tool
    
    This API provides comprehensive data quality assessment capabilities:
    
    * **Duplicate Detection**: Hash-based and semantic duplicate detection
    * **Content Quality**: Ambiguity, contradiction, and readability analysis  
    * **Metadata Validation**: Completeness and format validation
    * **Quality Scoring**: Configurable scoring engine with business rules
    * **Batch Processing**: Efficient processing of multiple documents
    * **Real-time Monitoring**: Health checks and performance metrics
    
    ### Authentication
    API uses JWT tokens. Include in Authorization header: `Bearer <token>`
    
    ### Rate Limiting  
    - Standard users: 100 requests/hour
    - Premium users: 1000 requests/hour
    
    ### Error Handling
    All errors follow RFC 7807 Problem Details format.
    """,
    version="1.0.0",
    terms_of_service="http://company.com/terms/",
    contact={
        "name": "Data Quality Team",
        "url": "http://company.com/contact/",
        "email": "data-quality@company.com",
    },
    license_info={
        "name": "Internal Use License",
        "url": "http://company.com/license/",
    },
)

@app.post(
    "/api/v1/assess-quality",
    response_model=QualityAssessmentResponse,
    summary="Assess Document Quality",
    description="""
    Perform comprehensive quality assessment on a single document.
    
    The assessment includes:
    1. **Duplicate Detection**: Checks against existing documents
    2. **Content Analysis**: Evaluates readability, ambiguity, contradictions
    3. **Metadata Validation**: Ensures required fields are present and valid
    4. **Quality Scoring**: Calculates overall quality score (0-100)
    
    ### Processing Time
    - Small documents (<10KB): ~2-5 seconds
    - Medium documents (10-100KB): ~5-15 seconds  
    - Large documents (>100KB): ~15-60 seconds
    
    ### Quality Thresholds
    - **90-100**: Excellent quality, auto-approved
    - **75-89**: Good quality, minor issues flagged
    - **60-74**: Acceptable quality, review recommended
    - **40-59**: Needs improvement, manual review required
    - **0-39**: Poor quality, rejected
    """,
    responses={
        200: {"description": "Assessment completed successfully"},
        400: {"description": "Invalid document format or missing required fields"},
        422: {"description": "Validation error in request data"},
        429: {"description": "Rate limit exceeded"},
        500: {"description": "Internal server error during assessment"},
    }
)
async def assess_document_quality(
    request: QualityAssessmentRequest,
    background_tasks: BackgroundTasks,
    current_user: dict = Depends(get_current_user)
):
    """Implementation here"""
    pass
```

### 10.2 Team Handover Document

```markdown
# HANDOVER DOCUMENT - DATA QUALITY ASSESSMENT MODULE

## 📋 OVERVIEW
Module: Data Quality Assessment Tool (FR-03.2)
Status: Production Ready
Handover Date: [Current Date]
Team: Backend Development Team → Operations Team

## 🎯 WHAT THIS MODULE DOES
- Detects duplicate documents (exact, semantic, keyword-based)
- Analyzes content quality (readability, ambiguity, contradictions)
- Validates metadata completeness and format
- Generates quality scores and reports
- Provides visual dashboards for quality monitoring

## 🏗️ TECHNICAL ARCHITECTURE

### Core Components
1. **Duplicate Detection Engine**
   - Hash-based: Uses SHA256 for exact matches
   - Semantic: Uses sentence-transformers for meaning-based detection
   - Keyword: Uses TF-IDF and fuzzy matching

2. **Content Quality Analyzer**
   - Readability: Flesch-Kincaid, SMOG, Coleman-Liau scores
   - Ambiguity: NLP-based detection of unclear content
   - Contradictions: Uses NLI models to find conflicting statements

3. **Metadata Validator**
   - Required fields validation
   - Format validation (dates, emails, enums)
   - Business rule validation

4. **Scoring Engine**
   - Configurable weights and thresholds
   - Business rules engine
   - Pass/fail determination

### Dependencies
- **Python 3.9+**: Core runtime
- **PostgreSQL 15+**: Metadata and results storage
- **Redis 7+**: Caching and session storage
- **sentence-transformers**: Embedding generation
- **FastAPI**: REST API framework
- **Docker**: Containerization

## 📁 KEY FILES AND LOCATIONS

```
/opt/data_quality/
├── src/
│   ├── duplicate_detection/     # Duplicate detection logic
│   ├── content_analysis/        # Content quality analysis
│   ├── metadata_validation/     # Metadata validation
│   ├── scoring/                 # Quality scoring engine
│   ├── api/                     # REST API endpoints
│   └── dashboard/               # Web dashboard
├── config/
│   ├── config.yaml             # Main configuration
│   ├── database.yaml           # Database settings
│   └── scoring_rules.yaml      # Quality scoring rules
├── logs/                       # Application logs
├── scripts/                    # Maintenance scripts
└── tests/                      # Test suites
```

## 🔧 OPERATIONS GUIDE

### Daily Tasks
1. **Monitor System Health**
   ```bash
   curl -f http://localhost:8000/health
   ```

2. **Check Log Files**
   ```bash
   tail -f /opt/data_quality/logs/data_quality.log
   ```

3. **Database Maintenance**
   ```bash
   /opt/data_quality/scripts/daily_maintenance.sh
   ```

### Weekly Tasks
1. **Performance Review**
   - Check Grafana dashboards
   - Review slow query logs
   - Analyze quality trends

2. **Capacity Planning**
   - Monitor disk usage
   - Check memory consumption
   - Review database growth

### Monthly Tasks
1. **Model Updates**
   - Update embedding models if available
   - Retrain custom models with new data
   - Update business rules

2. **Security Review**
   - Update dependencies
   - Review access logs
   - Security scan

## 🚨 ALERT RESPONSES

### Critical Alerts (Immediate Response Required)

#### Service Down
```bash
# Check service status
sudo systemctl status data-quality-api

# Restart if needed
sudo systemctl restart data-quality-api

# Check logs for errors
sudo journalctl -u data-quality-api -f
```

#### Database Connection Failed
```bash
# Check PostgreSQL status
sudo systemctl status postgresql

# Check connections
sudo -u postgres psql -c "SELECT count(*) FROM pg_stat_activity;"

# Restart connection pool if needed
sudo systemctl restart data-quality-api
```

#### High Memory Usage
```bash
# Check memory usage
free -h
ps aux --sort=-%mem | head -10

# Restart service to clear memory
sudo systemctl restart data-quality-api
```

### Warning Alerts (Response Within 1 Hour)

#### High Response Time
- Check slow query log
- Review current processing load
- Consider scaling resources

#### High Error Rate
- Review application logs
- Check for pattern in errors
- Investigate root cause

## 📊 MONITORING AND METRICS

### Key Metrics to Watch
1. **Performance Metrics**
   - Average response time: < 30 seconds
   - Throughput: > 2 documents/second
   - Error rate: < 1%

2. **Resource Metrics**
   - Memory usage: < 8GB
   - CPU usage: < 80%
   - Disk usage: < 90%

3. **Business Metrics**
   - Daily assessments processed
   - Quality score distribution
   - Approval/rejection rates

### Dashboards
- **Grafana**: http://localhost:3000/d/data-quality-overview
- **Application Dashboard**: http://localhost:8000/dashboard
- **Prometheus Metrics**: http://localhost:9090

### Log Locations
- **Application Logs**: `/opt/data_quality/logs/data_quality.log`
- **API Access Logs**: `/opt/data_quality/logs/api_access.log`
- **Error Logs**: `/opt/data_quality/logs/errors.log`
- **System Logs**: `journalctl -u data-quality-api`

## 🛠️ CONFIGURATION MANAGEMENT

### Environment Variables
```bash
# Production environment
export ENVIRONMENT=production
export DATABASE_URL=postgresql://dq_prod_user:password@localhost:5432/data_quality_prod
export REDIS_URL=redis://localhost:6379/0
export LOG_LEVEL=INFO
export MAX_WORKERS=8
export CACHE_TTL=3600
```

### Configuration Files
1. **Main Config** (`config/config.yaml`)
   - Database connections
   - Redis settings  
   - Quality thresholds
   - Processing parameters

2. **Scoring Rules** (`config/scoring_rules.yaml`)
   - Weight configurations
   - Business rules
   - Threshold settings

3. **Model Config** (`config/models.yaml`)
   - Embedding model settings
   - NLP model parameters
   - Performance tuning

### Making Configuration Changes
1. Edit configuration file
2. Validate syntax: `python scripts/validate_config.py`
3. Test in staging environment
4. Apply to production: `sudo systemctl restart data-quality-api`
5. Monitor for issues

## 🔄 DEPLOYMENT PROCEDURES

### Regular Deployment (Code Updates)
```bash
# 1. Backup current version
sudo cp -r /opt/data_quality /opt/data_quality_backup_$(date +%Y%m%d)

# 2. Pull latest code
cd /opt/data_quality
git pull origin main

# 3. Update dependencies
source venv_prod/bin/activate
pip install -r requirements.txt

# 4. Run database migrations
python scripts/migrate_db.py

# 5. Run tests
python -m pytest tests/ -v

# 6. Restart service
sudo systemctl restart data-quality-api

# 7. Health check
sleep 10
curl -f http://localhost:8000/health || echo "Deployment failed!"
```

### Emergency Rollback
```bash
# 1. Stop current service
sudo systemctl stop data-quality-api

# 2. Restore backup
sudo rm -rf /opt/data_quality
sudo mv /opt/data_quality_backup_$(date +%Y%m%d) /opt/data_quality

# 3. Restore database if needed
sudo -u postgres psql data_quality_prod < /opt/backups/db_backup_$(date +%Y%m%d).sql

# 4. Start service
sudo systemctl start data-quality-api

# 5. Verify
curl -f http://localhost:8000/health
```

### Model Updates
```bash
# 1. Download new model
cd /opt/data_quality
python scripts/download_model.py --model sentence-transformers/all-mpnet-base-v2

# 2. Test model performance
python scripts/benchmark_model.py --model all-mpnet-base-v2

# 3. Update configuration
vim config/models.yaml

# 4. Restart with new model
sudo systemctl restart data-quality-api
```

## 🐛 TROUBLESHOOTING GUIDE

### Common Issues và Quick Fixes

#### 1. "Model loading timeout"
```bash
# Check available memory
free -h

# If memory is low, restart service
sudo systemctl restart data-quality-api

# If persistent, switch to CPU-only mode
vim config/models.yaml
# Set: device: 'cpu'
```

#### 2. "Database connection pool exhausted"
```bash
# Check active connections
sudo -u postgres psql data_quality_prod -c "SELECT count(*) FROM pg_stat_activity WHERE state='active';"

# Kill long-running queries
sudo -u postgres psql data_quality_prod -c "SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE state='active' AND query_start < NOW() - INTERVAL '5 minutes';"

# Restart API service
sudo systemctl restart data-quality-api
```

#### 3. "Redis connection failed"
```bash
# Check Redis status
sudo systemctl status redis-server

# Test connection
redis-cli ping

# Clear Redis if corrupted
redis-cli flushall
sudo systemctl restart redis-server
```

#### 4. "High memory usage"
```bash
# Check memory usage by process
ps aux --sort=-%mem | head -10

# Check for memory leaks
python scripts/memory_profile.py

# Restart service
sudo systemctl restart data-quality-api
```

### Log Analysis Commands
```bash
# Find errors in last 24 hours
grep -i error /opt/data_quality/logs/data_quality.log | grep $(date +"%Y-%m-%d")

# Performance analysis
grep "processing_time" /opt/data_quality/logs/data_quality.log | awk '{print $NF}' | sort -n | tail -10

# Memory warnings
grep -i "memory" /opt/data_quality/logs/data_quality.log | tail -20

# Database errors
grep -i "database\|connection" /opt/data_quality/logs/data_quality.log | grep ERROR
```

## 📞 ESCALATION CONTACTS

### Primary Contacts
- **Technical Lead**: John Smith (john.smith@company.com, +1-xxx-xxx-xxxx)
- **DevOps Engineer**: Jane Doe (jane.doe@company.com, +1-xxx-xxx-xxxx)  
- **Database Administrator**: Mike Johnson (mike.johnson@company.com, +1-xxx-xxx-xxxx)

### Secondary Contacts  
- **Product Owner**: Sarah Wilson (sarah.wilson@company.com)
- **IT Support**: support@company.com, +1-xxx-xxx-xxxx

### Emergency Escalation
1. **Level 1**: Service degradation → DevOps Engineer
2. **Level 2**: Service down → Technical Lead  
3. **Level 3**: Data corruption → Database Administrator + Technical Lead
4. **Level 4**: Security incident → CISO + Technical Lead

## 📚 KNOWLEDGE BASE

### Useful Commands
```bash
# Check service status
sudo systemctl status data-quality-api

# View real-time logs
sudo journalctl -u data-quality-api -f

# Check API health
curl -s http://localhost:8000/health | jq '.'

# Monitor system resources
htop

# Check database performance
sudo -u postgres psql data_quality_prod -c "\x" -c "SELECT * FROM pg_stat_statements ORDER BY total_time DESC LIMIT 5;"

# Redis monitoring
redis-cli monitor

# Test embedding model
python -c "from src.models import EmbeddingService; es = EmbeddingService(); print('Model loaded successfully')"
```

### Configuration Examples
```yaml
# High-performance configuration for busy periods
processing:
  max_concurrent_tasks: 12
  batch_processing_size: 200
  timeout_seconds: 120

# Memory-conservative configuration
processing:
  max_concurrent_tasks: 4
  batch_processing_size: 50
  timeout_seconds: 300

# Quality-focused configuration
scoring:
  thresholds:
    excellent: 95.0
    good: 85.0
    acceptable: 70.0
```

## 🔧 MAINTENANCE SCRIPTS

### Daily Maintenance (`scripts/daily_maintenance.sh`)
- Database cleanup
- Log rotation
- Cache optimization
- Health checks

### Weekly Reports (`scripts/weekly_report.py`)
- Performance metrics
- Quality trends
- Error analysis
- Resource utilization

### Monthly Tasks (`scripts/monthly_tasks.sh`)
- Database vacuum and reindex
- Model performance review
- Security updates
- Capacity planning

## 📈 PERFORMANCE BENCHMARKS

### Expected Performance (Production Load)
- **Document Processing**: 2-5 documents/second
- **API Response Time**: < 30 seconds (95th percentile)
- **Memory Usage**: 4-8GB under normal load
- **CPU Usage**: 40-70% under normal load
- **Database Response**: < 100ms for typical queries

### Load Test Results
```bash
# Run load test
python scripts/load_test.py --concurrent 50 --duration 300

# Expected results:
# - 50 concurrent users
# - 300 second duration  
# - 0% error rate
# - Average response time < 45 seconds
```

## 🚀 FUTURE ROADMAP

### Planned Improvements (Next 3 months)
1. **Performance Optimization**
   - Implement async batch processing
   - Add GPU acceleration for embeddings
   - Optimize database queries

2. **Feature Enhancements**
   - Multi-language support
   - Custom quality rules per department
   - Advanced visualization dashboards

3. **Operational Improvements**
   - Automated scaling
   - Enhanced monitoring
   - Predictive alerting

### Technical Debt
1. **Code Refactoring**
   - Split monolithic modules
   - Improve error handling
   - Add more comprehensive tests

2. **Infrastructure**
   - Migrate to Kubernetes
   - Implement blue-green deployment
   - Add disaster recovery

## ✅ HANDOVER CHECKLIST

### Technical Handover
- [ ] Code repository access provided
- [ ] Production environment access granted
- [ ] Database credentials shared securely
- [ ] Monitoring dashboards configured
- [ ] Alert configurations verified
- [ ] Documentation reviewed and updated
- [ ] Runbooks tested and validated

### Operational Handover
- [ ] Maintenance schedules documented
- [ ] Escalation procedures established
- [ ] Team contacts updated
- [ ] Knowledge transfer sessions completed
- [ ] Emergency procedures tested
- [ ] Backup and recovery verified

### Knowledge Transfer
- [ ] Architecture walkthrough completed
- [ ] Troubleshooting scenarios practiced  
- [ ] Configuration management explained
- [ ] Performance expectations set
- [ ] Future roadmap discussed

---

**Document Version**: 1.0
**Last Updated**: [Current Date]
**Next Review**: [Current Date + 3 months]

For questions or clarifications, contact the Technical Lead or refer to the internal wiki at: `http://wiki.company.com/data-quality-module`
```

---

## 📋 FINAL SUMMARY

Đây là tài liệu thiết kế hoàn chỉnh cho **Module Công cụ Đánh giá Chất lượng Dữ liệu (FR-03.2)** bao gồm:

### ✅ **ĐÃ HOÀN THÀNH**

1. **📋 Tổng quan Module** - Mục đích, vị trí trong hệ thống, Input/Output
2. **💻 Chuẩn bị Môi trường** - Hardware, software, dependencies chi tiết
3. **🏗️ Kiến trúc Module** - Architecture diagram với Mermaid, luồng xử lý
4. **📅 Implementation Roadmap** - 4 tuần chi tiết với tasks cụ thể
5. **🔧 Technical Implementation** - Database schema, API spec, configuration
6. **⚠️ Error Handling** - Comprehensive error management với retry logic
7. **⚡ Performance Optimization** - Caching, batch processing, memory management
8. **🧪 Testing Strategy** - Unit, integration, performance testing
9. **🚀 Deployment Guide** - Production setup, Docker, monitoring
10. **🛠️ Troubleshooting** - Common issues, health checks, log analysis
11. **📚 Documentation** - API docs, handover document, operations guide

### 🎯 **KEY FEATURES**

- **Comprehensive Quality Assessment**: Duplicate detection, content analysis, metadata validation
- **Production-Ready**: Full deployment pipeline với monitoring và alerting
- **Scalable Architecture**: Async processing, caching, load balancing
- **Robust Error Handling**: Retry logic, fallback mechanisms, graceful degradation
- **Operational Excellence**: Health checks, metrics, troubleshooting guides

### 📊 **SUCCESS CRITERIA**

- ✅ **Performance**: < 60 giây response time, 2+ docs/second throughput
- ✅ **Reliability**: 99.5% uptime, comprehensive error handling
- ✅ **Maintainability**: Clear documentation, monitoring, troubleshooting
- ✅ **Scalability**: Support 100+ concurrent users, horizontal scaling ready

Team kỹ thuật có thể **follow step-by-step** guide này để implement module một cách tối ưu và production-ready! 🚀
