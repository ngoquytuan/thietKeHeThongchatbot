Náº¿u Ä‘Ã£ Ä‘á»§ thÃ´ng tin hÃ£y giÃºp tÃ´i viáº¿t tÃ i liá»‡u thiáº¿t káº¿ module dÆ°á»›i Ä‘Ã¢y.
TÃ i liá»‡u nÃ y cung cáº¥p roadmap Ä‘áº§y Ä‘á»§ Ä‘á»ƒ implement FR-03.2 mÃ  khÃ´ng cáº§n code cá»¥ thá»ƒ nhÆ°ng 
cÃ³ cÃ¡c bÆ°á»›c lÃ m cá»¥ thá»ƒ nhÆ° chuáº©n bá»‹ mÃ´i trÆ°á»ng PC tháº¿ nÃ o, pháº§n má»m gÃ¬?
 Ä‘á»ƒ Team ká»¹ thuáº­t cÃ³ thá»ƒ follow step-by-step guide nÃ y Ä‘á»ƒ thiáº¿t káº¿ tá»‘i Æ°u cho há»‡ thá»‘ng chatbot
 vÃ  cÃ³ mermaidchart luá»“ng cÃ´ng viá»‡c, Ä‘iá»ƒm quay Ä‘áº§u khi bá»‹ fail.

3.3 Module Xá»­ lÃ½ Dá»¯ liá»‡u (FR-03)
**FR-03.2 - CÃ´ng cá»¥ ÄÃ¡nh giÃ¡ Cháº¥t lÆ°á»£ng Dá»¯ liá»‡u**
- PhÃ¡t hiá»‡n tÃ i liá»‡u trÃ¹ng láº·p (ngá»¯ nghÄ©a vÃ  tá»« khÃ³a)
- XÃ¡c Ä‘á»‹nh ná»™i dung mÆ¡ há»“, mÃ¢u thuáº«n
- ÄÃ¡nh giÃ¡ Ä‘á»™ hoÃ n chá»‰nh cá»§a metadata
- BÃ¡o cÃ¡o cháº¥t lÆ°á»£ng trá»±c quan
---
# TÃ€I LIá»†U THIáº¾T Káº¾ MODULE - CÃ”NG Cá»¤ ÄÃNH GIÃ CHáº¤T LÆ¯á»¢NG Dá»® LIá»†U
## DATA QUALITY ASSESSMENT TOOL (FR-03.2)

---

**Module ID:** FR-03.2  
**TÃªn Module:** CÃ´ng cá»¥ ÄÃ¡nh giÃ¡ Cháº¥t lÆ°á»£ng Dá»¯ liá»‡u  
**PhiÃªn báº£n:** 1.0  
**NgÃ y:** 31/08/2025  
**NgÆ°á»i soáº¡n tháº£o:** ChuyÃªn gia Thiáº¿t káº¿ Há»‡ thá»‘ng  
**Tráº¡ng thÃ¡i:** Ready for Implementation  

---

## 1. Tá»”NG QUAN MODULE

### 1.1 Má»¥c Ä‘Ã­ch
Module nÃ y Ä‘áº£m báº£o cháº¥t lÆ°á»£ng dá»¯ liá»‡u Ä‘áº§u vÃ o cho há»‡ thá»‘ng RAG báº±ng cÃ¡ch:
- PhÃ¡t hiá»‡n vÃ  loáº¡i bá» tÃ i liá»‡u trÃ¹ng láº·p (semantic & keyword-based)
- XÃ¡c Ä‘á»‹nh ná»™i dung mÆ¡ há»“, mÃ¢u thuáº«n trong tÃ i liá»‡u
- ÄÃ¡nh giÃ¡ Ä‘á»™ hoÃ n chá»‰nh cá»§a metadata
- Táº¡o bÃ¡o cÃ¡o cháº¥t lÆ°á»£ng trá»±c quan cho stakeholders

### 1.2 Vá»‹ trÃ­ trong há»‡ thá»‘ng
```
Raw Documents â†’ [FR-03.1 Raw-to-Clean] â†’ [FR-03.2 Quality Assessment] â†’ [FR-03.3 Data Ingestion] â†’ Vector DB
```

### 1.3 Input/Output
- **Input**: Clean documents tá»« FR-03.1 vá»›i metadata
- **Output**: Quality-validated documents + Quality Report + Rejected items log

---

## 2. CHUáº¨N Bá»Š MÃ”I TRÆ¯á»œNG PHÃT TRIá»‚N

### 2.1 YÃªu cáº§u Hardware tá»‘i thiá»ƒu

| Component | Specification | LÃ½ do |
|-----------|---------------|-------|
| **CPU** | 8 cores, 2.4GHz+ | Xá»­ lÃ½ song song nhiá»u documents |
| **RAM** | 32GB+ | Load embedding models + vector operations |
| **Storage** | 1TB SSD | Fast I/O cho document processing |
| **GPU** | 8GB VRAM (optional) | TÄƒng tá»‘c embedding generation |

### 2.2 MÃ´i trÆ°á»ng pháº§n má»m

#### **Operating System**
```bash
# Recommended: Ubuntu 22.04 LTS hoáº·c CentOS 8+
# Windows 11 Pro (vá»›i WSL2) cÅ©ng Ä‘Æ°á»£c há»— trá»£
```

#### **Core Development Tools**

| Tool | Version | Installation Command | Purpose |
|------|---------|---------------------|---------|
| **Python** | 3.9+ | `sudo apt install python3.9 python3.9-venv` | Core language |
| **Node.js** | 18+ | `curl -fsSL https://deb.nodesource.com/setup_18.x \| sudo -E bash -` | Frontend dashboard |
| **Docker** | 24+ | `curl -fsSL https://get.docker.com -o get-docker.sh && sh get-docker.sh` | Containerization |
| **PostgreSQL** | 15+ | `sudo apt install postgresql-15 postgresql-client-15` | Metadata storage |
| **Redis** | 7+ | `sudo apt install redis-server` | Caching layer |

#### **Python Dependencies**
```bash
# Táº¡o virtual environment
python3 -m venv data_quality_env
source data_quality_env/bin/activate

# Core ML/NLP libraries
pip install torch>=2.0.0 transformers>=4.30.0
pip install sentence-transformers>=2.2.0
pip install scikit-learn>=1.3.0 numpy>=1.24.0 pandas>=2.0.0

# Text processing
pip install spacy>=3.6.0 nltk>=3.8.0
pip install textdistance>=4.5.0 fuzzywuzzy>=0.18.0

# Database & API
pip install psycopg2-binary>=2.9.0 redis>=4.5.0
pip install fastapi>=0.100.0 uvicorn>=0.22.0
pip install sqlalchemy>=2.0.0 alembic>=1.11.0

# Visualization & Reporting  
pip install plotly>=5.15.0 dash>=2.11.0
pip install matplotlib>=3.7.0 seaborn>=0.12.0
pip install reportlab>=4.0.0 jinja2>=3.1.0

# Quality assessment specific
pip install dedupe>=2.0.0 recordlinkage>=0.15.0
pip install textstat>=0.7.0 language-tool-python>=2.7.0
```

#### **Development Environment Setup**
```bash
# IDE Setup (chá»n 1 trong cÃ¡c options)
# Option 1: VS Code
sudo snap install --classic code
code --install-extension ms-python.python
code --install-extension ms-toolsai.jupyter

# Option 2: PyCharm Professional
# Download tá»« JetBrains website

# Option 3: Jupyter Lab
pip install jupyterlab>=4.0.0
jupyter lab --generate-config
```

### 2.3 Database Setup

#### **PostgreSQL Configuration**
```sql
-- Táº¡o database vÃ  user
CREATE DATABASE data_quality_db;
CREATE USER dq_user WITH PASSWORD 'secure_password_here';
GRANT ALL PRIVILEGES ON DATABASE data_quality_db TO dq_user;

-- Táº¡o extensions cáº§n thiáº¿t
\c data_quality_db;
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pg_trgm"; -- Cho fuzzy text matching
CREATE EXTENSION IF NOT EXISTS "btree_gin"; -- Tá»‘i Æ°u indexing
```

#### **Redis Configuration**
```bash
# Chá»‰nh sá»­a /etc/redis/redis.conf
maxmemory 8gb
maxmemory-policy allkeys-lru
save 900 1
save 300 10
```

---

## 3. KIáº¾N TRÃšC MODULE VÃ€ LUá»’NG CÃ”NG VIá»†C

### 3.1 Module Architecture Overview

```mermaid
graph TB
    %% Input
    Input[ğŸ“„ Clean Documents<br/>+ Metadata]
    
    %% Main Processing Engine
    subgraph "ğŸ” DATA QUALITY ASSESSMENT ENGINE"
        subgraph "1ï¸âƒ£ Duplicate Detection"
            HashDup[ğŸ”¢ Hash-based<br/>Duplicate Detection]
            SemDup[ğŸ§  Semantic<br/>Duplicate Detection]
            KeywordDup[ğŸ”¤ Keyword<br/>Duplicate Detection]
        end
        
        subgraph "2ï¸âƒ£ Content Quality Analysis"
            AmbiguityDet[â“ Ambiguity<br/>Detector]
            ContradictDet[âš¡ Contradiction<br/>Detector]
            ReadabilityCheck[ğŸ“š Readability<br/>Checker]
        end
        
        subgraph "3ï¸âƒ£ Metadata Completeness"
            MetadataVal[âœ… Metadata<br/>Validator]
            RequiredFields[ğŸ“‹ Required Fields<br/>Checker]
            DataTypes[ğŸ·ï¸ Data Type<br/>Validator]
        end
        
        subgraph "4ï¸âƒ£ Quality Scoring"
            ScoreCalc[ğŸ“Š Quality Score<br/>Calculator]
            RuleEngine[âš™ï¸ Business Rules<br/>Engine]
            ThresholdCheck[ğŸ¯ Threshold<br/>Checker]
        end
    end
    
    %% Decision Point
    Decision{ğŸ“‹ Quality<br/>Passed?}
    
    %% Outputs
    Approved[âœ… Approved<br/>Documents]
    Rejected[âŒ Rejected<br/>Documents]
    Report[ğŸ“Š Quality<br/>Report]
    
    %% Storage
    QualityDB[(ğŸ—„ï¸ Quality<br/>Database)]
    Cache[(âš¡ Redis<br/>Cache)]
    
    %% Flow
    Input --> HashDup
    Input --> SemDup
    Input --> KeywordDup
    Input --> AmbiguityDet
    Input --> ContradictDet
    Input --> ReadabilityCheck
    Input --> MetadataVal
    Input --> RequiredFields
    Input --> DataTypes
    
    HashDup --> ScoreCalc
    SemDup --> ScoreCalc
    KeywordDup --> ScoreCalc
    AmbiguityDet --> ScoreCalc
    ContradictDet --> ScoreCalc
    ReadabilityCheck --> ScoreCalc
    MetadataVal --> ScoreCalc
    RequiredFields --> ScoreCalc
    DataTypes --> ScoreCalc
    
    ScoreCalc --> RuleEngine
    RuleEngine --> ThresholdCheck
    ThresholdCheck --> Decision
    
    Decision -->|Yes| Approved
    Decision -->|No| Rejected
    
    Approved --> QualityDB
    Rejected --> QualityDB
    ScoreCalc --> Report
    
    %% Caching
    SemDup -.-> Cache
    ScoreCalc -.-> Cache
    
    %% Styling
    classDef input fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef duplicate fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef content fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef metadata fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef scoring fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    classDef decision fill:#ffebee,stroke:#c62828,stroke-width:3px
    classDef output fill:#e0f2f1,stroke:#00796b,stroke-width:2px
    classDef storage fill:#f1f8e9,stroke:#689f38,stroke-width:2px
    
    class Input input
    class HashDup,SemDup,KeywordDup duplicate
    class AmbiguityDet,ContradictDet,ReadabilityCheck content
    class MetadataVal,RequiredFields,DataTypes metadata
    class ScoreCalc,RuleEngine,ThresholdCheck scoring
    class Decision decision
    class Approved,Rejected,Report output
    class QualityDB,Cache storage
```

### 3.2 Detailed Workflow vá»›i Retry Logic

```mermaid
flowchart TD
    Start([ğŸš€ Start Process])
    
    %% Input Validation
    InputCheck{ğŸ“‹ Input<br/>Valid?}
    InputFail[âŒ Invalid Input<br/>Log Error]
    
    %% Duplicate Detection Phase
    DupPhase[ğŸ” Phase 1:<br/>Duplicate Detection]
    HashCheck[ğŸ”¢ Hash Duplicate<br/>Check]
    HashFail{âš ï¸ Hash Check<br/>Failed?}
    HashRetry[ğŸ”„ Retry Hash<br/>Check _3x_]
    
    SemCheck[ğŸ§  Semantic Duplicate<br/>Check]
    SemFail{âš ï¸ Semantic Check<br/>Failed?}
    SemRetry[ğŸ”„ Retry Semantic<br/>Check _3x_]
    
    KeywordCheck[ğŸ”¤ Keyword Duplicate<br/>Check]
    KeywordFail{âš ï¸ Keyword Check<br/>Failed?}
    KeywordRetry[ğŸ”„ Retry Keyword<br/>Check _3x_]
    
    %% Content Quality Phase
    ContentPhase[ğŸ“š Phase 2:<br/>Content Quality]
    AmbiguityCheck[â“ Ambiguity<br/>Detection]
    AmbFail{âš ï¸ Ambiguity Check<br/>Failed?}
    AmbRetry[ğŸ”„ Retry Ambiguity<br/>Check _3x_]
    
    ContradictCheck[âš¡ Contradiction<br/>Detection]
    ConFail{âš ï¸ Contradiction Check<br/>Failed?}
    ConRetry[ğŸ”„ Retry Contradiction<br/>Check _3x_]
    
    ReadabilityCheck[ğŸ“– Readability<br/>Assessment]
    ReadFail{âš ï¸ Readability Check<br/>Failed?}
    ReadRetry[ğŸ”„ Retry Readability<br/>Check 3x]
    
    %% Metadata Phase
    MetadataPhase[ğŸ·ï¸ Phase 3:<br/>Metadata Validation]
    MetaCheck[âœ… Metadata<br/>Completeness]
    MetaFail{âš ï¸ Metadata Check<br/>Failed?}
    MetaRetry[ğŸ”„ Retry Metadata<br/>Check 3x]
    
    %% Scoring Phase
    ScoringPhase[ğŸ“Š Phase 4:<br/>Quality Scoring]
    ScoreCalc[ğŸ§® Calculate<br/>Quality Score]
    ScoreFail{âš ï¸ Scoring<br/>Failed?}
    ScoreRetry[ğŸ”„ Retry Scoring<br/>3x]
    
    %% Decision Phase
    Decision{ğŸ¯ Quality Score<br/>>= Threshold?}
    
    %% Output Handling
    Approve[âœ… Approve<br/>Document]
    Reject[âŒ Reject<br/>Document]
    
    %% Database Operations
    SaveApproved[ğŸ’¾ Save to<br/>Approved DB]
    SaveRejected[ğŸ’¾ Save to<br/>Rejected DB]
    
    SaveFail{âš ï¸ DB Save<br/>Failed?}
    SaveRetry[ğŸ”„ Retry DB Save<br/>5x]
    
    %% Report Generation
    GenReport[ğŸ“Š Generate<br/>Quality Report]
    ReportFail{âš ï¸ Report Gen<br/>Failed?}
    ReportRetry[ğŸ”„ Retry Report<br/>Gen 3x]
    
    %% Final States
    Success([âœ… Process<br/>Completed])
    CriticalFail([ğŸ’€ Critical<br/>Failure])
    
    %% Main Flow
    Start --> InputCheck
    InputCheck -->|Valid| DupPhase
    InputCheck -->|Invalid| InputFail
    InputFail --> CriticalFail
    
    %% Phase 1: Duplicate Detection
    DupPhase --> HashCheck
    HashCheck --> HashFail
    HashFail -->|Yes| HashRetry
    HashFail -->|No| SemCheck
    HashRetry -->|Max Retry| CriticalFail
    HashRetry -->|Retry| HashCheck
    
    SemCheck --> SemFail
    SemFail -->|Yes| SemRetry
    SemFail -->|No| KeywordCheck
    SemRetry -->|Max Retry| CriticalFail
    SemRetry -->|Retry| SemCheck
    
    KeywordCheck --> KeywordFail
    KeywordFail -->|Yes| KeywordRetry
    KeywordFail -->|No| ContentPhase
    KeywordRetry -->|Max Retry| CriticalFail
    KeywordRetry -->|Retry| KeywordCheck
    
    %% Phase 2: Content Quality
    ContentPhase --> AmbiguityCheck
    AmbiguityCheck --> AmbFail
    AmbFail -->|Yes| AmbRetry
    AmbFail -->|No| ContradictCheck
    AmbRetry -->|Max Retry| CriticalFail
    AmbRetry -->|Retry| AmbiguityCheck
    
    ContradictCheck --> ConFail
    ConFail -->|Yes| ConRetry
    ConFail -->|No| ReadabilityCheck
    ConRetry -->|Max Retry| CriticalFail
    ConRetry -->|Retry| ContradictCheck
    
    ReadabilityCheck --> ReadFail
    ReadFail -->|Yes| ReadRetry
    ReadFail -->|No| MetadataPhase
    ReadRetry -->|Max Retry| CriticalFail
    ReadRetry -->|Retry| ReadabilityCheck
    
    %% Phase 3: Metadata
    MetadataPhase --> MetaCheck
    MetaCheck --> MetaFail
    MetaFail -->|Yes| MetaRetry
    MetaFail -->|No| ScoringPhase
    MetaRetry -->|Max Retry| CriticalFail
    MetaRetry -->|Retry| MetaCheck
    
    %% Phase 4: Scoring
    ScoringPhase --> ScoreCalc
    ScoreCalc --> ScoreFail
    ScoreFail -->|Yes| ScoreRetry
    ScoreFail -->|No| Decision
    ScoreRetry -->|Max Retry| CriticalFail
    ScoreRetry -->|Retry| ScoreCalc
    
    %% Decision & Output
    Decision -->|Pass| Approve
    Decision -->|Fail| Reject
    
    Approve --> SaveApproved
    Reject --> SaveRejected
    
    SaveApproved --> SaveFail
    SaveRejected --> SaveFail
    
    SaveFail -->|Yes| SaveRetry
    SaveFail -->|No| GenReport
    SaveRetry -->|Max Retry| CriticalFail
    SaveRetry -->|Retry| SaveApproved
    
    GenReport --> ReportFail
    ReportFail -->|Yes| ReportRetry
    ReportFail -->|No| Success
    ReportRetry -->|Max Retry| CriticalFail
    ReportRetry -->|Retry| GenReport
    
    %% Styling
    classDef startEnd fill:#e3f2fd,stroke:#1976d2,stroke-width:3px
    classDef phase fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef process fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef decision fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef retry fill:#ffecb3,stroke:#f57f17,stroke-width:2px
    classDef fail fill:#ffebee,stroke:#c62828,stroke-width:2px
    classDef success fill:#e0f2f1,stroke:#00796b,stroke-width:2px
    
    class Start,Success,CriticalFail startEnd
    class DupPhase,ContentPhase,MetadataPhase,ScoringPhase phase
    class HashCheck,SemCheck,KeywordCheck,AmbiguityCheck,ContradictCheck,ReadabilityCheck,MetaCheck,ScoreCalc,SaveApproved,SaveRejected,GenReport process
    class InputCheck,HashFail,SemFail,KeywordFail,AmbFail,ConFail,ReadFail,MetaFail,ScoreFail,SaveFail,ReportFail,Decision decision
    class HashRetry,SemRetry,KeywordRetry,AmbRetry,ConRetry,ReadRetry,MetaRetry,ScoreRetry,SaveRetry,ReportRetry retry
    class InputFail fail
    class Approve,Reject success
```

---

## 4. IMPLEMENTATION ROADMAP (4 TUáº¦N)

### 4.1 Week 1: Core Infrastructure & Duplicate Detection

#### **Day 1-2: Environment Setup**
```bash
# Step 1: Clone repository structure
mkdir data_quality_module
cd data_quality_module
mkdir -p {src,tests,config,docs,scripts,data/{input,output,temp}}

# Step 2: Setup Python environment
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# Step 3: Database setup
sudo -u postgres createdb data_quality_db
python scripts/init_db.py

# Step 4: Configuration
cp config/config.example.yaml config/config.yaml
# Edit config.yaml vá»›i database credentials
```

#### **Day 3-5: Hash-based Duplicate Detection**
```python
# File: src/duplicate_detection/hash_detector.py

"""
Implementation tasks:
1. MD5/SHA256 hash generation cho documents
2. Hash comparison algorithm
3. Near-duplicate detection using simhash
4. Database storage cho hash values
5. Batch processing optimization
"""

# Key algorithms to implement:
# - Exact hash matching: MD5, SHA256
# - Fuzzy hash matching: SimHash, TLSH
# - Content fingerprinting: MinHash, LSH
```

#### **Day 6-7: Semantic Duplicate Detection**
```python
# File: src/duplicate_detection/semantic_detector.py

"""
Implementation tasks:
1. Load pre-trained embedding model (sentence-transformers)
2. Generate document embeddings
3. Cosine similarity calculation
4. Clustering similar documents
5. Threshold tuning for semantic similarity
"""

# Key components:
# - Embedding model: all-MiniLM-L6-v2 or multilingual variants
# - Similarity metrics: Cosine, Euclidean, Jaccard
# - Clustering: KMeans, DBSCAN
# - Optimization: Approximate nearest neighbor (Annoy, FAISS)
```

### 4.2 Week 2: Content Quality Analysis

#### **Day 8-10: Ambiguity Detection**
```python
# File: src/content_analysis/ambiguity_detector.py

"""
Implementation approach:
1. Sentence-level ambiguity detection
   - Multiple interpretations
   - Unclear pronouns/references
   - Missing context

2. Word-level ambiguity
   - Polysemic words
   - Context-dependent meanings
   
3. Structural ambiguity
   - Grammar parsing issues
   - Sentence structure problems
"""

# Algorithms to implement:
# - Named Entity Recognition (NER) for unclear references
# - Part-of-speech tagging for grammatical ambiguity
# - Dependency parsing for structural issues
# - Custom rule-based patterns for domain-specific ambiguity
```

#### **Day 11-12: Contradiction Detection**
```python
# File: src/content_analysis/contradiction_detector.py

"""
Implementation strategy:
1. Within-document contradictions
   - Fact extraction and comparison
   - Negation detection
   - Temporal contradictions

2. Cross-document contradictions
   - Compare statements across documents
   - Version control conflicts
   - Policy contradictions
"""

# Technical approach:
# - Natural Language Inference (NLI) models
# - Fact extraction using Open Information Extraction
# - Negation scope detection
# - Temporal relation extraction
```

#### **Day 13-14: Readability Assessment**
```python
# File: src/content_analysis/readability_checker.py

"""
Metrics to implement:
1. Traditional readability scores
   - Flesch Reading Ease
   - Flesch-Kincaid Grade Level
   - SMOG Index
   - Coleman-Liau Index

2. Modern NLP-based metrics
   - Sentence complexity analysis
   - Vocabulary difficulty assessment
   - Technical jargon density
"""

# Implementation:
# - Use textstat library for traditional metrics
# - Custom analysis for technical content
# - Domain-specific vocabulary lists
# - Readability recommendations generator
```

### 4.3 Week 3: Metadata Validation & Scoring

#### **Day 15-17: Metadata Completeness Checker**
```python
# File: src/metadata_validation/completeness_checker.py

"""
Validation rules:
1. Required fields validation
   - document_id, title, author, department
   - creation_date, last_updated
   - access_level, document_type

2. Field format validation
   - Date format consistency
   - Email format for authors
   - Enum values for categories

3. Business logic validation
   - Department-author relationship
   - Access level permissions
   - Version numbering consistency
"""

# Schema definition:
metadata_schema = {
    "required_fields": ["document_id", "title", "author"],
    "optional_fields": ["tags", "description", "version"],
    "field_types": {
        "document_id": "uuid",
        "creation_date": "datetime",
        "access_level": "enum"
    },
    "business_rules": {
        "author_department_match": True,
        "version_increment": True
    }
}
```

#### **Day 18-19: Quality Scoring Engine**
```python
# File: src/scoring/quality_calculator.py

"""
Scoring algorithm:
Total Score = Î£(Component Score Ã— Weight)

Components:
1. Duplicate Score (30%)
   - No duplicates: 100 points
   - Semantic similarity < 0.8: 80 points
   - High similarity: 0-50 points

2. Content Quality Score (40%)
   - Readability: 25%
   - Ambiguity level: 25%
   - Contradiction: 50%

3. Metadata Completeness (30%)
   - Required fields: 60%
   - Optional fields: 20%
   - Format compliance: 20%
"""

# Configurable thresholds:
quality_thresholds = {
    "excellent": 90,
    "good": 75,
    "acceptable": 60,
    "needs_improvement": 40,
    "reject": 0
}
```

#### **Day 20-21: Business Rules Engine**
```python
# File: src/scoring/rules_engine.py

"""
Rule categories:
1. Mandatory rules (auto-reject if failed)
   - Critical metadata missing
   - Exact duplicate found
   - Contains forbidden content

2. Scoring rules (affect quality score)
   - Readability below threshold
   - Ambiguous content detected
   - Incomplete metadata

3. Warning rules (flag for review)
   - Potential outdated information
   - Cross-references missing
   - Inconsistent formatting
"""

# Rule definition format:
rule_example = {
    "id": "META_001",
    "name": "Required Author Field",
    "type": "mandatory",
    "condition": "metadata.author is not null",
    "action": "reject",
    "message": "Author field is mandatory"
}
```

### 4.4 Week 4: Reporting & Integration

#### **Day 22-24: Quality Report Generation**
```python
# File: src/reporting/report_generator.py

"""
Report types:
1. Executive Summary
   - Overall quality metrics
   - Pass/fail statistics
   - Trending analysis

2. Technical Report
   - Detailed scoring breakdown
   - Failed validation rules
   - Recommended actions

3. Visual Dashboard
   - Interactive charts
   - Quality trends over time
   - Department-wise analysis
"""

# Report formats:
# - HTML dashboard (Plotly Dash)
# - PDF report (ReportLab)
# - JSON API response
# - Excel export
```

#### **Day 25-26: Web Dashboard Development**
```python
# File: src/dashboard/app.py

"""
Dashboard features:
1. Real-time quality monitoring
2. Document processing status
3. Quality trends visualization
4. Drill-down capability for failed documents
5. Bulk action interface
"""

# Tech stack:
# - Frontend: Plotly Dash
# - Backend: FastAPI
# - Database: PostgreSQL
# - Caching: Redis
```

#### **Day 27-28: Integration & Testing**
```python
# Integration checklist:
# 1. API endpoints cho external integration
# 2. Database migration scripts
# 3. Configuration management
# 4. Error handling vÃ  logging
# 5. Performance optimization
# 6. Unit tests vÃ  integration tests
# 7. Documentation update
```

---

## 5. CHI TIáº¾T TECHNICAL IMPLEMENTATION

### 5.1 Database Schema

```sql
-- Quality Assessment Tables
CREATE TABLE quality_assessments (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    document_id UUID NOT NULL,
    assessment_date TIMESTAMP DEFAULT NOW(),
    overall_score DECIMAL(5,2),
    status VARCHAR(20) CHECK (status IN ('approved', 'rejected', 'needs_review')),
    
    -- Duplicate detection results
    hash_duplicate_found BOOLEAN DEFAULT FALSE,
    semantic_similarity_score DECIMAL(5,4),
    keyword_similarity_score DECIMAL(5,4),
    
    -- Content quality results
    readability_score DECIMAL(5,2),
    ambiguity_score DECIMAL(5,2),
    contradiction_found BOOLEAN DEFAULT FALSE,
    
    -- Metadata quality
    metadata_completeness_score DECIMAL(5,2),
    required_fields_missing TEXT[],
    
    -- Additional metadata
    processing_time_ms INTEGER,
    quality_rules_applied JSONB,
    recommendations TEXT[],
    
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

CREATE TABLE duplicate_pairs (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    document_id_1 UUID NOT NULL,
    document_id_2 UUID NOT NULL,
    duplicate_type VARCHAR(20) CHECK (duplicate_type IN ('exact', 'semantic', 'keyword')),
    similarity_score DECIMAL(5,4),
    detected_at TIMESTAMP DEFAULT NOW()
);

CREATE TABLE quality_rules (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    rule_name VARCHAR(100) NOT NULL,
    rule_type VARCHAR(20) CHECK (rule_type IN ('mandatory', 'scoring', 'warning')),
    rule_condition TEXT NOT NULL,
    rule_action VARCHAR(50),
    rule_weight DECIMAL(3,2),
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT NOW()
);

-- Indexes for performance
CREATE INDEX idx_quality_assessments_document_id ON quality_assessments(document_id);
CREATE INDEX idx_quality_assessments_status ON quality_assessments(status);
CREATE INDEX idx_quality_assessments_date ON quality_assessments(assessment_date);
CREATE INDEX idx_duplicate_pairs_doc1 ON duplicate_pairs(document_id_1);
CREATE INDEX idx_duplicate_pairs_doc2 ON duplicate_pairs(document_id_2);
```

### 5.2 API Specification

```python
# File: src/api/endpoints.py

from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel
from typing import List, Optional

app = FastAPI(title="Data Quality Assessment API", version="1.0.0")

class QualityAssessmentRequest(BaseModel):
    document_id: str
    document_content: str
    metadata: dict
    assessment_options: Optional[dict] = {}

class QualityAssessmentResponse(BaseModel):
    document_id: str
    overall_score: float
    status: str
    detailed_results: dict
    recommendations: List[str]
    processing_time_ms: int

@app.post("/api/v1/assess-quality", response_model=QualityAssessmentResponse)
async def assess_document_quality(request: QualityAssessmentRequest):
    """
    Assess quality of a single document
    """
    # Implementation here
    pass

@app.post("/api/v1/batch-assess")
async def batch_assess_quality(
    documents: List[QualityAssessmentRequest],
    background_tasks: BackgroundTasks
):
    """
    Batch process multiple documents
    """
    # Implementation here
    pass

@app.get("/api/v1/quality-report/{document_id}")
async def get_quality_report(document_id: str):
    """
    Retrieve quality assessment report for specific document
    """
    # Implementation here
    pass

@app.get("/api/v1/quality-dashboard")
async def get_quality_dashboard():
    """
    Get dashboard data for quality overview
    """
    # Implementation here
    pass
```

### 5.3 Configuration Management

```yaml
# File: config/config.yaml

# Database Configuration
database:
  host: localhost
  port: 5432
  name: data_quality_db
  user: dq_user
  password: ${DB_PASSWORD}
  pool_size: 20
  max_overflow: 0

# Redis Configuration  
redis:
  host: localhost
  port: 6379
  db: 0
```
### 5.3 Configuration Management (tiáº¿p theo)

```yaml
# File: config/config.yaml (tiáº¿p)

# Quality Assessment Settings
quality_assessment:
  # Duplicate Detection
  duplicate_detection:
    hash_algorithm: "sha256"
    semantic_similarity_threshold: 0.85
    keyword_similarity_threshold: 0.75
    embedding_model: "sentence-transformers/all-MiniLM-L6-v2"
    batch_size: 32
    
  # Content Quality
  content_quality:
    readability:
      target_grade_level: 12
      min_acceptable_score: 30
    ambiguity:
      max_ambiguous_sentences_ratio: 0.1
      confidence_threshold: 0.7
    contradiction:
      nli_model: "microsoft/DialoGPT-medium"
      confidence_threshold: 0.8
      
  # Metadata Validation
  metadata_validation:
    required_fields:
      - document_id
      - title
      - author
      - department_owner
      - access_level
      - document_type
    optional_fields:
      - description
      - tags
      - version
      - keywords
    field_validation:
      email_pattern: "^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$"
      date_format: "%Y-%m-%d"
      version_pattern: "^\\d+\\.\\d+\\.\\d+$"

# Scoring Configuration
scoring:
  weights:
    duplicate_score: 0.30
    content_quality: 0.40
    metadata_completeness: 0.30
  
  thresholds:
    excellent: 90.0
    good: 75.0
    acceptable: 60.0
    needs_improvement: 40.0
    reject: 30.0
    
  penalties:
    exact_duplicate: -100  # Auto-reject
    high_semantic_similarity: -50
    missing_required_field: -20
    contradiction_found: -30

# Processing Configuration
processing:
  max_concurrent_tasks: 8
  timeout_seconds: 300
  retry_attempts: 3
  retry_delay_seconds: 5
  batch_processing_size: 100

# Reporting Configuration
reporting:
  formats: ["html", "pdf", "json"]
  dashboard_refresh_interval: 30  # seconds
  report_retention_days: 90
  export_formats: ["xlsx", "csv"]

# Logging Configuration
logging:
  level: INFO
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "logs/data_quality.log"
  max_file_size: "10MB"
  backup_count: 5
```

### 5.4 Error Handling Strategy

```python
# File: src/utils/error_handler.py

import logging
import traceback
from enum import Enum
from typing import Optional, Dict, Any
from datetime import datetime

class ErrorSeverity(Enum):
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"

class QualityAssessmentError(Exception):
    """Base exception cho Data Quality Assessment module"""
    
    def __init__(
        self, 
        message: str, 
        error_code: str,
        severity: ErrorSeverity = ErrorSeverity.MEDIUM,
        details: Optional[Dict[str, Any]] = None
    ):
        self.message = message
        self.error_code = error_code
        self.severity = severity
        self.details = details or {}
        self.timestamp = datetime.now()
        super().__init__(self.message)

class DuplicateDetectionError(QualityAssessmentError):
    """Errors trong quÃ¡ trÃ¬nh duplicate detection"""
    pass

class ContentAnalysisError(QualityAssessmentError):
    """Errors trong content quality analysis"""
    pass

class MetadataValidationError(QualityAssessmentError):
    """Errors trong metadata validation"""
    pass

class ErrorHandler:
    def __init__(self, logger: logging.Logger):
        self.logger = logger
        self.error_counts = {}
        
    def handle_error(
        self, 
        error: Exception, 
        context: Dict[str, Any] = None,
        retry_count: int = 0
    ) -> Dict[str, Any]:
        """
        Centralized error handling vá»›i retry logic
        """
        error_info = {
            "error_type": type(error).__name__,
            "message": str(error),
            "timestamp": datetime.now().isoformat(),
            "context": context or {},
            "retry_count": retry_count,
            "traceback": traceback.format_exc()
        }
        
        if isinstance(error, QualityAssessmentError):
            error_info.update({
                "error_code": error.error_code,
                "severity": error.severity.value,
                "details": error.details
            })
            
            # Log based on severity
            if error.severity == ErrorSeverity.CRITICAL:
                self.logger.critical(f"CRITICAL ERROR: {error.message}", extra=error_info)
            elif error.severity == ErrorSeverity.HIGH:
                self.logger.error(f"HIGH SEVERITY: {error.message}", extra=error_info)
            elif error.severity == ErrorSeverity.MEDIUM:
                self.logger.warning(f"MEDIUM SEVERITY: {error.message}", extra=error_info)
            else:
                self.logger.info(f"LOW SEVERITY: {error.message}", extra=error_info)
        else:
            self.logger.error(f"Unexpected error: {error.message}", extra=error_info)
            
        # Track error frequency
        error_key = f"{type(error).__name__}:{getattr(error, 'error_code', 'unknown')}"
        self.error_counts[error_key] = self.error_counts.get(error_key, 0) + 1
        
        return error_info
        
    def should_retry(self, error: Exception, retry_count: int, max_retries: int = 3) -> bool:
        """
        Determine if operation should be retried
        """
        if retry_count >= max_retries:
            return False
            
        # Don't retry critical errors or validation errors
        if isinstance(error, QualityAssessmentError):
            if error.severity == ErrorSeverity.CRITICAL:
                return False
            if isinstance(error, MetadataValidationError):
                return False
                
        # Retry for transient errors
        transient_errors = [
            ConnectionError,
            TimeoutError,
            "rate_limit_exceeded",
            "temporary_service_unavailable"
        ]
        
        return any(
            isinstance(error, err_type) or 
            (hasattr(error, 'error_code') and error.error_code == err_type)
            for err_type in transient_errors
        )
```

### 5.5 Performance Optimization

```python
# File: src/optimization/performance_optimizer.py

import asyncio
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache, wraps
from typing import List, Dict, Any, Callable
import redis
import hashlib
import pickle

class PerformanceOptimizer:
    def __init__(self, redis_client: redis.Redis):
        self.redis_client = redis_client
        self.executor = ThreadPoolExecutor(max_workers=8)
        self.cache_ttl = 3600  # 1 hour
        
    def async_cache(self, ttl: int = 3600):
        """
        Decorator for caching async function results
        """
        def decorator(func: Callable):
            @wraps(func)
            async def wrapper(*args, **kwargs):
                # Create cache key
                cache_key = f"{func.__name__}:{hashlib.md5(str(args + tuple(kwargs.items())).encode()).hexdigest()}"
                
                # Try to get from cache
                cached_result = self.redis_client.get(cache_key)
                if cached_result:
                    return pickle.loads(cached_result)
                
                # Execute function and cache result
                result = await func(*args, **kwargs)
                self.redis_client.setex(
                    cache_key, 
                    ttl, 
                    pickle.dumps(result)
                )
                return result
            return wrapper
        return decorator
    
    async def batch_process_documents(
        self, 
        documents: List[Dict[str, Any]], 
        processing_func: Callable,
        batch_size: int = 32
    ) -> List[Dict[str, Any]]:
        """
        Process documents in batches for optimal performance
        """
        results = []
        
        for i in range(0, len(documents), batch_size):
            batch = documents[i:i + batch_size]
            
            # Process batch concurrently
            tasks = []
            for doc in batch:
                task = asyncio.create_task(processing_func(doc))
                tasks.append(task)
            
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)
            results.extend(batch_results)
            
            # Small delay to prevent overwhelming resources
            await asyncio.sleep(0.1)
            
        return results
    
    def preload_models(self):
        """
        Preload ML models to memory for faster inference
        """
        # Preload embedding model
        self._load_embedding_model()
        
        # Preload NLP models
        self._load_nlp_models()
        
        # Warm up cache
        self._warmup_cache()
    
    @lru_cache(maxsize=1)
    def _load_embedding_model(self):
        from sentence_transformers import SentenceTransformer
        model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
        return model
    
    @lru_cache(maxsize=128)
    def compute_text_hash(self, text: str) -> str:
        """Cached hash computation"""
        return hashlib.sha256(text.encode()).hexdigest()
    
    async def parallel_similarity_computation(
        self, 
        target_embedding: List[float], 
        candidate_embeddings: List[List[float]]
    ) -> List[float]:
        """
        Compute similarities in parallel
        """
        from sklearn.metrics.pairwise import cosine_similarity
        import numpy as np
        
        def compute_batch_similarity(batch_embeddings):
            return cosine_similarity([target_embedding], batch_embeddings)[0]
        
        # Process in smaller batches for memory efficiency
        batch_size = 1000
        similarity_scores = []
        
        for i in range(0, len(candidate_embeddings), batch_size):
            batch = candidate_embeddings[i:i + batch_size]
            batch_similarities = await asyncio.get_event_loop().run_in_executor(
                self.executor, 
                compute_batch_similarity, 
                batch
            )
            similarity_scores.extend(batch_similarities)
            
        return similarity_scores
```

---

## 6. TESTING STRATEGY

### 6.1 Unit Testing Framework

```python
# File: tests/test_duplicate_detection.py

import pytest
import asyncio
from unittest.mock import Mock, patch, AsyncMock
from src.duplicate_detection.hash_detector import HashDuplicateDetector
from src.duplicate_detection.semantic_detector import SemanticDuplicateDetector

class TestHashDuplicateDetector:
    
    @pytest.fixture
    def hash_detector(self):
        return HashDuplicateDetector()
    
    def test_exact_duplicate_detection(self, hash_detector):
        """Test exact duplicate detection using hash"""
        doc1 = "This is a test document with specific content."
        doc2 = "This is a test document with specific content."
        doc3 = "This is a different document with other content."
        
        hash1 = hash_detector.compute_hash(doc1)
        hash2 = hash_detector.compute_hash(doc2)
        hash3 = hash_detector.compute_hash(doc3)
        
        assert hash1 == hash2  # Exact duplicates
        assert hash1 != hash3  # Different content
    
    def test_near_duplicate_detection(self, hash_detector):
        """Test near-duplicate detection using SimHash"""
        doc1 = "The quick brown fox jumps over the lazy dog."
        doc2 = "The quick brown fox jumped over the lazy dog."  # Minor change
        doc3 = "A completely different sentence about cats and mice."
        
        similarity_1_2 = hash_detector.compute_simhash_similarity(doc1, doc2)
        similarity_1_3 = hash_detector.compute_simhash_similarity(doc1, doc3)
        
        assert similarity_1_2 > 0.8  # High similarity
        assert similarity_1_3 < 0.3  # Low similarity
    
    @pytest.mark.asyncio
    async def test_batch_duplicate_detection(self, hash_detector):
        """Test batch processing of duplicate detection"""
        documents = [
            {"id": "doc1", "content": "Content A"},
            {"id": "doc2", "content": "Content A"},  # Duplicate
            {"id": "doc3", "content": "Content B"},
            {"id": "doc4", "content": "Content C"}
        ]
        
        duplicates = await hash_detector.find_duplicates_batch(documents)
        
        assert len(duplicates) == 1
        assert duplicates[0]["documents"] == ["doc1", "doc2"]

class TestSemanticDuplicateDetector:
    
    @pytest.fixture
    def semantic_detector(self):
        return SemanticDuplicateDetector()
    
    @pytest.mark.asyncio
    async def test_semantic_similarity(self, semantic_detector):
        """Test semantic similarity detection"""
        doc1 = "The company's revenue increased significantly last quarter."
        doc2 = "Last quarter, the firm saw a substantial rise in income."  # Semantic duplicate
        doc3 = "Today is a sunny day with clear blue skies."  # Unrelated
        
        similarity_1_2 = await semantic_detector.compute_similarity(doc1, doc2)
        similarity_1_3 = await semantic_detector.compute_similarity(doc1, doc3)
        
        assert similarity_1_2 > 0.7  # High semantic similarity
        assert similarity_1_3 < 0.2  # Low semantic similarity
    
    @pytest.mark.asyncio
    async def test_clustering_similar_documents(self, semantic_detector):
        """Test clustering of semantically similar documents"""
        documents = [
            {"id": "doc1", "content": "Machine learning algorithms"},
            {"id": "doc2", "content": "AI and ML techniques"},  
            {"id": "doc3", "content": "Cooking recipes for dinner"},
            {"id": "doc4", "content": "Food preparation methods"}
        ]
        
        clusters = await semantic_detector.cluster_documents(documents)
        
        assert len(clusters) == 2  # Two clusters expected
        # Verify AI/ML documents are clustered together
        # Verify cooking documents are clustered together
```

### 6.2 Integration Testing

```python
# File: tests/test_integration.py

import pytest
import asyncio
from src.quality_assessment_engine import QualityAssessmentEngine
from src.database.models import QualityAssessment
from tests.fixtures import sample_documents

class TestQualityAssessmentIntegration:
    
    @pytest.fixture
    async def assessment_engine(self):
        """Setup quality assessment engine with test database"""
        engine = QualityAssessmentEngine(config_path="config/test_config.yaml")
        await engine.initialize()
        return engine
    
    @pytest.mark.asyncio
    async def test_end_to_end_quality_assessment(self, assessment_engine):
        """Test complete quality assessment pipeline"""
        test_document = {
            "document_id": "test_doc_001",
            "content": "This is a well-written document with clear content and proper structure.",
            "metadata": {
                "title": "Test Document",
                "author": "test@company.com",
                "department_owner": "IT",
                "access_level": "employee_only",
                "document_type": "policy"
            }
        }
        
        result = await assessment_engine.assess_document(test_document)
        
        # Verify all components were executed
        assert "duplicate_detection" in result
        assert "content_quality" in result
        assert "metadata_validation" in result
        assert "overall_score" in result
        assert "status" in result
        
        # Verify score is calculated correctly
        assert 0 <= result["overall_score"] <= 100
        
        # Verify status is assigned
        assert result["status"] in ["approved", "rejected", "needs_review"]
    
    @pytest.mark.asyncio
    async def test_batch_processing(self, assessment_engine):
        """Test batch processing of multiple documents"""
        test_documents = sample_documents.get_test_batch(50)  # 50 test documents
        
        start_time = time.time()
        results = await assessment_engine.assess_documents_batch(test_documents)
        processing_time = time.time() - start_time
        
        # Verify all documents were processed
        assert len(results) == len(test_documents)
        
        # Verify performance requirement (should process 50 docs within reasonable time)
        assert processing_time < 300  # 5 minutes max for 50 documents
        
        # Verify results structure
        for result in results:
            assert "document_id" in result
            assert "overall_score" in result
            assert "processing_time_ms" in result
    
    @pytest.mark.asyncio
    async def test_database_persistence(self, assessment_engine):
        """Test database storage of assessment results"""
        test_document = sample_documents.get_single_document()
        
        result = await assessment_engine.assess_document(test_document)
        
        # Verify result was saved to database
        saved_assessment = await QualityAssessment.get_by_document_id(
            test_document["document_id"]
        )
        
        assert saved_assessment is not None
        assert saved_assessment.overall_score == result["overall_score"]
        assert saved_assessment.status == result["status"]
    
    @pytest.mark.asyncio
    async def test_error_recovery(self, assessment_engine):
        """Test error handling and recovery mechanisms"""
        # Test with malformed document
        malformed_document = {
            "document_id": "malformed_doc",
            "content": "",  # Empty content
            "metadata": {}  # Missing required fields
        }
        
        with pytest.raises(MetadataValidationError):
            await assessment_engine.assess_document(malformed_document)
        
        # Test with corrupted content
        corrupted_document = {
            "document_id": "corrupted_doc", 
            "content": "\x00\x01\x02",  # Binary content
            "metadata": sample_documents.get_valid_metadata()
        }
        
        result = await assessment_engine.assess_document(corrupted_document)
        # Should not crash, but should have low quality score
        assert result["status"] in ["rejected", "needs_review"]
```

### 6.3 Performance Testing

```python
# File: tests/test_performance.py

import pytest
import asyncio
import time
import psutil
from concurrent.futures import ThreadPoolExecutor
from src.quality_assessment_engine import QualityAssessmentEngine

class TestPerformance:
    
    @pytest.fixture
    async def assessment_engine(self):
        engine = QualityAssessmentEngine()
        await engine.initialize()
        return engine
    
    @pytest.mark.performance
    async def test_throughput_performance(self, assessment_engine):
        """Test system throughput with concurrent requests"""
        num_documents = 100
        test_documents = [
            sample_documents.generate_random_document(i) 
            for i in range(num_documents)
        ]
        
        start_time = time.time()
        start_memory = psutil.virtual_memory().used
        
        # Process documents concurrently
        tasks = []
        for doc in test_documents:
            task = asyncio.create_task(assessment_engine.assess_document(doc))
            tasks.append(task)
        
        results = await asyncio.gather(*tasks)
        
        end_time = time.time()
        end_memory = psutil.virtual_memory().used
        
        processing_time = end_time - start_time
        memory_used = (end_memory - start_memory) / 1024 / 1024  # MB
        
        # Performance assertions
        throughput = num_documents / processing_time
        assert throughput >= 2.0  # At least 2 documents per second
        assert processing_time < 60  # Should complete within 1 minute
        assert memory_used < 1000  # Should use less than 1GB additional memory
        
        print(f"Throughput: {throughput:.2f} docs/sec")
        print(f"Memory usage: {memory_used:.2f} MB")
        print(f"Total time: {processing_time:.2f} seconds")
    
    @pytest.mark.performance
    async def test_memory_efficiency(self, assessment_engine):
        """Test memory usage with large documents"""
        # Generate large document (1MB text)
        large_content = "A" * (1024 * 1024)
        large_document = {
            "document_id": "large_doc",
            "content": large_content,
            "metadata": sample_documents.get_valid_metadata()
        }
        
        memory_before = psutil.virtual_memory().used
        
        result = await assessment_engine.assess_document(large_document)
        
        memory_after = psutil.virtual_memory().used
        memory_increase = (memory_after - memory_before) / 1024 / 1024
        
        # Should not use excessive memory for large documents
        assert memory_increase < 500  # Less than 500MB increase
        assert result is not None
    
    @pytest.mark.performance  
    async def test_cache_effectiveness(self, assessment_engine):
        """Test caching improves performance for repeated operations"""
        test_document = sample_documents.get_single_document()
        
        # First assessment (cold cache)
        start_time = time.time()
        result1 = await assessment_engine.assess_document(test_document)
        first_time = time.time() - start_time
        
        # Second assessment (warm cache)
        start_time = time.time()
        result2 = await assessment_engine.assess_document(test_document)
        second_time = time.time() - start_time
        
        # Cache should improve performance significantly
        assert second_time < first_time * 0.5  # At least 50% faster
        assert result1 == result2  # Results should be identical
```

---

## 7. DEPLOYMENT GUIDE

### 7.1 Production Environment Setup

```bash
#!/bin/bash
# File: scripts/production_setup.sh

# Production deployment script
set -e

echo "ğŸš€ Starting Data Quality Assessment Module Deployment"

# 1. System requirements check
echo "ğŸ“‹ Checking system requirements..."
python3 --version | grep -E "3\.[9-1][0-9]\." || {
    echo "âŒ Python 3.9+ required"
    exit 1
}

docker --version || {
    echo "âŒ Docker required"
    exit 1
}

# 2. Create production directories
echo "ğŸ“ Creating directory structure..."
sudo mkdir -p /opt/data_quality/{logs,data,config,backups}
sudo chown -R $USER:$USER /opt/data_quality

# 3. Setup Python environment
echo "ğŸ Setting up Python environment..."
cd /opt/data_quality
python3 -m venv venv_prod
source venv_prod/bin/activate
pip install --upgrade pip
pip install -r requirements.txt

# 4. Database setup
echo "ğŸ—„ï¸ Setting up PostgreSQL database..."
sudo -u postgres createdb data_quality_prod
sudo -u postgres createuser dq_prod_user
sudo -u postgres psql -c "ALTER USER dq_prod_user WITH PASSWORD 'prod_password_here';"
sudo -u postgres psql -c "GRANT ALL PRIVILEGES ON DATABASE data_quality_prod TO dq_prod_user;"

# 5. Redis setup
echo "ğŸ”´ Configuring Redis..."
sudo systemctl enable redis-server
sudo systemctl start redis-server

# 6. Environment variables
echo "âš™ï¸ Setting up environment variables..."
cat > /opt/data_quality/.env << EOF
ENVIRONMENT=production
DATABASE_URL=postgresql://dq_prod_user:prod_password_here@localhost:5432/data_quality_prod
REDIS_URL=redis://localhost:6379/0
SECRET_KEY=$(openssl rand -base64 32)
LOG_LEVEL=INFO
MAX_WORKERS=8
CACHE_TTL=3600
EOF

# 7. System service setup
echo "ğŸ”§ Creating systemd service..."
sudo tee /etc/systemd/system/data-quality-api.service > /dev/null << EOF
[Unit]
Description=Data Quality Assessment API
After=network.target postgresql.service redis.service

[Service]
Type=simple
User=$USER
WorkingDirectory=/opt/data_quality
Environment=PATH=/opt/data_quality/venv_prod/bin
EnvironmentFile=/opt/data_quality/.env
ExecStart=/opt/data_quality/venv_prod/bin/uvicorn src.api.main:app --host 0.0.0.0 --port 8000 --workers 4
Restart=always
RestartSec=3

[Install]
WantedBy=multi-user.target
EOF

# 8. Start services
echo "ğŸš€ Starting services..."
sudo systemctl daemon-reload
sudo systemctl enable data-quality-api
sudo systemctl start data-quality-api

# 9. Health check
echo "ğŸ¥ Running health check..."
sleep 10
curl -f http://localhost:8000/health || {
    echo "âŒ Health check failed"
    sudo systemctl status data-quality-api
    exit 1
}

echo "âœ… Deployment completed successfully!"
echo "ğŸ“Š Dashboard available at: http://localhost:8000/dashboard"
echo "ğŸ” API documentation at: http://localhost:8000/docs"
```

### 7.2 Docker Deployment

```dockerfile
# File: Dockerfile
FROM python:3.11-slim

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    curl \
    postgresql-client \
    && rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /app

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Download NLTK data and spaCy models
RUN python -m nltk.downloader punkt stopwords averaged_perceptron_tagger
RUN python -m spacy download en_core_web_sm

# Copy application code
COPY src/ ./src/
COPY config/ ./config/
COPY scripts/ ./scripts/

# Create non-root user
RUN useradd --create-home --shell /bin/bash appuser
RUN chown -R appuser:appuser /app
USER appuser

# Expose port
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Start application
CMD ["uvicorn", "src.api.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

```yaml
# File: docker-compose.yml
version: '3.8'

services:
  data-quality-api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://dq_user:password@postgres:5432/data_quality_db
      - REDIS_URL=redis://redis:6379/0
      - ENVIRONMENT=production
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_started
    volumes:
      - ./logs:/app/logs
      - ./data:/app/data
    restart: unless-stopped

  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: data_quality_db
      POSTGRES_USER: dq_user
      POSTGRES_PASSWORD: password
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init_db.sql:/docker-entrypoint-initdb.d/init_db.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U dq_user -d data_quality_db"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data
    restart: unless-stopped

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - data-quality-api
    restart: unless-stopped

volumes:
  postgres_data:
  redis_data:
```

### 7.3 Monitoring vÃ  Alerting

```python
# File: src/monitoring/metrics.py

import time
import psutil
from prometheus_client import Counter, Histogram, Gauge, start_http_server
from functools import wraps

# Prometheus metrics
REQUEST_COUNT = Counter('data_quality_requests_total', 'Total requests', ['method', 'endpoint'])
REQUEST_DURATION = Histogram('data_quality_request_duration_seconds', 'Request duration')
PROCESSING_TIME = Histogram('document_processing_seconds', 'Document processing time')
ACTIVE_ASSESSMENTS = Gauge('active_assessments', 'Number of active quality assessments')
MEMORY_USAGE = Gauge('memory_usage_bytes', 'Memory usage in bytes')
CPU_USAGE = Gauge('cpu_usage_percent', 'CPU usage percentage')

# Error metrics
ERROR_COUNT = Counter('data_quality_errors_total', 'Total errors', ['error_type'])
QUALITY_SCORES = Histogram('quality_scores', 'Distribution of quality scores')

class MetricsCollector:
    def __init__(self):
        self.start_metrics_server()
        
    def start_metrics_server(self, port=9090):
        """Start Prometheus metrics server"""
        start_http_server(port)
        
    def track_request(self, method: str, endpoint: str):
        """Decorator to track API requests"""
        def decorator(func):
            @wraps(func)
            async def wrapper(*args, **kwargs):
                REQUEST_COUNT.labels(method=method, endpoint=endpoint).inc()
                
                with REQUEST_DURATION.time():
                    result = await func(*args, **kwargs)
                    
                return result
            return wrapper
        return decorator
        
    def track_processing_time(self, func):
        """Decorator to track document processing time"""
        @wraps(func)
        async def wrapper(*args, **kwargs):
            start_time = time.time()
            try:
                result = await func(*args, **kwargs)
                PROCESSING_TIME.observe(time.time() - start_time)
                return result
            except Exception as e:
                ERROR_COUNT.labels(error_type=type(e).__name__).inc()
                raise
        return wrapper
        
    def update_system_metrics(self):
        """Update system resource metrics"""
        MEMORY_USAGE.set(psutil.virtual_memory().used)
        CPU_USAGE.set(psutil.cpu_percent())
        
    def record_quality_score(self, score: float):
        """Record quality assessment score"""
        QUALITY_SCORES.observe(score)
        
    def increment_active_assessments(self):
        """Increment active assessment counter"""
        ACTIVE_ASSESSMENTS.inc()
        
    def decrement_active_assessments(self):
        """Decrement active assessment counter"""
        ACTIVE_ASSESSMENTS.dec()

# Usage example
metrics = MetricsCollector()

@metrics.track_request('POST', '/assess-quality')
@metrics.track_processing_time
async def assess_document_quality(document):
    metrics.increment_active_assessments()
    try:
        # Processing logic here
        result = await process_document(document)
        metrics.record_quality_score(result['overall_score'])
        return result
    finally:
        metrics.decrement_active_assessments()
```

```yaml
# File: monitoring/alerting_rules.yml

groups:
  - name: data_quality_alerts
    rules:
      # High error rate
      - alert: HighErrorRate
        expr: rate(data_quality_errors_total[5m]) > 0.1
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "High error rate in data quality assessment"
          description: "Error rate is {{ $value }} errors per second"
          
      # High response time
      - alert: HighResponseTime
        expr: histogram_quantile(0.95, rate(data_quality_request_duration_seconds_bucket[5m])) > 30
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High response time detected"
          description: "95th percentile response time is {{ $value }} seconds"
          
      # High memory usage
      - alert: HighMemoryUsage
        expr: memory_usage_bytes / (1024^3) > 8
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage"
          description: "Memory usage is {{ $value }}GB"
          
      # Service down
      - alert: ServiceDown
        expr: up{job="data-quality-api"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Data Quality API is down"
          description: "Service has been down for more than 1 minute"
          
      # Database connection issues
      - alert: DatabaseConnectionFail
        expr: increase(data_quality_errors_total{error_type="DatabaseError"}[5m]) > 5
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Database connection issues"
          description: "Multiple database connection failures detected"
```

---

## 8. TROUBLESHOOTING GUIDE

### 8.1 Common Issues vÃ  Solutions

#### **Issue 1: Memory Usage quÃ¡ cao**
```bash
# Symptoms
- Process bá»‹ kill bá»Ÿi OOM killer
- Slow response times
- High swap usage

# Diagnosis
htop
free -h
ps aux --sort=-%mem | head -10

# Solutions
# 1. TÄƒng batch size Ä‘á»ƒ giáº£m concurrent processing
# File: config/config.yaml
processing:
  batch_processing_size: 50  # Giáº£m tá»« 100
  max_concurrent_tasks: 4    # Giáº£m tá»« 8

# 2. Implement memory cleanup
import gc
import psutil

def cleanup_memory():
    gc.collect()
    if psutil.virtual_memory().percent > 85:
        # Force cleanup embedding cache
        embedding_cache.clear()
        
# 3. Monitor memory usage
def memory_monitor(func):
    @wraps(func)
    async def wrapper(*args, **kwargs):
        before = psutil.virtual_memory().used
        result = await func(*args, **kwargs)
        after = psutil.virtual_memory().used
        if (after - before) > 500 * 1024 * 1024:  # 500MB
            logging.warning(f"High memory usage: {(after-before)/1024/1024:.2f}MB")
        return result
    return wrapper
```

#### **Issue 2: Embedding Model Loading Lá»—i**
```python
# Symptoms
- ImportError cho sentence-transformers
- CUDA out of memory errors
- Model loading timeout

# Diagnosis
try:
    from sentence_transformers import SentenceTransformer
    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
    print("âœ… Model loaded successfully")
except Exception as e:
    print(f"âŒ Model loading failed: {e}")

# Solutions
# 1. Fallback model loading
def load_embedding_model_with_fallback():
    models_to_try = [
        'sentence-transformers/all-MiniLM-L6-v2',
        'sentence-transformers/all-mpnet-base-v2',
        'sentence-transformers/paraphrase-MiniLM-L6-v2'
    ]
    
    for model_name in models_to_try:
        try:
            model = SentenceTransformer(model_name)
            logging.info(f"âœ… Loaded model: {model_name}")
            return model
        except Exception as e:
            logging.warning(f"âŒ Failed to load {model_name}: {e}")
            continue
    
    raise RuntimeError("Failed to load any embedding model")

# 2. CPU-only mode for memory constraints
model = SentenceTransformer(
    'sentence-transformers/all-MiniLM-L6-v2',
    device='cpu'  # Force CPU mode
)
```

#### **Issue 3: Database Connection Pool Exhausted**
```python
# Symptoms
- "pool limit exceeded" errors
- Slow database queries
- Connection timeout errors

# Diagnosis
# Monitor active connections
SELECT count(*) as active_connections 
FROM pg_stat_activity 
WHERE state = 'active';

SELECT pid, state, query_start, query 
FROM pg_stat_activity 
WHERE state != 'idle' 
ORDER BY query_start;

# Solutions
# 1. Connection pool tuning
# File: src/database/connection.py
from sqlalchemy.pool import QueuePool

engine = create_engine(
    DATABASE_URL,
    poolclass=QueuePool,
    pool_size=20,          # Increase pool size
    max_overflow=30,       # Allow overflow
    pool_recycle=3600,     # Recycle connections every hour
    pool_pre_ping=True,    # Verify connections before use
    pool_timeout=30        # Timeout for getting connection
)

# 2. Proper connection management
async def get_db_session():
    async with AsyncSession() as session:
        try:
            yield session
            await session.commit()
        except Exception:
            await session.rollback()
            raise
        finally:
            await session.close()

# 3. Connection monitoring
async def monitor_db_connections():
    while True:
        active = await get_active_connection_count()
        if active > 15:  # 75% of pool size
            logging.warning(f"High DB connection usage: {active}")
        await asyncio.sleep(30)
```

#### **Issue 4: Redis Cache Performance Issues**
```python
# Symptoms
- Slow cache lookups
- Memory errors in Redis
- Cache miss rate cao

# Diagnosis
redis-cli info memory
redis-cli info stats
redis-cli monitor  # Watch real-time commands

# Solutions
# 1. Memory optimization
# redis.conf
maxmemory 4gb
maxmemory-policy allkeys-lru
maxmemory-samples 10

# 2. Connection pooling
import aioredis

redis_pool = aioredis.ConnectionPool.from_url(
    "redis://localhost:6379",
    max_connections=20,
    retry_on_timeout=True,
    health_check_interval=30
)

# 3. Async Redis operations vá»›i timeout
async def safe_redis_get(key: str, timeout: int = 5):
    try:
        return await asyncio.wait_for(
            redis_client.get(key),
            timeout=timeout
        )
    except asyncio.TimeoutError:
        logging.warning(f"Redis timeout for key: {key}")
        return None
```

### 8.2 Health Check System

```python
# File: src/monitoring/health_check.py

import asyncio
import time
from typing import Dict, Any
from enum import Enum

class HealthStatus(Enum):
    HEALTHY = "healthy"
    DEGRADED = "degraded"
    UNHEALTHY = "unhealthy"

class HealthChecker:
    def __init__(self):
        self.checks = {}
        
    def register_check(self, name: str, check_func, timeout: int = 10):
        """Register a health check function"""
        self.checks[name] = {
            'func': check_func,
            'timeout': timeout,
            'last_result': None,
            'last_check': None
        }
    
    async def run_all_checks(self) -> Dict[str, Any]:
        """Run all registered health checks"""
        results = {
            'status': HealthStatus.HEALTHY.value,
            'timestamp': time.time(),
            'checks': {}
        }
        
        overall_healthy = True
        
        for name, check_info in self.checks.items():
            try:
                result = await asyncio.wait_for(
                    check_info['func'](),
                    timeout=check_info['timeout']
                )
                
                results['checks'][name] = {
                    'status': HealthStatus.HEALTHY.value,
                    'response_time_ms': result.get('response_time_ms', 0),
                    'details': result.get('details', {})
                }
                
            except asyncio.TimeoutError:
                results['checks'][name] = {
                    'status': HealthStatus.UNHEALTHY.value,
                    'error': 'Timeout',
                    'response_time_ms': check_info['timeout'] * 1000
                }
                overall_healthy = False
                
            except Exception as e:
                results['checks'][name] = {
                    'status': HealthStatus.UNHEALTHY.value,
                    'error': str(e),
                    'response_time_ms': 0
                }
                overall_healthy = False
        
        # Determine overall status
        unhealthy_count = sum(
            1 for check in results['checks'].values()
            if check['status'] == HealthStatus.UNHEALTHY.value
        )
        
        if not overall_healthy:
            if unhealthy_count >= len(self.checks) / 2:
                results['status'] = HealthStatus.UNHEALTHY.value
            else:
                results['status'] = HealthStatus.DEGRADED.value
        
        return results

# Health check functions
async def check_database():
    """Check database connectivity and performance"""
    start_time = time.time()
    try:
        # Simple query to test connection
        result = await database.fetch_one("SELECT 1 as test")
        response_time = (time.time() - start_time) * 1000
        
        return {
            'response_time_ms': response_time,
            'details': {'connection': 'ok', 'query_result': result['test']}
        }
    except Exception as e:
        raise Exception(f"Database check failed: {e}")

async def check_redis():
    """Check Redis connectivity and performance"""
    start_time = time.time()
    try:
        test_key = "health_check_test"
        await redis_client.set(test_key, "test_value", ex=60)
        value = await redis_client.get(test_key)
        await redis_client.delete(test_key)
        
        response_time = (time.time() - start_time) * 1000
        
        return {
            'response_time_ms': response_time,
            'details': {'connection': 'ok', 'read_write': 'ok'}
        }
    except Exception as e:
        raise Exception(f"Redis check failed: {e}")

async def check_embedding_model():
    """Check embedding model availability"""
    start_time = time.time()
    try:
        test_text = "This is a test sentence for health check"
        embedding = await embedding_service.get_embedding(test_text)
        response_time = (time.time() - start_time) * 1000
        
        return {
            'response_time_ms': response_time,
            'details': {
                'model_loaded': True,
                'embedding_size': len(embedding)
            }
        }
    except Exception as e:
        raise Exception(f"Embedding model check failed: {e}")

async def check_disk_space():
    """Check available disk space"""
    import shutil
    try:
        total, used, free = shutil.disk_usage('/opt/data_quality')
        free_percent = (free / total) * 100
        
        if free_percent < 10:
            raise Exception(f"Low disk space: {free_percent:.1f}% free")
        
        return {
            'response_time_ms': 1,
            'details': {
                'total_gb': round(total / (1024**3), 2),
                'free_gb': round(free / (1024**3), 2),
                'free_percent': round(free_percent, 1)
            }
        }
    except Exception as e:
        raise Exception(f"Disk space check failed: {e}")

# Initialize health checker
health_checker = HealthChecker()
health_checker.register_check('database', check_database, timeout=10)
health_checker.register_check('redis', check_redis, timeout=5)
health_checker.register_check('embedding_model', check_embedding_model, timeout=15)
health_checker.register_check('disk_space', check_disk_space, timeout=5)
```

### 8.3 Log Analysis Tools

```bash
#!/bin/bash
# File: scripts/analyze_logs.sh

# Log analysis script for troubleshooting

LOG_FILE="/opt/data_quality/logs/data_quality.log"
DATE_PATTERN=$(date +"%Y-%m-%d")

echo "ğŸ“Š Data Quality Module Log Analysis - $DATE_PATTERN"
echo "=================================================="

# Error summary
echo ""
echo "ğŸ”¥ ERROR SUMMARY:"
echo "----------------"
grep "ERROR" $LOG_FILE | grep "$DATE_PATTERN" | \
    sed 's/.*ERROR - //' | sort | uniq -c | sort -nr

# Performance issues
echo ""
echo "â±ï¸ SLOW OPERATIONS (>30 seconds):"
echo "---------------------------------"
grep "processing_time" $LOG_FILE | grep "$DATE_PATTERN" | \
    awk '$NF > 30000 {print $0}' | tail -10

# Memory warnings
echo ""
echo "ğŸ’¾ MEMORY WARNINGS:"
echo "------------------"
grep -i "memory" $LOG_FILE | grep "$DATE_PATTERN" | tail -10

# Database connection issues
echo ""
echo "ğŸ—„ï¸ DATABASE ISSUES:"
echo "------------------"
grep -i "database\|connection\|pool" $LOG_FILE | grep "ERROR\|WARNING" | \
    grep "$DATE_PATTERN" | tail -10

# Quality assessment statistics
echo ""
echo "ğŸ“ˆ QUALITY ASSESSMENT STATS:"
echo "----------------------------"
echo "Total assessments today: $(grep "assessment completed" $LOG_FILE | grep "$DATE_PATTERN" | wc -l)"
echo "Approved documents: $(grep "status.*approved" $LOG_FILE | grep "$DATE_PATTERN" | wc -l)"
echo "Rejected documents: $(grep "status.*rejected" $LOG_FILE | grep "$DATE_PATTERN" | wc -l)"

# Top error types
echo ""
echo "ğŸš¨ TOP ERROR TYPES:"
echo "------------------"
grep "ERROR" $LOG_FILE | grep "$DATE_PATTERN" | \
    sed -n 's/.*\(Error\|Exception\): \([^:]*\).*/\2/p' | \
    sort | uniq -c | sort -nr | head -5
```

---

## 9. MAINTENANCE VÃ€ OPTIMIZATION

### 9.1 Database Maintenance

```sql
-- File: scripts/db_maintenance.sql

-- Daily maintenance tasks
-- 1. Update table statistics
ANALYZE quality_assessments;
ANALYZE duplicate_pairs;
ANALYZE quality_rules;

-- 2. Clean old assessment data (older than 90 days)
DELETE FROM quality_assessments 
WHERE created_at < NOW() - INTERVAL '90 days';

-- 3. Rebuild indexes if fragmented
REINDEX INDEX CONCURRENTLY idx_quality_assessments_document_id;
REINDEX INDEX CONCURRENTLY idx_quality_assessments_date;

-- 4. Check for unused indexes
SELECT 
    schemaname, 
    tablename, 
    attname, 
    n_distinct, 
    correlation 
FROM pg_stats 
WHERE schemaname = 'public' 
    AND n_distinct < 10;

-- 5. Monitor query performance
SELECT 
    query,
    calls,
    total_time,
    mean_time,
    rows
FROM pg_stat_statements 
WHERE query LIKE '%quality_assessments%'
ORDER BY total_time DESC
LIMIT 10;
```

### 9.2 Performance Optimization

```python
# File: src/optimization/batch_optimizer.py

import asyncio
from typing import List, Dict, Any
from concurrent.futures import ThreadPoolExecutor

class BatchOptimizer:
    def __init__(self, max_workers: int = 8):
        self.max_workers = max_workers
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        
    async def optimize_duplicate_detection(self, documents: List[Dict[str, Any]]):
        """
        Optimize duplicate detection using clustering vÃ  parallel processing
        """
        # Step 1: Quick hash-based filtering
        hash_groups = self._group_by_hash(documents)
        
        # Step 2: Semantic clustering for remaining documents
        semantic_candidates = []
        for group in hash_groups.values():
            if len(group) == 1:  # No hash duplicates
                semantic_candidates.extend(group)
        
        # Step 3: Parallel semantic analysis
        if semantic_candidates:
            semantic_duplicates = await self._parallel_semantic_analysis(
                semantic_candidates
            )
        
        return {
            'hash_duplicates': [g for g in hash_groups.values() if len(g) > 1],
            'semantic_duplicates': semantic_duplicates
        }
    
    def _group_by_hash(self, documents: List[Dict[str, Any]]) -> Dict[str, List]:
        """Group documents by content hash"""
        hash_groups = {}
        for doc in documents:
            content_hash = hashlib.sha256(doc['content'].encode()).hexdigest()
            if content_hash not in hash_groups:
                hash_groups[content_hash] = []
            hash_groups[content_hash].append(doc)
        return hash_groups
    
    async def _parallel_semantic_analysis(self, documents: List[Dict[str, Any]]):
        """Run semantic analysis in parallel batches"""
        batch_size = 50
        batches = [
            documents[i:i + batch_size] 
            for i in range(0, len(documents), batch_size)
        ]
        
        tasks = []
        for batch in batches:
            task = asyncio.create_task(self._process_semantic_batch(batch))
            tasks.append(task)
        
        results = await asyncio.gather(*tasks)
        
        # Flatten results
        all_duplicates = []
        for batch_result in results:
            all_duplicates.extend(batch_result)
        
        return all_duplicates
    
    async def _process_semantic_batch(self, batch: List[Dict[str, Any]]):
        """Process a batch of documents for semantic similarity"""
        # Implementation would use clustering algorithms
        # like DBSCAN or hierarchical clustering
        pass

# Usage
optimizer = BatchOptimizer(max_workers=8)
```

### 9.3 Caching Strategy

```python
# File: src/caching/cache_manager.py

import json
import hashlib
from typing import Any, Optional, Dict
from datetime import datetime, timedelta

class CacheManager:
    def __init__(self, redis_client):
        self.redis = redis_client
        self.default_ttl = 3600  # 1 hour
        
    async def get_cached_result(
        self, 
        key: str, 
        compute_func: callable,
        ttl: Optional[int] = None
    ) -> Any:
        """
        Get cached result or compute vÃ  cache new result
        """
        ttl = ttl or self.default_ttl
        
        # Try to get from cache
        cached = await self.redis.get(key)
        if cached:
            try:
                return json.loads(cached)
            except json.JSONDecodeError:
                await self.redis.delete(key)  # Invalid cache entry
        
        # Compute new result
        result = await compute_func()
        
        # Cache result
        await self.redis.setex(
            key, 
            ttl, 
            json.dumps(result, default=str)  # Handle datetime serialization
        )
        
        return result
    
    def generate_cache_key(self, prefix: str, **kwargs) -> str:
        """Generate consistent cache key tá»« parameters"""
        key_parts = [prefix]
        
        # Sort kwargs for consistent ordering
        for k, v in sorted(kwargs.items()):
            if isinstance(v, (dict, list)):
                v = json.dumps(v, sort_keys=True)
            key_parts.append(f"{k}:{v}")
        
        key_string = "|".join(key_parts)
        
        # Hash long keys
        if len(key_string) > 200:
            key_hash = hashlib.md5(key_string.encode()).hexdigest()
            return f"{prefix}:hash:{key_hash}"
        
        return key_string
    
    async def invalidate_pattern(self, pattern: str):
        """Invalidate cache keys matching pattern"""
        keys = await self.redis.keys(pattern)
        if keys:
            await self.redis.delete(*keys)
    
    async def warm_up_cache(self, document_ids: List[str]):
        """Pre-warm cache vá»›i commonly accessed documents"""
        for doc_id in document_ids:
            cache_key = self.generate_cache_key("quality_assessment", document_id=doc_id)
            
            # Check if already cached
            exists = await self.redis.exists(cache_key)
            if not exists:
                # Compute vÃ  cache result
                await self.get_cached_result(
                    cache_key,
                    lambda: self._compute_quality_assessment(doc_id),
                    ttl=7200  # 2 hours for warm-up cache
                )

# Cache decorators
def cached_method(ttl: int = 3600, key_prefix: str = None):
    """Decorator for caching method results"""
    def decorator(func):
        @wraps(func)
        async def wrapper(self, *args, **kwargs):
            cache_key = cache_manager.generate_cache_key(
                key_prefix or func.__name__,
                args=args,
                kwargs=kwargs
            )
            
            return await cache_manager.get_cached_result(
                cache_key,
                lambda: func(self, *args, **kwargs),
                ttl=ttl
            )
        return wrapper
    return decorator
```

---

## 10. DOCUMENTATION VÃ€ HANDOVER

### 10.1 API Documentation

```python
# File: src/api/main.py - Enhanced vá»›i detailed documentation

from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks
from fastapi.openapi.docs import get_swagger_ui_html
from fastapi.openapi.utils import get_openapi

app = FastAPI(
    title="Data Quality Assessment API",
    description="""
    ## Data Quality Assessment Tool
    
    This API provides comprehensive data quality assessment capabilities:
    
    * **Duplicate Detection**: Hash-based and semantic duplicate detection
    * **Content Quality**: Ambiguity, contradiction, and readability analysis  
    * **Metadata Validation**: Completeness and format validation
    * **Quality Scoring**: Configurable scoring engine with business rules
    * **Batch Processing**: Efficient processing of multiple documents
    * **Real-time Monitoring**: Health checks and performance metrics
    
    ### Authentication
    API uses JWT tokens. Include in Authorization header: `Bearer <token>`
    
    ### Rate Limiting  
    - Standard users: 100 requests/hour
    - Premium users: 1000 requests/hour
    
    ### Error Handling
    All errors follow RFC 7807 Problem Details format.
    """,
    version="1.0.0",
    terms_of_service="http://company.com/terms/",
    contact={
        "name": "Data Quality Team",
        "url": "http://company.com/contact/",
        "email": "data-quality@company.com",
    },
    license_info={
        "name": "Internal Use License",
        "url": "http://company.com/license/",
    },
)

@app.post(
    "/api/v1/assess-quality",
    response_model=QualityAssessmentResponse,
    summary="Assess Document Quality",
    description="""
    Perform comprehensive quality assessment on a single document.
    
    The assessment includes:
    1. **Duplicate Detection**: Checks against existing documents
    2. **Content Analysis**: Evaluates readability, ambiguity, contradictions
    3. **Metadata Validation**: Ensures required fields are present and valid
    4. **Quality Scoring**: Calculates overall quality score (0-100)
    
    ### Processing Time
    - Small documents (<10KB): ~2-5 seconds
    - Medium documents (10-100KB): ~5-15 seconds  
    - Large documents (>100KB): ~15-60 seconds
    
    ### Quality Thresholds
    - **90-100**: Excellent quality, auto-approved
    - **75-89**: Good quality, minor issues flagged
    - **60-74**: Acceptable quality, review recommended
    - **40-59**: Needs improvement, manual review required
    - **0-39**: Poor quality, rejected
    """,
    responses={
        200: {"description": "Assessment completed successfully"},
        400: {"description": "Invalid document format or missing required fields"},
        422: {"description": "Validation error in request data"},
        429: {"description": "Rate limit exceeded"},
        500: {"description": "Internal server error during assessment"},
    }
)
async def assess_document_quality(
    request: QualityAssessmentRequest,
    background_tasks: BackgroundTasks,
    current_user: dict = Depends(get_current_user)
):
    """Implementation here"""
    pass
```

### 10.2 Team Handover Document

```markdown
# HANDOVER DOCUMENT - DATA QUALITY ASSESSMENT MODULE

## ğŸ“‹ OVERVIEW
Module: Data Quality Assessment Tool (FR-03.2)
Status: Production Ready
Handover Date: [Current Date]
Team: Backend Development Team â†’ Operations Team

## ğŸ¯ WHAT THIS MODULE DOES
- Detects duplicate documents (exact, semantic, keyword-based)
- Analyzes content quality (readability, ambiguity, contradictions)
- Validates metadata completeness and format
- Generates quality scores and reports
- Provides visual dashboards for quality monitoring

## ğŸ—ï¸ TECHNICAL ARCHITECTURE

### Core Components
1. **Duplicate Detection Engine**
   - Hash-based: Uses SHA256 for exact matches
   - Semantic: Uses sentence-transformers for meaning-based detection
   - Keyword: Uses TF-IDF and fuzzy matching

2. **Content Quality Analyzer**
   - Readability: Flesch-Kincaid, SMOG, Coleman-Liau scores
   - Ambiguity: NLP-based detection of unclear content
   - Contradictions: Uses NLI models to find conflicting statements

3. **Metadata Validator**
   - Required fields validation
   - Format validation (dates, emails, enums)
   - Business rule validation

4. **Scoring Engine**
   - Configurable weights and thresholds
   - Business rules engine
   - Pass/fail determination

### Dependencies
- **Python 3.9+**: Core runtime
- **PostgreSQL 15+**: Metadata and results storage
- **Redis 7+**: Caching and session storage
- **sentence-transformers**: Embedding generation
- **FastAPI**: REST API framework
- **Docker**: Containerization

## ğŸ“ KEY FILES AND LOCATIONS

```
/opt/data_quality/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ duplicate_detection/     # Duplicate detection logic
â”‚   â”œâ”€â”€ content_analysis/        # Content quality analysis
â”‚   â”œâ”€â”€ metadata_validation/     # Metadata validation
â”‚   â”œâ”€â”€ scoring/                 # Quality scoring engine
â”‚   â”œâ”€â”€ api/                     # REST API endpoints
â”‚   â””â”€â”€ dashboard/               # Web dashboard
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ config.yaml             # Main configuration
â”‚   â”œâ”€â”€ database.yaml           # Database settings
â”‚   â””â”€â”€ scoring_rules.yaml      # Quality scoring rules
â”œâ”€â”€ logs/                       # Application logs
â”œâ”€â”€ scripts/                    # Maintenance scripts
â””â”€â”€ tests/                      # Test suites
```

## ğŸ”§ OPERATIONS GUIDE

### Daily Tasks
1. **Monitor System Health**
   ```bash
   curl -f http://localhost:8000/health
   ```

2. **Check Log Files**
   ```bash
   tail -f /opt/data_quality/logs/data_quality.log
   ```

3. **Database Maintenance**
   ```bash
   /opt/data_quality/scripts/daily_maintenance.sh
   ```

### Weekly Tasks
1. **Performance Review**
   - Check Grafana dashboards
   - Review slow query logs
   - Analyze quality trends

2. **Capacity Planning**
   - Monitor disk usage
   - Check memory consumption
   - Review database growth

### Monthly Tasks
1. **Model Updates**
   - Update embedding models if available
   - Retrain custom models with new data
   - Update business rules

2. **Security Review**
   - Update dependencies
   - Review access logs
   - Security scan

## ğŸš¨ ALERT RESPONSES

### Critical Alerts (Immediate Response Required)

#### Service Down
```bash
# Check service status
sudo systemctl status data-quality-api

# Restart if needed
sudo systemctl restart data-quality-api

# Check logs for errors
sudo journalctl -u data-quality-api -f
```

#### Database Connection Failed
```bash
# Check PostgreSQL status
sudo systemctl status postgresql

# Check connections
sudo -u postgres psql -c "SELECT count(*) FROM pg_stat_activity;"

# Restart connection pool if needed
sudo systemctl restart data-quality-api
```

#### High Memory Usage
```bash
# Check memory usage
free -h
ps aux --sort=-%mem | head -10

# Restart service to clear memory
sudo systemctl restart data-quality-api
```

### Warning Alerts (Response Within 1 Hour)

#### High Response Time
- Check slow query log
- Review current processing load
- Consider scaling resources

#### High Error Rate
- Review application logs
- Check for pattern in errors
- Investigate root cause

## ğŸ“Š MONITORING AND METRICS

### Key Metrics to Watch
1. **Performance Metrics**
   - Average response time: < 30 seconds
   - Throughput: > 2 documents/second
   - Error rate: < 1%

2. **Resource Metrics**
   - Memory usage: < 8GB
   - CPU usage: < 80%
   - Disk usage: < 90%

3. **Business Metrics**
   - Daily assessments processed
   - Quality score distribution
   - Approval/rejection rates

### Dashboards
- **Grafana**: http://localhost:3000/d/data-quality-overview
- **Application Dashboard**: http://localhost:8000/dashboard
- **Prometheus Metrics**: http://localhost:9090

### Log Locations
- **Application Logs**: `/opt/data_quality/logs/data_quality.log`
- **API Access Logs**: `/opt/data_quality/logs/api_access.log`
- **Error Logs**: `/opt/data_quality/logs/errors.log`
- **System Logs**: `journalctl -u data-quality-api`

## ğŸ› ï¸ CONFIGURATION MANAGEMENT

### Environment Variables
```bash
# Production environment
export ENVIRONMENT=production
export DATABASE_URL=postgresql://dq_prod_user:password@localhost:5432/data_quality_prod
export REDIS_URL=redis://localhost:6379/0
export LOG_LEVEL=INFO
export MAX_WORKERS=8
export CACHE_TTL=3600
```

### Configuration Files
1. **Main Config** (`config/config.yaml`)
   - Database connections
   - Redis settings  
   - Quality thresholds
   - Processing parameters

2. **Scoring Rules** (`config/scoring_rules.yaml`)
   - Weight configurations
   - Business rules
   - Threshold settings

3. **Model Config** (`config/models.yaml`)
   - Embedding model settings
   - NLP model parameters
   - Performance tuning

### Making Configuration Changes
1. Edit configuration file
2. Validate syntax: `python scripts/validate_config.py`
3. Test in staging environment
4. Apply to production: `sudo systemctl restart data-quality-api`
5. Monitor for issues

## ğŸ”„ DEPLOYMENT PROCEDURES

### Regular Deployment (Code Updates)
```bash
# 1. Backup current version
sudo cp -r /opt/data_quality /opt/data_quality_backup_$(date +%Y%m%d)

# 2. Pull latest code
cd /opt/data_quality
git pull origin main

# 3. Update dependencies
source venv_prod/bin/activate
pip install -r requirements.txt

# 4. Run database migrations
python scripts/migrate_db.py

# 5. Run tests
python -m pytest tests/ -v

# 6. Restart service
sudo systemctl restart data-quality-api

# 7. Health check
sleep 10
curl -f http://localhost:8000/health || echo "Deployment failed!"
```

### Emergency Rollback
```bash
# 1. Stop current service
sudo systemctl stop data-quality-api

# 2. Restore backup
sudo rm -rf /opt/data_quality
sudo mv /opt/data_quality_backup_$(date +%Y%m%d) /opt/data_quality

# 3. Restore database if needed
sudo -u postgres psql data_quality_prod < /opt/backups/db_backup_$(date +%Y%m%d).sql

# 4. Start service
sudo systemctl start data-quality-api

# 5. Verify
curl -f http://localhost:8000/health
```

### Model Updates
```bash
# 1. Download new model
cd /opt/data_quality
python scripts/download_model.py --model sentence-transformers/all-mpnet-base-v2

# 2. Test model performance
python scripts/benchmark_model.py --model all-mpnet-base-v2

# 3. Update configuration
vim config/models.yaml

# 4. Restart with new model
sudo systemctl restart data-quality-api
```

## ğŸ› TROUBLESHOOTING GUIDE

### Common Issues vÃ  Quick Fixes

#### 1. "Model loading timeout"
```bash
# Check available memory
free -h

# If memory is low, restart service
sudo systemctl restart data-quality-api

# If persistent, switch to CPU-only mode
vim config/models.yaml
# Set: device: 'cpu'
```

#### 2. "Database connection pool exhausted"
```bash
# Check active connections
sudo -u postgres psql data_quality_prod -c "SELECT count(*) FROM pg_stat_activity WHERE state='active';"

# Kill long-running queries
sudo -u postgres psql data_quality_prod -c "SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE state='active' AND query_start < NOW() - INTERVAL '5 minutes';"

# Restart API service
sudo systemctl restart data-quality-api
```

#### 3. "Redis connection failed"
```bash
# Check Redis status
sudo systemctl status redis-server

# Test connection
redis-cli ping

# Clear Redis if corrupted
redis-cli flushall
sudo systemctl restart redis-server
```

#### 4. "High memory usage"
```bash
# Check memory usage by process
ps aux --sort=-%mem | head -10

# Check for memory leaks
python scripts/memory_profile.py

# Restart service
sudo systemctl restart data-quality-api
```

### Log Analysis Commands
```bash
# Find errors in last 24 hours
grep -i error /opt/data_quality/logs/data_quality.log | grep $(date +"%Y-%m-%d")

# Performance analysis
grep "processing_time" /opt/data_quality/logs/data_quality.log | awk '{print $NF}' | sort -n | tail -10

# Memory warnings
grep -i "memory" /opt/data_quality/logs/data_quality.log | tail -20

# Database errors
grep -i "database\|connection" /opt/data_quality/logs/data_quality.log | grep ERROR
```

## ğŸ“ ESCALATION CONTACTS

### Primary Contacts
- **Technical Lead**: John Smith (john.smith@company.com, +1-xxx-xxx-xxxx)
- **DevOps Engineer**: Jane Doe (jane.doe@company.com, +1-xxx-xxx-xxxx)  
- **Database Administrator**: Mike Johnson (mike.johnson@company.com, +1-xxx-xxx-xxxx)

### Secondary Contacts  
- **Product Owner**: Sarah Wilson (sarah.wilson@company.com)
- **IT Support**: support@company.com, +1-xxx-xxx-xxxx

### Emergency Escalation
1. **Level 1**: Service degradation â†’ DevOps Engineer
2. **Level 2**: Service down â†’ Technical Lead  
3. **Level 3**: Data corruption â†’ Database Administrator + Technical Lead
4. **Level 4**: Security incident â†’ CISO + Technical Lead

## ğŸ“š KNOWLEDGE BASE

### Useful Commands
```bash
# Check service status
sudo systemctl status data-quality-api

# View real-time logs
sudo journalctl -u data-quality-api -f

# Check API health
curl -s http://localhost:8000/health | jq '.'

# Monitor system resources
htop

# Check database performance
sudo -u postgres psql data_quality_prod -c "\x" -c "SELECT * FROM pg_stat_statements ORDER BY total_time DESC LIMIT 5;"

# Redis monitoring
redis-cli monitor

# Test embedding model
python -c "from src.models import EmbeddingService; es = EmbeddingService(); print('Model loaded successfully')"
```

### Configuration Examples
```yaml
# High-performance configuration for busy periods
processing:
  max_concurrent_tasks: 12
  batch_processing_size: 200
  timeout_seconds: 120

# Memory-conservative configuration
processing:
  max_concurrent_tasks: 4
  batch_processing_size: 50
  timeout_seconds: 300

# Quality-focused configuration
scoring:
  thresholds:
    excellent: 95.0
    good: 85.0
    acceptable: 70.0
```

## ğŸ”§ MAINTENANCE SCRIPTS

### Daily Maintenance (`scripts/daily_maintenance.sh`)
- Database cleanup
- Log rotation
- Cache optimization
- Health checks

### Weekly Reports (`scripts/weekly_report.py`)
- Performance metrics
- Quality trends
- Error analysis
- Resource utilization

### Monthly Tasks (`scripts/monthly_tasks.sh`)
- Database vacuum and reindex
- Model performance review
- Security updates
- Capacity planning

## ğŸ“ˆ PERFORMANCE BENCHMARKS

### Expected Performance (Production Load)
- **Document Processing**: 2-5 documents/second
- **API Response Time**: < 30 seconds (95th percentile)
- **Memory Usage**: 4-8GB under normal load
- **CPU Usage**: 40-70% under normal load
- **Database Response**: < 100ms for typical queries

### Load Test Results
```bash
# Run load test
python scripts/load_test.py --concurrent 50 --duration 300

# Expected results:
# - 50 concurrent users
# - 300 second duration  
# - 0% error rate
# - Average response time < 45 seconds
```

## ğŸš€ FUTURE ROADMAP

### Planned Improvements (Next 3 months)
1. **Performance Optimization**
   - Implement async batch processing
   - Add GPU acceleration for embeddings
   - Optimize database queries

2. **Feature Enhancements**
   - Multi-language support
   - Custom quality rules per department
   - Advanced visualization dashboards

3. **Operational Improvements**
   - Automated scaling
   - Enhanced monitoring
   - Predictive alerting

### Technical Debt
1. **Code Refactoring**
   - Split monolithic modules
   - Improve error handling
   - Add more comprehensive tests

2. **Infrastructure**
   - Migrate to Kubernetes
   - Implement blue-green deployment
   - Add disaster recovery

## âœ… HANDOVER CHECKLIST

### Technical Handover
- [ ] Code repository access provided
- [ ] Production environment access granted
- [ ] Database credentials shared securely
- [ ] Monitoring dashboards configured
- [ ] Alert configurations verified
- [ ] Documentation reviewed and updated
- [ ] Runbooks tested and validated

### Operational Handover
- [ ] Maintenance schedules documented
- [ ] Escalation procedures established
- [ ] Team contacts updated
- [ ] Knowledge transfer sessions completed
- [ ] Emergency procedures tested
- [ ] Backup and recovery verified

### Knowledge Transfer
- [ ] Architecture walkthrough completed
- [ ] Troubleshooting scenarios practiced  
- [ ] Configuration management explained
- [ ] Performance expectations set
- [ ] Future roadmap discussed

---

**Document Version**: 1.0
**Last Updated**: [Current Date]
**Next Review**: [Current Date + 3 months]

For questions or clarifications, contact the Technical Lead or refer to the internal wiki at: `http://wiki.company.com/data-quality-module`
```

---

## ğŸ“‹ FINAL SUMMARY

ÄÃ¢y lÃ  tÃ i liá»‡u thiáº¿t káº¿ hoÃ n chá»‰nh cho **Module CÃ´ng cá»¥ ÄÃ¡nh giÃ¡ Cháº¥t lÆ°á»£ng Dá»¯ liá»‡u (FR-03.2)** bao gá»“m:

### âœ… **ÄÃƒ HOÃ€N THÃ€NH**

1. **ğŸ“‹ Tá»•ng quan Module** - Má»¥c Ä‘Ã­ch, vá»‹ trÃ­ trong há»‡ thá»‘ng, Input/Output
2. **ğŸ’» Chuáº©n bá»‹ MÃ´i trÆ°á»ng** - Hardware, software, dependencies chi tiáº¿t
3. **ğŸ—ï¸ Kiáº¿n trÃºc Module** - Architecture diagram vá»›i Mermaid, luá»“ng xá»­ lÃ½
4. **ğŸ“… Implementation Roadmap** - 4 tuáº§n chi tiáº¿t vá»›i tasks cá»¥ thá»ƒ
5. **ğŸ”§ Technical Implementation** - Database schema, API spec, configuration
6. **âš ï¸ Error Handling** - Comprehensive error management vá»›i retry logic
7. **âš¡ Performance Optimization** - Caching, batch processing, memory management
8. **ğŸ§ª Testing Strategy** - Unit, integration, performance testing
9. **ğŸš€ Deployment Guide** - Production setup, Docker, monitoring
10. **ğŸ› ï¸ Troubleshooting** - Common issues, health checks, log analysis
11. **ğŸ“š Documentation** - API docs, handover document, operations guide

### ğŸ¯ **KEY FEATURES**

- **Comprehensive Quality Assessment**: Duplicate detection, content analysis, metadata validation
- **Production-Ready**: Full deployment pipeline vá»›i monitoring vÃ  alerting
- **Scalable Architecture**: Async processing, caching, load balancing
- **Robust Error Handling**: Retry logic, fallback mechanisms, graceful degradation
- **Operational Excellence**: Health checks, metrics, troubleshooting guides

### ğŸ“Š **SUCCESS CRITERIA**

- âœ… **Performance**: < 60 giÃ¢y response time, 2+ docs/second throughput
- âœ… **Reliability**: 99.5% uptime, comprehensive error handling
- âœ… **Maintainability**: Clear documentation, monitoring, troubleshooting
- âœ… **Scalability**: Support 100+ concurrent users, horizontal scaling ready

Team ká»¹ thuáº­t cÃ³ thá»ƒ **follow step-by-step** guide nÃ y Ä‘á»ƒ implement module má»™t cÃ¡ch tá»‘i Æ°u vÃ  production-ready! ğŸš€
