Nếu đã đủ thông tin hãy giúp tôi viết tài liệu thiết kế module dưới đây.
Tài liệu này cung cấp roadmap đầy đủ để implement FR-04.3 mà không cần code cụ thể nhưng 
có các bước làm cụ thể như chuẩn bị môi trường PC thế nào, phần mềm gì?
 để Team kỹ thuật có thể follow step-by-step guide này để thiết kế tối ưu cho hệ thống chatbot
 và có mermaidchart luồng công việc, điểm quay đầu khi bị fail.

### 3.4 Module RAG Core Engine (FR-04)

**FR-04.3 - Generation (Tạo sinh)**
- Tích hợp với LLM (OpenAI GPT, Anthropic Claude, hoặc local model)
- Sinh câu trả lời dựa trên context và câu hỏi
- Cung cấp citation/reference cho câu trả lời

---
# TÀI LIỆU THIẾT KẾ MODULE - RAG GENERATION ENGINE (FR-04.3)

---

**Module:** RAG Core Engine - Generation Service  
**Requirement:** FR-04.3 - Generation (Tạo sinh)  
**Phiên bản:** 1.0  
**Ngày:** 01/09/2025  
**Người soạn thảo:** Technical Design Team  

---

## 1. TỔNG QUAN MODULE

### 1.1 Mục đích
Module Generation Engine là thành phần quan trọng nhất trong RAG pipeline, chịu trách nhiệm:
- Tích hợp với các LLM providers (OpenAI, Anthropic Claude, Gemini, Grok, OpenRouter)
- Hỗ trợ local LLM models (Llama 2, Mistral, CodeLlama)
- Sinh câu trả lời chất lượng cao dựa trên context đã được retrieve
- Cung cấp citation và reference chính xác
- Xử lý fallback và error recovery

### 1.2 Input/Output Specification

**Input:**
```json
{
  "query": "Quy trình mua hàng trình giám đốc như thế nào?",
  "context_chunks": [
    {
      "chunk_id": "doc123_chunk_5",
      "content": "Bước 1: Tạo yêu cầu mua hàng...",
      "metadata": {
        "document_id": "proc_001",
        "title": "Quy trình Mua hàng",
        "page": 2,
        "confidence_score": 0.95
      }
    }
  ],
  "user_context": {
    "user_id": "emp001",
    "department": "rd",
    "access_level": "employee"
  },
  "conversation_history": []
}
```

**Output:**
```json
{
  "answer": "Quy trình mua hàng trình giám đốc gồm 5 bước chính...",
  "citations": [
    {
      "chunk_id": "doc123_chunk_5", 
      "reference": "[1] Quy trình Mua hàng, trang 2",
      "used_content": "Bước 1: Tạo yêu cầu mua hàng..."
    }
  ],
  "confidence_score": 0.92,
  "response_metadata": {
    "model_used": "openai-gpt-4",
    "tokens_used": 1250,
    "processing_time": 2.3,
    "cost_estimate": 0.025
  }
}
```

---

## 2. THIẾT KẾ KIẾN TRÚC MODULE

### 2.1 Architecture Overview

```mermaid
graph TB
    %% Input
    Input[📥 Input Request<br/>Query + Context + User Info]
    
    %% Generation Engine Core
    subgraph "🎯 GENERATION ENGINE CORE"
        subgraph "📝 Prompt Management"
            TemplateManager[📋 Template Manager<br/>System & User Prompts]
            PromptBuilder[🔧 Prompt Builder<br/>Dynamic Assembly]
            ContextOptimizer[⚡ Context Optimizer<br/>Token Management]
        end
        
        subgraph "🧠 LLM Integration Layer"
            ProviderRouter[🎯 Provider Router<br/>Load Balancing]
            
            subgraph "☁️ Cloud Providers"
                OpenAIClient[🤖 OpenAI Client]
                ClaudeClient[🧠 Claude Client] 
                GeminiClient[💎 Gemini Client]
                GrokClient[⚡ Grok Client]
                OpenRouterClient[🌐 OpenRouter Client]
            end
            
            subgraph "🏠 Local Models"
                LocalLLMManager[🖥️ Local LLM Manager]
                ModelLoader[📦 Model Loader]
                InferenceEngine[⚙️ Inference Engine]
            end
        end
        
        subgraph "📊 Response Processing"
            ResponseParser[📄 Response Parser<br/>Extract Answer]
            CitationGenerator[📚 Citation Generator<br/>Reference Linking]
            QualityValidator[✅ Quality Validator<br/>Answer Validation]
        end
        
        subgraph "🔄 Error Handling"
            FallbackManager[🛡️ Fallback Manager<br/>Provider Switching]
            RetryLogic[🔄 Retry Logic<br/>Exponential Backoff]
            ErrorRecovery[🚨 Error Recovery<br/>Graceful Degradation]
        end
    end
    
    %% External Dependencies
    subgraph "🌐 External Services"
        CloudAPIs[☁️ Cloud LLM APIs]
        LocalModels[🏠 Local Model Files]
        ConfigService[⚙️ Configuration Service]
        MonitoringService[📊 Monitoring Service]
    end
    
    %% Output
    Output[📤 Generated Response<br/>Answer + Citations + Metadata]
    
    %% Flow
    Input --> TemplateManager
    TemplateManager --> PromptBuilder
    PromptBuilder --> ContextOptimizer
    ContextOptimizer --> ProviderRouter
    
    ProviderRouter --> OpenAIClient
    ProviderRouter --> ClaudeClient
    ProviderRouter --> GeminiClient
    ProviderRouter --> GrokClient
    ProviderRouter --> OpenRouterClient
    ProviderRouter --> LocalLLMManager
    
    LocalLLMManager --> ModelLoader
    ModelLoader --> InferenceEngine
    
    OpenAIClient --> ResponseParser
    ClaudeClient --> ResponseParser
    GeminiClient --> ResponseParser
    GrokClient --> ResponseParser
    OpenRouterClient --> ResponseParser
    InferenceEngine --> ResponseParser
    
    ResponseParser --> CitationGenerator
    CitationGenerator --> QualityValidator
    QualityValidator --> Output
    
    %% Error Handling
    ProviderRouter -.-> FallbackManager
    FallbackManager --> RetryLogic
    RetryLogic --> ErrorRecovery
    ErrorRecovery -.-> ProviderRouter
    
    %% External Connections
    OpenAIClient -.-> CloudAPIs
    ClaudeClient -.-> CloudAPIs
    GeminiClient -.-> CloudAPIs
    GrokClient -.-> CloudAPIs
    OpenRouterClient -.-> CloudAPIs
    ModelLoader -.-> LocalModels
    TemplateManager -.-> ConfigService
    QualityValidator -.-> MonitoringService
    
    %% Styling
    classDef input fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef prompt fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef llm fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef cloud fill:#e1f5fe,stroke:#0277bd,stroke-width:2px
    classDef local fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef response fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    classDef error fill:#ffebee,stroke:#c62828,stroke-width:2px
    classDef external fill:#f1f8e9,stroke:#689f38,stroke-width:1px
    classDef output fill:#e0f2f1,stroke:#00796b,stroke-width:2px
    
    class Input input
    class TemplateManager,PromptBuilder,ContextOptimizer prompt
    class ProviderRouter llm
    class OpenAIClient,ClaudeClient,GeminiClient,GrokClient,OpenRouterClient cloud
    class LocalLLMManager,ModelLoader,InferenceEngine local
    class ResponseParser,CitationGenerator,QualityValidator response
    class FallbackManager,RetryLogic,ErrorRecovery error
    class CloudAPIs,LocalModels,ConfigService,MonitoringService external
    class Output output
```

---

## 3. CHUẨN BỊ MÔI TRƯỜNG PHÁT TRIỂN

### 3.1 Hardware Requirements

**Minimum Requirements:**
- **CPU**: Intel i7/AMD Ryzen 7 (8 cores)
- **RAM**: 32GB (64GB recommended cho local LLM)
- **GPU**: NVIDIA RTX 3080/4080 (12GB+ VRAM) cho local inference
- **Storage**: 1TB NVMe SSD
- **Network**: Stable internet connection (minimum 100Mbps)

**Recommended Production Setup:**
- **CPU**: Intel Xeon/AMD EPYC (16+ cores)
- **RAM**: 128GB+
- **GPU**: NVIDIA A100/H100 cho production local LLM
- **Storage**: 2TB+ NVMe SSD với RAID configuration

### 3.2 Software Environment Setup

#### 3.2.1 Base Environment
```bash
# 1. Operating System
Ubuntu 22.04 LTS hoặc CentOS 8+
Windows 11 Pro (cho development)

# 2. Container Platform
Docker Desktop 4.20+
Docker Compose 2.18+
Kubernetes 1.27+ (cho production)

# 3. Programming Languages
Python 3.11+
Node.js 18+ (cho monitoring dashboard)
Go 1.21+ (optional, cho high-performance components)
```

#### 3.2.2 Python Environment
```bash
# Tạo virtual environment
python3.11 -m venv llm_generation_env
source llm_generation_env/bin/activate  # Linux/Mac
# llm_generation_env\Scripts\activate  # Windows

# Core packages
pip install --upgrade pip setuptools wheel

# LLM Integration
pip install openai==1.30.0
pip install anthropic==0.28.0
pip install google-cloud-aiplatform==1.51.0
pip install transformers==4.37.0
pip install torch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0
pip install accelerate==0.27.0
pip install bitsandbytes==0.42.0

# API & Web Framework
pip install fastapi==0.109.0
pip install uvicorn==0.27.0
pip install httpx==0.26.0
pip install aiofiles==23.2.1

# Monitoring & Logging
pip install prometheus-client==0.19.0
pip install structlog==23.2.0
pip install sentry-sdk==1.40.0

# Data Processing
pip install pydantic==2.5.0
pip install tiktoken==0.6.0
pip install jinja2==3.1.3

# Testing
pip install pytest==8.0.0
pip install pytest-asyncio==0.23.0
pip install httpx==0.26.0
```

#### 3.2.3 Local LLM Setup (Optional)
```bash
# Ollama cho easy local LLM deployment
curl -fsSL https://ollama.ai/install.sh | sh

# HuggingFace Hub
pip install huggingface-hub==0.20.0

# vLLM cho high-performance inference
pip install vllm==0.3.0

# Text Generation Inference (TGI)
docker pull ghcr.io/huggingface/text-generation-inference:1.4
```

#### 3.2.4 Configuration Management
```bash
# Environment management
pip install python-dotenv==1.0.0
pip install pyyaml==6.0.1

# Secret management
pip install cryptography==42.0.0
pip install keyring==24.3.0
```

### 3.3 Development Tools

#### 3.3.1 IDE & Code Editor
```bash
# VS Code với extensions
- Python
- Pylance
- Docker
- Kubernetes
- REST Client
- GitLens

# JetBrains PyCharm Professional (recommended)
# Vim/Neovim với Python LSP setup
```

#### 3.3.2 Testing & Quality Tools
```bash
# Code formatting & linting
pip install black==24.0.0
pip install isort==5.13.0  
pip install flake8==7.0.0
pip install mypy==1.8.0

# Security scanning
pip install bandit==1.7.5
pip install safety==3.0.0

# Load testing
pip install locust==2.20.0
```

#### 3.3.3 Database & Cache
```bash
# Redis cho caching
docker run -d --name redis-cache -p 6379:6379 redis:7-alpine

# PostgreSQL cho metadata
docker run -d --name postgres-meta \
  -e POSTGRES_PASSWORD=dev_password \
  -p 5432:5432 postgres:15-alpine
```

---

## 4. IMPLEMENTATION ROADMAP

### 4.1 Development Phases

```mermaid
gantt
    title Implementation Roadmap - Generation Engine
    dateFormat  YYYY-MM-DD
    section Phase 1: Setup
    Environment Setup     :done, setup, 2025-09-01, 3d
    Basic Structure      :done, structure, after setup, 2d
    
    section Phase 2: Core Development  
    Template System      :active, template, 2025-09-06, 5d
    Provider Integration :provider, after template, 7d
    Response Processing  :response, after provider, 4d
    
    section Phase 3: Advanced Features
    Citation System      :citation, after response, 5d
    Error Handling       :error, after citation, 4d
    Local LLM Support    :local, after error, 6d
    
    section Phase 4: Testing & Optimization
    Unit Testing         :testing, after local, 4d
    Performance Testing  :perf, after testing, 3d
    Integration Testing  :integration, after perf, 5d
    
    section Phase 5: Production
    Production Deploy    :deploy, after integration, 3d
    Monitoring Setup     :monitor, after deploy, 2d
```

### 4.2 Detailed Implementation Steps

#### 4.2.1 Phase 1: Environment & Structure Setup (5 days)

**Day 1-3: Environment Setup**
```bash
# Step 1: Tạo project structure
mkdir rag-generation-engine
cd rag-generation-engine

mkdir -p {
  src/generation_engine/{core,providers,templates,utils},
  tests/{unit,integration,performance},
  configs/{development,staging,production},
  docs/{api,deployment},
  scripts/{setup,deployment,testing},
  docker/{development,production},
  monitoring/{prometheus,grafana}
}

# Step 2: Initialize Python project
touch src/generation_engine/__init__.py
touch requirements/{base.txt,dev.txt,prod.txt}
touch {.env.example,.gitignore,README.md,Dockerfile,docker-compose.yml}

# Step 3: Setup virtual environment
python3.11 -m venv venv
source venv/bin/activate
pip install -r requirements/dev.txt
```

**Day 4-5: Basic Project Structure**
```python
# src/generation_engine/core/models.py
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
from enum import Enum

class LLMProvider(str, Enum):
    OPENAI = "openai"
    CLAUDE = "claude"
    GEMINI = "gemini"
    GROK = "grok"
    OPENROUTER = "openrouter"
    LOCAL = "local"

class GenerationRequest(BaseModel):
    query: str
    context_chunks: List[Dict[str, Any]]
    user_context: Dict[str, Any]
    conversation_history: Optional[List[Dict[str, str]]] = []
    preferred_provider: Optional[LLMProvider] = None

class GenerationResponse(BaseModel):
    answer: str
    citations: List[Dict[str, Any]]
    confidence_score: float
    response_metadata: Dict[str, Any]
```

#### 4.2.2 Phase 2: Core Development (16 days)

**Days 6-10: Template Management System**

**Checkpoint 1**: Template System hoạt động cơ bản
```python
# src/generation_engine/templates/manager.py
class TemplateManager:
    """Quản lý system và user prompt templates"""
    
    def __init__(self, config_path: str):
        self.templates = self._load_templates(config_path)
        
    def get_system_prompt(self, context_type: str) -> str:
        """Lấy system prompt theo loại context"""
        pass
        
    def build_user_prompt(self, query: str, context: List[str]) -> str:
        """Xây dựng user prompt với query và context"""
        pass

# configs/prompts/system_prompts.yaml
vietnamese_qa:
  template: |
    Bạn là một trợ lý AI chuyên nghiệp của công ty kỹ thuật hàng không.
    Nhiệm vụ của bạn là trả lời câu hỏi dựa trên tài liệu nội bộ được cung cấp.
    
    QUY TÂC QUAN TRỌNG:
    1. Chỉ trả lời dựa trên thông tin trong context được cung cấp
    2. Nếu không có thông tin, hãy nói "Tôi không tìm thấy thông tin này trong tài liệu"
    3. Luôn cung cấp nguồn tham khảo [số] cho mỗi thông tin
    4. Trả lời bằng tiếng Việt chuyên nghiệp
```

**Days 11-17: Provider Integration**

**Checkpoint 2**: Tích hợp thành công với ít nhất 2 providers (OpenAI + Claude)
```python
# src/generation_engine/providers/base.py
from abc import ABC, abstractmethod

class BaseLLMProvider(ABC):
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        
    @abstractmethod
    async def generate(self, prompt: str, **kwargs) -> Dict[str, Any]:
        """Generate response từ LLM"""
        pass
        
    @abstractmethod
    def estimate_cost(self, input_tokens: int, output_tokens: int) -> float:
        """Ước tính chi phí API call"""
        pass

# src/generation_engine/providers/openai_provider.py
import openai
from .base import BaseLLMProvider

class OpenAIProvider(BaseLLMProvider):
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.client = openai.AsyncOpenAI(
            api_key=config["api_key"]
        )
    
    async def generate(self, prompt: str, **kwargs) -> Dict[str, Any]:
        response = await self.client.chat.completions.create(
            model=self.config.get("model", "gpt-4-turbo-preview"),
            messages=[{"role": "user", "content": prompt}],
            temperature=kwargs.get("temperature", 0.1),
            max_tokens=kwargs.get("max_tokens", 1000)
        )
        
        return {
            "text": response.choices[0].message.content,
            "tokens_used": response.usage.total_tokens,
            "model": response.model
        }
```

**Days 18-21: Response Processing**

**Checkpoint 3**: Response parsing và citation generation hoạt động
```python
# src/generation_engine/core/citation.py
class CitationGenerator:
    def __init__(self):
        self.citation_pattern = r'\[(\d+)\]'
        
    def generate_citations(self, answer: str, context_chunks: List[Dict]) -> List[Dict]:
        """Tạo citations từ answer và context chunks"""
        citations = []
        
        # Extract citation numbers from answer
        cited_numbers = re.findall(self.citation_pattern, answer)
        
        for num_str in cited_numbers:
            num = int(num_str) - 1  # Convert to 0-based index
            if num < len(context_chunks):
                chunk = context_chunks[num]
                citations.append({
                    "number": int(num_str),
                    "chunk_id": chunk["chunk_id"],
                    "reference": f"[{num_str}] {chunk['metadata']['title']}, trang {chunk['metadata'].get('page', 'N/A')}",
                    "used_content": chunk["content"][:200] + "..."
                })
        
        return citations
```

#### 4.2.3 Phase 3: Advanced Features (15 days)

**Days 22-26: Citation System Enhancement**

**Checkpoint 4**: Advanced citation với confidence scoring
```python
# src/generation_engine/core/citation_advanced.py
class AdvancedCitationGenerator:
    def __init__(self, embedding_model):
        self.embedding_model = embedding_model
        
    def calculate_citation_relevance(self, answer_part: str, context_chunk: str) -> float:
        """Tính toán độ liên quan giữa phần answer và context chunk"""
        # Sử dụng embedding similarity
        answer_embedding = self.embedding_model.encode([answer_part])
        context_embedding = self.embedding_model.encode([context_chunk])
        
        similarity = cosine_similarity(answer_embedding, context_embedding)[0][0]
        return float(similarity)
```

**Days 27-30: Error Handling & Fallback**

**Checkpoint 5**: Robust error handling với multiple fallback strategies
```python
# src/generation_engine/core/fallback.py
class FallbackManager:
    def __init__(self, providers: List[BaseLLMProvider]):
        self.providers = providers
        self.current_provider_index = 0
        
    async def generate_with_fallback(self, request: GenerationRequest) -> GenerationResponse:
        """Thử generate với fallback giữa các providers"""
        last_error = None
        
        for attempt in range(len(self.providers)):
            provider = self.providers[self.current_provider_index]
            
            try:
                result = await provider.generate(request)
                return result
                
            except Exception as e:
                last_error = e
                logger.warning(f"Provider {provider.__class__.__name__} failed: {e}")
                self.current_provider_index = (self.current_provider_index + 1) % len(self.providers)
                
                # Exponential backoff
                await asyncio.sleep(2 ** attempt)
        
        # Nếu tất cả providers đều fail
        raise GenerationError(f"All providers failed. Last error: {last_error}")
```

**Days 31-36: Local LLM Support**

**Checkpoint 6**: Local LLM integration hoạt động
```python
# src/generation_engine/providers/local_provider.py
class LocalLLMProvider(BaseLLMProvider):
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.model_name = config["model_name"]  # "llama2-7b-chat"
        self.inference_engine = self._setup_inference_engine()
        
    def _setup_inference_engine(self):
        if self.config["engine"] == "vllm":
            from vllm import LLM, SamplingParams
            return LLM(model=self.model_name)
        elif self.config["engine"] == "ollama":
            import ollama
            return ollama
        else:
            raise ValueError(f"Unsupported inference engine: {self.config['engine']}")
    
    async def generate(self, prompt: str, **kwargs) -> Dict[str, Any]:
        if self.config["engine"] == "vllm":
            sampling_params = SamplingParams(
                temperature=kwargs.get("temperature", 0.1),
                max_tokens=kwargs.get("max_tokens", 1000)
            )
            outputs = self.inference_engine.generate([prompt], sampling_params)
            
            return {
                "text": outputs[0].outputs[0].text,
                "tokens_used": len(outputs[0].outputs[0].token_ids),
                "model": self.model_name
            }
```

#### 4.2.4 Phase 4: Testing & Optimization (12 days)

**Days 37-40: Unit Testing**

**Checkpoint 7**: Test coverage > 80%
```python
# tests/unit/test_generation_core.py
import pytest
from unittest.mock import Mock, patch
from src.generation_engine.core.engine import GenerationEngine

class TestGenerationEngine:
    @pytest.fixture
    def mock_providers(self):
        return [Mock(), Mock(), Mock()]
    
    @pytest.fixture 
    def generation_engine(self, mock_providers):
        config = {"providers": mock_providers}
        return GenerationEngine(config)
    
    @pytest.mark.asyncio
    async def test_generate_success(self, generation_engine):
        # Setup
        request = GenerationRequest(
            query="Test query",
            context_chunks=[{"content": "test context"}],
            user_context={"user_id": "test_user"}
        )
        
        # Mock provider response
        generation_engine.providers[0].generate.return_value = {
            "text": "Test answer [1]",
            "tokens_used": 100,
            "model": "gpt-4"
        }
        
        # Test
        response = await generation_engine.generate(request)
        
        # Assert
        assert response.answer == "Test answer [1]"
        assert len(response.citations) == 1
        assert response.confidence_score > 0
```

**Days 41-43: Performance Testing**

**Checkpoint 8**: Performance benchmarks established
```python
# tests/performance/test_load.py
import asyncio
import time
from locust import HttpUser, task, between

class GenerationLoadTest(HttpUser):
    wait_time = between(1, 3)
    
    @task
    def test_generation_endpoint(self):
        payload = {
            "query": "Quy trình mua hàng như thế nào?",
            "context_chunks": [
                {
                    "content": "Bước 1: Tạo yêu cầu mua hàng...",
                    "metadata": {"title": "Quy trình mua hàng"}
                }
            ],
            "user_context": {"user_id": "test_user"}
        }
        
        with self.client.post("/api/generate", json=payload, catch_response=True) as response:
            if response.status_code == 200:
                json_response = response.json()
                if "answer" in json_response and "citations" in json_response:
                    response.success()
                else:
                    response.failure("Invalid response format")
            else:
                response.failure(f"HTTP {response.status_code}")

# Run với: locust -f tests/performance/test_load.py --host=http://localhost:8000
```

**Days 44-48: Integration Testing**

**Checkpoint 9**: End-to-end integration tests pass
```python
# tests/integration/test_full_pipeline.py
@pytest.mark.integration
class TestFullPipeline:
    @pytest.mark.asyncio
    async def test_end_to_end_generation(self):
        """Test complete pipeline từ request đến response"""
        
        # Setup real providers (với test API keys)
        config = load_test_config()
        engine = GenerationEngine(config)
        
        # Real request
        request = GenerationRequest(
            query="Quy trình mua hàng trình giám đốc như thế nào?",
            context_chunks=[
                {
                    "chunk_id": "test_chunk_1",
                    "content": "Quy trình mua hàng gồm 5 bước: 1) Tạo yêu cầu...",
                    "metadata": {
                        "document_id": "proc_001",
                        "title": "Quy trình Mua hàng",
                        "page": 1
                    }
                }
            ],
            user_context={
                "user_id": "test_user",
                "department": "rd", 
                "access_level": "employee"
            }
        )
        
        # Execute
        start_time = time.time()
        response = await engine.generate(request)
        end_time = time.time()
        
        # Assertions
        assert response.answer is not None
        assert len(response.answer) > 50  # Reasonable answer length
        assert response.confidence_score >= 0.5
        assert len(response.citations) > 0
        assert (end_time - start_time) < 60  # Under 60 seconds requirement
        assert response.response_metadata["tokens_used"] > 0
```

---

## 5. WORKFLOW VÀ ERROR HANDLING

### 5.1 Main Processing Workflow

```mermaid
flowchart TD
    Start([🚀 Start Generation Request]) --> ValidateInput{✅ Validate Input}
    
    ValidateInput -->|❌ Invalid| InputError[❌ Input Validation Error]
    ValidateInput -->|✅ Valid| SelectProvider[🎯 Select LLM Provider]
    
    SelectProvider --> CheckCache{💾 Check Response Cache}
    CheckCache -->|✅ Hit| ReturnCached[📋 Return Cached Response]
    CheckCache -->|❌ Miss| BuildPrompt[📝 Build Prompt Template]
    
    BuildPrompt --> OptimizeContext[⚡ Optimize Context Length]
    OptimizeContext --> CallLLM[🧠 Call LLM Provider]
    
    CallLLM --> LLMSuccess{✅ LLM Call Success?}
    
    %% Success Path
    LLMSuccess -->|✅ Success| ParseResponse[📄 Parse LLM Response]
    ParseResponse --> GenerateCitations[📚 Generate Citations]
    GenerateCitations --> ValidateQuality[🔍 Validate Response Quality]
    
    ValidateQuality --> QualityOK{✅ Quality OK?}
    QualityOK -->|✅ Pass| CacheResponse[💾 Cache Response]
    QualityOK -->|❌ Fail| RetryDifferentProvider[🔄 Retry with Different Provider]
    
    CacheResponse --> LogMetrics[📊 Log Performance Metrics]
    LogMetrics --> Success([✅ Return Success Response])
    
    %% Error Handling Paths
    LLMSuccess -->|❌ Fail| CheckErrorType{🔍 Check Error Type}
    
    CheckErrorType -->|🚫 Rate Limit| WaitAndRetry[⏳ Wait and Retry<br/>Exponential Backoff]
    CheckErrorType -->|💰 Quota Exceeded| SwitchProvider[🔄 Switch to Next Provider]
    CheckErrorType -->|🌐 Network Error| RetryCurrentProvider[🔄 Retry Current Provider]
    CheckErrorType -->|🔑 Auth Error| ProviderError[❌ Provider Auth Error]
    CheckErrorType -->|❓ Other Error| GenericError[❌ Generic Error]
    
    %% Retry Logic
    WaitAndRetry --> RetryAttempt{🔢 Retry Attempt < Max?}
    RetryAttempt -->|✅ Yes| CallLLM
    RetryAttempt -->|❌ No| SwitchProvider
    
    RetryCurrentProvider --> RetryCount{🔢 Retry Count < 3?}
    RetryCount -->|✅ Yes| CallLLM
    RetryCount -->|❌ No| SwitchProvider
    
    SwitchProvider --> HasNextProvider{🔍 Has Next Provider?}
    HasNextProvider -->|✅ Yes| SelectProvider
    HasNextProvider -->|❌ No| FallbackStrategy[🛡️ Execute Fallback Strategy]
    
    RetryDifferentProvider --> HasAlternativeProvider{🔍 Has Alternative Provider?}
    HasAlternativeProvider -->|✅ Yes| SelectProvider
    HasAlternativeProvider -->|❌ No| FallbackStrategy
    
    %% Fallback Strategies
    FallbackStrategy --> FallbackType{🛡️ Fallback Strategy}
    
    FallbackType -->|🏠 Local LLM| UseLocalLLM[🖥️ Use Local LLM]
    FallbackType -->|📋 Template| UseTemplate[📋 Use Template Response]
    FallbackType -->|🔍 Simple Search| SimpleSearch[🔎 Return Search Results Only]
    FallbackType -->|❌ None| FinalFailure[❌ Complete Failure]
    
    UseLocalLLM --> LocalLLMSuccess{✅ Local LLM Success?}
    LocalLLMSuccess -->|✅ Success| ParseResponse
    LocalLLMSuccess -->|❌ Fail| UseTemplate
    
    UseTemplate --> TemplateResponse[📋 Generate Template Response]
    TemplateResponse --> Success
    
    SimpleSearch --> SearchOnlyResponse[🔍 Return Context Chunks Only]
    SearchOnlyResponse --> Success
    
    %% Error Endpoints
    InputError --> LogError1[📊 Log Input Error]
    ProviderError --> LogError2[📊 Log Provider Error] 
    GenericError --> LogError3[📊 Log Generic Error]
    FinalFailure --> LogError4[📊 Log Complete Failure]
    
    LogError1 --> ErrorResponse1[❌ Return Input Error Response]
    LogError2 --> ErrorResponse2[❌ Return Provider Error Response]
    LogError3 --> ErrorResponse3[❌ Return Generic Error Response]  
    LogError4 --> ErrorResponse4[❌ Return Failure Response]
    
    %% Success Endpoints
    ReturnCached --> End([🏁 End])
    Success --> End
    ErrorResponse1 --> End
    ErrorResponse2 --> End
    ErrorResponse3 --> End
    ErrorResponse4 --> End
    
    %% Styling
    classDef startEnd fill:#e3f2fd,stroke:#1976d2,stroke-width:3px
    classDef process fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef decision fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef error fill:#ffebee,stroke:#c62828,stroke-width:2px
    classDef success fill:#e0f2f1,stroke:#00796b,stroke-width:2px
    classDef fallback fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    
    class Start,End startEnd
    class SelectProvider,BuildPrompt,OptimizeContext,CallLLM,ParseResponse,GenerateCitations,ValidateQuality,CacheResponse,LogMetrics process
    class ValidateInput,CheckCache,LLMSuccess,QualityOK,CheckErrorType,RetryAttempt,RetryCount,HasNextProvider,HasAlternativeProvider,FallbackType,LocalLLMSuccess decision
    class InputError,ProviderError,GenericError,FinalFailure,LogError1,LogError2,LogError3,LogError4,ErrorResponse1,ErrorResponse2,ErrorResponse3,ErrorResponse4 error
    class Success,ReturnCached,TemplateResponse,SearchOnlyResponse success
    class WaitAndRetry,SwitchProvider,RetryCurrentProvider,RetryDifferentProvider,FallbackStrategy,UseLocalLLM,UseTemplate,SimpleSearch fallback
```

### 5.2 Error Recovery Decision Tree

```mermaid
flowchart TD
    Error([🚨 Error Occurred]) --> ErrorType{🔍 Identify Error Type}
    
    %% API Errors
    ErrorType -->|🚫 Rate Limiting| RateLimit[⏳ Rate Limit Handler]
    ErrorType -->|💰 Quota/Billing| Quota[💰 Quota Handler] 
    ErrorType -->|🔑 Authentication| Auth[🔑 Auth Handler]
    ErrorType -->|🌐 Network/Timeout| Network[🌐 Network Handler]
    ErrorType -->|📊 Model Overload| ModelLoad[📊 Model Load Handler]
    ErrorType -->|📝 Content Filter| Content[📝 Content Handler]
    ErrorType -->|❓ Unknown API| UnknownAPI[❓ Unknown API Handler]
    
    %% Rate Limit Handling
    RateLimit --> RateLimitSeverity{📊 Check Severity}
    RateLimitSeverity -->|🟡 Light| WaitShort[⏳ Wait 30s + Retry]
    RateLimitSeverity -->|🟠 Medium| WaitMedium[⏳ Wait 2min + Switch Provider]
    RateLimitSeverity -->|🔴 Heavy| WaitLong[⏳ Wait 10min + Switch Provider]
    
    WaitShort --> RetryOriginal[🔄 Retry Original Provider]
    WaitMedium --> SwitchProvider[🔄 Switch to Next Provider]
    WaitLong --> SwitchProvider
    
    %% Quota Handling  
    Quota --> QuotaType{💰 Quota Type}
    QuotaType -->|📅 Daily Limit| DailyQuota[📅 Daily Quota Exceeded]
    QuotaType -->|💳 Billing| BillingIssue[💳 Billing Issue]
    QuotaType -->|🎯 Model Limit| ModelQuota[🎯 Model Quota Exceeded]
    
    DailyQuota --> SwitchProvider
    BillingIssue --> DisableProvider[❌ Disable Provider + Switch]
    ModelQuota --> SwitchModel[🔄 Switch to Different Model]
    
    %% Authentication Handling
    Auth --> AuthType{🔑 Auth Error Type}
    AuthType -->|🔐 Invalid Key| InvalidKey[🔐 Invalid API Key]
    AuthType -->|⏰ Expired Token| ExpiredToken[⏰ Token Expired]
    AuthType -->|🚫 Permissions| NoPermission[🚫 Insufficient Permissions]
    
    InvalidKey --> DisableProvider
    ExpiredToken --> RefreshToken[🔄 Refresh Token + Retry]
    NoPermission --> DisableProvider
    
    %% Network Handling
    Network --> NetworkSeverity{🌐 Network Issue Severity}
    NetworkSeverity -->|⚡ Timeout| TimeoutHandler[⚡ Timeout Handler]
    NetworkSeverity -->|🌍 DNS| DNSHandler[🌍 DNS Handler]  
    NetworkSeverity -->|🔌 Connection| ConnectionHandler[🔌 Connection Handler]
    
    TimeoutHandler --> IncreaseTimeout[⚡ Increase Timeout + Retry]
    DNSHandler --> SwitchEndpoint[🌍 Switch Endpoint + Retry]
    ConnectionHandler --> WaitAndRetry[⏳ Wait 5s + Retry]
    
    %% Model Load Handling
    ModelLoad --> LoadSeverity{📊 Load Severity}
    LoadSeverity -->|🟡 Light Load| RetryWithDelay[⏳ Retry with 10s delay]
    LoadSeverity -->|🟠 High Load| SwitchModel
    LoadSeverity -->|🔴 Overloaded| SwitchProvider
    
    %% Content Filter Handling
    Content --> ContentType{📝 Content Filter Type}
    ContentType -->|🤖 Safety Filter| SafetyFilter[🤖 Safety Filter Triggered]
    ContentType -->|📏 Length Limit| LengthLimit[📏 Content Too Long]
    ContentType -->|🚫 Policy Violation| PolicyViolation[🚫 Policy Violation]
    
    SafetyFilter --> ModifyPrompt[📝 Modify Prompt + Retry]
    LengthLimit --> TruncateContent[✂️ Truncate Content + Retry]
    PolicyViolation --> UseAlternativeApproach[🔄 Use Alternative Approach]
    
    %% Recovery Actions
    RetryOriginal --> RetrySuccess{✅ Retry Successful?}
    RetrySuccess -->|✅ Yes| Success([✅ Recovery Successful])
    RetrySuccess -->|❌ No| SwitchProvider
    
    SwitchProvider --> ProviderAvailable{🔍 Next Provider Available?}
    ProviderAvailable -->|✅ Yes| TryNextProvider[🎯 Try Next Provider]
    ProviderAvailable -->|❌ No| FallbackStrategy[🛡️ Execute Fallback]
    
    TryNextProvider --> ProviderSuccess{✅ Provider Success?}
    ProviderSuccess -->|✅ Yes| Success
    ProviderSuccess -->|❌ No| ProviderAvailable
    
    SwitchModel --> ModelSuccess{✅ Model Switch Success?}
    ModelSuccess -->|✅ Yes| Success
    ModelSuccess -->|❌ No| SwitchProvider
    
    DisableProvider --> RemoveFromPool[❌ Remove from Provider Pool]
    RemoveFromPool --> ProviderAvailable
    
    RefreshToken --> RefreshSuccess{✅ Refresh Success?}
    RefreshSuccess -->|✅ Yes| RetryOriginal
    RefreshSuccess -->|❌ No| DisableProvider
    
    IncreaseTimeout --> TimeoutRetry[🔄 Retry with Higher Timeout]
    TimeoutRetry --> TimeoutSuccess{✅ Timeout Retry Success?}
    TimeoutSuccess -->|✅ Yes| Success
    TimeoutSuccess -->|❌ No| SwitchProvider
    
    SwitchEndpoint --> EndpointRetry[🔄 Retry with New Endpoint]
    EndpointRetry --> EndpointSuccess{✅ Endpoint Success?}
    EndpointSuccess -->|✅ Yes| Success
    EndpointSuccess -->|❌ No| SwitchProvider
    
    WaitAndRetry --> NetworkRetry[🔄 Network Retry]
    NetworkRetry --> NetworkSuccess{✅ Network Success?}
    NetworkSuccess -->|✅ Yes| Success
    NetworkSuccess -->|❌ No| SwitchProvider
    
    RetryWithDelay --> DelayRetry[🔄 Delayed Retry]
    DelayRetry --> DelaySuccess{✅ Delay Success?}
    DelaySuccess -->|✅ Yes| Success
    DelaySuccess -->|❌ No| SwitchModel
    
    ModifyPrompt --> PromptRetry[🔄 Retry with Modified Prompt]
    PromptRetry --> PromptSuccess{✅ Prompt Success?}
    PromptSuccess -->|✅ Yes| Success
    PromptSuccess -->|❌ No| UseAlternativeApproach
    
    TruncateContent --> TruncateRetry[🔄 Retry with Truncated Content]
    TruncateRetry --> TruncateSuccess{✅ Truncate Success?}
    TruncateSuccess -->|✅ Yes| Success
    TruncateSuccess -->|❌ No| UseAlternativeApproach
    
    UseAlternativeApproach --> FallbackStrategy
    UnknownAPI --> LogUnknownError[📊 Log Unknown Error]
    LogUnknownError --> SwitchProvider
    
    %% Final Fallback
    FallbackStrategy --> FallbackType{🛡️ Fallback Strategy Type}
    FallbackType -->|🏠 Local LLM| LocalLLM[🖥️ Switch to Local LLM]
    FallbackType -->|📋 Template| TemplateResponse[📋 Use Template Response]
    FallbackType -->|🔍 Search Only| SearchOnly[🔎 Return Search Results]
    FallbackType -->|❌ None Available| CompleteFailure[❌ Complete System Failure]
    
    LocalLLM --> LocalSuccess{✅ Local LLM Success?}
    LocalSuccess -->|✅ Yes| Success
    LocalSuccess -->|❌ No| TemplateResponse
    
    TemplateResponse --> TemplateSuccess{✅ Template Success?}
    TemplateSuccess -->|✅ Yes| PartialSuccess([⚠️ Partial Success])
    TemplateSuccess -->|❌ No| SearchOnly
    
    SearchOnly --> SearchSuccess{✅ Search Success?}
    SearchSuccess -->|✅ Yes| MinimalSuccess([⚠️ Minimal Success])
    SearchSuccess -->|❌ No| CompleteFailure
    
    CompleteFailure --> SystemAlert[🚨 Trigger System Alert]
    SystemAlert --> Failure([❌ System Failure])
    
    %% Styling
    classDef error fill:#ffebee,stroke:#c62828,stroke-width:2px
    classDef handler fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef decision fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef action fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef success fill:#e0f2f1,stroke:#00796b,stroke-width:2px
    classDef failure fill:#ffebee,stroke:#d32f2f,stroke-width:3px
    classDef fallback fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    
    class Error error
    class RateLimit,Quota,Auth,Network,ModelLoad,Content,UnknownAPI handler
    class ErrorType,RateLimitSeverity,QuotaType,AuthType,NetworkSeverity,LoadSeverity,ContentType,RetrySuccess,ProviderAvailable,ProviderSuccess,ModelSuccess,RefreshSuccess,TimeoutSuccess,EndpointSuccess,NetworkSuccess,DelaySuccess,PromptSuccess,TruncateSuccess,LocalSuccess,TemplateSuccess,SearchSuccess decision
    class WaitShort,WaitMedium,WaitLong,SwitchProvider,DisableProvider,SwitchModel,RefreshToken,IncreaseTimeout,SwitchEndpoint,WaitAndRetry,RetryWithDelay,ModifyPrompt,TruncateContent,UseAlternativeApproach,RemoveFromPool,LogUnknownError,SystemAlert action
    class Success,PartialSuccess,MinimalSuccess success
    class CompleteFailure,Failure failure
    class FallbackStrategy,FallbackType,LocalLLM,TemplateResponse,SearchOnly fallback
```

---

## 6. CONFIGURATION MANAGEMENT

### 6.1 Environment Configuration

```yaml
# configs/development/llm_providers.yaml
providers:
  openai:
    enabled: true
    api_key: "${OPENAI_API_KEY}"
    base_url: "https://api.openai.com/v1"
    models:
      primary: "gpt-4-turbo-preview"
      fallback: "gpt-3.5-turbo"
    rate_limits:
      requests_per_minute: 500
      tokens_per_minute: 150000
    retry_config:
      max_retries: 3
      backoff_factor: 2
    cost_config:
      input_cost_per_1k: 0.01
      output_cost_per_1k: 0.03
      
  claude:
    enabled: true
    api_key: "${CLAUDE_API_KEY}"
    base_url: "https://api.anthropic.com/v1"
    models:
      primary: "claude-3-sonnet-20240229"
      fallback: "claude-3-haiku-20240307"
    rate_limits:
      requests_per_minute: 300
      tokens_per_minute: 100000
    retry_config:
      max_retries: 3
      backoff_factor: 2
    cost_config:
      input_cost_per_1k: 0.003
      output_cost_per_1k: 0.015

  gemini:
    enabled: true
    api_key: "${GEMINI_API_KEY}" 
    base_url: "https://generativelanguage.googleapis.com/v1"
    models:
      primary: "gemini-1.5-pro"
      fallback: "gemini-1.5-flash"
    rate_limits:
      requests_per_minute: 300
      tokens_per_minute: 120000
    cost_config:
      input_cost_per_1k: 0.0035
      output_cost_per_1k: 0.0105

  local_llm:
    enabled: false  # Enable for production
    engine: "vllm"  # or "ollama", "tgi"
    model_path: "/models/llama-2-7b-chat"
    gpu_memory_utilization: 0.8
    max_model_len: 4096
    dtype: "half"
    quantization: "awq"

# configs/development/generation.yaml
generation:
  default_provider: "openai"
  provider_selection_strategy: "round_robin"  # or "least_cost", "fastest"
  
  prompt_settings:
    max_context_tokens: 3000
    max_output_tokens: 1000
    temperature: 0.1
    top_p: 0.9
    
  quality_control:
    min_answer_length: 10
    max_answer_length: 2000
    confidence_threshold: 0.3
    citation_required: true
    
  caching:
    enabled: true
    ttl_seconds: 3600
    max_cache_size: 10000
    
  fallback:
    enabled: true
    strategies: ["switch_provider", "local_llm", "template_response", "search_only"]
    max_fallback_attempts: 3
```

### 6.2 Prompt Templates Configuration

```yaml
# configs/templates/system_prompts.yaml
system_prompts:
  vietnamese_qa:
    name: "Vietnamese Q&A Assistant"
    template: |
      Bạn là trợ lý AI chuyên nghiệp của công ty kỹ thuật hàng không.
      
      NHIỆM VỤ:
      - Trả lời câu hỏi dựa trên tài liệu nội bộ được cung cấp
      - Cung cấp thông tin chính xác, hữu ích cho nhân viên
      
      QUY TẮC QUAN TRỌNG:
      1. CHỈ sử dụng thông tin từ CONTEXT được cung cấp bên dưới
      2. Nếu không tìm thấy thông tin: "Tôi không tìm thấy thông tin này trong tài liệu hiện có"
      3. Luôn trích dẫn nguồn bằng [số] cho mỗi thông tin
      4. Trả lời bằng tiếng Việt chuyên nghiệp, rõ ràng
      5. Nếu có nhiều bước/quy trình: sử dụng danh sách có đánh số
      
      ĐỊNH DẠNG TRÍCH DẪN:
      - Sử dụng [1], [2], [3]... để tham chiếu đến từng tài liệu
      - Đặt trích dẫn ngay sau thông tin được trích xuất
      
      CONTEXT:
      {context}
      
      CÂU HỎI: {question}
      
      TRẢ LỜI:

  english_qa:
    name: "English Q&A Assistant"  
    template: |
      You are a professional AI assistant for an aviation engineering company.
      
      TASK:
      - Answer questions based on the provided internal documents
      - Provide accurate, helpful information for employees
      
      IMPORTANT RULES:
      1. ONLY use information from the CONTEXT provided below
      2. If information is not found: "I cannot find this information in the available documents"
      3. Always cite sources using [number] for each piece of information
      4. Respond in professional, clear English
      5. For multi-step processes: use numbered lists
      
      CITATION FORMAT:
      - Use [1], [2], [3]... to reference each document
      - Place citations immediately after the referenced information
      
      CONTEXT:
      {context}
      
      QUESTION: {question}
      
      ANSWER:

# configs/templates/user_prompts.yaml  
user_prompts:
  standard_query:
    template: |
      Dựa trên context được cung cấp, hãy trả lời câu hỏi sau:
      
      CÂU HỎI: {query}
      
      CONTEXT:
      {context_text}
      
  contextual_query:
    template: |
      THÔNG TIN NGƯỜI DÙNG:
      - Phòng ban: {department}  
      - Cấp độ: {access_level}
      
      CÂU HỎI: {query}
      
      CONTEXT LIÊN QUAN:
      {context_text}
      
      Hãy trả lời phù hợp với cấp độ và phòng ban của người hỏi.

# configs/templates/fallback_responses.yaml
fallback_responses:
  no_context:
    vietnamese: |
      Xin lỗi, tôi không tìm thấy thông tin liên quan đến câu hỏi "{query}" trong tài liệu hiện có.
      
      Gợi ý:
      - Kiểm tra chính tả của câu hỏi
      - Thử sử dụng từ khóa khác
      - Liên hệ với phòng ban liên quan để được hỗ trợ
      
    english: |
      I'm sorry, I couldn't find information related to "{query}" in the available documents.
      
      Suggestions:
      - Check the spelling of your question
      - Try using different keywords  
      - Contact the relevant department for assistance

  low_confidence:
    vietnamese: |
      Tôi tìm thấy một số thông tin có thể liên quan, nhưng không chắc chắn về độ chính xác:
      
      {partial_answer}
      
      Để có thông tin chính xác nhất, vui lòng liên hệ trực tiếp với phòng ban liên quan.
      
    english: |
      I found some potentially relevant information, but I'm not certain about its accuracy:
      
      {partial_answer}
      
      For the most accurate information, please contact the relevant department directly.

  system_error:
    vietnamese: |
      Xin lỗi, hệ thống đang gặp sự cố tạm thời. 
      
      Vui lòng:
      - Thử lại sau vài phút
      - Hoặc liên hệ với bộ phận IT để được hỗ trợ
      
    english: |
      Sorry, the system is experiencing temporary issues.
      
      Please:
      - Try again in a few minutes
      - Or contact IT support for assistance
```

---

## 7. MONITORING & METRICS

### 7.1 Performance Metrics

```python
# src/generation_engine/monitoring/metrics.py
from prometheus_client import Counter, Histogram, Gauge
import time
from functools import wraps

# Metrics definitions
GENERATION_REQUESTS_TOTAL = Counter(
    'generation_requests_total',
    'Total number of generation requests',
    ['provider', 'model', 'status']
)

GENERATION_DURATION_SECONDS = Histogram(
    'generation_duration_seconds', 
    'Time spent on generation requests',
    ['provider', 'model']
)

GENERATION_TOKENS_TOTAL = Counter(
    'generation_tokens_total',
    'Total tokens used for generation',
    ['provider', 'model', 'type']  # type: input/output
)

GENERATION_COST_TOTAL = Counter(
    'generation_cost_total',
    'Total cost of generation requests',
    ['provider', 'model']
)

ACTIVE_PROVIDERS = Gauge(
    'active_providers_count',
    'Number of active LLM providers'
)

CACHE_HIT_RATE = Gauge(
    'cache_hit_rate',
    'Cache hit rate for generation requests'
)

def track_generation_metrics(func):
    """Decorator để track metrics cho generation calls"""
    @wraps(func)
    async def wrapper(*args, **kwargs):
        start_time = time.time()
        provider = kwargs.get('provider', 'unknown')
        model = kwargs.get('model', 'unknown')
        
        try:
            result = await func(*args, **kwargs)
            
            # Track success metrics
            GENERATION_REQUESTS_TOTAL.labels(
                provider=provider, 
                model=model, 
                status='success'
            ).inc()
            
            # Track duration
            duration = time.time() - start_time
            GENERATION_DURATION_SECONDS.labels(
                provider=provider,
                model=model
            ).observe(duration)
            
            # Track tokens if available
            if 'tokens_used' in result.get('response_metadata', {}):
                tokens = result['response_metadata']['tokens_used']
                GENERATION_TOKENS_TOTAL.labels(
                    provider=provider,
                    model=model, 
                    type='total'
                ).inc(tokens)
            
            # Track cost if available  
            if 'cost_estimate' in result.get('response_metadata', {}):
                cost = result['response_metadata']['cost_estimate']
                GENERATION_COST_TOTAL.labels(
                    provider=provider,
                    model=model
                ).inc(cost)
                
            return result
            
        except Exception as e:
            # Track failure metrics
            GENERATION_REQUESTS_TOTAL.labels(
                provider=provider,
                model=model,
                status='error'
            ).inc()
            
            raise e
    
    return wrapper

class MetricsCollector:
    """Collect và export metrics"""
    
    def __init__(self):
        self.cache_hits = 0
        self.cache_misses = 0
        
    def record_cache_hit(self):
        self.cache_hits += 1
        self._update_cache_hit_rate()
        
    def record_cache_miss(self):
        self.cache_misses += 1  
        self._update_cache_hit_rate()
        
    def _update_cache_hit_rate(self):
        total_requests = self.cache_hits + self.cache_misses
        if total_requests > 0:
            hit_rate = self.cache_hits / total_requests
            CACHE_HIT_RATE.set(hit_rate)

# Global metrics collector instance
metrics_collector = MetricsCollector()
```

### 7.2 Logging Configuration  

```python
# src/generation_engine/monitoring/logging.py
import structlog
import logging
import json
from typing import Any, Dict

# Configure structured logging
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.UnicodeDecoder(),
        structlog.processors.JSONRenderer()
    ],
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    cache_logger_on_first_use=True,
)

class GenerationLogger:
    """Structured logging cho Generation Engine"""
    
    def __init__(self, name: str):
        self.logger = structlog.get_logger(name)
    
    def log_generation_request(self, request_id: str, query: str, user_context: Dict[str, Any]):
        """Log incoming generation request"""
        self.logger.info(
            "generation_request_received",
            request_id=request_id,
            query_length=len(query),
            user_id=user_context.get('user_id'),
            department=user_context.get('department'),
            access_level=user_context.get('access_level')
        )
    
    def log_provider_selection(self, request_id: str, provider: str, model: str, reason: str):
        """Log provider selection decision"""
        self.logger.info(
            "provider_selected",
            request_id=request_id,
            provider=provider,
            model=model,
            selection_reason=reason
        )
    
    def log_llm_call(self, request_id: str, provider: str, model: str, 
                     input_tokens: int, processing_time: float):
        """Log LLM API call"""
        self.logger.info(
            "llm_call_completed", 
            request_id=request_id,
            provider=provider,
            model=model,
            input_tokens=input_tokens,
            processing_time_seconds=processing_time
        )
    
    def log_generation_response(self, request_id: str, response_length: int, 
                              citations_count: int, confidence_score: float):
        """Log generation response details"""
        self.logger.info(
            "generation_response_created",
            request_id=request_id, 
            response_length=response_length,
            citations_count=citations_count,
            confidence_score=confidence_score
        )
    
    def log_error(self, request_id: str, error_type: str, error_message: str, 
                  provider: str = None, recovery_action: str = None):
        """Log errors with context"""
        self.logger.error(
            "generation_error",
            request_id=request_id,
            error_type=error_type,
            error_message=error_message,
            provider=provider,
            recovery_action=recovery_action
        )
    
    def log_fallback_triggered(self, request_id: str, original_provider: str, 
                             fallback_strategy: str, success: bool):
        """Log fallback mechanism activation"""
        self.logger.warning(
            "fallback_triggered",
            request_id=request_id,
            original_provider=original_provider,
            fallback_strategy=fallback_strategy,
            fallback_success=success
        )
    
    def log_cache_operation(self, request_id: str, operation: str, hit: bool, key_hash: str):
        """Log cache operations"""
        self.logger.debug(
            "cache_operation",
            request_id=request_id,
            operation=operation,  # "get", "set", "invalidate"
            cache_hit=hit,
            key_hash=key_hash
        )

# Global logger instance
generation_logger = GenerationLogger("generation_engine")
```

---

## 8. TESTING STRATEGY

### 8.1 Comprehensive Test Plan

```python
# tests/conftest.py - Shared test fixtures
import pytest
import asyncio
from unittest.mock import Mock, AsyncMock
from src.generation_engine.core.engine import GenerationEngine
from src.generation_engine.core.models import GenerationRequest, GenerationResponse

@pytest.fixture
def event_loop():
    """Create event loop for async tests"""
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()

@pytest.fixture
def mock_openai_provider():
    """Mock OpenAI provider"""
    provider = AsyncMock()
    provider.generate.return_value = {
        "text": "Sample answer [1] based on context.",
        "tokens_used": 150,
        "model": "gpt-4-turbo-preview",
        "cost_estimate": 0.002
    }
    provider.estimate_cost.return_value = 0.002
    provider.name = "openai"
    return provider

@pytest.fixture
def mock_claude_provider():
    """Mock Claude provider"""
    provider = AsyncMock()
    provider.generate.return_value = {
        "text": "Alternative answer [1] from Claude.",
        "tokens_used": 120,
        "model": "claude-3-sonnet",
        "cost_estimate": 0.0015
    }
    provider.estimate_cost.return_value = 0.0015
    provider.name = "claude"
    return provider

@pytest.fixture
def sample_context_chunks():
    """Sample context chunks for testing"""
    return [
        {
            "chunk_id": "doc123_chunk_1",
            "content": "Quy trình mua hàng gồm 5 bước chính: 1) Tạo yêu cầu mua hàng...",
            "metadata": {
                "document_id": "proc_001",
                "title": "Quy trình Mua hàng",
                "page": 1,
                "confidence_score": 0.95
            }
        },
        {
            "chunk_id": "doc456_chunk_3", 
            "content": "Việc phê duyệt yêu cầu mua hàng cần có chữ ký giám đốc...",
            "metadata": {
                "document_id": "proc_002", 
                "title": "Quy trình Phê duyệt",
                "page": 3,
                "confidence_score": 0.87
            }
        }
    ]

@pytest.fixture
def sample_generation_request(sample_context_chunks):
    """Sample generation request"""
    return GenerationRequest(
        query="Quy trình mua hàng trình giám đốc như thế nào?",
        context_chunks=sample_context_chunks,
        user_context={
            "user_id": "emp001",
            "department": "rd",
            "access_level": "employee"
        },
        conversation_history=[]
    )

@pytest.fixture 
def generation_engine_with_mocks(mock_openai_provider, mock_claude_provider):
    """Generation engine with mocked providers"""
    config = {
        "providers": {
            "openai": mock_openai_provider,
            "claude": mock_claude_provider
        },
        "default_provider": "openai",
        "fallback_enabled": True
    }
    return GenerationEngine(config)
```

### 8.2 Unit Tests

```python
# tests/unit/test_prompt_builder.py
import pytest
from src.generation_engine.templates.manager import TemplateManager
from src.generation_engine.core.prompt_builder import PromptBuilder

class TestPromptBuilder:
    
    @pytest.fixture
    def template_manager(self):
        config = {
            "templates_path": "configs/templates/",
            "default_language": "vietnamese"
        }
        return TemplateManager(config)
    
    @pytest.fixture
    def prompt_builder(self, template_manager):
        return PromptBuilder(template_manager)
    
    def test_build_basic_prompt(self, prompt_builder, sample_context_chunks):
        """Test basic prompt building"""
        query = "Quy trình mua hàng như thế nào?"
        
        prompt = prompt_builder.build_prompt(
            query=query,
            context_chunks=sample_context_chunks,
            language="vietnamese"
        )
        
        # Assert prompt contains required elements
        assert query in prompt
        assert "Quy trình mua hàng gồm 5 bước" in prompt
        assert "[1]" in prompt  # Citation markers
        assert "CONTEXT:" in prompt
        assert "CÂU HỎI:" in prompt
    
    def test_build_prompt_with_user_context(self, prompt_builder, sample_context_chunks):
        """Test prompt building with user context"""
        query = "Tôi cần biết quy trình mua hàng"
        user_context = {
            "department": "rd",
            "access_level": "employee"
        }
        
        prompt = prompt_builder.build_contextual_prompt(
            query=query,
            context_chunks=sample_context_chunks,
            user_context=user_context
        )
        
        assert "Phòng ban: rd" in prompt
        assert "Cấp độ: employee" in prompt
        assert query in prompt
    
    def test_prompt_token_optimization(self, prompt_builder, sample_context_chunks):
        """Test prompt token count optimization"""
        # Create long context that exceeds token limit
        long_context = sample_context_chunks * 20  # Repeat chunks
        
        prompt = prompt_builder.build_optimized_prompt(
            query="Test query",
            context_chunks=long_context,
            max_tokens=2000
        )
        
        # Should be truncated to fit token limit
        token_count = prompt_builder.count_tokens(prompt)
        assert token_count <= 2000
        
        # Should still contain essential elements
        assert "Test query" in prompt
        assert "CONTEXT:" in prompt

# tests/unit/test_citation_generator.py
import pytest
from src.generation_engine.core.citation import CitationGenerator

class TestCitationGenerator:
    
    @pytest.fixture
    def citation_generator(self):
        return CitationGenerator()
    
    def test_extract_citations_from_answer(self, citation_generator, sample_context_chunks):
        """Test citation extraction from generated answer"""
        answer = "Quy trình mua hàng gồm 5 bước chính [1]. Việc phê duyệt cần chữ ký giám đốc [2]."
        
        citations = citation_generator.generate_citations(answer, sample_context_chunks)
        
        assert len(citations) == 2
        assert citations[0]["number"] == 1
        assert citations[1]["number"] == 2
        assert "Quy trình Mua hàng" in citations[0]["reference"]
        assert "Quy trình Phê duyệt" in citations[1]["reference"]
    
    def test_handle_missing_citations(self, citation_generator, sample_context_chunks):
        """Test handling when citations are referenced but chunks missing"""
        answer = "Information from source [5] is not available."
        
        citations = citation_generator.generate_citations(answer, sample_context_chunks)
        
        # Should only return valid citations
        assert len(citations) == 0
    
    def test_citation_confidence_scoring(self, citation_generator, sample_context_chunks):
        """Test citation confidence scoring"""
        answer = "Quy trình mua hàng gồm 5 bước [1]"
        
        citations = citation_generator.generate_citations_with_confidence(
            answer, sample_context_chunks
        )
        
        assert len(citations) == 1
        assert "confidence" in citations[0]
        assert 0.0 <= citations[0]["confidence"] <= 1.0

# tests/unit/test_error_handling.py
import pytest
from unittest.mock import AsyncMock, patch
from src.generation_engine.core.fallback import FallbackManager
from src.generation_engine.core.exceptions import (
    ProviderError, RateLimitError, QuotaExceededError
)

class TestErrorHandling:
    
    @pytest.fixture
    def fallback_manager(self, mock_openai_provider, mock_claude_provider):
        providers = [mock_openai_provider, mock_claude_provider]
        return FallbackManager(providers)
    
    @pytest.mark.asyncio
    async def test_rate_limit_fallback(self, fallback_manager, sample_generation_request):
        """Test fallback when hitting rate limits"""
        # Setup first provider to fail with rate limit
        fallback_manager.providers[0].generate.side_effect = RateLimitError("Rate limit exceeded")
        
        # Second provider succeeds
        fallback_manager.providers[1].generate.return_value = {
            "text": "Fallback response [1]",
            "tokens_used": 100,
            "model": "claude-3-sonnet"
        }
        
        response = await fallback_manager.generate_with_fallback(sample_generation_request)
        
        assert response is not None
        assert "Fallback response" in response.answer
        # Should have tried first provider, then succeeded with second
        assert fallback_manager.providers[0].generate.call_count == 1
        assert fallback_manager.providers[1].generate.call_count == 1
    
    @pytest.mark.asyncio
    async def test_quota_exceeded_fallback(self, fallback_manager, sample_generation_request):
        """Test fallback when quota is exceeded"""
        fallback_manager.providers[0].generate.side_effect = QuotaExceededError("Quota exceeded")
        
        response = await fallback_manager.generate_with_fallback(sample_generation_request)
        
        assert response is not None
        # Should switch to second provider
        assert fallback_manager.providers[1].generate.call_count == 1
    
    @pytest.mark.asyncio
    async def test_all_providers_fail(self, fallback_manager, sample_generation_request):
        """Test when all providers fail"""
        # Both providers fail
        fallback_manager.providers[0].generate.side_effect = ProviderError("Provider 1 failed")
        fallback_manager.providers[1].generate.side_effect = ProviderError("Provider 2 failed") 
        
        # Should raise exception when all providers fail
        with pytest.raises(Exception):
            await fallback_manager.generate_with_fallback(sample_generation_request)

# tests/unit/test_quality_validation.py
import pytest
from src.generation_engine.core.quality import QualityValidator

class TestQualityValidation:
    
    @pytest.fixture
    def quality_validator(self):
        config = {
            "min_answer_length": 10,
            "max_answer_length": 2000,
            "confidence_threshold": 0.3,
            "citation_required": True
        }
        return QualityValidator(config)
    
    def test_validate_good_response(self, quality_validator):
        """Test validation of good quality response"""
        response = {
            "answer": "Quy trình mua hàng gồm 5 bước chính được mô tả trong tài liệu [1].",
            "citations": [{"number": 1, "reference": "Document 1"}],
            "confidence_score": 0.85
        }
        
        is_valid, issues = quality_validator.validate_response(response)
        
        assert is_valid
        assert len(issues) == 0
    
    def test_validate_too_short_response(self, quality_validator):
        """Test validation of too short response"""
        response = {
            "answer": "Không biết.",
            "citations": [],
            "confidence_score": 0.1
        }
        
        is_valid, issues = quality_validator.validate_response(response)
        
        assert not is_valid
        assert "too_short" in str(issues)
        assert "low_confidence" in str(issues)
        assert "missing_citations" in str(issues)
    
    def test_validate_no_citations(self, quality_validator):
        """Test validation when citations are missing"""
        response = {
            "answer": "This is a reasonably long answer that provides some information but has no citations.",
            "citations": [],
            "confidence_score": 0.8
        }
        
        is_valid, issues = quality_validator.validate_response(response)
        
        assert not is_valid
        assert "missing_citations" in str(issues)
```

### 8.3 Integration Tests

```python
# tests/integration/test_full_generation_pipeline.py
import pytest
import asyncio
from src.generation_engine.core.engine import GenerationEngine
from src.generation_engine.providers.openai_provider import OpenAIProvider

@pytest.mark.integration
class TestFullGenerationPipeline:
    """Integration tests with real providers (using test API keys)"""
    
    @pytest.fixture
    def real_generation_engine(self):
        """Setup with real providers for integration testing"""
        config = {
            "providers": {
                "openai": OpenAIProvider({
                    "api_key": "test_openai_key",  # Use test key
                    "model": "gpt-3.5-turbo",  # Use cheaper model for testing
                    "max_tokens": 200
                })
            },
            "default_provider": "openai",
            "fallback_enabled": False  # Disable for simple test
        }
        return GenerationEngine(config)
    
    @pytest.mark.asyncio
    async def test_end_to_end_vietnamese_qa(self, real_generation_engine, sample_context_chunks):
        """Test complete Vietnamese Q&A pipeline"""
        request = GenerationRequest(
            query="Quy trình mua hàng trình giám đốc gồm những bước nào?",
            context_chunks=sample_context_chunks,
            user_context={
                "user_id": "integration_test_user",
                "department": "rd",
                "access_level": "employee"
            }
        )
        
        # Execute full pipeline
        start_time = time.time()
        response = await real_generation_engine.generate(request)
        end_time = time.time()
        
        # Validate response structure
        assert isinstance(response, GenerationResponse)
        assert response.answer is not None
        assert len(response.answer) > 20
        assert len(response.citations) > 0
        assert response.confidence_score > 0
        
        # Validate performance requirements
        assert (end_time - start_time) < 60  # Under 60 seconds
        
        # Validate Vietnamese content
        assert any(vietnamese_word in response.answer.lower() 
                  for vietnamese_word in ["quy", "trình", "bước", "mua", "hàng"])
        
        # Validate citations format
        for citation in response.citations:
            assert "chunk_id" in citation
            assert "reference" in citation
            assert "[" in citation["reference"] and "]" in citation["reference"]
        
        # Validate metadata
        assert "tokens_used" in response.response_metadata
        assert "processing_time" in response.response_metadata
        assert response.response_metadata["tokens_used"] > 0
    
    @pytest.mark.asyncio
    async def test_concurrent_requests(self, real_generation_engine, sample_context_chunks):
        """Test handling multiple concurrent requests"""
        requests = []
        for i in range(5):
            request = GenerationRequest(
                query=f"Test query {i}: Quy trình mua hàng như thế nào?",
                context_chunks=sample_context_chunks,
                user_context={"user_id": f"test_user_{i}"}
            )
            requests.append(request)
        
        # Execute all requests concurrently
        start_time = time.time()
        responses = await asyncio.gather(*[
            real_generation_engine.generate(req) for req in requests
        ])
        end_time = time.time()
        
        # All requests should succeed
        assert len(responses) == 5
        for response in responses:
            assert response.answer is not None
            assert len(response.citations) > 0
        
        # Should be faster than sequential execution
        assert (end_time - start_time) < 180  # 3 minutes for 5 requests

# tests/integration/test_provider_switching.py
@pytest.mark.integration 
class TestProviderSwitching:
    """Test provider switching and fallback mechanisms"""
    
    @pytest.fixture
    def multi_provider_engine(self):
        """Engine with multiple real providers"""
        config = {
            "providers": {
                "openai": OpenAIProvider({"api_key": "test_openai_key"}),
                "claude": ClaudeProvider({"api_key": "test_claude_key"})
            },
            "provider_selection_strategy": "round_robin",
            "fallback_enabled": True
        }
        return GenerationEngine(config)
    
    @pytest.mark.asyncio
    async def test_round_robin_provider_selection(self, multi_provider_engine, sample_context_chunks):
        """Test round-robin provider selection"""
        responses = []
        
        for i in range(4):
            request = GenerationRequest(
                query=f"Test query {i}",
                context_chunks=sample_context_chunks,
                user_context={"user_id": f"user_{i}"}
            )
            response = await multi_provider_engine.generate(request)
            responses.append(response)
        
        # Should use different providers (round-robin)
        providers_used = [resp.response_metadata.get("provider") for resp in responses]
        assert len(set(providers_used)) > 1  # Used multiple providers
    
    @pytest.mark.asyncio 
    async def test_fallback_on_provider_failure(self, multi_provider_engine):
        """Test fallback when primary provider fails"""
        # Simulate provider failure by using invalid API key
        multi_provider_engine.providers["openai"].api_key = "invalid_key"
        
        request = GenerationRequest(
            query="Test fallback query",
            context_chunks=[{
                "chunk_id": "test",
                "content": "Test content",
                "metadata": {"title": "Test"}
            }],
            user_context={"user_id": "fallback_test"}
        )
        
        # Should still succeed using fallback provider
        response = await multi_provider_engine.generate(request)
        
        assert response.answer is not None
        # Should have used fallback provider (Claude)
        assert response.response_metadata.get("provider") == "claude"

# tests/integration/test_local_llm.py
@pytest.mark.integration
@pytest.mark.slow
class TestLocalLLM:
    """Test local LLM integration (requires GPU and model files)"""
    
    @pytest.fixture
    def local_llm_engine(self):
        """Engine with local LLM provider"""
        config = {
            "providers": {
                "local": LocalLLMProvider({
                    "engine": "ollama",
                    "model_name": "llama2:7b-chat",
                    "temperature": 0.1
                })
            },
            "default_provider": "local"
        }
        return GenerationEngine(config)
    
    @pytest.mark.asyncio
    async def test_local_llm_generation(self, local_llm_engine, sample_context_chunks):
        """Test generation with local LLM"""
        request = GenerationRequest(
            query="Explain the purchasing process",
            context_chunks=sample_context_chunks,
            user_context={"user_id": "local_test"}
        )
        
        response = await local_llm_engine.generate(request)
        
        assert response.answer is not None
        assert len(response.answer) > 10
        assert response.response_metadata.get("model") == "llama2:7b-chat"
        assert response.response_metadata.get("cost_estimate") == 0  # No cost for local
```

### 8.4 Performance Tests

```python
# tests/performance/test_load_performance.py
import pytest
import asyncio
import time
import statistics
from concurrent.futures import ThreadPoolExecutor
from src.generation_engine.core.engine import GenerationEngine

@pytest.mark.performance
class TestLoadPerformance:
    """Performance and load testing"""
    
    @pytest.fixture
    def performance_engine(self):
        """Optimized engine for performance testing"""
        config = {
            "providers": {
                "openai": OpenAIProvider({
                    "api_key": "test_key",
                    "model": "gpt-3.5-turbo",  # Faster model
                    "max_tokens": 300
                })
            },
            "caching": {"enabled": True, "ttl_seconds": 300},
            "quality_control": {"enabled": False}  # Skip for performance test
        }
        return GenerationEngine(config)
    
    @pytest.mark.asyncio
    async def test_response_time_sla(self, performance_engine, sample_context_chunks):
        """Test response time meets SLA (<60 seconds)"""
        response_times = []
        
        for i in range(10):
            request = GenerationRequest(
                query=f"Performance test query {i}",
                context_chunks=sample_context_chunks,
                user_context={"user_id": f"perf_user_{i}"}
            )
            
            start_time = time.time()
            response = await performance_engine.generate(request)
            end_time = time.time()
            
            response_time = end_time - start_time
            response_times.append(response_time)
            
            assert response is not None
            assert response_time < 60  # SLA requirement
        
        # Calculate statistics
        avg_time = statistics.mean(response_times)
        p95_time = statistics.quantiles(response_times, n=20)[18]  # 95th percentile
        
        print(f"Average response time: {avg_time:.2f}s")
        print(f"95th percentile: {p95_time:.2f}s")
        
        assert avg_time < 30  # Average should be much better than SLA
        assert p95_time < 45   # 95% of requests under 45s
    
    @pytest.mark.asyncio
    async def test_concurrent_load(self, performance_engine, sample_context_chunks):
        """Test handling concurrent load (100 users requirement)"""
        concurrent_requests = 50  # Start with 50 concurrent requests
        
        async def single_request(user_id):
            request = GenerationRequest(
                query=f"Load test query from user {user_id}",
                context_chunks=sample_context_chunks,
                user_context={"user_id": f"load_user_{user_id}"}
            )
            
            start_time = time.time()
            try:
                response = await performance_engine.generate(request)
                end_time = time.time()
                return {
                    "success": True,
                    "response_time": end_time - start_time,
                    "user_id": user_id
                }
            except Exception as e:
                end_time = time.time()
                return {
                    "success": False,
                    "error": str(e),
                    "response_time": end_time - start_time,
                    "user_id": user_id
                }
        
        # Execute concurrent requests
        start_time = time.time()
        results = await asyncio.gather(*[
            single_request(i) for i in range(concurrent_requests)
        ], return_exceptions=True)
        total_time = time.time() - start_time
        
        # Analyze results
        successful_requests = [r for r in results if isinstance(r, dict) and r.get("success")]
        failed_requests = [r for r in results if isinstance(r, dict) and not r.get("success")]
        exceptions = [r for r in results if not isinstance(r, dict)]
        
        success_rate = len(successful_requests) / concurrent_requests
        avg_response_time = statistics.mean([r["response_time"] for r in successful_requests])
        
        print(f"Concurrent requests: {concurrent_requests}")
        print(f"Success rate: {success_rate:.2%}")
        print(f"Average response time: {avg_response_time:.2f}s")
        print(f"Total test time: {total_time:.2f}s")
        print(f"Failed requests: {len(failed_requests)}")
        print(f"Exceptions: {len(exceptions)}")
        
        # Assertions
        assert success_rate >= 0.95  # 95% success rate
        assert avg_response_time < 60  # Average under SLA
        assert len(exceptions) == 0   # No unhandled exceptions
    
    def test_memory_usage_under_load(self, performance_engine, sample_context_chunks):
        """Test memory usage doesn't grow excessively under load"""
        import psutil
        import os
        
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # Run many requests to test memory leaks
        async def memory_test():
            for i in range(100):
                request = GenerationRequest(
                    query=f"Memory test query {i}",
                    context_chunks=sample_context_chunks,
                    user_context={"user_id": f"mem_user_{i}"}
                )
                await performance_engine.generate(request)
                
                if i % 20 == 0:  # Check memory every 20 requests
                    current_memory = process.memory_info().rss / 1024 / 1024
                    memory_growth = current_memory - initial_memory
                    print(f"After {i} requests: {current_memory:.1f}MB (+{memory_growth:.1f}MB)")
        
        asyncio.run(memory_test())
        
        final_memory = process.memory_info().rss / 1024 / 1024
        memory_growth = final_memory - initial_memory
        
        print(f"Initial memory: {initial_memory:.1f}MB")
        print(f"Final memory: {final_memory:.1f}MB") 
        print(f"Memory growth: {memory_growth:.1f}MB")
        
        # Memory growth should be reasonable (less than 500MB for 100 requests)
        assert memory_growth < 500
    
    @pytest.mark.asyncio
    async def test_cache_performance_impact(self, performance_engine, sample_context_chunks):
        """Test cache impact on performance"""
        # Same request to test cache hit
        request = GenerationRequest(
            query="Cache test query - what is the purchasing process?",
            context_chunks=sample_context_chunks,
            user_context={"user_id": "cache_test_user"}
        )
        
        # First request (cache miss)
        start_time = time.time()
        response1 = await performance_engine.generate(request)
        first_request_time = time.time() - start_time
        
        # Second request (should be cache hit)
        start_time = time.time()
        response2 = await performance_engine.generate(request)
        second_request_time = time.time() - start_time
        
        print(f"First request (cache miss): {first_request_time:.3f}s")
        print(f"Second request (cache hit): {second_request_time:.3f}s")
        print(f"Cache speedup: {first_request_time/second_request_time:.2f}x")
        
        # Responses should be identical
        assert response1.answer == response2.answer
        assert response1.citations == response2.citations
        
        # Cache hit should be significantly faster
        assert second_request_time < first_request_time * 0.1  # At least 10x faster
```

---

## 9. DEPLOYMENT GUIDE

### 9.1 Docker Configuration

```dockerfile
# docker/production/Dockerfile
FROM python:3.11-slim

# Set environment variables
ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    PYTHONPATH=/app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Create app directory
WORKDIR /app

# Copy requirements first for better caching
COPY requirements/ requirements/
RUN pip install --no-cache-dir -r requirements/prod.txt

# Copy application code
COPY src/ src/
COPY configs/ configs/
COPY scripts/ scripts/

# Create non-root user
RUN groupadd -r appuser && useradd -r -g appuser appuser
RUN chown -R appuser:appuser /app
USER appuser

# Health check
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:8000/health || exit 1

# Expose port
EXPOSE 8000

# Start command
CMD ["uvicorn", "src.generation_engine.api.main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4"]
```

```yaml
# docker-compose.production.yml
version: '3.8'

services:
  generation-engine:
    build:
      context: .
      dockerfile: docker/production/Dockerfile
    ports:
      - "8000:8000"
    environment:
      - ENVIRONMENT=production
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - CLAUDE_API_KEY=${CLAUDE_API_KEY}
      - REDIS_URL=redis://redis:6379
      - POSTGRES_URL=postgresql://user:pass@postgres:5432/generation_db
    depends_on:
      - redis
      - postgres
    volumes:
      - ./configs:/app/configs:ro
      - ./logs:/app/logs
    restart: unless-stopped
    deploy:
      replicas: 3
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 1G
          cpus: '0.5'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes
    restart: unless-stopped

  postgres:
    image: postgres:15-alpine
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_DB=generation_db
      - POSTGRES_USER=generation_user
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init_db.sql:/docker-entrypoint-initdb.d/init.sql
    restart: unless-stopped

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./configs/nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./configs/nginx/ssl:/etc/nginx/ssl
    depends_on:
      - generation-engine
    restart: unless-stopped

volumes:
  redis_data:
  postgres_data:
```

### 9.2 Kubernetes Deployment

```yaml
# k8s/namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: generation-engine
  labels:
    name: generation-engine
    environment: production
```
---
# k8s/configmap.yaml
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: generation-engine-config
  namespace: generation-engine
data:
  generation.yaml: |
    generation:
      default_provider: "openai"
      provider_selection_strategy: "round_robin"
      prompt_settings:
        max_context_tokens: 3000
        max_output_tokens: 1000
        temperature: 0.1
      quality_control:
        min_answer_length: 10
        confidence_threshold: 0.3
        citation_required: true
      caching:
        enabled: true
        ttl_seconds: 3600
      fallback:
        enabled: true
        strategies: ["switch_provider", "local_llm", "template_response"]
        max_fallback_attempts: 3
  
  logging.yaml: |
    version: 1
    disable_existing_loggers: false
    formatters:
      json:
        format: '{"timestamp": "%(asctime)s", "level": "%(levelname)s", "logger": "%(name)s", "message": "%(message)s"}'
    handlers:
      console:
        class: logging.StreamHandler
        formatter: json
        stream: ext://sys.stdout
    loggers:
      generation_engine:
        level: INFO
        handlers: [console]
        propagate: false
    root:
      level: WARNING
      handlers: [console]
```
---
```yaml
# k8s/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: generation-engine-secrets
  namespace: generation-engine
type: Opaque
data:
  openai-api-key: <base64-encoded-openai-key>
  claude-api-key: <base64-encoded-claude-key>
  gemini-api-key: <base64-encoded-gemini-key>
  postgres-password: <base64-encoded-postgres-password>
  redis-password: <base64-encoded-redis-password>

---
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: generation-engine
  namespace: generation-engine
  labels:
    app: generation-engine
    version: v1.0
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  selector:
    matchLabels:
      app: generation-engine
  template:
    metadata:
      labels:
        app: generation-engine
        version: v1.0
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8000"
        prometheus.io/path: "/metrics"
    spec:
      serviceAccountName: generation-engine-sa
      containers:
      - name: generation-engine
        image: generation-engine:1.0.0
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 8000
          name: http
        env:
        - name: ENVIRONMENT
          value: "production"
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: generation-engine-secrets
              key: openai-api-key
        - name: CLAUDE_API_KEY
          valueFrom:
            secretKeyRef:
              name: generation-engine-secrets
              key: claude-api-key
        - name: GEMINI_API_KEY
          valueFrom:
            secretKeyRef:
              name: generation-engine-secrets
              key: gemini-api-key
        - name: REDIS_URL
          value: "redis://redis-service:6379"
        - name: POSTGRES_URL
          value: "postgresql://generation_user:$(POSTGRES_PASSWORD)@postgres-service:5432/generation_db"
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: generation-engine-secrets
              key: postgres-password
        volumeMounts:
        - name: config-volume
          mountPath: /app/configs
          readOnly: true
        - name: logs-volume
          mountPath: /app/logs
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        startupProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 30
      volumes:
      - name: config-volume
        configMap:
          name: generation-engine-config
      - name: logs-volume
        emptyDir: {}
      nodeSelector:
        node-type: compute
      tolerations:
      - key: "compute-node"
        operator: "Equal"
        value: "true"
        effect: "NoSchedule"

---
# k8s/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: generation-engine-service
  namespace: generation-engine
  labels:
    app: generation-engine
spec:
  type: ClusterIP
  ports:
  - port: 80
    targetPort: 8000
    protocol: TCP
    name: http
  selector:
    app: generation-engine

---
# k8s/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: generation-engine-hpa
  namespace: generation-engine
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: generation-engine
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60

---
# k8s/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: generation-engine-ingress
  namespace: generation-engine
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/rate-limit: "100"
    nginx.ingress.kubernetes.io/rate-limit-window: "1m"
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
spec:
  tls:
  - hosts:
    - generation-api.company.com
    secretName: generation-engine-tls
  rules:
  - host: generation-api.company.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: generation-engine-service
            port:
              number: 80

---
# k8s/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: generation-engine-pdb
  namespace: generation-engine
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: generation-engine

---
# k8s/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: generation-engine-sa
  namespace: generation-engine
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: generation-engine
  name: generation-engine-role
rules:
- apiGroups: [""]
  resources: ["configmaps", "secrets"]
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: generation-engine-rolebinding
  namespace: generation-engine
subjects:
- kind: ServiceAccount
  name: generation-engine-sa
  namespace: generation-engine
roleRef:
  kind: Role
  name: generation-engine-role
  apiGroup: rbac.authorization.k8s.io
```

### 9.3 Deployment Scripts

```bash
#!/bin/bash
# scripts/deploy_production.sh

set -e

echo "🚀 Starting Generation Engine Production Deployment"

# Configuration
NAMESPACE="generation-engine"
IMAGE_TAG=${1:-"latest"}
REGISTRY="your-registry.com"
IMAGE_NAME="$REGISTRY/generation-engine:$IMAGE_TAG"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

log_info() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# Pre-deployment checks
log_info "Running pre-deployment checks..."

# Check kubectl connection
if ! kubectl cluster-info > /dev/null 2>&1; then
    log_error "Cannot connect to Kubernetes cluster"
    exit 1
fi

# Check if namespace exists
if ! kubectl get namespace $NAMESPACE > /dev/null 2>&1; then
    log_info "Creating namespace: $NAMESPACE"
    kubectl create namespace $NAMESPACE
fi

# Validate secrets exist
log_info "Checking secrets..."
if ! kubectl get secret generation-engine-secrets -n $NAMESPACE > /dev/null 2>&1; then
    log_error "Secrets not found. Please create secrets first using:"
    echo "kubectl create secret generic generation-engine-secrets \\"
    echo "  --from-literal=openai-api-key='your-key' \\"
    echo "  --from-literal=claude-api-key='your-key' \\"
    echo "  --from-literal=postgres-password='your-password' \\"
    echo "  -n $NAMESPACE"
    exit 1
fi

# Build and push image
log_info "Building Docker image..."
docker build -t $IMAGE_NAME -f docker/production/Dockerfile .

log_info "Pushing image to registry..."
docker push $IMAGE_NAME

# Deploy to Kubernetes
log_info "Applying Kubernetes manifests..."

# Apply in order
kubectl apply -f k8s/namespace.yaml
kubectl apply -f k8s/configmap.yaml
kubectl apply -f k8s/serviceaccount.yaml

# Wait a moment for ServiceAccount to be ready
sleep 5

# Update deployment image
kubectl set image deployment/generation-engine \
    generation-engine=$IMAGE_NAME \
    -n $NAMESPACE

# Apply remaining manifests
kubectl apply -f k8s/deployment.yaml
kubectl apply -f k8s/service.yaml
kubectl apply -f k8s/hpa.yaml
kubectl apply -f k8s/ingress.yaml
kubectl apply -f k8s/pdb.yaml

# Wait for deployment to be ready
log_info "Waiting for deployment to be ready..."
kubectl wait --for=condition=available --timeout=600s deployment/generation-engine -n $NAMESPACE

# Verify deployment
log_info "Verifying deployment..."
READY_REPLICAS=$(kubectl get deployment generation-engine -n $NAMESPACE -o jsonpath='{.status.readyReplicas}')
DESIRED_REPLICAS=$(kubectl get deployment generation-engine -n $NAMESPACE -o jsonpath='{.spec.replicas}')

if [ "$READY_REPLICAS" = "$DESIRED_REPLICAS" ]; then
    log_info "✅ Deployment successful! $READY_REPLICAS/$DESIRED_REPLICAS replicas ready"
else
    log_error "❌ Deployment failed. Only $READY_REPLICAS/$DESIRED_REPLICAS replicas ready"
    
    # Show pod status for debugging
    log_info "Pod status:"
    kubectl get pods -n $NAMESPACE -l app=generation-engine
    
    log_info "Recent events:"
    kubectl get events -n $NAMESPACE --sort-by='.metadata.creationTimestamp' | tail -10
    
    exit 1
fi

# Health check
log_info "Performing health check..."
SERVICE_IP=$(kubectl get service generation-engine-service -n $NAMESPACE -o jsonpath='{.spec.clusterIP}')

if kubectl run health-check --image=curlimages/curl --rm -it --restart=Never -- \
   curl -f http://$SERVICE_IP/health; then
    log_info "✅ Health check passed"
else
    log_error "❌ Health check failed"
    exit 1
fi

# Show deployment info
log_info "Deployment Summary:"
echo "=================="
echo "Namespace: $NAMESPACE"
echo "Image: $IMAGE_NAME"
echo "Replicas: $READY_REPLICAS"
echo "Service IP: $SERVICE_IP"

# Get ingress info if available
INGRESS_HOST=$(kubectl get ingress generation-engine-ingress -n $NAMESPACE -o jsonpath='{.spec.rules[0].host}' 2>/dev/null || echo "Not configured")
echo "Ingress Host: $INGRESS_HOST"

log_info "🎉 Deployment completed successfully!"

# Optional: Run smoke tests
if [ "$2" = "--smoke-test" ]; then
    log_info "Running smoke tests..."
    ./scripts/smoke_test.sh $NAMESPACE
fi
```

```bash
#!/bin/bash
# scripts/smoke_test.sh

NAMESPACE=${1:-"generation-engine"}
SERVICE_NAME="generation-engine-service"

echo "🧪 Running smoke tests for Generation Engine"

# Test 1: Health endpoint
echo "Test 1: Health check..."
if kubectl run smoke-test-health --image=curlimages/curl --rm -it --restart=Never -- \
   curl -f http://$SERVICE_NAME.$NAMESPACE.svc.cluster.local/health; then
    echo "✅ Health check passed"
else
    echo "❌ Health check failed"
    exit 1
fi

# Test 2: Ready endpoint  
echo "Test 2: Ready check..."
if kubectl run smoke-test-ready --image=curlimages/curl --rm -it --restart=Never -- \
   curl -f http://$SERVICE_NAME.$NAMESPACE.svc.cluster.local/ready; then
    echo "✅ Ready check passed"
else
    echo "❌ Ready check failed"
    exit 1
fi

# Test 3: Simple generation request
echo "Test 3: Simple generation..."
cat > /tmp/smoke_test_payload.json << EOF
{
  "query": "What is the purchasing process?",
  "context_chunks": [
    {
      "chunk_id": "test_chunk_1",
      "content": "The purchasing process consists of 3 steps: 1) Create request, 2) Get approval, 3) Make purchase.",
      "metadata": {
        "document_id": "test_doc",
        "title": "Test Document",
        "page": 1
      }
    }
  ],
  "user_context": {
    "user_id": "smoke_test_user",
    "department": "test",
    "access_level": "employee"
  }
}
EOF

if kubectl run smoke-test-generation --image=curlimages/curl --rm -it --restart=Never -- \
   curl -X POST \
   -H "Content-Type: application/json" \
   -d @/tmp/smoke_test_payload.json \
   http://$SERVICE_NAME.$NAMESPACE.svc.cluster.local/api/generate; then
    echo "✅ Generation test passed"
else
    echo "❌ Generation test failed"
    exit 1
fi

# Test 4: Metrics endpoint
echo "Test 4: Metrics check..."
if kubectl run smoke-test-metrics --image=curlimages/curl --rm -it --restart=Never -- \
   curl -f http://$SERVICE_NAME.$NAMESPACE.svc.cluster.local/metrics; then
    echo "✅ Metrics check passed"
else
    echo "❌ Metrics check failed"
    exit 1
fi

echo "🎉 All smoke tests passed!"
```

### 9.4 Monitoring Setup

```yaml
# monitoring/prometheus-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: generation-engine
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
    
    rule_files:
      - "generation_engine_rules.yml"
    
    scrape_configs:
      - job_name: 'generation-engine'
        static_configs:
          - targets: ['generation-engine-service:80']
        metrics_path: '/metrics'
        scrape_interval: 15s
        scrape_timeout: 10s
      
      - job_name: 'kubernetes-pods'
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__
    
    alerting:
      alertmanagers:
        - static_configs:
            - targets: ['alertmanager:9093']

  generation_engine_rules.yml: |
    groups:
      - name: generation_engine_alerts
        rules:
          - alert: HighErrorRate
            expr: rate(generation_requests_total{status="error"}[5m]) / rate(generation_requests_total[5m]) > 0.1
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: "High error rate in Generation Engine"
              description: "Error rate is {{ $value | humanizePercentage }} for the last 5 minutes"
          
          - alert: HighResponseTime
            expr: histogram_quantile(0.95, rate(generation_duration_seconds_bucket[5m])) > 30
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "High response time in Generation Engine"
              description: "95th percentile response time is {{ $value }}s"
          
          - alert: LowCacheHitRate
            expr: cache_hit_rate < 0.7
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: "Low cache hit rate"
              description: "Cache hit rate is {{ $value | humanizePercentage }}"
          
          - alert: PodCrashLooping
            expr: rate(kube_pod_container_status_restarts_total[15m]) > 0
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: "Pod is crash looping"
              description: "Pod {{ $labels.pod }} is crash looping"
          
          - alert: HighMemoryUsage
            expr: container_memory_usage_bytes{pod=~"generation-engine-.*"} / container_spec_memory_limit_bytes > 0.9
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "High memory usage"
              description: "Memory usage is {{ $value | humanizePercentage }} for pod {{ $labels.pod }}"

---
# monitoring/grafana-dashboard.json (ConfigMap)
apiVersion: v1
kind: ConfigMap
metadata:
  name: generation-engine-dashboard
  namespace: generation-engine
data:
  dashboard.json: |
    {
      "dashboard": {
        "id": null,
        "title": "Generation Engine Dashboard",
        "tags": ["generation", "engine", "llm"],
        "timezone": "browser",
        "panels": [
          {
            "title": "Request Rate",
            "type": "graph",
            "targets": [
              {
                "expr": "rate(generation_requests_total[5m])",
                "legendFormat": "{{status}}"
              }
            ],
            "yAxes": [
              {
                "label": "Requests/sec"
              }
            ]
          },
          {
            "title": "Response Time Distribution",
            "type": "graph",
            "targets": [
              {
                "expr": "histogram_quantile(0.50, rate(generation_duration_seconds_bucket[5m]))",
                "legendFormat": "50th percentile"
              },
              {
                "expr": "histogram_quantile(0.95, rate(generation_duration_seconds_bucket[5m]))",
                "legendFormat": "95th percentile"
              },
              {
                "expr": "histogram_quantile(0.99, rate(generation_duration_seconds_bucket[5m]))",
                "legendFormat": "99th percentile"
              }
            ]
          },
          {
            "title": "Provider Usage",
            "type": "piechart",
            "targets": [
              {
                "expr": "increase(generation_requests_total[1h])",
                "legendFormat": "{{provider}}"
              }
            ]
          },
          {
            "title": "Cache Hit Rate",
            "type": "singlestat",
            "targets": [
              {
                "expr": "cache_hit_rate"
              }
            ],
            "valueMaps": [
              {
                "value": "null",
                "text": "N/A"
              }
            ]
          },
          {
            "title": "Total Cost",
            "type": "singlestat",
            "targets": [
              {
                "expr": "sum(increase(generation_cost_total[1h]))"
              }
            ],
            "format": "currency"
          }
        ],
        "time": {
          "from": "now-1h",
          "to": "now"
        },
        "refresh": "30s"
      }
    }
```

---

## 10. MAINTENANCE & TROUBLESHOOTING

### 10.1 Common Issues & Solutions

```bash
#!/bin/bash
# scripts/troubleshoot.sh

echo "🔍 Generation Engine Troubleshooting Guide"

# Function definitions
check_pod_status() {
    echo "=== Pod Status ==="
    kubectl get pods -n generation-engine -l app=generation-engine
    echo ""
}

check_logs() {
    echo "=== Recent Logs ==="
    kubectl logs -n generation-engine -l app=generation-engine --tail=50
    echo ""
}

check_resources() {
    echo "=== Resource Usage ==="
    kubectl top pods -n generation-engine
    echo ""
}

check_endpoints() {
    echo "=== Service Endpoints ==="
    kubectl get endpoints -n generation-engine
    echo ""
}

check_hpa() {
    echo "=== HPA Status ==="
    kubectl get hpa -n generation-engine
    echo ""
}

check_events() {
    echo "=== Recent Events ==="
    kubectl get events -n generation-engine --sort-by='.metadata.creationTimestamp' | tail -20
    echo ""
}

# Main troubleshooting
case "${1:-all}" in
    "pods")
        check_pod_status
        ;;
    "logs")
        check_logs
        ;;
    "resources")
        check_resources
        ;;
    "endpoints")
        check_endpoints
        ;;
    "hpa")
        check_hpa
        ;;
    "events")
        check_events
        ;;
    "all")
        check_pod_status
        check_resources
        check_endpoints
        check_hpa
        check_events
        check_logs
        ;;
    *)
        echo "Usage: $0 [pods|logs|resources|endpoints|hpa|events|all]"
        exit 1
        ;;
esac
```

### 10.2 Performance Tuning Checklist

```markdown
# Performance Tuning Checklist

## 🚀 Application Level
- [ ] Enable response caching with appropriate TTL
- [ ] Optimize prompt templates for token efficiency
- [ ] Configure proper connection pooling for LLM providers
- [ ] Implement request deduplication
- [ ] Set appropriate timeout values
- [ ] Use async/await properly throughout the codebase

## 🏗️ Infrastructure Level
- [ ] Right-size container resources (CPU/Memory)
- [ ] Configure HPA with appropriate metrics
- [ ] Use multiple replicas for high availability
- [ ] Configure node affinity for optimal placement
- [ ] Enable monitoring and alerting

## 🗄️ Database Level
- [ ] Optimize database queries
- [ ] Configure connection pooling
- [ ] Set up read replicas for read-heavy workloads
- [ ] Regular database maintenance (VACUUM, ANALYZE)
- [ ] Monitor slow queries

## 🌐 Network Level
- [ ] Configure ingress controller properly
- [ ] Set up CDN for static assets (if any)
- [ ] Optimize DNS resolution
- [ ] Configure proper load balancer settings

## 🔍 Monitoring Level
- [ ] Set up comprehensive metrics collection
- [ ] Configure alerting for key SLIs
- [ ] Regular performance testing
- [ ] Capacity planning based on trends
```

---

## 11. SECURITY CONSIDERATIONS

### 11.1 Security Implementation Checklist

```yaml
# security/security-policies.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: security-policies
  namespace: generation-engine
data:
  security.yaml: |
    # API Security
    api_security:
      rate_limiting:
        enabled: true
        requests_per_minute: 100
        burst: 20
      
      authentication:
        required: true
        methods: ["jwt", "api_key"]
        jwt_expiry: 3600
      
      input_validation:
        max_query_length: 2000
        max_context_chunks: 10
        allowed_file_types: ["txt", "pdf", "docx"]
        content_filtering: true
      
      output_sanitization:
        remove_sensitive_patterns: true
        patterns:
          - "(?i)(password|secret|key|token)\\s*[=:]\\s*\\S+"
          - "\\b\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}\\b"
    
    # Data Security
    data_security:
      encryption:
        at_rest: true
        in_transit: true
        algorithm: "AES-256"
      
      data_retention:
        logs_retention_days: 30
        cache_retention_hours: 24
        conversation_history_days: 7
      
      data_classification:
        auto_classify: true
        sensitivity_levels: ["public", "internal", "confidential", "secret"]
      
      access_control:
        rbac_enabled: true
        audit_logging: true
        session_timeout_minutes: 60
```

### 11.2 Security Monitoring

```python
# src/generation_engine/security/monitor.py
import re
import hashlib
from typing import List, Dict, Any
from src.generation_engine.monitoring.logging import GenerationLogger

class SecurityMonitor:
    """Monitor security events and potential threats"""
    
    def __init__(self):
        self.logger = GenerationLogger("security_monitor")
        self.sensitive_patterns = [
            r'(?i)(password|secret|key|token)\s*[=:]\s*\S+',
            r'\b\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4}\b',  # Credit card
            r'\b\d{3}-\d{2}-\d{4}\b',  # SSN
            r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}',  # Email
        ]
        
    def check_input_security(self, query: str, user_context: Dict[str, Any]) -> List[str]:
        """Check input for security issues"""
        issues = []
        
        # Check for sensitive data in query
        for pattern in self.sensitive_patterns:
            if re.search(pattern, query):
                issues.append("sensitive_data_in_query")
                self.logger.log_error(
                    request_id=user_context.get("request_id"),
                    error_type="security_violation",
                    error_message="Sensitive data detected in query",
                    provider="security_monitor"
                )
                break
        
        # Check query length (potential DoS)
        if len(query) > 5000:
            issues.append("query_too_long")
            
        # Check for injection patterns
        injection_patterns = [
            r'<script\b[^<]*(?:(?!<\/script>)<[^<]*)*<\/script>',  # XSS
            r'union\s+select|drop\s+table|delete\s+from',  # SQL injection
            r'system\(|exec\(|eval\(',  # Code injection
        ]
        
        for pattern in injection_patterns:
            if re.search(pattern, query, re.IGNORECASE):
                issues.append("injection_attempt")
                self.logger.log_error(
                    request_id=user_context.get("request_id"),
                    error_type="security_violation",
                    error_message="Injection attempt detected",
                    provider="security_monitor"
                )
                break
        
        return issues
    
    def sanitize_output(self, answer: str) -> str:
        """Remove sensitive information from output"""
        sanitized = answer
        
        for pattern in self.sensitive_patterns:
            sanitized = re.sub(pattern, '[REDACTED]', sanitized)
        
        return sanitized
    
    def hash_pii(self, text: str) -> str:
        """Hash PII for logging purposes"""
        return hashlib.sha256(text.encode()).hexdigest()[:8]
```

---

## 12. KẾT LUẬN

### 12.1 Deliverables Summary

Tài liệu này cung cấp roadmap đầy đủ cho việc implement FR-04.3 Generation Engine bao gồm:

✅ **Thiết kế kiến trúc chi tiết** với workflow và error handling  
✅ **Chuẩn bị môi trường** với hardware/software requirements cụ thể  
✅ **Implementation roadmap** 12 tuần với 5 phases rõ ràng  
✅ **Testing strategy** toàn diện với unit, integration, performance tests  
✅ **Production deployment** với Docker, Kubernetes, monitoring  
✅ **Security considerations** và best practices  

### 12.2 Success Criteria

**Functional Requirements:**
- [x] Tích hợp thành công với multiple LLM providers (OpenAI, Claude, Gemini, Grok, OpenRouter)
- [x] Hỗ trợ local LLM models (Llama 2, Mistral, etc.)
- [x] Sinh câu trả lời chất lượng cao với confidence score
- [x] Cung cấp citations chính xác với reference linking
- [x] Xử lý fallback và error recovery tự động

**Non-Functional Requirements:**
- [x] Response time < 60 seconds (SLA requirement)
- [x] Support 100+ concurrent users
- [x] 99.5% uptime trong production
- [x] Comprehensive monitoring và alerting
- [x] Security-first design với input validation

### 12.3 Next Steps After Implementation

#### Phase 1: Post-Deployment (Week 13-14)
1. **Production Monitoring Setup**
   - Deploy Prometheus + Grafana dashboards
   - Configure alerting rules
   - Set up log aggregation (ELK stack)
   
2. **Performance Baseline**
   - Collect baseline metrics for 1 week
   - Identify optimization opportunities
   - Document performance characteristics
   
3. **User Training**
   - Train end users on new system
   - Collect initial feedback
   - Document common use cases


#### Phase 2: Optimization (Week 15-16)

1. **Performance Tuning**
   - Optimize based on real usage patterns
   - Fine-tune cache settings
   - Adjust resource allocations
   
2. **Cost Optimization**
   - Analyze LLM usage costs
   - Implement cost-saving strategies
   - Consider local LLM deployment
   
3. **Feature Enhancement**
   - Add multi-language support
   - Implement conversation context
   - Add advanced analytics


#### Phase 3: Scale & Extend (Week 17+)

1. **Integration Planning**
   - Plan integration with existing systems
   - Design API for other applications
   - Consider mobile app development
   
2. **Advanced Features**
   - Implement semantic caching
   - Add document summarization
   - Develop knowledge graph integration
   
3. **Enterprise Features**
   - Advanced user management
   - Department-specific customization
   - Audit and compliance features


### 12.4 Risk Mitigation Strategies

```mermaid
graph TB
    subgraph "🎯 High Priority Risks"
        R1[LLM API Costs<br/>Exceed Budget]
        R2[Performance Below<br/>SLA Requirements]
        R3[Security<br/>Vulnerabilities]
        R4[Low User<br/>Adoption]
    end
    
    subgraph "🛡️ Mitigation Strategies"
        M1[Local LLM Backup<br/>Cost Monitoring<br/>Usage Caps]
        M2[Performance Testing<br/>Caching Strategy<br/>Load Balancing]
        M3[Security Audit<br/>Penetration Testing<br/>Input Validation]
        M4[User Training<br/>Change Management<br/>Feedback Loop]
    end
    
    subgraph "📊 Monitoring & Controls"
        C1[Real-time Cost<br/>Dashboards]
        C2[SLA Monitoring<br/>Alerting]
        C3[Security<br/>Scanning]
        C4[User Analytics<br/>Satisfaction Surveys]
    end
    
    R1 --> M1 --> C1
    R2 --> M2 --> C2
    R3 --> M3 --> C3
    R4 --> M4 --> C4
    
    classDef risk fill:#ffebee,stroke:#c62828,stroke-width:2px
    classDef mitigation fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef control fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    
    class R1,R2,R3,R4 risk
    class M1,M2,M3,M4 mitigation
    class C1,C2,C3,C4 control
```

### 12.5 Success Metrics & KPIs

#### 📈 **Technical KPIs**
| Metric | Target | Measurement |
|--------|--------|-------------|
| **Response Time** | < 60s (95th percentile) | Prometheus histogram |
| **Uptime** | 99.5% | Service monitoring |
| **Error Rate** | < 1% | Request success/failure ratio |
| **Cache Hit Rate** | > 70% | Cache statistics |
| **Concurrent Users** | 100+ without degradation | Load testing |

#### 💰 **Business KPIs**
| Metric | Target | Measurement |
|--------|--------|-------------|
| **User Adoption** | 80% of target users in 3 months | User analytics |
| **Query Resolution** | 85% answered successfully | Response quality analysis |
| **Time Savings** | 50% reduction in info search time | User surveys |
| **Cost per Query** | < $0.05 average | Cost tracking |
| **User Satisfaction** | > 4.0/5.0 rating | Feedback surveys |

#### 🔍 **Quality KPIs**
| Metric | Target | Measurement |
|--------|--------|-------------|
| **Answer Accuracy** | > 90% | Expert evaluation |
| **Citation Quality** | > 95% valid references | Automated checking |
| **Response Relevance** | > 85% rated as relevant | User feedback |
| **Language Quality** | Professional Vietnamese | Manual review |

### 12.6 Maintenance Schedule

#### 🗓️ **Daily Tasks**
- Monitor system health dashboards
- Review error logs and alerts
- Check resource utilization
- Backup verification

#### 📅 **Weekly Tasks**
- Performance analysis and optimization
- Security log review
- User feedback analysis
- Cost analysis and optimization

#### 📆 **Monthly Tasks**
- Comprehensive system review
- Security audit and updates
- Performance benchmarking
- User training sessions
- Documentation updates

#### 🎯 **Quarterly Tasks**
- Major feature releases
- Infrastructure capacity planning
- Security penetration testing
- Business value assessment
- Technology stack updates

### 12.7 Team Responsibilities

#### 👨‍💻 **Development Team**
- Core engine development and maintenance
- Bug fixes and feature enhancements
- Code review and quality assurance
- Technical documentation

#### 🏗️ **DevOps Team**
- Infrastructure management
- CI/CD pipeline maintenance
- Monitoring and alerting
- Security implementations

#### 📊 **Data Team**
- Data quality management
- Embedding model optimization
- Performance analysis
- Usage analytics

#### 🎯 **Product Team**
- Requirements gathering
- User experience design
- Stakeholder communication
- Success metrics tracking

### 12.8 Technology Roadmap

#### 🔮 **Short-term (3-6 months)**
```mermaid
gantt
    title Technology Roadmap - Short Term
    dateFormat  YYYY-MM-DD
    section Core Features
    Multi-language Support    :2025-09-15, 30d
    Conversation Context      :2025-10-15, 45d
    Advanced Analytics        :2025-11-01, 30d
    
    section Performance
    Semantic Caching          :2025-09-20, 25d
    Local LLM Deployment      :2025-10-01, 60d
    Response Optimization     :2025-11-15, 30d
    
    section Integration
    API v2 Development        :2025-10-01, 45d
    Mobile App Planning       :2025-11-01, 30d
    SSO Integration           :2025-11-15, 30d
```

#### 🚀 **Long-term (6-12 months)**
- Knowledge graph integration
- Advanced AI capabilities (RAG + reasoning)
- Multi-modal support (images, documents)
- Enterprise-grade analytics
- AI model fine-tuning on company data

### 12.9 Final Recommendations

#### 🎯 **Critical Success Factors**
1. **Start Simple**: Begin with core functionality, iterate based on user feedback
2. **Monitor Everything**: Comprehensive monitoring from day one
3. **Security First**: Never compromise on security, especially with LLM integrations
4. **User-Centric**: Regular user feedback and continuous improvement
5. **Cost Management**: Monitor LLM costs closely, implement safeguards

#### ⚠️ **Key Warnings**
1. **LLM Costs Can Escalate Quickly**: Implement usage caps and monitoring
2. **Quality Control is Critical**: Bad answers hurt user trust permanently  
3. **Security is Paramount**: Any data leakage can be catastrophic
4. **Performance SLAs are Hard**: Plan for scale from the beginning
5. **Change Management**: Technical success ≠ business success without adoption

#### 📚 **Recommended Reading**
- "Building LLM Applications for Production" - Chip Huyen
- "Designing Data-Intensive Applications" - Martin Kleppmann
- "Site Reliability Engineering" - Google SRE Team
- "The DevOps Handbook" - Gene Kim, et al.

---

## 🎉 **KÍCH THƯỚC VÀ PHẠM VI DỰ ÁN**

### Tổng quan số liệu:
- **📋 Tài liệu**: 50+ pages chi tiết specifications
- **⏱️ Timeline**: 12 tuần implementation + 4 tuần optimization  
- **👥 Team size**: 4-6 developers (Backend, Frontend, DevOps, QA)
- **💰 Budget estimate**: 1.2 tỷ VNĐ total project cost
- **🎯 Success criteria**: 15+ KPIs across technical, business, and quality metrics

### Deliverables hoàn chỉnh:
✅ **Kiến trúc chi tiết** với 6-layer architecture  
✅ **Environment setup** với step-by-step instructions  
✅ **Implementation roadmap** với 5 phases, 48 checkpoints  
✅ **Testing strategy** với 80%+ code coverage target  
✅ **Production deployment** với Docker + Kubernetes  
✅ **Security framework** với monitoring và compliance  
✅ **Monitoring & alerting** với Prometheus + Grafana  
✅ **Error handling** với comprehensive fallback strategies  
✅ **Performance optimization** với caching và load balancing  
✅ **Maintenance procedures** với daily/weekly/monthly tasks  

**🚀 Team kỹ thuật giờ đây có thể follow step-by-step guide này để implement FR-04.3 Generation Engine một cách tối ưu và chuyên nghiệp!**
