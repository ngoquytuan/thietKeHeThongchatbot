Náº¿u Ä‘Ã£ Ä‘á»§ thÃ´ng tin hÃ£y giÃºp tÃ´i viáº¿t tÃ i liá»‡u thiáº¿t káº¿ module dÆ°á»›i Ä‘Ã¢y.
TÃ i liá»‡u nÃ y cung cáº¥p roadmap Ä‘áº§y Ä‘á»§ Ä‘á»ƒ implement FR-04.3 mÃ  khÃ´ng cáº§n code cá»¥ thá»ƒ nhÆ°ng 
cÃ³ cÃ¡c bÆ°á»›c lÃ m cá»¥ thá»ƒ nhÆ° chuáº©n bá»‹ mÃ´i trÆ°á»ng PC tháº¿ nÃ o, pháº§n má»m gÃ¬?
 Ä‘á»ƒ Team ká»¹ thuáº­t cÃ³ thá»ƒ follow step-by-step guide nÃ y Ä‘á»ƒ thiáº¿t káº¿ tá»‘i Æ°u cho há»‡ thá»‘ng chatbot
 vÃ  cÃ³ mermaidchart luá»“ng cÃ´ng viá»‡c, Ä‘iá»ƒm quay Ä‘áº§u khi bá»‹ fail.

### 3.4 Module RAG Core Engine (FR-04)

**FR-04.3 - Generation (Táº¡o sinh)**
- TÃ­ch há»£p vá»›i LLM (OpenAI GPT, Anthropic Claude, hoáº·c local model)
- Sinh cÃ¢u tráº£ lá»i dá»±a trÃªn context vÃ  cÃ¢u há»i
- Cung cáº¥p citation/reference cho cÃ¢u tráº£ lá»i

---
# TÃ€I LIá»†U THIáº¾T Káº¾ MODULE - RAG GENERATION ENGINE (FR-04.3)

---

**Module:** RAG Core Engine - Generation Service  
**Requirement:** FR-04.3 - Generation (Táº¡o sinh)  
**PhiÃªn báº£n:** 1.0  
**NgÃ y:** 01/09/2025  
**NgÆ°á»i soáº¡n tháº£o:** Technical Design Team  

---

## 1. Tá»”NG QUAN MODULE

### 1.1 Má»¥c Ä‘Ã­ch
Module Generation Engine lÃ  thÃ nh pháº§n quan trá»ng nháº¥t trong RAG pipeline, chá»‹u trÃ¡ch nhiá»‡m:
- TÃ­ch há»£p vá»›i cÃ¡c LLM providers (OpenAI, Anthropic Claude, Gemini, Grok, OpenRouter)
- Há»— trá»£ local LLM models (Llama 2, Mistral, CodeLlama)
- Sinh cÃ¢u tráº£ lá»i cháº¥t lÆ°á»£ng cao dá»±a trÃªn context Ä‘Ã£ Ä‘Æ°á»£c retrieve
- Cung cáº¥p citation vÃ  reference chÃ­nh xÃ¡c
- Xá»­ lÃ½ fallback vÃ  error recovery

### 1.2 Input/Output Specification

**Input:**
```json
{
  "query": "Quy trÃ¬nh mua hÃ ng trÃ¬nh giÃ¡m Ä‘á»‘c nhÆ° tháº¿ nÃ o?",
  "context_chunks": [
    {
      "chunk_id": "doc123_chunk_5",
      "content": "BÆ°á»›c 1: Táº¡o yÃªu cáº§u mua hÃ ng...",
      "metadata": {
        "document_id": "proc_001",
        "title": "Quy trÃ¬nh Mua hÃ ng",
        "page": 2,
        "confidence_score": 0.95
      }
    }
  ],
  "user_context": {
    "user_id": "emp001",
    "department": "rd",
    "access_level": "employee"
  },
  "conversation_history": []
}
```

**Output:**
```json
{
  "answer": "Quy trÃ¬nh mua hÃ ng trÃ¬nh giÃ¡m Ä‘á»‘c gá»“m 5 bÆ°á»›c chÃ­nh...",
  "citations": [
    {
      "chunk_id": "doc123_chunk_5", 
      "reference": "[1] Quy trÃ¬nh Mua hÃ ng, trang 2",
      "used_content": "BÆ°á»›c 1: Táº¡o yÃªu cáº§u mua hÃ ng..."
    }
  ],
  "confidence_score": 0.92,
  "response_metadata": {
    "model_used": "openai-gpt-4",
    "tokens_used": 1250,
    "processing_time": 2.3,
    "cost_estimate": 0.025
  }
}
```

---

## 2. THIáº¾T Káº¾ KIáº¾N TRÃšC MODULE

### 2.1 Architecture Overview

```mermaid
graph TB
    %% Input
    Input[ğŸ“¥ Input Request<br/>Query + Context + User Info]
    
    %% Generation Engine Core
    subgraph "ğŸ¯ GENERATION ENGINE CORE"
        subgraph "ğŸ“ Prompt Management"
            TemplateManager[ğŸ“‹ Template Manager<br/>System & User Prompts]
            PromptBuilder[ğŸ”§ Prompt Builder<br/>Dynamic Assembly]
            ContextOptimizer[âš¡ Context Optimizer<br/>Token Management]
        end
        
        subgraph "ğŸ§  LLM Integration Layer"
            ProviderRouter[ğŸ¯ Provider Router<br/>Load Balancing]
            
            subgraph "â˜ï¸ Cloud Providers"
                OpenAIClient[ğŸ¤– OpenAI Client]
                ClaudeClient[ğŸ§  Claude Client] 
                GeminiClient[ğŸ’ Gemini Client]
                GrokClient[âš¡ Grok Client]
                OpenRouterClient[ğŸŒ OpenRouter Client]
            end
            
            subgraph "ğŸ  Local Models"
                LocalLLMManager[ğŸ–¥ï¸ Local LLM Manager]
                ModelLoader[ğŸ“¦ Model Loader]
                InferenceEngine[âš™ï¸ Inference Engine]
            end
        end
        
        subgraph "ğŸ“Š Response Processing"
            ResponseParser[ğŸ“„ Response Parser<br/>Extract Answer]
            CitationGenerator[ğŸ“š Citation Generator<br/>Reference Linking]
            QualityValidator[âœ… Quality Validator<br/>Answer Validation]
        end
        
        subgraph "ğŸ”„ Error Handling"
            FallbackManager[ğŸ›¡ï¸ Fallback Manager<br/>Provider Switching]
            RetryLogic[ğŸ”„ Retry Logic<br/>Exponential Backoff]
            ErrorRecovery[ğŸš¨ Error Recovery<br/>Graceful Degradation]
        end
    end
    
    %% External Dependencies
    subgraph "ğŸŒ External Services"
        CloudAPIs[â˜ï¸ Cloud LLM APIs]
        LocalModels[ğŸ  Local Model Files]
        ConfigService[âš™ï¸ Configuration Service]
        MonitoringService[ğŸ“Š Monitoring Service]
    end
    
    %% Output
    Output[ğŸ“¤ Generated Response<br/>Answer + Citations + Metadata]
    
    %% Flow
    Input --> TemplateManager
    TemplateManager --> PromptBuilder
    PromptBuilder --> ContextOptimizer
    ContextOptimizer --> ProviderRouter
    
    ProviderRouter --> OpenAIClient
    ProviderRouter --> ClaudeClient
    ProviderRouter --> GeminiClient
    ProviderRouter --> GrokClient
    ProviderRouter --> OpenRouterClient
    ProviderRouter --> LocalLLMManager
    
    LocalLLMManager --> ModelLoader
    ModelLoader --> InferenceEngine
    
    OpenAIClient --> ResponseParser
    ClaudeClient --> ResponseParser
    GeminiClient --> ResponseParser
    GrokClient --> ResponseParser
    OpenRouterClient --> ResponseParser
    InferenceEngine --> ResponseParser
    
    ResponseParser --> CitationGenerator
    CitationGenerator --> QualityValidator
    QualityValidator --> Output
    
    %% Error Handling
    ProviderRouter -.-> FallbackManager
    FallbackManager --> RetryLogic
    RetryLogic --> ErrorRecovery
    ErrorRecovery -.-> ProviderRouter
    
    %% External Connections
    OpenAIClient -.-> CloudAPIs
    ClaudeClient -.-> CloudAPIs
    GeminiClient -.-> CloudAPIs
    GrokClient -.-> CloudAPIs
    OpenRouterClient -.-> CloudAPIs
    ModelLoader -.-> LocalModels
    TemplateManager -.-> ConfigService
    QualityValidator -.-> MonitoringService
    
    %% Styling
    classDef input fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef prompt fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef llm fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef cloud fill:#e1f5fe,stroke:#0277bd,stroke-width:2px
    classDef local fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef response fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    classDef error fill:#ffebee,stroke:#c62828,stroke-width:2px
    classDef external fill:#f1f8e9,stroke:#689f38,stroke-width:1px
    classDef output fill:#e0f2f1,stroke:#00796b,stroke-width:2px
    
    class Input input
    class TemplateManager,PromptBuilder,ContextOptimizer prompt
    class ProviderRouter llm
    class OpenAIClient,ClaudeClient,GeminiClient,GrokClient,OpenRouterClient cloud
    class LocalLLMManager,ModelLoader,InferenceEngine local
    class ResponseParser,CitationGenerator,QualityValidator response
    class FallbackManager,RetryLogic,ErrorRecovery error
    class CloudAPIs,LocalModels,ConfigService,MonitoringService external
    class Output output
```

---

## 3. CHUáº¨N Bá»Š MÃ”I TRÆ¯á»œNG PHÃT TRIá»‚N

### 3.1 Hardware Requirements

**Minimum Requirements:**
- **CPU**: Intel i7/AMD Ryzen 7 (8 cores)
- **RAM**: 32GB (64GB recommended cho local LLM)
- **GPU**: NVIDIA RTX 3080/4080 (12GB+ VRAM) cho local inference
- **Storage**: 1TB NVMe SSD
- **Network**: Stable internet connection (minimum 100Mbps)

**Recommended Production Setup:**
- **CPU**: Intel Xeon/AMD EPYC (16+ cores)
- **RAM**: 128GB+
- **GPU**: NVIDIA A100/H100 cho production local LLM
- **Storage**: 2TB+ NVMe SSD vá»›i RAID configuration

### 3.2 Software Environment Setup

#### 3.2.1 Base Environment
```bash
# 1. Operating System
Ubuntu 22.04 LTS hoáº·c CentOS 8+
Windows 11 Pro (cho development)

# 2. Container Platform
Docker Desktop 4.20+
Docker Compose 2.18+
Kubernetes 1.27+ (cho production)

# 3. Programming Languages
Python 3.11+
Node.js 18+ (cho monitoring dashboard)
Go 1.21+ (optional, cho high-performance components)
```

#### 3.2.2 Python Environment
```bash
# Táº¡o virtual environment
python3.11 -m venv llm_generation_env
source llm_generation_env/bin/activate  # Linux/Mac
# llm_generation_env\Scripts\activate  # Windows

# Core packages
pip install --upgrade pip setuptools wheel

# LLM Integration
pip install openai==1.30.0
pip install anthropic==0.28.0
pip install google-cloud-aiplatform==1.51.0
pip install transformers==4.37.0
pip install torch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0
pip install accelerate==0.27.0
pip install bitsandbytes==0.42.0

# API & Web Framework
pip install fastapi==0.109.0
pip install uvicorn==0.27.0
pip install httpx==0.26.0
pip install aiofiles==23.2.1

# Monitoring & Logging
pip install prometheus-client==0.19.0
pip install structlog==23.2.0
pip install sentry-sdk==1.40.0

# Data Processing
pip install pydantic==2.5.0
pip install tiktoken==0.6.0
pip install jinja2==3.1.3

# Testing
pip install pytest==8.0.0
pip install pytest-asyncio==0.23.0
pip install httpx==0.26.0
```

#### 3.2.3 Local LLM Setup (Optional)
```bash
# Ollama cho easy local LLM deployment
curl -fsSL https://ollama.ai/install.sh | sh

# HuggingFace Hub
pip install huggingface-hub==0.20.0

# vLLM cho high-performance inference
pip install vllm==0.3.0

# Text Generation Inference (TGI)
docker pull ghcr.io/huggingface/text-generation-inference:1.4
```

#### 3.2.4 Configuration Management
```bash
# Environment management
pip install python-dotenv==1.0.0
pip install pyyaml==6.0.1

# Secret management
pip install cryptography==42.0.0
pip install keyring==24.3.0
```

### 3.3 Development Tools

#### 3.3.1 IDE & Code Editor
```bash
# VS Code vá»›i extensions
- Python
- Pylance
- Docker
- Kubernetes
- REST Client
- GitLens

# JetBrains PyCharm Professional (recommended)
# Vim/Neovim vá»›i Python LSP setup
```

#### 3.3.2 Testing & Quality Tools
```bash
# Code formatting & linting
pip install black==24.0.0
pip install isort==5.13.0  
pip install flake8==7.0.0
pip install mypy==1.8.0

# Security scanning
pip install bandit==1.7.5
pip install safety==3.0.0

# Load testing
pip install locust==2.20.0
```

#### 3.3.3 Database & Cache
```bash
# Redis cho caching
docker run -d --name redis-cache -p 6379:6379 redis:7-alpine

# PostgreSQL cho metadata
docker run -d --name postgres-meta \
  -e POSTGRES_PASSWORD=dev_password \
  -p 5432:5432 postgres:15-alpine
```

---

## 4. IMPLEMENTATION ROADMAP

### 4.1 Development Phases

```mermaid
gantt
    title Implementation Roadmap - Generation Engine
    dateFormat  YYYY-MM-DD
    section Phase 1: Setup
    Environment Setup     :done, setup, 2025-09-01, 3d
    Basic Structure      :done, structure, after setup, 2d
    
    section Phase 2: Core Development  
    Template System      :active, template, 2025-09-06, 5d
    Provider Integration :provider, after template, 7d
    Response Processing  :response, after provider, 4d
    
    section Phase 3: Advanced Features
    Citation System      :citation, after response, 5d
    Error Handling       :error, after citation, 4d
    Local LLM Support    :local, after error, 6d
    
    section Phase 4: Testing & Optimization
    Unit Testing         :testing, after local, 4d
    Performance Testing  :perf, after testing, 3d
    Integration Testing  :integration, after perf, 5d
    
    section Phase 5: Production
    Production Deploy    :deploy, after integration, 3d
    Monitoring Setup     :monitor, after deploy, 2d
```

### 4.2 Detailed Implementation Steps

#### 4.2.1 Phase 1: Environment & Structure Setup (5 days)

**Day 1-3: Environment Setup**
```bash
# Step 1: Táº¡o project structure
mkdir rag-generation-engine
cd rag-generation-engine

mkdir -p {
  src/generation_engine/{core,providers,templates,utils},
  tests/{unit,integration,performance},
  configs/{development,staging,production},
  docs/{api,deployment},
  scripts/{setup,deployment,testing},
  docker/{development,production},
  monitoring/{prometheus,grafana}
}

# Step 2: Initialize Python project
touch src/generation_engine/__init__.py
touch requirements/{base.txt,dev.txt,prod.txt}
touch {.env.example,.gitignore,README.md,Dockerfile,docker-compose.yml}

# Step 3: Setup virtual environment
python3.11 -m venv venv
source venv/bin/activate
pip install -r requirements/dev.txt
```

**Day 4-5: Basic Project Structure**
```python
# src/generation_engine/core/models.py
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
from enum import Enum

class LLMProvider(str, Enum):
    OPENAI = "openai"
    CLAUDE = "claude"
    GEMINI = "gemini"
    GROK = "grok"
    OPENROUTER = "openrouter"
    LOCAL = "local"

class GenerationRequest(BaseModel):
    query: str
    context_chunks: List[Dict[str, Any]]
    user_context: Dict[str, Any]
    conversation_history: Optional[List[Dict[str, str]]] = []
    preferred_provider: Optional[LLMProvider] = None

class GenerationResponse(BaseModel):
    answer: str
    citations: List[Dict[str, Any]]
    confidence_score: float
    response_metadata: Dict[str, Any]
```

#### 4.2.2 Phase 2: Core Development (16 days)

**Days 6-10: Template Management System**

**Checkpoint 1**: Template System hoáº¡t Ä‘á»™ng cÆ¡ báº£n
```python
# src/generation_engine/templates/manager.py
class TemplateManager:
    """Quáº£n lÃ½ system vÃ  user prompt templates"""
    
    def __init__(self, config_path: str):
        self.templates = self._load_templates(config_path)
        
    def get_system_prompt(self, context_type: str) -> str:
        """Láº¥y system prompt theo loáº¡i context"""
        pass
        
    def build_user_prompt(self, query: str, context: List[str]) -> str:
        """XÃ¢y dá»±ng user prompt vá»›i query vÃ  context"""
        pass

# configs/prompts/system_prompts.yaml
vietnamese_qa:
  template: |
    Báº¡n lÃ  má»™t trá»£ lÃ½ AI chuyÃªn nghiá»‡p cá»§a cÃ´ng ty ká»¹ thuáº­t hÃ ng khÃ´ng.
    Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  tráº£ lá»i cÃ¢u há»i dá»±a trÃªn tÃ i liá»‡u ná»™i bá»™ Ä‘Æ°á»£c cung cáº¥p.
    
    QUY TÃ‚C QUAN TRá»ŒNG:
    1. Chá»‰ tráº£ lá»i dá»±a trÃªn thÃ´ng tin trong context Ä‘Æ°á»£c cung cáº¥p
    2. Náº¿u khÃ´ng cÃ³ thÃ´ng tin, hÃ£y nÃ³i "TÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin nÃ y trong tÃ i liá»‡u"
    3. LuÃ´n cung cáº¥p nguá»“n tham kháº£o [sá»‘] cho má»—i thÃ´ng tin
    4. Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t chuyÃªn nghiá»‡p
```

**Days 11-17: Provider Integration**

**Checkpoint 2**: TÃ­ch há»£p thÃ nh cÃ´ng vá»›i Ã­t nháº¥t 2 providers (OpenAI + Claude)
```python
# src/generation_engine/providers/base.py
from abc import ABC, abstractmethod

class BaseLLMProvider(ABC):
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        
    @abstractmethod
    async def generate(self, prompt: str, **kwargs) -> Dict[str, Any]:
        """Generate response tá»« LLM"""
        pass
        
    @abstractmethod
    def estimate_cost(self, input_tokens: int, output_tokens: int) -> float:
        """Æ¯á»›c tÃ­nh chi phÃ­ API call"""
        pass

# src/generation_engine/providers/openai_provider.py
import openai
from .base import BaseLLMProvider

class OpenAIProvider(BaseLLMProvider):
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.client = openai.AsyncOpenAI(
            api_key=config["api_key"]
        )
    
    async def generate(self, prompt: str, **kwargs) -> Dict[str, Any]:
        response = await self.client.chat.completions.create(
            model=self.config.get("model", "gpt-4-turbo-preview"),
            messages=[{"role": "user", "content": prompt}],
            temperature=kwargs.get("temperature", 0.1),
            max_tokens=kwargs.get("max_tokens", 1000)
        )
        
        return {
            "text": response.choices[0].message.content,
            "tokens_used": response.usage.total_tokens,
            "model": response.model
        }
```

**Days 18-21: Response Processing**

**Checkpoint 3**: Response parsing vÃ  citation generation hoáº¡t Ä‘á»™ng
```python
# src/generation_engine/core/citation.py
class CitationGenerator:
    def __init__(self):
        self.citation_pattern = r'\[(\d+)\]'
        
    def generate_citations(self, answer: str, context_chunks: List[Dict]) -> List[Dict]:
        """Táº¡o citations tá»« answer vÃ  context chunks"""
        citations = []
        
        # Extract citation numbers from answer
        cited_numbers = re.findall(self.citation_pattern, answer)
        
        for num_str in cited_numbers:
            num = int(num_str) - 1  # Convert to 0-based index
            if num < len(context_chunks):
                chunk = context_chunks[num]
                citations.append({
                    "number": int(num_str),
                    "chunk_id": chunk["chunk_id"],
                    "reference": f"[{num_str}] {chunk['metadata']['title']}, trang {chunk['metadata'].get('page', 'N/A')}",
                    "used_content": chunk["content"][:200] + "..."
                })
        
        return citations
```

#### 4.2.3 Phase 3: Advanced Features (15 days)

**Days 22-26: Citation System Enhancement**

**Checkpoint 4**: Advanced citation vá»›i confidence scoring
```python
# src/generation_engine/core/citation_advanced.py
class AdvancedCitationGenerator:
    def __init__(self, embedding_model):
        self.embedding_model = embedding_model
        
    def calculate_citation_relevance(self, answer_part: str, context_chunk: str) -> float:
        """TÃ­nh toÃ¡n Ä‘á»™ liÃªn quan giá»¯a pháº§n answer vÃ  context chunk"""
        # Sá»­ dá»¥ng embedding similarity
        answer_embedding = self.embedding_model.encode([answer_part])
        context_embedding = self.embedding_model.encode([context_chunk])
        
        similarity = cosine_similarity(answer_embedding, context_embedding)[0][0]
        return float(similarity)
```

**Days 27-30: Error Handling & Fallback**

**Checkpoint 5**: Robust error handling vá»›i multiple fallback strategies
```python
# src/generation_engine/core/fallback.py
class FallbackManager:
    def __init__(self, providers: List[BaseLLMProvider]):
        self.providers = providers
        self.current_provider_index = 0
        
    async def generate_with_fallback(self, request: GenerationRequest) -> GenerationResponse:
        """Thá»­ generate vá»›i fallback giá»¯a cÃ¡c providers"""
        last_error = None
        
        for attempt in range(len(self.providers)):
            provider = self.providers[self.current_provider_index]
            
            try:
                result = await provider.generate(request)
                return result
                
            except Exception as e:
                last_error = e
                logger.warning(f"Provider {provider.__class__.__name__} failed: {e}")
                self.current_provider_index = (self.current_provider_index + 1) % len(self.providers)
                
                # Exponential backoff
                await asyncio.sleep(2 ** attempt)
        
        # Náº¿u táº¥t cáº£ providers Ä‘á»u fail
        raise GenerationError(f"All providers failed. Last error: {last_error}")
```

**Days 31-36: Local LLM Support**

**Checkpoint 6**: Local LLM integration hoáº¡t Ä‘á»™ng
```python
# src/generation_engine/providers/local_provider.py
class LocalLLMProvider(BaseLLMProvider):
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.model_name = config["model_name"]  # "llama2-7b-chat"
        self.inference_engine = self._setup_inference_engine()
        
    def _setup_inference_engine(self):
        if self.config["engine"] == "vllm":
            from vllm import LLM, SamplingParams
            return LLM(model=self.model_name)
        elif self.config["engine"] == "ollama":
            import ollama
            return ollama
        else:
            raise ValueError(f"Unsupported inference engine: {self.config['engine']}")
    
    async def generate(self, prompt: str, **kwargs) -> Dict[str, Any]:
        if self.config["engine"] == "vllm":
            sampling_params = SamplingParams(
                temperature=kwargs.get("temperature", 0.1),
                max_tokens=kwargs.get("max_tokens", 1000)
            )
            outputs = self.inference_engine.generate([prompt], sampling_params)
            
            return {
                "text": outputs[0].outputs[0].text,
                "tokens_used": len(outputs[0].outputs[0].token_ids),
                "model": self.model_name
            }
```

#### 4.2.4 Phase 4: Testing & Optimization (12 days)

**Days 37-40: Unit Testing**

**Checkpoint 7**: Test coverage > 80%
```python
# tests/unit/test_generation_core.py
import pytest
from unittest.mock import Mock, patch
from src.generation_engine.core.engine import GenerationEngine

class TestGenerationEngine:
    @pytest.fixture
    def mock_providers(self):
        return [Mock(), Mock(), Mock()]
    
    @pytest.fixture 
    def generation_engine(self, mock_providers):
        config = {"providers": mock_providers}
        return GenerationEngine(config)
    
    @pytest.mark.asyncio
    async def test_generate_success(self, generation_engine):
        # Setup
        request = GenerationRequest(
            query="Test query",
            context_chunks=[{"content": "test context"}],
            user_context={"user_id": "test_user"}
        )
        
        # Mock provider response
        generation_engine.providers[0].generate.return_value = {
            "text": "Test answer [1]",
            "tokens_used": 100,
            "model": "gpt-4"
        }
        
        # Test
        response = await generation_engine.generate(request)
        
        # Assert
        assert response.answer == "Test answer [1]"
        assert len(response.citations) == 1
        assert response.confidence_score > 0
```

**Days 41-43: Performance Testing**

**Checkpoint 8**: Performance benchmarks established
```python
# tests/performance/test_load.py
import asyncio
import time
from locust import HttpUser, task, between

class GenerationLoadTest(HttpUser):
    wait_time = between(1, 3)
    
    @task
    def test_generation_endpoint(self):
        payload = {
            "query": "Quy trÃ¬nh mua hÃ ng nhÆ° tháº¿ nÃ o?",
            "context_chunks": [
                {
                    "content": "BÆ°á»›c 1: Táº¡o yÃªu cáº§u mua hÃ ng...",
                    "metadata": {"title": "Quy trÃ¬nh mua hÃ ng"}
                }
            ],
            "user_context": {"user_id": "test_user"}
        }
        
        with self.client.post("/api/generate", json=payload, catch_response=True) as response:
            if response.status_code == 200:
                json_response = response.json()
                if "answer" in json_response and "citations" in json_response:
                    response.success()
                else:
                    response.failure("Invalid response format")
            else:
                response.failure(f"HTTP {response.status_code}")

# Run vá»›i: locust -f tests/performance/test_load.py --host=http://localhost:8000
```

**Days 44-48: Integration Testing**

**Checkpoint 9**: End-to-end integration tests pass
```python
# tests/integration/test_full_pipeline.py
@pytest.mark.integration
class TestFullPipeline:
    @pytest.mark.asyncio
    async def test_end_to_end_generation(self):
        """Test complete pipeline tá»« request Ä‘áº¿n response"""
        
        # Setup real providers (vá»›i test API keys)
        config = load_test_config()
        engine = GenerationEngine(config)
        
        # Real request
        request = GenerationRequest(
            query="Quy trÃ¬nh mua hÃ ng trÃ¬nh giÃ¡m Ä‘á»‘c nhÆ° tháº¿ nÃ o?",
            context_chunks=[
                {
                    "chunk_id": "test_chunk_1",
                    "content": "Quy trÃ¬nh mua hÃ ng gá»“m 5 bÆ°á»›c: 1) Táº¡o yÃªu cáº§u...",
                    "metadata": {
                        "document_id": "proc_001",
                        "title": "Quy trÃ¬nh Mua hÃ ng",
                        "page": 1
                    }
                }
            ],
            user_context={
                "user_id": "test_user",
                "department": "rd", 
                "access_level": "employee"
            }
        )
        
        # Execute
        start_time = time.time()
        response = await engine.generate(request)
        end_time = time.time()
        
        # Assertions
        assert response.answer is not None
        assert len(response.answer) > 50  # Reasonable answer length
        assert response.confidence_score >= 0.5
        assert len(response.citations) > 0
        assert (end_time - start_time) < 60  # Under 60 seconds requirement
        assert response.response_metadata["tokens_used"] > 0
```

---

## 5. WORKFLOW VÃ€ ERROR HANDLING

### 5.1 Main Processing Workflow

```mermaid
flowchart TD
    Start([ğŸš€ Start Generation Request]) --> ValidateInput{âœ… Validate Input}
    
    ValidateInput -->|âŒ Invalid| InputError[âŒ Input Validation Error]
    ValidateInput -->|âœ… Valid| SelectProvider[ğŸ¯ Select LLM Provider]
    
    SelectProvider --> CheckCache{ğŸ’¾ Check Response Cache}
    CheckCache -->|âœ… Hit| ReturnCached[ğŸ“‹ Return Cached Response]
    CheckCache -->|âŒ Miss| BuildPrompt[ğŸ“ Build Prompt Template]
    
    BuildPrompt --> OptimizeContext[âš¡ Optimize Context Length]
    OptimizeContext --> CallLLM[ğŸ§  Call LLM Provider]
    
    CallLLM --> LLMSuccess{âœ… LLM Call Success?}
    
    %% Success Path
    LLMSuccess -->|âœ… Success| ParseResponse[ğŸ“„ Parse LLM Response]
    ParseResponse --> GenerateCitations[ğŸ“š Generate Citations]
    GenerateCitations --> ValidateQuality[ğŸ” Validate Response Quality]
    
    ValidateQuality --> QualityOK{âœ… Quality OK?}
    QualityOK -->|âœ… Pass| CacheResponse[ğŸ’¾ Cache Response]
    QualityOK -->|âŒ Fail| RetryDifferentProvider[ğŸ”„ Retry with Different Provider]
    
    CacheResponse --> LogMetrics[ğŸ“Š Log Performance Metrics]
    LogMetrics --> Success([âœ… Return Success Response])
    
    %% Error Handling Paths
    LLMSuccess -->|âŒ Fail| CheckErrorType{ğŸ” Check Error Type}
    
    CheckErrorType -->|ğŸš« Rate Limit| WaitAndRetry[â³ Wait and Retry<br/>Exponential Backoff]
    CheckErrorType -->|ğŸ’° Quota Exceeded| SwitchProvider[ğŸ”„ Switch to Next Provider]
    CheckErrorType -->|ğŸŒ Network Error| RetryCurrentProvider[ğŸ”„ Retry Current Provider]
    CheckErrorType -->|ğŸ”‘ Auth Error| ProviderError[âŒ Provider Auth Error]
    CheckErrorType -->|â“ Other Error| GenericError[âŒ Generic Error]
    
    %% Retry Logic
    WaitAndRetry --> RetryAttempt{ğŸ”¢ Retry Attempt < Max?}
    RetryAttempt -->|âœ… Yes| CallLLM
    RetryAttempt -->|âŒ No| SwitchProvider
    
    RetryCurrentProvider --> Retry
