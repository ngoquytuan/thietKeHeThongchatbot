Náº¿u Ä‘Ã£ Ä‘á»§ thÃ´ng tin hÃ£y giÃºp tÃ´i viáº¿t tÃ i liá»‡u thiáº¿t káº¿ module dÆ°á»›i Ä‘Ã¢y.
TÃ i liá»‡u nÃ y cung cáº¥p roadmap Ä‘áº§y Ä‘á»§ Ä‘á»ƒ implement FR-04.3 mÃ  khÃ´ng cáº§n code cá»¥ thá»ƒ nhÆ°ng 
cÃ³ cÃ¡c bÆ°á»›c lÃ m cá»¥ thá»ƒ nhÆ° chuáº©n bá»‹ mÃ´i trÆ°á»ng PC tháº¿ nÃ o, pháº§n má»m gÃ¬?
 Ä‘á»ƒ Team ká»¹ thuáº­t cÃ³ thá»ƒ follow step-by-step guide nÃ y Ä‘á»ƒ thiáº¿t káº¿ tá»‘i Æ°u cho há»‡ thá»‘ng chatbot
 vÃ  cÃ³ mermaidchart luá»“ng cÃ´ng viá»‡c, Ä‘iá»ƒm quay Ä‘áº§u khi bá»‹ fail.

### 3.4 Module RAG Core Engine (FR-04)

**FR-04.3 - Generation (Táº¡o sinh)**
- TÃ­ch há»£p vá»›i LLM (OpenAI GPT, Anthropic Claude, hoáº·c local model)
- Sinh cÃ¢u tráº£ lá»i dá»±a trÃªn context vÃ  cÃ¢u há»i
- Cung cáº¥p citation/reference cho cÃ¢u tráº£ lá»i

---
# TÃ€I LIá»†U THIáº¾T Káº¾ MODULE - RAG GENERATION ENGINE (FR-04.3)

---

**Module:** RAG Core Engine - Generation Service  
**Requirement:** FR-04.3 - Generation (Táº¡o sinh)  
**PhiÃªn báº£n:** 1.0  
**NgÃ y:** 01/09/2025  
**NgÆ°á»i soáº¡n tháº£o:** Technical Design Team  

---

## 1. Tá»”NG QUAN MODULE

### 1.1 Má»¥c Ä‘Ã­ch
Module Generation Engine lÃ  thÃ nh pháº§n quan trá»ng nháº¥t trong RAG pipeline, chá»‹u trÃ¡ch nhiá»‡m:
- TÃ­ch há»£p vá»›i cÃ¡c LLM providers (OpenAI, Anthropic Claude, Gemini, Grok, OpenRouter)
- Há»— trá»£ local LLM models (Llama 2, Mistral, CodeLlama)
- Sinh cÃ¢u tráº£ lá»i cháº¥t lÆ°á»£ng cao dá»±a trÃªn context Ä‘Ã£ Ä‘Æ°á»£c retrieve
- Cung cáº¥p citation vÃ  reference chÃ­nh xÃ¡c
- Xá»­ lÃ½ fallback vÃ  error recovery

### 1.2 Input/Output Specification

**Input:**
```json
{
  "query": "Quy trÃ¬nh mua hÃ ng trÃ¬nh giÃ¡m Ä‘á»‘c nhÆ° tháº¿ nÃ o?",
  "context_chunks": [
    {
      "chunk_id": "doc123_chunk_5",
      "content": "BÆ°á»›c 1: Táº¡o yÃªu cáº§u mua hÃ ng...",
      "metadata": {
        "document_id": "proc_001",
        "title": "Quy trÃ¬nh Mua hÃ ng",
        "page": 2,
        "confidence_score": 0.95
      }
    }
  ],
  "user_context": {
    "user_id": "emp001",
    "department": "rd",
    "access_level": "employee"
  },
  "conversation_history": []
}
```

**Output:**
```json
{
  "answer": "Quy trÃ¬nh mua hÃ ng trÃ¬nh giÃ¡m Ä‘á»‘c gá»“m 5 bÆ°á»›c chÃ­nh...",
  "citations": [
    {
      "chunk_id": "doc123_chunk_5", 
      "reference": "[1] Quy trÃ¬nh Mua hÃ ng, trang 2",
      "used_content": "BÆ°á»›c 1: Táº¡o yÃªu cáº§u mua hÃ ng..."
    }
  ],
  "confidence_score": 0.92,
  "response_metadata": {
    "model_used": "openai-gpt-4",
    "tokens_used": 1250,
    "processing_time": 2.3,
    "cost_estimate": 0.025
  }
}
```

---

## 2. THIáº¾T Káº¾ KIáº¾N TRÃšC MODULE

### 2.1 Architecture Overview

```mermaid
graph TB
    %% Input
    Input[ğŸ“¥ Input Request<br/>Query + Context + User Info]
    
    %% Generation Engine Core
    subgraph "ğŸ¯ GENERATION ENGINE CORE"
        subgraph "ğŸ“ Prompt Management"
            TemplateManager[ğŸ“‹ Template Manager<br/>System & User Prompts]
            PromptBuilder[ğŸ”§ Prompt Builder<br/>Dynamic Assembly]
            ContextOptimizer[âš¡ Context Optimizer<br/>Token Management]
        end
        
        subgraph "ğŸ§  LLM Integration Layer"
            ProviderRouter[ğŸ¯ Provider Router<br/>Load Balancing]
            
            subgraph "â˜ï¸ Cloud Providers"
                OpenAIClient[ğŸ¤– OpenAI Client]
                ClaudeClient[ğŸ§  Claude Client] 
                GeminiClient[ğŸ’ Gemini Client]
                GrokClient[âš¡ Grok Client]
                OpenRouterClient[ğŸŒ OpenRouter Client]
            end
            
            subgraph "ğŸ  Local Models"
                LocalLLMManager[ğŸ–¥ï¸ Local LLM Manager]
                ModelLoader[ğŸ“¦ Model Loader]
                InferenceEngine[âš™ï¸ Inference Engine]
            end
        end
        
        subgraph "ğŸ“Š Response Processing"
            ResponseParser[ğŸ“„ Response Parser<br/>Extract Answer]
            CitationGenerator[ğŸ“š Citation Generator<br/>Reference Linking]
            QualityValidator[âœ… Quality Validator<br/>Answer Validation]
        end
        
        subgraph "ğŸ”„ Error Handling"
            FallbackManager[ğŸ›¡ï¸ Fallback Manager<br/>Provider Switching]
            RetryLogic[ğŸ”„ Retry Logic<br/>Exponential Backoff]
            ErrorRecovery[ğŸš¨ Error Recovery<br/>Graceful Degradation]
        end
    end
    
    %% External Dependencies
    subgraph "ğŸŒ External Services"
        CloudAPIs[â˜ï¸ Cloud LLM APIs]
        LocalModels[ğŸ  Local Model Files]
        ConfigService[âš™ï¸ Configuration Service]
        MonitoringService[ğŸ“Š Monitoring Service]
    end
    
    %% Output
    Output[ğŸ“¤ Generated Response<br/>Answer + Citations + Metadata]
    
    %% Flow
    Input --> TemplateManager
    TemplateManager --> PromptBuilder
    PromptBuilder --> ContextOptimizer
    ContextOptimizer --> ProviderRouter
    
    ProviderRouter --> OpenAIClient
    ProviderRouter --> ClaudeClient
    ProviderRouter --> GeminiClient
    ProviderRouter --> GrokClient
    ProviderRouter --> OpenRouterClient
    ProviderRouter --> LocalLLMManager
    
    LocalLLMManager --> ModelLoader
    ModelLoader --> InferenceEngine
    
    OpenAIClient --> ResponseParser
    ClaudeClient --> ResponseParser
    GeminiClient --> ResponseParser
    GrokClient --> ResponseParser
    OpenRouterClient --> ResponseParser
    InferenceEngine --> ResponseParser
    
    ResponseParser --> CitationGenerator
    CitationGenerator --> QualityValidator
    QualityValidator --> Output
    
    %% Error Handling
    ProviderRouter -.-> FallbackManager
    FallbackManager --> RetryLogic
    RetryLogic --> ErrorRecovery
    ErrorRecovery -.-> ProviderRouter
    
    %% External Connections
    OpenAIClient -.-> CloudAPIs
    ClaudeClient -.-> CloudAPIs
    GeminiClient -.-> CloudAPIs
    GrokClient -.-> CloudAPIs
    OpenRouterClient -.-> CloudAPIs
    ModelLoader -.-> LocalModels
    TemplateManager -.-> ConfigService
    QualityValidator -.-> MonitoringService
    
    %% Styling
    classDef input fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef prompt fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef llm fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef cloud fill:#e1f5fe,stroke:#0277bd,stroke-width:2px
    classDef local fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef response fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    classDef error fill:#ffebee,stroke:#c62828,stroke-width:2px
    classDef external fill:#f1f8e9,stroke:#689f38,stroke-width:1px
    classDef output fill:#e0f2f1,stroke:#00796b,stroke-width:2px
    
    class Input input
    class TemplateManager,PromptBuilder,ContextOptimizer prompt
    class ProviderRouter llm
    class OpenAIClient,ClaudeClient,GeminiClient,GrokClient,OpenRouterClient cloud
    class LocalLLMManager,ModelLoader,InferenceEngine local
    class ResponseParser,CitationGenerator,QualityValidator response
    class FallbackManager,RetryLogic,ErrorRecovery error
    class CloudAPIs,LocalModels,ConfigService,MonitoringService external
    class Output output
```

---

## 3. CHUáº¨N Bá»Š MÃ”I TRÆ¯á»œNG PHÃT TRIá»‚N

### 3.1 Hardware Requirements

**Minimum Requirements:**
- **CPU**: Intel i7/AMD Ryzen 7 (8 cores)
- **RAM**: 32GB (64GB recommended cho local LLM)
- **GPU**: NVIDIA RTX 3080/4080 (12GB+ VRAM) cho local inference
- **Storage**: 1TB NVMe SSD
- **Network**: Stable internet connection (minimum 100Mbps)

**Recommended Production Setup:**
- **CPU**: Intel Xeon/AMD EPYC (16+ cores)
- **RAM**: 128GB+
- **GPU**: NVIDIA A100/H100 cho production local LLM
- **Storage**: 2TB+ NVMe SSD vá»›i RAID configuration

### 3.2 Software Environment Setup

#### 3.2.1 Base Environment
```bash
# 1. Operating System
Ubuntu 22.04 LTS hoáº·c CentOS 8+
Windows 11 Pro (cho development)

# 2. Container Platform
Docker Desktop 4.20+
Docker Compose 2.18+
Kubernetes 1.27+ (cho production)

# 3. Programming Languages
Python 3.11+
Node.js 18+ (cho monitoring dashboard)
Go 1.21+ (optional, cho high-performance components)
```

#### 3.2.2 Python Environment
```bash
# Táº¡o virtual environment
python3.11 -m venv llm_generation_env
source llm_generation_env/bin/activate  # Linux/Mac
# llm_generation_env\Scripts\activate  # Windows

# Core packages
pip install --upgrade pip setuptools wheel

# LLM Integration
pip install openai==1.30.0
pip install anthropic==0.28.0
pip install google-cloud-aiplatform==1.51.0
pip install transformers==4.37.0
pip install torch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0
pip install accelerate==0.27.0
pip install bitsandbytes==0.42.0

# API & Web Framework
pip install fastapi==0.109.0
pip install uvicorn==0.27.0
pip install httpx==0.26.0
pip install aiofiles==23.2.1

# Monitoring & Logging
pip install prometheus-client==0.19.0
pip install structlog==23.2.0
pip install sentry-sdk==1.40.0

# Data Processing
pip install pydantic==2.5.0
pip install tiktoken==0.6.0
pip install jinja2==3.1.3

# Testing
pip install pytest==8.0.0
pip install pytest-asyncio==0.23.0
pip install httpx==0.26.0
```

#### 3.2.3 Local LLM Setup (Optional)
```bash
# Ollama cho easy local LLM deployment
curl -fsSL https://ollama.ai/install.sh | sh

# HuggingFace Hub
pip install huggingface-hub==0.20.0

# vLLM cho high-performance inference
pip install vllm==0.3.0

# Text Generation Inference (TGI)
docker pull ghcr.io/huggingface/text-generation-inference:1.4
```

#### 3.2.4 Configuration Management
```bash
# Environment management
pip install python-dotenv==1.0.0
pip install pyyaml==6.0.1

# Secret management
pip install cryptography==42.0.0
pip install keyring==24.3.0
```

### 3.3 Development Tools

#### 3.3.1 IDE & Code Editor
```bash
# VS Code vá»›i extensions
- Python
- Pylance
- Docker
- Kubernetes
- REST Client
- GitLens

# JetBrains PyCharm Professional (recommended)
# Vim/Neovim vá»›i Python LSP setup
```

#### 3.3.2 Testing & Quality Tools
```bash
# Code formatting & linting
pip install black==24.0.0
pip install isort==5.13.0  
pip install flake8==7.0.0
pip install mypy==1.8.0

# Security scanning
pip install bandit==1.7.5
pip install safety==3.0.0

# Load testing
pip install locust==2.20.0
```

#### 3.3.3 Database & Cache
```bash
# Redis cho caching
docker run -d --name redis-cache -p 6379:6379 redis:7-alpine

# PostgreSQL cho metadata
docker run -d --name postgres-meta \
  -e POSTGRES_PASSWORD=dev_password \
  -p 5432:5432 postgres:15-alpine
```

---

## 4. IMPLEMENTATION ROADMAP

### 4.1 Development Phases

```mermaid
gantt
    title Implementation Roadmap - Generation Engine
    dateFormat  YYYY-MM-DD
    section Phase 1: Setup
    Environment Setup     :done, setup, 2025-09-01, 3d
    Basic Structure      :done, structure, after setup, 2d
    
    section Phase 2: Core Development  
    Template System      :active, template, 2025-09-06, 5d
    Provider Integration :provider, after template, 7d
    Response Processing  :response, after provider, 4d
    
    section Phase 3: Advanced Features
    Citation System      :citation, after response, 5d
    Error Handling       :error, after citation, 4d
    Local LLM Support    :local, after error, 6d
    
    section Phase 4: Testing & Optimization
    Unit Testing         :testing, after local, 4d
    Performance Testing  :perf, after testing, 3d
    Integration Testing  :integration, after perf, 5d
    
    section Phase 5: Production
    Production Deploy    :deploy, after integration, 3d
    Monitoring Setup     :monitor, after deploy, 2d
```

### 4.2 Detailed Implementation Steps

#### 4.2.1 Phase 1: Environment & Structure Setup (5 days)

**Day 1-3: Environment Setup**
```bash
# Step 1: Táº¡o project structure
mkdir rag-generation-engine
cd rag-generation-engine

mkdir -p {
  src/generation_engine/{core,providers,templates,utils},
  tests/{unit,integration,performance},
  configs/{development,staging,production},
  docs/{api,deployment},
  scripts/{setup,deployment,testing},
  docker/{development,production},
  monitoring/{prometheus,grafana}
}

# Step 2: Initialize Python project
touch src/generation_engine/__init__.py
touch requirements/{base.txt,dev.txt,prod.txt}
touch {.env.example,.gitignore,README.md,Dockerfile,docker-compose.yml}

# Step 3: Setup virtual environment
python3.11 -m venv venv
source venv/bin/activate
pip install -r requirements/dev.txt
```

**Day 4-5: Basic Project Structure**
```python
# src/generation_engine/core/models.py
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
from enum import Enum

class LLMProvider(str, Enum):
    OPENAI = "openai"
    CLAUDE = "claude"
    GEMINI = "gemini"
    GROK = "grok"
    OPENROUTER = "openrouter"
    LOCAL = "local"

class GenerationRequest(BaseModel):
    query: str
    context_chunks: List[Dict[str, Any]]
    user_context: Dict[str, Any]
    conversation_history: Optional[List[Dict[str, str]]] = []
    preferred_provider: Optional[LLMProvider] = None

class GenerationResponse(BaseModel):
    answer: str
    citations: List[Dict[str, Any]]
    confidence_score: float
    response_metadata: Dict[str, Any]
```

#### 4.2.2 Phase 2: Core Development (16 days)

**Days 6-10: Template Management System**

**Checkpoint 1**: Template System hoáº¡t Ä‘á»™ng cÆ¡ báº£n
```python
# src/generation_engine/templates/manager.py
class TemplateManager:
    """Quáº£n lÃ½ system vÃ  user prompt templates"""
    
    def __init__(self, config_path: str):
        self.templates = self._load_templates(config_path)
        
    def get_system_prompt(self, context_type: str) -> str:
        """Láº¥y system prompt theo loáº¡i context"""
        pass
        
    def build_user_prompt(self, query: str, context: List[str]) -> str:
        """XÃ¢y dá»±ng user prompt vá»›i query vÃ  context"""
        pass

# configs/prompts/system_prompts.yaml
vietnamese_qa:
  template: |
    Báº¡n lÃ  má»™t trá»£ lÃ½ AI chuyÃªn nghiá»‡p cá»§a cÃ´ng ty ká»¹ thuáº­t hÃ ng khÃ´ng.
    Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  tráº£ lá»i cÃ¢u há»i dá»±a trÃªn tÃ i liá»‡u ná»™i bá»™ Ä‘Æ°á»£c cung cáº¥p.
    
    QUY TÃ‚C QUAN TRá»ŒNG:
    1. Chá»‰ tráº£ lá»i dá»±a trÃªn thÃ´ng tin trong context Ä‘Æ°á»£c cung cáº¥p
    2. Náº¿u khÃ´ng cÃ³ thÃ´ng tin, hÃ£y nÃ³i "TÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin nÃ y trong tÃ i liá»‡u"
    3. LuÃ´n cung cáº¥p nguá»“n tham kháº£o [sá»‘] cho má»—i thÃ´ng tin
    4. Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t chuyÃªn nghiá»‡p
```

**Days 11-17: Provider Integration**

**Checkpoint 2**: TÃ­ch há»£p thÃ nh cÃ´ng vá»›i Ã­t nháº¥t 2 providers (OpenAI + Claude)
```python
# src/generation_engine/providers/base.py
from abc import ABC, abstractmethod

class BaseLLMProvider(ABC):
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        
    @abstractmethod
    async def generate(self, prompt: str, **kwargs) -> Dict[str, Any]:
        """Generate response tá»« LLM"""
        pass
        
    @abstractmethod
    def estimate_cost(self, input_tokens: int, output_tokens: int) -> float:
        """Æ¯á»›c tÃ­nh chi phÃ­ API call"""
        pass

# src/generation_engine/providers/openai_provider.py
import openai
from .base import BaseLLMProvider

class OpenAIProvider(BaseLLMProvider):
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.client = openai.AsyncOpenAI(
            api_key=config["api_key"]
        )
    
    async def generate(self, prompt: str, **kwargs) -> Dict[str, Any]:
        response = await self.client.chat.completions.create(
            model=self.config.get("model", "gpt-4-turbo-preview"),
            messages=[{"role": "user", "content": prompt}],
            temperature=kwargs.get("temperature", 0.1),
            max_tokens=kwargs.get("max_tokens", 1000)
        )
        
        return {
            "text": response.choices[0].message.content,
            "tokens_used": response.usage.total_tokens,
            "model": response.model
        }
```

**Days 18-21: Response Processing**

**Checkpoint 3**: Response parsing vÃ  citation generation hoáº¡t Ä‘á»™ng
```python
# src/generation_engine/core/citation.py
class CitationGenerator:
    def __init__(self):
        self.citation_pattern = r'\[(\d+)\]'
        
    def generate_citations(self, answer: str, context_chunks: List[Dict]) -> List[Dict]:
        """Táº¡o citations tá»« answer vÃ  context chunks"""
        citations = []
        
        # Extract citation numbers from answer
        cited_numbers = re.findall(self.citation_pattern, answer)
        
        for num_str in cited_numbers:
            num = int(num_str) - 1  # Convert to 0-based index
            if num < len(context_chunks):
                chunk = context_chunks[num]
                citations.append({
                    "number": int(num_str),
                    "chunk_id": chunk["chunk_id"],
                    "reference": f"[{num_str}] {chunk['metadata']['title']}, trang {chunk['metadata'].get('page', 'N/A')}",
                    "used_content": chunk["content"][:200] + "..."
                })
        
        return citations
```

#### 4.2.3 Phase 3: Advanced Features (15 days)

**Days 22-26: Citation System Enhancement**

**Checkpoint 4**: Advanced citation vá»›i confidence scoring
```python
# src/generation_engine/core/citation_advanced.py
class AdvancedCitationGenerator:
    def __init__(self, embedding_model):
        self.embedding_model = embedding_model
        
    def calculate_citation_relevance(self, answer_part: str, context_chunk: str) -> float:
        """TÃ­nh toÃ¡n Ä‘á»™ liÃªn quan giá»¯a pháº§n answer vÃ  context chunk"""
        # Sá»­ dá»¥ng embedding similarity
        answer_embedding = self.embedding_model.encode([answer_part])
        context_embedding = self.embedding_model.encode([context_chunk])
        
        similarity = cosine_similarity(answer_embedding, context_embedding)[0][0]
        return float(similarity)
```

**Days 27-30: Error Handling & Fallback**

**Checkpoint 5**: Robust error handling vá»›i multiple fallback strategies
```python
# src/generation_engine/core/fallback.py
class FallbackManager:
    def __init__(self, providers: List[BaseLLMProvider]):
        self.providers = providers
        self.current_provider_index = 0
        
    async def generate_with_fallback(self, request: GenerationRequest) -> GenerationResponse:
        """Thá»­ generate vá»›i fallback giá»¯a cÃ¡c providers"""
        last_error = None
        
        for attempt in range(len(self.providers)):
            provider = self.providers[self.current_provider_index]
            
            try:
                result = await provider.generate(request)
                return result
                
            except Exception as e:
                last_error = e
                logger.warning(f"Provider {provider.__class__.__name__} failed: {e}")
                self.current_provider_index = (self.current_provider_index + 1) % len(self.providers)
                
                # Exponential backoff
                await asyncio.sleep(2 ** attempt)
        
        # Náº¿u táº¥t cáº£ providers Ä‘á»u fail
        raise GenerationError(f"All providers failed. Last error: {last_error}")
```

**Days 31-36: Local LLM Support**

**Checkpoint 6**: Local LLM integration hoáº¡t Ä‘á»™ng
```python
# src/generation_engine/providers/local_provider.py
class LocalLLMProvider(BaseLLMProvider):
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.model_name = config["model_name"]  # "llama2-7b-chat"
        self.inference_engine = self._setup_inference_engine()
        
    def _setup_inference_engine(self):
        if self.config["engine"] == "vllm":
            from vllm import LLM, SamplingParams
            return LLM(model=self.model_name)
        elif self.config["engine"] == "ollama":
            import ollama
            return ollama
        else:
            raise ValueError(f"Unsupported inference engine: {self.config['engine']}")
    
    async def generate(self, prompt: str, **kwargs) -> Dict[str, Any]:
        if self.config["engine"] == "vllm":
            sampling_params = SamplingParams(
                temperature=kwargs.get("temperature", 0.1),
                max_tokens=kwargs.get("max_tokens", 1000)
            )
            outputs = self.inference_engine.generate([prompt], sampling_params)
            
            return {
                "text": outputs[0].outputs[0].text,
                "tokens_used": len(outputs[0].outputs[0].token_ids),
                "model": self.model_name
            }
```

#### 4.2.4 Phase 4: Testing & Optimization (12 days)

**Days 37-40: Unit Testing**

**Checkpoint 7**: Test coverage > 80%
```python
# tests/unit/test_generation_core.py
import pytest
from unittest.mock import Mock, patch
from src.generation_engine.core.engine import GenerationEngine

class TestGenerationEngine:
    @pytest.fixture
    def mock_providers(self):
        return [Mock(), Mock(), Mock()]
    
    @pytest.fixture 
    def generation_engine(self, mock_providers):
        config = {"providers": mock_providers}
        return GenerationEngine(config)
    
    @pytest.mark.asyncio
    async def test_generate_success(self, generation_engine):
        # Setup
        request = GenerationRequest(
            query="Test query",
            context_chunks=[{"content": "test context"}],
            user_context={"user_id": "test_user"}
        )
        
        # Mock provider response
        generation_engine.providers[0].generate.return_value = {
            "text": "Test answer [1]",
            "tokens_used": 100,
            "model": "gpt-4"
        }
        
        # Test
        response = await generation_engine.generate(request)
        
        # Assert
        assert response.answer == "Test answer [1]"
        assert len(response.citations) == 1
        assert response.confidence_score > 0
```

**Days 41-43: Performance Testing**

**Checkpoint 8**: Performance benchmarks established
```python
# tests/performance/test_load.py
import asyncio
import time
from locust import HttpUser, task, between

class GenerationLoadTest(HttpUser):
    wait_time = between(1, 3)
    
    @task
    def test_generation_endpoint(self):
        payload = {
            "query": "Quy trÃ¬nh mua hÃ ng nhÆ° tháº¿ nÃ o?",
            "context_chunks": [
                {
                    "content": "BÆ°á»›c 1: Táº¡o yÃªu cáº§u mua hÃ ng...",
                    "metadata": {"title": "Quy trÃ¬nh mua hÃ ng"}
                }
            ],
            "user_context": {"user_id": "test_user"}
        }
        
        with self.client.post("/api/generate", json=payload, catch_response=True) as response:
            if response.status_code == 200:
                json_response = response.json()
                if "answer" in json_response and "citations" in json_response:
                    response.success()
                else:
                    response.failure("Invalid response format")
            else:
                response.failure(f"HTTP {response.status_code}")

# Run vá»›i: locust -f tests/performance/test_load.py --host=http://localhost:8000
```

**Days 44-48: Integration Testing**

**Checkpoint 9**: End-to-end integration tests pass
```python
# tests/integration/test_full_pipeline.py
@pytest.mark.integration
class TestFullPipeline:
    @pytest.mark.asyncio
    async def test_end_to_end_generation(self):
        """Test complete pipeline tá»« request Ä‘áº¿n response"""
        
        # Setup real providers (vá»›i test API keys)
        config = load_test_config()
        engine = GenerationEngine(config)
        
        # Real request
        request = GenerationRequest(
            query="Quy trÃ¬nh mua hÃ ng trÃ¬nh giÃ¡m Ä‘á»‘c nhÆ° tháº¿ nÃ o?",
            context_chunks=[
                {
                    "chunk_id": "test_chunk_1",
                    "content": "Quy trÃ¬nh mua hÃ ng gá»“m 5 bÆ°á»›c: 1) Táº¡o yÃªu cáº§u...",
                    "metadata": {
                        "document_id": "proc_001",
                        "title": "Quy trÃ¬nh Mua hÃ ng",
                        "page": 1
                    }
                }
            ],
            user_context={
                "user_id": "test_user",
                "department": "rd", 
                "access_level": "employee"
            }
        )
        
        # Execute
        start_time = time.time()
        response = await engine.generate(request)
        end_time = time.time()
        
        # Assertions
        assert response.answer is not None
        assert len(response.answer) > 50  # Reasonable answer length
        assert response.confidence_score >= 0.5
        assert len(response.citations) > 0
        assert (end_time - start_time) < 60  # Under 60 seconds requirement
        assert response.response_metadata["tokens_used"] > 0
```

---

## 5. WORKFLOW VÃ€ ERROR HANDLING

### 5.1 Main Processing Workflow

```mermaid
flowchart TD
    Start([ğŸš€ Start Generation Request]) --> ValidateInput{âœ… Validate Input}
    
    ValidateInput -->|âŒ Invalid| InputError[âŒ Input Validation Error]
    ValidateInput -->|âœ… Valid| SelectProvider[ğŸ¯ Select LLM Provider]
    
    SelectProvider --> CheckCache{ğŸ’¾ Check Response Cache}
    CheckCache -->|âœ… Hit| ReturnCached[ğŸ“‹ Return Cached Response]
    CheckCache -->|âŒ Miss| BuildPrompt[ğŸ“ Build Prompt Template]
    
    BuildPrompt --> OptimizeContext[âš¡ Optimize Context Length]
    OptimizeContext --> CallLLM[ğŸ§  Call LLM Provider]
    
    CallLLM --> LLMSuccess{âœ… LLM Call Success?}
    
    %% Success Path
    LLMSuccess -->|âœ… Success| ParseResponse[ğŸ“„ Parse LLM Response]
    ParseResponse --> GenerateCitations[ğŸ“š Generate Citations]
    GenerateCitations --> ValidateQuality[ğŸ” Validate Response Quality]
    
    ValidateQuality --> QualityOK{âœ… Quality OK?}
    QualityOK -->|âœ… Pass| CacheResponse[ğŸ’¾ Cache Response]
    QualityOK -->|âŒ Fail| RetryDifferentProvider[ğŸ”„ Retry with Different Provider]
    
    CacheResponse --> LogMetrics[ğŸ“Š Log Performance Metrics]
    LogMetrics --> Success([âœ… Return Success Response])
    
    %% Error Handling Paths
    LLMSuccess -->|âŒ Fail| CheckErrorType{ğŸ” Check Error Type}
    
    CheckErrorType -->|ğŸš« Rate Limit| WaitAndRetry[â³ Wait and Retry<br/>Exponential Backoff]
    CheckErrorType -->|ğŸ’° Quota Exceeded| SwitchProvider[ğŸ”„ Switch to Next Provider]
    CheckErrorType -->|ğŸŒ Network Error| RetryCurrentProvider[ğŸ”„ Retry Current Provider]
    CheckErrorType -->|ğŸ”‘ Auth Error| ProviderError[âŒ Provider Auth Error]
    CheckErrorType -->|â“ Other Error| GenericError[âŒ Generic Error]
    
    %% Retry Logic
    WaitAndRetry --> RetryAttempt{ğŸ”¢ Retry Attempt < Max?}
    RetryAttempt -->|âœ… Yes| CallLLM
    RetryAttempt -->|âŒ No| SwitchProvider
    
    RetryCurrentProvider --> RetryCount{ğŸ”¢ Retry Count < 3?}
    RetryCount -->|âœ… Yes| CallLLM
    RetryCount -->|âŒ No| SwitchProvider
    
    SwitchProvider --> HasNextProvider{ğŸ” Has Next Provider?}
    HasNextProvider -->|âœ… Yes| SelectProvider
    HasNextProvider -->|âŒ No| FallbackStrategy[ğŸ›¡ï¸ Execute Fallback Strategy]
    
    RetryDifferentProvider --> HasAlternativeProvider{ğŸ” Has Alternative Provider?}
    HasAlternativeProvider -->|âœ… Yes| SelectProvider
    HasAlternativeProvider -->|âŒ No| FallbackStrategy
    
    %% Fallback Strategies
    FallbackStrategy --> FallbackType{ğŸ›¡ï¸ Fallback Strategy}
    
    FallbackType -->|ğŸ  Local LLM| UseLocalLLM[ğŸ–¥ï¸ Use Local LLM]
    FallbackType -->|ğŸ“‹ Template| UseTemplate[ğŸ“‹ Use Template Response]
    FallbackType -->|ğŸ” Simple Search| SimpleSearch[ğŸ” Return Search Results Only]
    FallbackType -->|âŒ None| FinalFailure[âŒ Complete Failure]
    
    UseLocalLLM --> LocalLLMSuccess{âœ… Local LLM Success?}
    LocalLLMSuccess -->|âœ… Success| ParseResponse
    LocalLLMSuccess -->|âŒ Fail| UseTemplate
    
    UseTemplate --> TemplateResponse[ğŸ“‹ Generate Template Response]
    TemplateResponse --> Success
    
    SimpleSearch --> SearchOnlyResponse[ğŸ” Return Context Chunks Only]
    SearchOnlyResponse --> Success
    
    %% Error Endpoints
    InputError --> LogError1[ğŸ“Š Log Input Error]
    ProviderError --> LogError2[ğŸ“Š Log Provider Error] 
    GenericError --> LogError3[ğŸ“Š Log Generic Error]
    FinalFailure --> LogError4[ğŸ“Š Log Complete Failure]
    
    LogError1 --> ErrorResponse1[âŒ Return Input Error Response]
    LogError2 --> ErrorResponse2[âŒ Return Provider Error Response]
    LogError3 --> ErrorResponse3[âŒ Return Generic Error Response]  
    LogError4 --> ErrorResponse4[âŒ Return Failure Response]
    
    %% Success Endpoints
    ReturnCached --> End([ğŸ End])
    Success --> End
    ErrorResponse1 --> End
    ErrorResponse2 --> End
    ErrorResponse3 --> End
    ErrorResponse4 --> End
    
    %% Styling
    classDef startEnd fill:#e3f2fd,stroke:#1976d2,stroke-width:3px
    classDef process fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef decision fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef error fill:#ffebee,stroke:#c62828,stroke-width:2px
    classDef success fill:#e0f2f1,stroke:#00796b,stroke-width:2px
    classDef fallback fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    
    class Start,End startEnd
    class SelectProvider,BuildPrompt,OptimizeContext,CallLLM,ParseResponse,GenerateCitations,ValidateQuality,CacheResponse,LogMetrics process
    class ValidateInput,CheckCache,LLMSuccess,QualityOK,CheckErrorType,RetryAttempt,RetryCount,HasNextProvider,HasAlternativeProvider,FallbackType,LocalLLMSuccess decision
    class InputError,ProviderError,GenericError,FinalFailure,LogError1,LogError2,LogError3,LogError4,ErrorResponse1,ErrorResponse2,ErrorResponse3,ErrorResponse4 error
    class Success,ReturnCached,TemplateResponse,SearchOnlyResponse success
    class WaitAndRetry,SwitchProvider,RetryCurrentProvider,RetryDifferentProvider,FallbackStrategy,UseLocalLLM,UseTemplate,SimpleSearch fallback
```

### 5.2 Error Recovery Decision Tree

```mermaid
flowchart TD
    Error([ğŸš¨ Error Occurred]) --> ErrorType{ğŸ” Identify Error Type}
    
    %% API Errors
    ErrorType -->|ğŸš« Rate Limiting| RateLimit[â³ Rate Limit Handler]
    ErrorType -->|ğŸ’° Quota/Billing| Quota[ğŸ’° Quota Handler] 
    ErrorType -->|ğŸ”‘ Authentication| Auth[ğŸ”‘ Auth Handler]
    ErrorType -->|ğŸŒ Network/Timeout| Network[ğŸŒ Network Handler]
    ErrorType -->|ğŸ“Š Model Overload| ModelLoad[ğŸ“Š Model Load Handler]
    ErrorType -->|ğŸ“ Content Filter| Content[ğŸ“ Content Handler]
    ErrorType -->|â“ Unknown API| UnknownAPI[â“ Unknown API Handler]
    
    %% Rate Limit Handling
    RateLimit --> RateLimitSeverity{ğŸ“Š Check Severity}
    RateLimitSeverity -->|ğŸŸ¡ Light| WaitShort[â³ Wait 30s + Retry]
    RateLimitSeverity -->|ğŸŸ  Medium| WaitMedium[â³ Wait 2min + Switch Provider]
    RateLimitSeverity -->|ğŸ”´ Heavy| WaitLong[â³ Wait 10min + Switch Provider]
    
    WaitShort --> RetryOriginal[ğŸ”„ Retry Original Provider]
    WaitMedium --> SwitchProvider[ğŸ”„ Switch to Next Provider]
    WaitLong --> SwitchProvider
    
    %% Quota Handling  
    Quota --> QuotaType{ğŸ’° Quota Type}
    QuotaType -->|ğŸ“… Daily Limit| DailyQuota[ğŸ“… Daily Quota Exceeded]
    QuotaType -->|ğŸ’³ Billing| BillingIssue[ğŸ’³ Billing Issue]
    QuotaType -->|ğŸ¯ Model Limit| ModelQuota[ğŸ¯ Model Quota Exceeded]
    
    DailyQuota --> SwitchProvider
    BillingIssue --> DisableProvider[âŒ Disable Provider + Switch]
    ModelQuota --> SwitchModel[ğŸ”„ Switch to Different Model]
    
    %% Authentication Handling
    Auth --> AuthType{ğŸ”‘ Auth Error Type}
    AuthType -->|ğŸ” Invalid Key| InvalidKey[ğŸ” Invalid API Key]
    AuthType -->|â° Expired Token| ExpiredToken[â° Token Expired]
    AuthType -->|ğŸš« Permissions| NoPermission[ğŸš« Insufficient Permissions]
    
    InvalidKey --> DisableProvider
    ExpiredToken --> RefreshToken[ğŸ”„ Refresh Token + Retry]
    NoPermission --> DisableProvider
    
    %% Network Handling
    Network --> NetworkSeverity{ğŸŒ Network Issue Severity}
    NetworkSeverity -->|âš¡ Timeout| TimeoutHandler[âš¡ Timeout Handler]
    NetworkSeverity -->|ğŸŒ DNS| DNSHandler[ğŸŒ DNS Handler]  
    NetworkSeverity -->|ğŸ”Œ Connection| ConnectionHandler[ğŸ”Œ Connection Handler]
    
    TimeoutHandler --> IncreaseTimeout[âš¡ Increase Timeout + Retry]
    DNSHandler --> SwitchEndpoint[ğŸŒ Switch Endpoint + Retry]
    ConnectionHandler --> WaitAndRetry[â³ Wait 5s + Retry]
    
    %% Model Load Handling
    ModelLoad --> LoadSeverity{ğŸ“Š Load Severity}
    LoadSeverity -->|ğŸŸ¡ Light Load| RetryWithDelay[â³ Retry with 10s delay]
    LoadSeverity -->|ğŸŸ  High Load| SwitchModel
    LoadSeverity -->|ğŸ”´ Overloaded| SwitchProvider
    
    %% Content Filter Handling
    Content --> ContentType{ğŸ“ Content Filter Type}
    ContentType -->|ğŸ¤– Safety Filter| SafetyFilter[ğŸ¤– Safety Filter Triggered]
    ContentType -->|ğŸ“ Length Limit| LengthLimit[ğŸ“ Content Too Long]
    ContentType -->|ğŸš« Policy Violation| PolicyViolation[ğŸš« Policy Violation]
    
    SafetyFilter --> ModifyPrompt[ğŸ“ Modify Prompt + Retry]
    LengthLimit --> TruncateContent[âœ‚ï¸ Truncate Content + Retry]
    PolicyViolation --> UseAlternativeApproach[ğŸ”„ Use Alternative Approach]
    
    %% Recovery Actions
    RetryOriginal --> RetrySuccess{âœ… Retry Successful?}
    RetrySuccess -->|âœ… Yes| Success([âœ… Recovery Successful])
    RetrySuccess -->|âŒ No| SwitchProvider
    
    SwitchProvider --> ProviderAvailable{ğŸ” Next Provider Available?}
    ProviderAvailable -->|âœ… Yes| TryNextProvider[ğŸ¯ Try Next Provider]
    ProviderAvailable -->|âŒ No| FallbackStrategy[ğŸ›¡ï¸ Execute Fallback]
    
    TryNextProvider --> ProviderSuccess{âœ… Provider Success?}
    ProviderSuccess -->|âœ… Yes| Success
    ProviderSuccess -->|âŒ No| ProviderAvailable
    
    SwitchModel --> ModelSuccess{âœ… Model Switch Success?}
    ModelSuccess -->|âœ… Yes| Success
    ModelSuccess -->|âŒ No| SwitchProvider
    
    DisableProvider --> RemoveFromPool[âŒ Remove from Provider Pool]
    RemoveFromPool --> ProviderAvailable
    
    RefreshToken --> RefreshSuccess{âœ… Refresh Success?}
    RefreshSuccess -->|âœ… Yes| RetryOriginal
    RefreshSuccess -->|âŒ No| DisableProvider
    
    IncreaseTimeout --> TimeoutRetry[ğŸ”„ Retry with Higher Timeout]
    TimeoutRetry --> TimeoutSuccess{âœ… Timeout Retry Success?}
    TimeoutSuccess -->|âœ… Yes| Success
    TimeoutSuccess -->|âŒ No| SwitchProvider
    
    SwitchEndpoint --> EndpointRetry[ğŸ”„ Retry with New Endpoint]
    EndpointRetry --> EndpointSuccess{âœ… Endpoint Success?}
    EndpointSuccess -->|âœ… Yes| Success
    EndpointSuccess -->|âŒ No| SwitchProvider
    
    WaitAndRetry --> NetworkRetry[ğŸ”„ Network Retry]
    NetworkRetry --> NetworkSuccess{âœ… Network Success?}
    NetworkSuccess -->|âœ… Yes| Success
    NetworkSuccess -->|âŒ No| SwitchProvider
    
    RetryWithDelay --> DelayRetry[ğŸ”„ Delayed Retry]
    DelayRetry --> DelaySuccess{âœ… Delay Success?}
    DelaySuccess -->|âœ… Yes| Success
    DelaySuccess -->|âŒ No| SwitchModel
    
    ModifyPrompt --> PromptRetry[ğŸ”„ Retry with Modified Prompt]
    PromptRetry --> PromptSuccess{âœ… Prompt Success?}
    PromptSuccess -->|âœ… Yes| Success
    PromptSuccess -->|âŒ No| UseAlternativeApproach
    
    TruncateContent --> TruncateRetry[ğŸ”„ Retry with Truncated Content]
    TruncateRetry --> TruncateSuccess{âœ… Truncate Success?}
    TruncateSuccess -->|âœ… Yes| Success
    TruncateSuccess -->|âŒ No| UseAlternativeApproach
    
    UseAlternativeApproach --> FallbackStrategy
    UnknownAPI --> LogUnknownError[ğŸ“Š Log Unknown Error]
    LogUnknownError --> SwitchProvider
    
    %% Final Fallback
    FallbackStrategy --> FallbackType{ğŸ›¡ï¸ Fallback Strategy Type}
    FallbackType -->|ğŸ  Local LLM| LocalLLM[ğŸ–¥ï¸ Switch to Local LLM]
    FallbackType -->|ğŸ“‹ Template| TemplateResponse[ğŸ“‹ Use Template Response]
    FallbackType -->|ğŸ” Search Only| SearchOnly[ğŸ” Return Search Results]
    FallbackType -->|âŒ None Available| CompleteFailure[âŒ Complete System Failure]
    
    LocalLLM --> LocalSuccess{âœ… Local LLM Success?}
    LocalSuccess -->|âœ… Yes| Success
    LocalSuccess -->|âŒ No| TemplateResponse
    
    TemplateResponse --> TemplateSuccess{âœ… Template Success?}
    TemplateSuccess -->|âœ… Yes| PartialSuccess([âš ï¸ Partial Success])
    TemplateSuccess -->|âŒ No| SearchOnly
    
    SearchOnly --> SearchSuccess{âœ… Search Success?}
    SearchSuccess -->|âœ… Yes| MinimalSuccess([âš ï¸ Minimal Success])
    SearchSuccess -->|âŒ No| CompleteFailure
    
    CompleteFailure --> SystemAlert[ğŸš¨ Trigger System Alert]
    SystemAlert --> Failure([âŒ System Failure])
    
    %% Styling
    classDef error fill:#ffebee,stroke:#c62828,stroke-width:2px
    classDef handler fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef decision fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef action fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef success fill:#e0f2f1,stroke:#00796b,stroke-width:2px
    classDef failure fill:#ffebee,stroke:#d32f2f,stroke-width:3px
    classDef fallback fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    
    class Error error
    class RateLimit,Quota,Auth,Network,ModelLoad,Content,UnknownAPI handler
    class ErrorType,RateLimitSeverity,QuotaType,AuthType,NetworkSeverity,LoadSeverity,ContentType,RetrySuccess,ProviderAvailable,ProviderSuccess,ModelSuccess,RefreshSuccess,TimeoutSuccess,EndpointSuccess,NetworkSuccess,DelaySuccess,PromptSuccess,TruncateSuccess,LocalSuccess,TemplateSuccess,SearchSuccess decision
    class WaitShort,WaitMedium,WaitLong,SwitchProvider,DisableProvider,SwitchModel,RefreshToken,IncreaseTimeout,SwitchEndpoint,WaitAndRetry,RetryWithDelay,ModifyPrompt,TruncateContent,UseAlternativeApproach,RemoveFromPool,LogUnknownError,SystemAlert action
    class Success,PartialSuccess,MinimalSuccess success
    class CompleteFailure,Failure failure
    class FallbackStrategy,FallbackType,LocalLLM,TemplateResponse,SearchOnly fallback
```

---

## 6. CONFIGURATION MANAGEMENT

### 6.1 Environment Configuration

```yaml
# configs/development/llm_providers.yaml
providers:
  openai:
    enabled: true
    api_key: "${OPENAI_API_KEY}"
    base_url: "https://api.openai.com/v1"
    models:
      primary: "gpt-4-turbo-preview"
      fallback: "gpt-3.5-turbo"
    rate_limits:
      requests_per_minute: 500
      tokens_per_minute: 150000
    retry_config:
      max_retries: 3
      backoff_factor: 2
    cost_config:
      input_cost_per_1k: 0.01
      output_cost_per_1k: 0.03
      
  claude:
    enabled: true
    api_key: "${CLAUDE_API_KEY}"
    base_url: "https://api.anthropic.com/v1"
    models:
      primary: "claude-3-sonnet-20240229"
      fallback: "claude-3-haiku-20240307"
    rate_limits:
      requests_per_minute: 300
      tokens_per_minute: 100000
    retry_config:
      max_retries: 3
      backoff_factor: 2
    cost_config:
      input_cost_per_1k: 0.003
      output_cost_per_1k: 0.015

  gemini:
    enabled: true
    api_key: "${GEMINI_API_KEY}" 
    base_url: "https://generativelanguage.googleapis.com/v1"
    models:
      primary: "gemini-1.5-pro"
      fallback: "gemini-1.5-flash"
    rate_limits:
      requests_per_minute: 300
      tokens_per_minute: 120000
    cost_config:
      input_cost_per_1k: 0.0035
      output_cost_per_1k: 0.0105

  local_llm:
    enabled: false  # Enable for production
    engine: "vllm"  # or "ollama", "tgi"
    model_path: "/models/llama-2-7b-chat"
    gpu_memory_utilization: 0.8
    max_model_len: 4096
    dtype: "half"
    quantization: "awq"

# configs/development/generation.yaml
generation:
  default_provider: "openai"
  provider_selection_strategy: "round_robin"  # or "least_cost", "fastest"
  
  prompt_settings:
    max_context_tokens: 3000
    max_output_tokens: 1000
    temperature: 0.1
    top_p: 0.9
    
  quality_control:
    min_answer_length: 10
    max_answer_length: 2000
    confidence_threshold: 0.3
    citation_required: true
    
  caching:
    enabled: true
    ttl_seconds: 3600
    max_cache_size: 10000
    
  fallback:
    enabled: true
    strategies: ["switch_provider", "local_llm", "template_response", "search_only"]
    max_fallback_attempts: 3
```

### 6.2 Prompt Templates Configuration

```yaml
# configs/templates/system_prompts.yaml
system_prompts:
  vietnamese_qa:
    name: "Vietnamese Q&A Assistant"
    template: |
      Báº¡n lÃ  trá»£ lÃ½ AI chuyÃªn nghiá»‡p cá»§a cÃ´ng ty ká»¹ thuáº­t hÃ ng khÃ´ng.
      
      NHIá»†M Vá»¤:
      - Tráº£ lá»i cÃ¢u há»i dá»±a trÃªn tÃ i liá»‡u ná»™i bá»™ Ä‘Æ°á»£c cung cáº¥p
      - Cung cáº¥p thÃ´ng tin chÃ­nh xÃ¡c, há»¯u Ã­ch cho nhÃ¢n viÃªn
      
      QUY Táº®C QUAN TRá»ŒNG:
      1. CHá»ˆ sá»­ dá»¥ng thÃ´ng tin tá»« CONTEXT Ä‘Æ°á»£c cung cáº¥p bÃªn dÆ°á»›i
      2. Náº¿u khÃ´ng tÃ¬m tháº¥y thÃ´ng tin: "TÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin nÃ y trong tÃ i liá»‡u hiá»‡n cÃ³"
      3. LuÃ´n trÃ­ch dáº«n nguá»“n báº±ng [sá»‘] cho má»—i thÃ´ng tin
      4. Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t chuyÃªn nghiá»‡p, rÃµ rÃ ng
      5. Náº¿u cÃ³ nhiá»u bÆ°á»›c/quy trÃ¬nh: sá»­ dá»¥ng danh sÃ¡ch cÃ³ Ä‘Ã¡nh sá»‘
      
      Äá»ŠNH Dáº NG TRÃCH DáºªN:
      - Sá»­ dá»¥ng [1], [2], [3]... Ä‘á»ƒ tham chiáº¿u Ä‘áº¿n tá»«ng tÃ i liá»‡u
      - Äáº·t trÃ­ch dáº«n ngay sau thÃ´ng tin Ä‘Æ°á»£c trÃ­ch xuáº¥t
      
      CONTEXT:
      {context}
      
      CÃ‚U Há»I: {question}
      
      TRáº¢ Lá»œI:

  english_qa:
    name: "English Q&A Assistant"  
    template: |
      You are a professional AI assistant for an aviation engineering company.
      
      TASK:
      - Answer questions based on the provided internal documents
      - Provide accurate, helpful information for employees
      
      IMPORTANT RULES:
      1. ONLY use information from the CONTEXT provided below
      2. If information is not found: "I cannot find this information in the available documents"
      3. Always cite sources using [number] for each piece of information
      4. Respond in professional, clear English
      5. For multi-step processes: use numbered lists
      
      CITATION FORMAT:
      - Use [1], [2], [3]... to reference each document
      - Place citations immediately after the referenced information
      
      CONTEXT:
      {context}
      
      QUESTION: {question}
      
      ANSWER:

# configs/templates/user_prompts.yaml  
user_prompts:
  standard_query:
    template: |
      Dá»±a trÃªn context Ä‘Æ°á»£c cung cáº¥p, hÃ£y tráº£ lá»i cÃ¢u há»i sau:
      
      CÃ‚U Há»I: {query}
      
      CONTEXT:
      {context_text}
      
  contextual_query:
    template: |
      THÃ”NG TIN NGÆ¯á»œI DÃ™NG:
      - PhÃ²ng ban: {department}  
      - Cáº¥p Ä‘á»™: {access_level}
      
      CÃ‚U Há»I: {query}
      
      CONTEXT LIÃŠN QUAN:
      {context_text}
      
      HÃ£y tráº£ lá»i phÃ¹ há»£p vá»›i cáº¥p Ä‘á»™ vÃ  phÃ²ng ban cá»§a ngÆ°á»i há»i.

# configs/templates/fallback_responses.yaml
fallback_responses:
  no_context:
    vietnamese: |
      Xin lá»—i, tÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin liÃªn quan Ä‘áº¿n cÃ¢u há»i "{query}" trong tÃ i liá»‡u hiá»‡n cÃ³.
      
      Gá»£i Ã½:
      - Kiá»ƒm tra chÃ­nh táº£ cá»§a cÃ¢u há»i
      - Thá»­ sá»­ dá»¥ng tá»« khÃ³a khÃ¡c
      - LiÃªn há»‡ vá»›i phÃ²ng ban liÃªn quan Ä‘á»ƒ Ä‘Æ°á»£c há»— trá»£
      
    english: |
      I'm sorry, I couldn't find information related to "{query}" in the available documents.
      
      Suggestions:
      - Check the spelling of your question
      - Try using different keywords  
      - Contact the relevant department for assistance

  low_confidence:
    vietnamese: |
      TÃ´i tÃ¬m tháº¥y má»™t sá»‘ thÃ´ng tin cÃ³ thá»ƒ liÃªn quan, nhÆ°ng khÃ´ng cháº¯c cháº¯n vá» Ä‘á»™ chÃ­nh xÃ¡c:
      
      {partial_answer}
      
      Äá»ƒ cÃ³ thÃ´ng tin chÃ­nh xÃ¡c nháº¥t, vui lÃ²ng liÃªn há»‡ trá»±c tiáº¿p vá»›i phÃ²ng ban liÃªn quan.
      
    english: |
      I found some potentially relevant information, but I'm not certain about its accuracy:
      
      {partial_answer}
      
      For the most accurate information, please contact the relevant department directly.

  system_error:
    vietnamese: |
      Xin lá»—i, há»‡ thá»‘ng Ä‘ang gáº·p sá»± cá»‘ táº¡m thá»i. 
      
      Vui lÃ²ng:
      - Thá»­ láº¡i sau vÃ i phÃºt
      - Hoáº·c liÃªn há»‡ vá»›i bá»™ pháº­n IT Ä‘á»ƒ Ä‘Æ°á»£c há»— trá»£
      
    english: |
      Sorry, the system is experiencing temporary issues.
      
      Please:
      - Try again in a few minutes
      - Or contact IT support for assistance
```

---

## 7. MONITORING & METRICS

### 7.1 Performance Metrics

```python
# src/generation_engine/monitoring/metrics.py
from prometheus_client import Counter, Histogram, Gauge
import time
from functools import wraps

# Metrics definitions
GENERATION_REQUESTS_TOTAL = Counter(
    'generation_requests_total',
    'Total number of generation requests',
    ['provider', 'model', 'status']
)

GENERATION_DURATION_SECONDS = Histogram(
    'generation_duration_seconds', 
    'Time spent on generation requests',
    ['provider', 'model']
)

GENERATION_TOKENS_TOTAL = Counter(
    'generation_tokens_total',
    'Total tokens used for generation',
    ['provider', 'model', 'type']  # type: input/output
)

GENERATION_COST_TOTAL = Counter(
    'generation_cost_total',
    'Total cost of generation requests',
    ['provider', 'model']
)

ACTIVE_PROVIDERS = Gauge(
    'active_providers_count',
    'Number of active LLM providers'
)

CACHE_HIT_RATE = Gauge(
    'cache_hit_rate',
    'Cache hit rate for generation requests'
)

def track_generation_metrics(func):
    """Decorator Ä‘á»ƒ track metrics cho generation calls"""
    @wraps(func)
    async def wrapper(*args, **kwargs):
        start_time = time.time()
        provider = kwargs.get('provider', 'unknown')
        model = kwargs.get('model', 'unknown')
        
        try:
            result = await func(*args, **kwargs)
            
            # Track success metrics
            GENERATION_REQUESTS_TOTAL.labels(
                provider=provider, 
                model=model, 
                status='success'
            ).inc()
            
            # Track duration
            duration = time.time() - start_time
            GENERATION_DURATION_SECONDS.labels(
                provider=provider,
                model=model
            ).observe(duration)
            
            # Track tokens if available
            if 'tokens_used' in result.get('response_metadata', {}):
                tokens = result['response_metadata']['tokens_used']
                GENERATION_TOKENS_TOTAL.labels(
                    provider=provider,
                    model=model, 
                    type='total'
                ).inc(tokens)
            
            # Track cost if available  
            if 'cost_estimate' in result.get('response_metadata', {}):
                cost = result['response_metadata']['cost_estimate']
                GENERATION_COST_TOTAL.labels(
                    provider=provider,
                    model=model
                ).inc(cost)
                
            return result
            
        except Exception as e:
            # Track failure metrics
            GENERATION_REQUESTS_TOTAL.labels(
                provider=provider,
                model=model,
                status='error'
            ).inc()
            
            raise e
    
    return wrapper

class MetricsCollector:
    """Collect vÃ  export metrics"""
    
    def __init__(self):
        self.cache_hits = 0
        self.cache_misses = 0
        
    def record_cache_hit(self):
        self.cache_hits += 1
        self._update_cache_hit_rate()
        
    def record_cache_miss(self):
        self.cache_misses += 1  
        self._update_cache_hit_rate()
        
    def _update_cache_hit_rate(self):
        total_requests = self.cache_hits + self.cache_misses
        if total_requests > 0:
            hit_rate = self.cache_hits / total_requests
            CACHE_HIT_RATE.set(hit_rate)

# Global metrics collector instance
metrics_collector = MetricsCollector()
```

### 7.2 Logging Configuration  

```python
# src/generation_engine/monitoring/logging.py
import structlog
import logging
import json
from typing import Any, Dict

# Configure structured logging
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.UnicodeDecoder(),
        structlog.processors.JSONRenderer()
    ],
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    cache_logger_on_first_use=True,
)

class GenerationLogger:
    """Structured logging cho Generation Engine"""
    
    def __init__(self, name: str):
        self.logger = structlog.get_logger(name)
    
    def log_generation_request(self, request_id: str, query: str, user_context: Dict[str, Any]):
        """Log incoming generation request"""
        self.logger.info(
            "generation_request_received",
            request_id=request_id,
            query_length=len(query),
            user_id=user_context.get('user_id'),
            department=user_context.get('department'),
            access_level=user_context.get('access_level')
        )
    
    def log_provider_selection(self, request_id: str, provider: str, model: str, reason: str):
        """Log provider selection decision"""
        self.logger.info(
            "provider_selected",
            request_id=request_id,
            provider=provider,
            model=model,
            selection_reason=reason
        )
    
    def log_llm_call(self, request_id: str, provider: str, model: str, 
                     input_tokens: int, processing_time: float):
        """Log LLM API call"""
        self.logger.info(
            "llm_call_completed", 
            request_id=request_id,
            provider=provider,
            model=model,
            input_tokens=input_tokens,
            processing_time_seconds=processing_time
        )
    
    def log_generation_response(self, request_id: str, response_length: int, 
                              citations_count: int, confidence_score: float):
        """Log generation response details"""
        self.logger.info(
            "generation_response_created",
            request_id=request_id, 
            response_length=response_length,
            citations_count=citations_count,
            confidence_score=confidence_score
        )
    
    def log_error(self, request_id: str, error_type: str, error_message: str, 
                  provider: str = None, recovery_action: str = None):
        """Log errors with context"""
        self.logger.error(
            "generation_error",
            request_id=request_id,
            error_type=error_type,
            error_message=error_message,
            provider=provider,
            recovery_action=recovery_action
        )
    
    def log_fallback_triggered(self, request_id: str, original_provider: str, 
                             fallback_strategy: str, success: bool):
        """Log fallback mechanism activation"""
        self.logger.warning(
            "fallback_triggered",
            request_id=request_id,
            original_provider=original_provider,
            fallback_strategy=fallback_strategy,
            fallback_success=success
        )
    
    def log_cache_operation(self, request_id: str, operation: str, hit: bool, key_hash: str):
        """Log cache operations"""
        self.logger.debug(
            "cache_operation",
            request_id=request_id,
            operation=operation,  # "get", "set", "invalidate"
            cache_hit=hit,
            key_hash=key_hash
        )

# Global logger instance
generation_logger = GenerationLogger("generation_engine")
```

---

## 8. TESTING STRATEGY

### 8.1 Comprehensive Test Plan

```python
# tests/conftest.py - Shared test fixtures
import pytest
import asyncio
from unittest.mock import Mock, AsyncMock
from src.generation_engine.core.engine import GenerationEngine
from src.generation_engine.core.models import GenerationRequest, GenerationResponse

@pytest.fixture
def event_loop():
    """Create event loop for async tests"""
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()

@pytest.fixture
def mock_openai_provider():
    """Mock OpenAI provider"""
    provider = AsyncMock()
    provider.generate.return_value = {
        "text": "Sample answer [1] based on context.",
        "tokens_used": 150,
        "model": "gpt-4-turbo-preview",
        "cost_estimate": 0.002
    }
    provider.estimate_cost.return_value = 0.002
    provider.name = "openai"
    return provider

@pytest.fixture
def mock_claude_provider():
    """Mock Claude provider"""
    provider = AsyncMock()
    provider.generate.return_value = {
        "text": "Alternative answer [1] from Claude.",
        "tokens_used": 120,
        "model": "claude-3-sonnet",
        "cost_estimate": 0.0015
    }
    provider.estimate_cost.return_value = 0.0015
    provider.name = "claude"
    return provider

@pytest.fixture
def sample_context_chunks():
    """Sample context chunks for testing"""
    return [
        {
            "chunk_id": "doc123_chunk_1",
            "content": "Quy trÃ¬nh mua hÃ ng gá»“m 5 bÆ°á»›c chÃ­nh: 1) Táº¡o yÃªu cáº§u mua hÃ ng...",
            "metadata": {
                "document_id": "proc_001",
                "title": "Quy trÃ¬nh Mua hÃ ng",
                "page": 1,
                "confidence_score": 0.95
            }
        },
        {
            "chunk_id": "doc456_chunk_3", 
            "content": "Viá»‡c phÃª duyá»‡t yÃªu cáº§u mua hÃ ng cáº§n cÃ³ chá»¯ kÃ½ giÃ¡m Ä‘á»‘c...",
            "metadata": {
                "document_id": "proc_002", 
                "title": "Quy trÃ¬nh PhÃª duyá»‡t",
                "page": 3,
                "confidence_score": 0.87
            }
        }
    ]

@pytest.fixture
def sample_generation_request(sample_context_chunks):
    """Sample generation request"""
    return GenerationRequest(
        query="Quy trÃ¬nh mua hÃ ng trÃ¬nh giÃ¡m Ä‘á»‘c nhÆ° tháº¿ nÃ o?",
        context_chunks=sample_context_chunks,
        user_context={
            "user_id": "emp001",
            "department": "rd",
            "access_level": "employee"
        },
        conversation_history=[]
    )

@pytest.fixture 
def generation_engine_with_mocks(mock_openai_provider, mock_claude_provider):
    """Generation engine with mocked providers"""
    config = {
        "providers": {
            "openai": mock_openai_provider,
            "claude": mock_claude_provider
        },
        "default_provider": "openai",
        "fallback_enabled": True
    }
    return GenerationEngine(config)
```

### 8.2 Unit Tests

```python
# tests/unit/test_prompt_builder.py
import pytest
from src.generation_engine.templates.manager import TemplateManager
from src.generation_engine.core.prompt_builder import PromptBuilder

class TestPromptBuilder:
    
    @pytest.fixture
    def template_manager(self):
        config = {
            "templates_path": "configs/templates/",
            "default_language": "vietnamese"
        }
        return TemplateManager(config)
    
    @pytest.fixture
    def prompt_builder(self, template_manager):
        return PromptBuilder(template_manager)
    
    def test_build_basic_prompt(self, prompt_builder, sample_context_chunks):
        """Test basic prompt building"""
        query = "Quy trÃ¬nh mua hÃ ng nhÆ° tháº¿ nÃ o?"
        
        prompt = prompt_builder.build_prompt(
            query=query,
            context_chunks=sample_context_chunks,
            language="vietnamese"
        )
        
        # Assert prompt contains required elements
        assert query in prompt
        assert "Quy trÃ¬nh mua hÃ ng gá»“m 5 bÆ°á»›c" in prompt
        assert "[1]" in prompt  # Citation markers
        assert "CONTEXT:" in prompt
        assert "CÃ‚U Há»I:" in prompt
    
    def test_build_prompt_with_user_context(self, prompt_builder, sample_context_chunks):
        """Test prompt building with user context"""
        query = "TÃ´i cáº§n biáº¿t quy trÃ¬nh mua hÃ ng"
        user_context = {
            "department": "rd",
            "access_level": "employee"
        }
        
        prompt = prompt_builder.build_contextual_prompt(
            query=query,
            context_chunks=sample_context_chunks,
            user_context=user_context
        )
        
        assert "PhÃ²ng ban: rd" in prompt
        assert "Cáº¥p Ä‘á»™: employee" in prompt
        assert query in prompt
    
    def test_prompt_token_optimization(self, prompt_builder, sample_context_chunks):
        """Test prompt token count optimization"""
        # Create long context that exceeds token limit
        long_context = sample_context_chunks * 20  # Repeat chunks
        
        prompt = prompt_builder.build_optimized_prompt(
            query="Test query",
            context_chunks=long_context,
            max_tokens=2000
        )
        
        # Should be truncated to fit token limit
        token_count = prompt_builder.count_tokens(prompt)
        assert token_count <= 2000
        
        # Should still contain essential elements
        assert "Test query" in prompt
        assert "CONTEXT:" in prompt

# tests/unit/test_citation_generator.py
import pytest
from src.generation_engine.core.citation import CitationGenerator

class TestCitationGenerator:
    
    @pytest.fixture
    def citation_generator(self):
        return CitationGenerator()
    
    def test_extract_citations_from_answer(self, citation_generator, sample_context_chunks):
        """Test citation extraction from generated answer"""
        answer = "Quy trÃ¬nh mua hÃ ng gá»“m 5 bÆ°á»›c chÃ­nh [1]. Viá»‡c phÃª duyá»‡t cáº§n chá»¯ kÃ½ giÃ¡m Ä‘á»‘c [2]."
        
        citations = citation_generator.generate_citations(answer, sample_context_chunks)
        
        assert len(citations) == 2
        assert citations[0]["number"] == 1
        assert citations[1]["number"] == 2
        assert "Quy trÃ¬nh Mua hÃ ng" in citations[0]["reference"]
        assert "Quy trÃ¬nh PhÃª duyá»‡t" in citations[1]["reference"]
    
    def test_handle_missing_citations(self, citation_generator, sample_context_chunks):
        """Test handling when citations are referenced but chunks missing"""
        answer = "Information from source [5] is not available."
        
        citations = citation_generator.generate_citations(answer, sample_context_chunks)
        
        # Should only return valid citations
        assert len(citations) == 0
    
    def test_citation_confidence_scoring(self, citation_generator, sample_context_chunks):
        """Test citation confidence scoring"""
        answer = "Quy trÃ¬nh mua hÃ ng gá»“m 5 bÆ°á»›c [1]"
        
        citations = citation_generator.generate_citations_with_confidence(
            answer, sample_context_chunks
        )
        
        assert len(citations) == 1
        assert "confidence" in citations[0]
        assert 0.0 <= citations[0]["confidence"] <= 1.0

# tests/unit/test_error_handling.py
import pytest
from unittest.mock import AsyncMock, patch
from src.generation_engine.core.fallback import FallbackManager
from src.generation_engine.core.exceptions import (
    ProviderError, RateLimitError, QuotaExceededError
)

class TestErrorHandling:
    
    @pytest.fixture
    def fallback_manager(self, mock_openai_provider, mock_claude_provider):
        providers = [mock_openai_provider, mock_claude_provider]
        return FallbackManager(providers)
    
    @pytest.mark.asyncio
    async def test_rate_limit_fallback(self, fallback_manager, sample_generation_request):
        """Test fallback when hitting rate limits"""
        # Setup first provider to fail with rate limit
        fallback_manager.providers[0].generate.side_effect = RateLimitError("Rate limit exceeded")
        
        # Second provider succeeds
        fallback_manager.providers[1].generate.return_value = {
            "text": "Fallback response [1]",
            "tokens_used": 100,
            "model": "claude-3-sonnet"
        }
        
        response = await fallback_manager.generate_with_fallback(sample_generation_request)
        
        assert response is not None
        assert "Fallback response" in response.answer
        # Should have tried first provider, then succeeded with second
        assert fallback_manager.providers[0].generate.call_count == 1
        assert fallback_manager.providers[1].generate.call_count == 1
    
    @pytest.mark.asyncio
    async def test_quota_exceeded_fallback(self, fallback_manager, sample_generation_request):
        """Test fallback when quota is exceeded"""
        fallback_manager.providers[0].generate.side_effect = QuotaExceededError("Quota exceeded")
        
        response = await fallback_manager.generate_with_fallback(sample_generation_request)
        
        assert response is not None
        # Should switch to second provider
        assert fallback_manager.providers[1].generate.call_count == 1
    
    @pytest.mark.asyncio
    async def test_all_providers_fail(self, fallback_manager, sample_generation_request):
        """Test when all providers fail"""
        # Both providers fail
        fallback_manager.providers[0].generate.side_effect = ProviderError("Provider 1 failed")
        fallback_manager.providers[1].generate.side_effect = ProviderError("Provider 2 failed") 
        
        # Should raise exception when all providers fail
        with pytest.raises(Exception):
            await fallback_manager.generate_with_fallback(sample_generation_request)

# tests/unit/test_quality_validation.py
import pytest
from src.generation_engine.core.quality import QualityValidator

class TestQualityValidation:
    
    @pytest.fixture
    def quality_validator(self):
        config = {
            "min_answer_length": 10,
            "max_answer_length": 2000,
            "confidence_threshold": 0.3,
            "citation_required": True
        }
        return QualityValidator(config)
    
    def test_validate_good_response(self, quality_validator):
        """Test validation of good quality response"""
        response = {
            "answer": "Quy trÃ¬nh mua hÃ ng gá»“m 5 bÆ°á»›c chÃ­nh Ä‘Æ°á»£c mÃ´ táº£ trong tÃ i liá»‡u [1].",
            "citations": [{"number": 1, "reference": "Document 1"}],
            "confidence_score": 0.85
        }
        
        is_valid, issues = quality_validator.validate_response(response)
        
        assert is_valid
        assert len(issues) == 0
    
    def test_validate_too_short_response(self, quality_validator):
        """Test validation of too short response"""
        response = {
            "answer": "KhÃ´ng biáº¿t.",
            "citations": [],
            "confidence_score": 0.1
        }
        
        is_valid, issues = quality_validator.validate_response(response)
        
        assert not is_valid
        assert "too_short" in str(issues)
        assert "low_confidence" in str(issues)
        assert "missing_citations" in str(issues)
    
    def test_validate_no_citations(self, quality_validator):
        """Test validation when citations are missing"""
        response = {
            "answer": "This is a reasonably long answer that provides some information but has no citations.",
            "citations": [],
            "confidence_score": 0.8
        }
        
        is_valid, issues = quality_validator.validate_response(response)
        
        assert not is_valid
        assert "missing_citations" in str(issues)
```

### 8.3 Integration Tests

```python
# tests/integration/test_full_generation_pipeline.py
import pytest
import asyncio
from src.generation_engine.core.engine import GenerationEngine
from src.generation_engine.providers.openai_provider import OpenAIProvider

@pytest.mark.integration
class TestFullGenerationPipeline:
    """Integration tests with real providers (using test API keys)"""
    
    @pytest.fixture
    def real_generation_engine(self):
        """Setup with real providers for integration testing"""
        config = {
            "providers": {
                "openai": OpenAIProvider({
                    "api_key": "test_openai_key",  # Use test key
                    "model": "gpt-3.5-turbo",  # Use cheaper model for testing
                    "max_tokens": 200
                })
            },
            "default_provider": "openai",
            "fallback_enabled": False  # Disable for simple test
        }
        return GenerationEngine(config)
    
    @pytest.mark.asyncio
    async def test_end_to_end_vietnamese_qa(self, real_generation_engine, sample_context_chunks):
        """Test complete Vietnamese Q&A pipeline"""
        request = GenerationRequest(
            query="Quy trÃ¬nh mua hÃ ng trÃ¬nh giÃ¡m Ä‘á»‘c gá»“m nhá»¯ng bÆ°á»›c nÃ o?",
            context_chunks=sample_context_chunks,
            user_context={
                "user_id": "integration_test_user",
                "department": "rd",
                "access_level": "employee"
            }
        )
        
        # Execute full pipeline
        start_time = time.time()
        response = await real_generation_engine.generate(request)
        end_time = time.time()
        
        # Validate response structure
        assert isinstance(response, GenerationResponse)
        assert response.answer is not None
        assert len(response.answer) > 20
        assert len(response.citations) > 0
        assert response.confidence_score > 0
        
        # Validate performance requirements
        assert (end_time - start_time) < 60  # Under 60 seconds
        
        # Validate Vietnamese content
        assert any(vietnamese_word in response.answer.lower() 
                  for vietnamese_word in ["quy", "trÃ¬nh", "bÆ°á»›c", "mua", "hÃ ng"])
        
        # Validate citations format
        for citation in response.citations:
            assert "chunk_id" in citation
            assert "reference" in citation
            assert "[" in citation["reference"] and "]" in citation["reference"]
        
        # Validate metadata
        assert "tokens_used" in response.response_metadata
        assert "processing_time" in response.response_metadata
        assert response.response_metadata["tokens_used"] > 0
    
    @pytest.mark.asyncio
    async def test_concurrent_requests(self, real_generation_engine, sample_context_chunks):
        """Test handling multiple concurrent requests"""
        requests = []
        for i in range(5):
            request = GenerationRequest(
                query=f"Test query {i}: Quy trÃ¬nh mua hÃ ng nhÆ° tháº¿ nÃ o?",
                context_chunks=sample_context_chunks,
                user_context={"user_id": f"test_user_{i}"}
            )
            requests.append(request)
        
        # Execute all requests concurrently
        start_time = time.time()
        responses = await asyncio.gather(*[
            real_generation_engine.generate(req) for req in requests
        ])
        end_time = time.time()
        
        # All requests should succeed
        assert len(responses) == 5
        for response in responses:
            assert response.answer is not None
            assert len(response.citations) > 0
        
        # Should be faster than sequential execution
        assert (end_time - start_time) < 180  # 3 minutes for 5 requests

# tests/integration/test_provider_switching.py
@pytest.mark.integration 
class TestProviderSwitching:
    """Test provider switching and fallback mechanisms"""
    
    @pytest.fixture
    def multi_provider_engine(self):
        """Engine with multiple real providers"""
        config = {
            "providers": {
                "openai": OpenAIProvider({"api_key": "test_openai_key"}),
                "claude": ClaudeProvider({"api_key": "test_claude_key"})
            },
            "provider_selection_strategy": "round_robin",
            "fallback_enabled": True
        }
        return GenerationEngine(config)
    
    @pytest.mark.asyncio
    async def test_round_robin_provider_selection(self, multi_provider_engine, sample_context_chunks):
        """Test round-robin provider selection"""
        responses = []
        
        for i in range(4):
            request = GenerationRequest(
                query=f"Test query {i}",
                context_chunks=sample_context_chunks,
                user_context={"user_id": f"user_{i}"}
            )
            response = await multi_provider_engine.generate(request)
            responses.append(response)
        
        # Should use different providers (round-robin)
        providers_used = [resp.response_metadata.get("provider") for resp in responses]
        assert len(set(providers_used)) > 1  # Used multiple providers
    
    @pytest.mark.asyncio 
    async def test_fallback_on_provider_failure(self, multi_provider_engine):
        """Test fallback when primary provider fails"""
        # Simulate provider failure by using invalid API key
        multi_provider_engine.providers["openai"].api_key = "invalid_key"
        
        request = GenerationRequest(
            query="Test fallback query",
            context_chunks=[{
                "chunk_id": "test",
                "content": "Test content",
                "metadata": {"title": "Test"}
            }],
            user_context={"user_id": "fallback_test"}
        )
        
        # Should still succeed using fallback provider
        response = await multi_provider_engine.generate(request)
        
        assert response.answer is not None
        # Should have used fallback provider (Claude)
        assert response.response_metadata.get("provider") == "claude"

# tests/integration/test_local_llm.py
@pytest.mark.integration
@pytest.mark.slow
class TestLocalLLM:
    """Test local LLM integration (requires GPU and model files)"""
    
    @pytest.fixture
    def local_llm_engine(self):
        """Engine with local LLM provider"""
        config = {
            "providers": {
                "local": LocalLLMProvider({
                    "engine": "ollama",
                    "model_name": "llama2:7b-chat",
                    "temperature": 0.1
                })
            },
            "default_provider": "local"
        }
        return GenerationEngine(config)
    
    @pytest.mark.asyncio
    async def test_local_llm_generation(self, local_llm_engine, sample_context_chunks):
        """Test generation with local LLM"""
        request = GenerationRequest(
            query="Explain the purchasing process",
            context_chunks=sample_context_chunks,
            user_context={"user_id": "local_test"}
        )
        
        response = await local_llm_engine.generate(request)
        
        assert response.answer is not None
        assert len(response.answer) > 10
        assert response.response_metadata.get("model") == "llama2:7b-chat"
        assert response.response_metadata.get("cost_estimate") == 0  # No cost for local
```

### 8.4 Performance Tests

```python
# tests/performance/test_load_performance.py
import pytest
import asyncio
import time
import statistics
from concurrent.futures import ThreadPoolExecutor
from src.generation_engine.core.engine import GenerationEngine

@pytest.mark.performance
class TestLoadPerformance:
    """Performance and load testing"""
    
    @pytest.fixture
    def performance_engine(self):
        """Optimized engine for performance testing"""
        config = {
            "providers": {
                "openai": OpenAIProvider({
                    "api_key": "test_key",
                    "model": "gpt-3.5-turbo",  # Faster model
                    "max_tokens": 300
                })
            },
            "caching": {"enabled": True, "ttl_seconds": 300},
            "quality_control": {"enabled": False}  # Skip for performance test
        }
        return GenerationEngine(config)
    
    @pytest.mark.asyncio
    async def test_response_time_sla(self, performance_engine, sample_context_chunks):
        """Test response time meets SLA (<60 seconds)"""
        response_times = []
        
        for i in range(10):
            request = GenerationRequest(
                query=f"Performance test query {i}",
                context_chunks=sample_context_chunks,
                user_context={"user_id": f"perf_user_{i}"}
            )
            
            start_time = time.time()
            response = await performance_engine.generate(request)
            end_time = time.time()
            
            response_time = end_time - start_time
            response_times.append(response_time)
            
            assert response is not None
            assert response_time < 60  # SLA requirement
        
        # Calculate statistics
        avg_time = statistics.mean(response_times)
        p95_time = statistics.quantiles(response_times, n=20)[18]  # 95th percentile
        
        print(f"Average response time: {avg_time:.2f}s")
        print(f"95th percentile: {p95_time:.2f}s")
        
        assert avg_time < 30  # Average should be much better than SLA
        assert p95_time < 45   # 95% of requests under 45s
    
    @pytest.mark.asyncio
    async def test_concurrent_load(self, performance_engine, sample_context_chunks):
        """Test handling concurrent load (100 users requirement)"""
        concurrent_requests = 50  # Start with 50 concurrent requests
        
        async def single_request(user_id):
            request = GenerationRequest(
                query=f"Load test query from user {user_id}",
                context_chunks=sample_context_chunks,
                user_context={"user_id": f"load_user_{user_id}"}
            )
            
            start_time = time.time()
            try:
                response = await performance_engine.generate(request)
                end_time = time.time()
                return {
                    "success": True,
                    "response_time": end_time - start_time,
                    "user_id": user_id
                }
            except Exception as e:
                end_time = time.time()
                return {
                    "success": False,
                    "error": str(e),
                    "response_time": end_time - start_time,
                    "user_id": user_id
                }
        
        # Execute concurrent requests
        start_time = time.time()
        results = await asyncio.gather(*[
            single_request(i) for i in range(concurrent_requests)
        ], return_exceptions=True)
        total_time = time.time() - start_time
        
        # Analyze results
        successful_requests = [r for r in results if isinstance(r, dict) and r.get("success")]
        failed_requests = [r for r in results if isinstance(r, dict) and not r.get("success")]
        exceptions = [r for r in results if not isinstance(r, dict)]
        
        success_rate = len(successful_requests) / concurrent_requests
        avg_response_time = statistics.mean([r["response_time"] for r in successful_requests])
        
        print(f"Concurrent requests: {concurrent_requests}")
        print(f"Success rate: {success_rate:.2%}")
        print(f"Average response time: {avg_response_time:.2f}s")
        print(f"Total test time: {total_time:.2f}s")
        print(f"Failed requests: {len(failed_requests)}")
        print(f"Exceptions: {len(exceptions)}")
        
        # Assertions
        assert success_rate >= 0.95  # 95% success rate
        assert avg_response_time < 60  # Average under SLA
        assert len(exceptions) == 0   # No unhandled exceptions
    
    def test_memory_usage_under_load(self, performance_engine, sample_context_chunks):
        """Test memory usage doesn't grow excessively under load"""
        import psutil
        import os
        
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # Run many requests to test memory leaks
        async def memory_test():
            for i in range(100):
                request = GenerationRequest(
                    query=f"Memory test query {i}",
                    context_chunks=sample_context_chunks,
                    user_context={"user_id": f"mem_user_{i}"}
                )
                await performance_engine.generate(request)
                
                if i % 20 == 0:  # Check memory every 20 requests
                    current_memory = process.memory_info().rss / 1024 / 1024
                    memory_growth = current_memory - initial_memory
                    print(f"After {i} requests: {current_memory:.1f}MB (+{memory_growth:.1f}MB)")
        
        asyncio.run(memory_test())
        
        final_memory = process.memory_info().rss / 1024 / 1024
        memory_growth = final_memory - initial_memory
        
        print(f"Initial memory: {initial_memory:.1f}MB")
        print(f"Final memory: {final_memory:.1f}MB") 
        print(f"Memory growth: {memory_growth:.1f}MB")
        
        # Memory growth should be reasonable (less than 500MB for 100 requests)
        assert memory_growth < 500
    
    @pytest.mark.asyncio
    async def test_cache_performance_impact(self, performance_engine, sample_context_chunks):
        """Test cache impact on performance"""
        # Same request to test cache hit
        request = GenerationRequest(
            query="Cache test query - what is the purchasing process?",
            context_chunks=sample_context_chunks,
            user_context={"user_id": "cache_test_user"}
        )
        
        # First request (cache miss)
        start_time = time.time()
        response1 = await performance_engine.generate(request)
        first_request_time = time.time() - start_time
        
        # Second request (should be cache hit)
        start_time = time.time()
        response2 = await performance_engine.generate(request)
        second_request_time = time.time() - start_time
        
        print(f"First request (cache miss): {first_request_time:.3f}s")
        print(f"Second request (cache hit): {second_request_time:.3f}s")
        print(f"Cache speedup: {first_request_time/second_request_time:.2f}x")
        
        # Responses should be identical
        assert response1.answer == response2.answer
        assert response1.citations == response2.citations
        
        # Cache hit should be significantly faster
        assert second_request_time < first_request_time * 0.1  # At least 10x faster
```

---

## 9. DEPLOYMENT GUIDE

### 9.1 Docker Configuration

```dockerfile
# docker/production/Dockerfile
FROM python:3.11-slim

# Set environment variables
ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    PYTHONPATH=/app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Create app directory
WORKDIR /app

# Copy requirements first for better caching
COPY requirements/ requirements/
RUN pip install --no-cache-dir -r requirements/prod.txt

# Copy application code
COPY src/ src/
COPY configs/ configs/
COPY scripts/ scripts/

# Create non-root user
RUN groupadd -r appuser && useradd -r -g appuser appuser
RUN chown -R appuser:appuser /app
USER appuser

# Health check
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:8000/health || exit 1

# Expose port
EXPOSE 8000

# Start command
CMD ["uvicorn", "src.generation_engine.api.main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4"]
```

```yaml
# docker-compose.production.yml
version: '3.8'

services:
  generation-engine:
    build:
      context: .
      dockerfile: docker/production/Dockerfile
    ports:
      - "8000:8000"
    environment:
      - ENVIRONMENT=production
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - CLAUDE_API_KEY=${CLAUDE_API_KEY}
      - REDIS_URL=redis://redis:6379
      - POSTGRES_URL=postgresql://user:pass@postgres:5432/generation_db
    depends_on:
      - redis
      - postgres
    volumes:
      - ./configs:/app/configs:ro
      - ./logs:/app/logs
    restart: unless-stopped
    deploy:
      replicas: 3
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 1G
          cpus: '0.5'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes
    restart: unless-stopped

  postgres:
    image: postgres:15-alpine
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_DB=generation_db
      - POSTGRES_USER=generation_user
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init_db.sql:/docker-entrypoint-initdb.d/init.sql
    restart: unless-stopped

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./configs/nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./configs/nginx/ssl:/etc/nginx/ssl
    depends_on:
      - generation-engine
    restart: unless-stopped

volumes:
  redis_data:
  postgres_data:
```

### 9.2 Kubernetes Deployment

```yaml
# k8s/namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: generation-engine
  labels:
    name: generation-engine
    environment: production
```
---
# k8s/configmap.yaml
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: generation-engine-config
  namespace: generation-engine
data:
  generation.yaml: |
    generation:
      default_provider: "openai"
      provider_selection_strategy: "round_robin"
      prompt_settings:
        max_context_tokens: 3000
        max_output_tokens: 1000
        temperature: 0.1
      quality_control:
        min_answer_length: 10
        confidence_threshold: 0.3
        citation_required: true
      caching:
        enabled: true
        ttl_seconds: 3600
      fallback:
        enabled: true
        strategies: ["switch_provider", "local_llm", "template_response"]
        max_fallback_attempts: 3
  
  logging.yaml: |
    version: 1
    disable_existing_loggers: false
    formatters:
      json:
        format: '{"timestamp": "%(asctime)s", "level": "%(levelname)s", "logger": "%(name)s", "message": "%(message)s"}'
    handlers:
      console:
        class: logging.StreamHandler
        formatter: json
        stream: ext://sys.stdout
    loggers:
      generation_engine:
        level: INFO
        handlers: [console]
        propagate: false
    root:
      level: WARNING
      handlers: [console]
```
---
```yaml
# k8s/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: generation-engine-secrets
  namespace: generation-engine
type: Opaque
data:
  openai-api-key: <base64-encoded-openai-key>
  claude-api-key: <base64-encoded-claude-key>
  gemini-api-key: <base64-encoded-gemini-key>
  postgres-password: <base64-encoded-postgres-password>
  redis-password: <base64-encoded-redis-password>

---
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: generation-engine
  namespace: generation-engine
  labels:
    app: generation-engine
    version: v1.0
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  selector:
    matchLabels:
      app: generation-engine
  template:
    metadata:
      labels:
        app: generation-engine
        version: v1.0
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8000"
        prometheus.io/path: "/metrics"
    spec:
      serviceAccountName: generation-engine-sa
      containers:
      - name: generation-engine
        image: generation-engine:1.0.0
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 8000
          name: http
        env:
        - name: ENVIRONMENT
          value: "production"
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: generation-engine-secrets
              key: openai-api-key
        - name: CLAUDE_API_KEY
          valueFrom:
            secretKeyRef:
              name: generation-engine-secrets
              key: claude-api-key
        - name: GEMINI_API_KEY
          valueFrom:
            secretKeyRef:
              name: generation-engine-secrets
              key: gemini-api-key
        - name: REDIS_URL
          value: "redis://redis-service:6379"
        - name: POSTGRES_URL
          value: "postgresql://generation_user:$(POSTGRES_PASSWORD)@postgres-service:5432/generation_db"
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: generation-engine-secrets
              key: postgres-password
        volumeMounts:
        - name: config-volume
          mountPath: /app/configs
          readOnly: true
        - name: logs-volume
          mountPath: /app/logs
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        startupProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 30
      volumes:
      - name: config-volume
        configMap:
          name: generation-engine-config
      - name: logs-volume
        emptyDir: {}
      nodeSelector:
        node-type: compute
      tolerations:
      - key: "compute-node"
        operator: "Equal"
        value: "true"
        effect: "NoSchedule"

---
# k8s/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: generation-engine-service
  namespace: generation-engine
  labels:
    app: generation-engine
spec:
  type: ClusterIP
  ports:
  - port: 80
    targetPort: 8000
    protocol: TCP
    name: http
  selector:
    app: generation-engine

---
# k8s/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: generation-engine-hpa
  namespace: generation-engine
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: generation-engine
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60

---
# k8s/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: generation-engine-ingress
  namespace: generation-engine
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/rate-limit: "100"
    nginx.ingress.kubernetes.io/rate-limit-window: "1m"
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
spec:
  tls:
  - hosts:
    - generation-api.company.com
    secretName: generation-engine-tls
  rules:
  - host: generation-api.company.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: generation-engine-service
            port:
              number: 80

---
# k8s/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: generation-engine-pdb
  namespace: generation-engine
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: generation-engine

---
# k8s/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: generation-engine-sa
  namespace: generation-engine
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: generation-engine
  name: generation-engine-role
rules:
- apiGroups: [""]
  resources: ["configmaps", "secrets"]
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: generation-engine-rolebinding
  namespace: generation-engine
subjects:
- kind: ServiceAccount
  name: generation-engine-sa
  namespace: generation-engine
roleRef:
  kind: Role
  name: generation-engine-role
  apiGroup: rbac.authorization.k8s.io
```

### 9.3 Deployment Scripts

```bash
#!/bin/bash
# scripts/deploy_production.sh

set -e

echo "ğŸš€ Starting Generation Engine Production Deployment"

# Configuration
NAMESPACE="generation-engine"
IMAGE_TAG=${1:-"latest"}
REGISTRY="your-registry.com"
IMAGE_NAME="$REGISTRY/generation-engine:$IMAGE_TAG"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

log_info() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# Pre-deployment checks
log_info "Running pre-deployment checks..."

# Check kubectl connection
if ! kubectl cluster-info > /dev/null 2>&1; then
    log_error "Cannot connect to Kubernetes cluster"
    exit 1
fi

# Check if namespace exists
if ! kubectl get namespace $NAMESPACE > /dev/null 2>&1; then
    log_info "Creating namespace: $NAMESPACE"
    kubectl create namespace $NAMESPACE
fi

# Validate secrets exist
log_info "Checking secrets..."
if ! kubectl get secret generation-engine-secrets -n $NAMESPACE > /dev/null 2>&1; then
    log_error "Secrets not found. Please create secrets first using:"
    echo "kubectl create secret generic generation-engine-secrets \\"
    echo "  --from-literal=openai-api-key='your-key' \\"
    echo "  --from-literal=claude-api-key='your-key' \\"
    echo "  --from-literal=postgres-password='your-password' \\"
    echo "  -n $NAMESPACE"
    exit 1
fi

# Build and push image
log_info "Building Docker image..."
docker build -t $IMAGE_NAME -f docker/production/Dockerfile .

log_info "Pushing image to registry..."
docker push $IMAGE_NAME

# Deploy to Kubernetes
log_info "Applying Kubernetes manifests..."

# Apply in order
kubectl apply -f k8s/namespace.yaml
kubectl apply -f k8s/configmap.yaml
kubectl apply -f k8s/serviceaccount.yaml

# Wait a moment for ServiceAccount to be ready
sleep 5

# Update deployment image
kubectl set image deployment/generation-engine \
    generation-engine=$IMAGE_NAME \
    -n $NAMESPACE

# Apply remaining manifests
kubectl apply -f k8s/deployment.yaml
kubectl apply -f k8s/service.yaml
kubectl apply -f k8s/hpa.yaml
kubectl apply -f k8s/ingress.yaml
kubectl apply -f k8s/pdb.yaml

# Wait for deployment to be ready
log_info "Waiting for deployment to be ready..."
kubectl wait --for=condition=available --timeout=600s deployment/generation-engine -n $NAMESPACE

# Verify deployment
log_info "Verifying deployment..."
READY_REPLICAS=$(kubectl get deployment generation-engine -n $NAMESPACE -o jsonpath='{.status.readyReplicas}')
DESIRED_REPLICAS=$(kubectl get deployment generation-engine -n $NAMESPACE -o jsonpath='{.spec.replicas}')

if [ "$READY_REPLICAS" = "$DESIRED_REPLICAS" ]; then
    log_info "âœ… Deployment successful! $READY_REPLICAS/$DESIRED_REPLICAS replicas ready"
else
    log_error "âŒ Deployment failed. Only $READY_REPLICAS/$DESIRED_REPLICAS replicas ready"
    
    # Show pod status for debugging
    log_info "Pod status:"
    kubectl get pods -n $NAMESPACE -l app=generation-engine
    
    log_info "Recent events:"
    kubectl get events -n $NAMESPACE --sort-by='.metadata.creationTimestamp' | tail -10
    
    exit 1
fi

# Health check
log_info "Performing health check..."
SERVICE_IP=$(kubectl get service generation-engine-service -n $NAMESPACE -o jsonpath='{.spec.clusterIP}')

if kubectl run health-check --image=curlimages/curl --rm -it --restart=Never -- \
   curl -f http://$SERVICE_IP/health; then
    log_info "âœ… Health check passed"
else
    log_error "âŒ Health check failed"
    exit 1
fi

# Show deployment info
log_info "Deployment Summary:"
echo "=================="
echo "Namespace: $NAMESPACE"
echo "Image: $IMAGE_NAME"
echo "Replicas: $READY_REPLICAS"
echo "Service IP: $SERVICE_IP"

# Get ingress info if available
INGRESS_HOST=$(kubectl get ingress generation-engine-ingress -n $NAMESPACE -o jsonpath='{.spec.rules[0].host}' 2>/dev/null || echo "Not configured")
echo "Ingress Host: $INGRESS_HOST"

log_info "ğŸ‰ Deployment completed successfully!"

# Optional: Run smoke tests
if [ "$2" = "--smoke-test" ]; then
    log_info "Running smoke tests..."
    ./scripts/smoke_test.sh $NAMESPACE
fi
```

```bash
#!/bin/bash
# scripts/smoke_test.sh

NAMESPACE=${1:-"generation-engine"}
SERVICE_NAME="generation-engine-service"

echo "ğŸ§ª Running smoke tests for Generation Engine"

# Test 1: Health endpoint
echo "Test 1: Health check..."
if kubectl run smoke-test-health --image=curlimages/curl --rm -it --restart=Never -- \
   curl -f http://$SERVICE_NAME.$NAMESPACE.svc.cluster.local/health; then
    echo "âœ… Health check passed"
else
    echo "âŒ Health check failed"
    exit 1
fi

# Test 2: Ready endpoint  
echo "Test 2: Ready check..."
if kubectl run smoke-test-ready --image=curlimages/curl --rm -it --restart=Never -- \
   curl -f http://$SERVICE_NAME.$NAMESPACE.svc.cluster.local/ready; then
    echo "âœ… Ready check passed"
else
    echo "âŒ Ready check failed"
    exit 1
fi

# Test 3: Simple generation request
echo "Test 3: Simple generation..."
cat > /tmp/smoke_test_payload.json << EOF
{
  "query": "What is the purchasing process?",
  "context_chunks": [
    {
      "chunk_id": "test_chunk_1",
      "content": "The purchasing process consists of 3 steps: 1) Create request, 2) Get approval, 3) Make purchase.",
      "metadata": {
        "document_id": "test_doc",
        "title": "Test Document",
        "page": 1
      }
    }
  ],
  "user_context": {
    "user_id": "smoke_test_user",
    "department": "test",
    "access_level": "employee"
  }
}
EOF

if kubectl run smoke-test-generation --image=curlimages/curl --rm -it --restart=Never -- \
   curl -X POST \
   -H "Content-Type: application/json" \
   -d @/tmp/smoke_test_payload.json \
   http://$SERVICE_NAME.$NAMESPACE.svc.cluster.local/api/generate; then
    echo "âœ… Generation test passed"
else
    echo "âŒ Generation test failed"
    exit 1
fi

# Test 4: Metrics endpoint
echo "Test 4: Metrics check..."
if kubectl run smoke-test-metrics --image=curlimages/curl --rm -it --restart=Never -- \
   curl -f http://$SERVICE_NAME.$NAMESPACE.svc.cluster.local/metrics; then
    echo "âœ… Metrics check passed"
else
    echo "âŒ Metrics check failed"
    exit 1
fi

echo "ğŸ‰ All smoke tests passed!"
```

### 9.4 Monitoring Setup

```yaml
# monitoring/prometheus-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: generation-engine
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
    
    rule_files:
      - "generation_engine_rules.yml"
    
    scrape_configs:
      - job_name: 'generation-engine'
        static_configs:
          - targets: ['generation-engine-service:80']
        metrics_path: '/metrics'
        scrape_interval: 15s
        scrape_timeout: 10s
      
      - job_name: 'kubernetes-pods'
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__
    
    alerting:
      alertmanagers:
        - static_configs:
            - targets: ['alertmanager:9093']

  generation_engine_rules.yml: |
    groups:
      - name: generation_engine_alerts
        rules:
          - alert: HighErrorRate
            expr: rate(generation_requests_total{status="error"}[5m]) / rate(generation_requests_total[5m]) > 0.1
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: "High error rate in Generation Engine"
              description: "Error rate is {{ $value | humanizePercentage }} for the last 5 minutes"
          
          - alert: HighResponseTime
            expr: histogram_quantile(0.95, rate(generation_duration_seconds_bucket[5m])) > 30
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "High response time in Generation Engine"
              description: "95th percentile response time is {{ $value }}s"
          
          - alert: LowCacheHitRate
            expr: cache_hit_rate < 0.7
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: "Low cache hit rate"
              description: "Cache hit rate is {{ $value | humanizePercentage }}"
          
          - alert: PodCrashLooping
            expr: rate(kube_pod_container_status_restarts_total[15m]) > 0
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: "Pod is crash looping"
              description: "Pod {{ $labels.pod }} is crash looping"
          
          - alert: HighMemoryUsage
            expr: container_memory_usage_bytes{pod=~"generation-engine-.*"} / container_spec_memory_limit_bytes > 0.9
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "High memory usage"
              description: "Memory usage is {{ $value | humanizePercentage }} for pod {{ $labels.pod }}"

---
# monitoring/grafana-dashboard.json (ConfigMap)
apiVersion: v1
kind: ConfigMap
metadata:
  name: generation-engine-dashboard
  namespace: generation-engine
data:
  dashboard.json: |
    {
      "dashboard": {
        "id": null,
        "title": "Generation Engine Dashboard",
        "tags": ["generation", "engine", "llm"],
        "timezone": "browser",
        "panels": [
          {
            "title": "Request Rate",
            "type": "graph",
            "targets": [
              {
                "expr": "rate(generation_requests_total[5m])",
                "legendFormat": "{{status}}"
              }
            ],
            "yAxes": [
              {
                "label": "Requests/sec"
              }
            ]
          },
          {
            "title": "Response Time Distribution",
            "type": "graph",
            "targets": [
              {
                "expr": "histogram_quantile(0.50, rate(generation_duration_seconds_bucket[5m]))",
                "legendFormat": "50th percentile"
              },
              {
                "expr": "histogram_quantile(0.95, rate(generation_duration_seconds_bucket[5m]))",
                "legendFormat": "95th percentile"
              },
              {
                "expr": "histogram_quantile(0.99, rate(generation_duration_seconds_bucket[5m]))",
                "legendFormat": "99th percentile"
              }
            ]
          },
          {
            "title": "Provider Usage",
            "type": "piechart",
            "targets": [
              {
                "expr": "increase(generation_requests_total[1h])",
                "legendFormat": "{{provider}}"
              }
            ]
          },
          {
            "title": "Cache Hit Rate",
            "type": "singlestat",
            "targets": [
              {
                "expr": "cache_hit_rate"
              }
            ],
            "valueMaps": [
              {
                "value": "null",
                "text": "N/A"
              }
            ]
          },
          {
            "title": "Total Cost",
            "type": "singlestat",
            "targets": [
              {
                "expr": "sum(increase(generation_cost_total[1h]))"
              }
            ],
            "format": "currency"
          }
        ],
        "time": {
          "from": "now-1h",
          "to": "now"
        },
        "refresh": "30s"
      }
    }
```

---

## 10. MAINTENANCE & TROUBLESHOOTING

### 10.1 Common Issues & Solutions

```bash
#!/bin/bash
# scripts/troubleshoot.sh

echo "ğŸ” Generation Engine Troubleshooting Guide"

# Function definitions
check_pod_status() {
    echo "=== Pod Status ==="
    kubectl get pods -n generation-engine -l app=generation-engine
    echo ""
}

check_logs() {
    echo "=== Recent Logs ==="
    kubectl logs -n generation-engine -l app=generation-engine --tail=50
    echo ""
}

check_resources() {
    echo "=== Resource Usage ==="
    kubectl top pods -n generation-engine
    echo ""
}

check_endpoints() {
    echo "=== Service Endpoints ==="
    kubectl get endpoints -n generation-engine
    echo ""
}

check_hpa() {
    echo "=== HPA Status ==="
    kubectl get hpa -n generation-engine
    echo ""
}

check_events() {
    echo "=== Recent Events ==="
    kubectl get events -n generation-engine --sort-by='.metadata.creationTimestamp' | tail -20
    echo ""
}

# Main troubleshooting
case "${1:-all}" in
    "pods")
        check_pod_status
        ;;
    "logs")
        check_logs
        ;;
    "resources")
        check_resources
        ;;
    "endpoints")
        check_endpoints
        ;;
    "hpa")
        check_hpa
        ;;
    "events")
        check_events
        ;;
    "all")
        check_pod_status
        check_resources
        check_endpoints
        check_hpa
        check_events
        check_logs
        ;;
    *)
        echo "Usage: $0 [pods|logs|resources|endpoints|hpa|events|all]"
        exit 1
        ;;
esac
```

### 10.2 Performance Tuning Checklist

```markdown
# Performance Tuning Checklist

## ğŸš€ Application Level
- [ ] Enable response caching with appropriate TTL
- [ ] Optimize prompt templates for token efficiency
- [ ] Configure proper connection pooling for LLM providers
- [ ] Implement request deduplication
- [ ] Set appropriate timeout values
- [ ] Use async/await properly throughout the codebase

## ğŸ—ï¸ Infrastructure Level
- [ ] Right-size container resources (CPU/Memory)
- [ ] Configure HPA with appropriate metrics
- [ ] Use multiple replicas for high availability
- [ ] Configure node affinity for optimal placement
- [ ] Enable monitoring and alerting

## ğŸ—„ï¸ Database Level
- [ ] Optimize database queries
- [ ] Configure connection pooling
- [ ] Set up read replicas for read-heavy workloads
- [ ] Regular database maintenance (VACUUM, ANALYZE)
- [ ] Monitor slow queries

## ğŸŒ Network Level
- [ ] Configure ingress controller properly
- [ ] Set up CDN for static assets (if any)
- [ ] Optimize DNS resolution
- [ ] Configure proper load balancer settings

## ğŸ” Monitoring Level
- [ ] Set up comprehensive metrics collection
- [ ] Configure alerting for key SLIs
- [ ] Regular performance testing
- [ ] Capacity planning based on trends
```

---

## 11. SECURITY CONSIDERATIONS

### 11.1 Security Implementation Checklist

```yaml
# security/security-policies.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: security-policies
  namespace: generation-engine
data:
  security.yaml: |
    # API Security
    api_security:
      rate_limiting:
        enabled: true
        requests_per_minute: 100
        burst: 20
      
      authentication:
        required: true
        methods: ["jwt", "api_key"]
        jwt_expiry: 3600
      
      input_validation:
        max_query_length: 2000
        max_context_chunks: 10
        allowed_file_types: ["txt", "pdf", "docx"]
        content_filtering: true
      
      output_sanitization:
        remove_sensitive_patterns: true
        patterns:
          - "(?i)(password|secret|key|token)\\s*[=:]\\s*\\S+"
          - "\\b\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}\\b"
    
    # Data Security
    data_security:
      encryption:
        at_rest: true
        in_transit: true
        algorithm: "AES-256"
      
      data_retention:
        logs_retention_days: 30
        cache_retention_hours: 24
        conversation_history_days: 7
      
      data_classification:
        auto_classify: true
        sensitivity_levels: ["public", "internal", "confidential", "secret"]
      
      access_control:
        rbac_enabled: true
        audit_logging: true
        session_timeout_minutes: 60
```

### 11.2 Security Monitoring

```python
# src/generation_engine/security/monitor.py
import re
import hashlib
from typing import List, Dict, Any
from src.generation_engine.monitoring.logging import GenerationLogger

class SecurityMonitor:
    """Monitor security events and potential threats"""
    
    def __init__(self):
        self.logger = GenerationLogger("security_monitor")
        self.sensitive_patterns = [
            r'(?i)(password|secret|key|token)\s*[=:]\s*\S+',
            r'\b\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4}\b',  # Credit card
            r'\b\d{3}-\d{2}-\d{4}\b',  # SSN
            r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}',  # Email
        ]
        
    def check_input_security(self, query: str, user_context: Dict[str, Any]) -> List[str]:
        """Check input for security issues"""
        issues = []
        
        # Check for sensitive data in query
        for pattern in self.sensitive_patterns:
            if re.search(pattern, query):
                issues.append("sensitive_data_in_query")
                self.logger.log_error(
                    request_id=user_context.get("request_id"),
                    error_type="security_violation",
                    error_message="Sensitive data detected in query",
                    provider="security_monitor"
                )
                break
        
        # Check query length (potential DoS)
        if len(query) > 5000:
            issues.append("query_too_long")
            
        # Check for injection patterns
        injection_patterns = [
            r'<script\b[^<]*(?:(?!<\/script>)<[^<]*)*<\/script>',  # XSS
            r'union\s+select|drop\s+table|delete\s+from',  # SQL injection
            r'system\(|exec\(|eval\(',  # Code injection
        ]
        
        for pattern in injection_patterns:
            if re.search(pattern, query, re.IGNORECASE):
                issues.append("injection_attempt")
                self.logger.log_error(
                    request_id=user_context.get("request_id"),
                    error_type="security_violation",
                    error_message="Injection attempt detected",
                    provider="security_monitor"
                )
                break
        
        return issues
    
    def sanitize_output(self, answer: str) -> str:
        """Remove sensitive information from output"""
        sanitized = answer
        
        for pattern in self.sensitive_patterns:
            sanitized = re.sub(pattern, '[REDACTED]', sanitized)
        
        return sanitized
    
    def hash_pii(self, text: str) -> str:
        """Hash PII for logging purposes"""
        return hashlib.sha256(text.encode()).hexdigest()[:8]
```

---

## 12. Káº¾T LUáº¬N

### 12.1 Deliverables Summary

TÃ i liá»‡u nÃ y cung cáº¥p roadmap Ä‘áº§y Ä‘á»§ cho viá»‡c implement FR-04.3 Generation Engine bao gá»“m:

âœ… **Thiáº¿t káº¿ kiáº¿n trÃºc chi tiáº¿t** vá»›i workflow vÃ  error handling  
âœ… **Chuáº©n bá»‹ mÃ´i trÆ°á»ng** vá»›i hardware/software requirements cá»¥ thá»ƒ  
âœ… **Implementation roadmap** 12 tuáº§n vá»›i 5 phases rÃµ rÃ ng  
âœ… **Testing strategy** toÃ n diá»‡n vá»›i unit, integration, performance tests  
âœ… **Production deployment** vá»›i Docker, Kubernetes, monitoring  
âœ… **Security considerations** vÃ  best practices  

### 12.2 Success Criteria

**Functional Requirements:**
- [x] TÃ­ch há»£p thÃ nh cÃ´ng vá»›i multiple LLM providers (OpenAI, Claude, Gemini, Grok, OpenRouter)
- [x] Há»— trá»£ local LLM models (Llama 2, Mistral, etc.)
- [x] Sinh cÃ¢u tráº£ lá»i cháº¥t lÆ°á»£ng cao vá»›i confidence score
- [x] Cung cáº¥p citations chÃ­nh xÃ¡c vá»›i reference linking
- [x] Xá»­ lÃ½ fallback vÃ  error recovery tá»± Ä‘á»™ng

**Non-Functional Requirements:**
- [x] Response time < 60 seconds (SLA requirement)
- [x] Support 100+ concurrent users
- [x] 99.5% uptime trong production
- [x] Comprehensive monitoring vÃ  alerting
- [x] Security-first design vá»›i input validation

### 12.3 Next Steps After Implementation

#### Phase 1: Post-Deployment (Week 13-14)
1. **Production Monitoring Setup**
   - Deploy Prometheus + Grafana dashboards
   - Configure alerting rules
   - Set up log aggregation (ELK stack)
   
2. **Performance Baseline**
   - Collect baseline metrics for 1 week
   - Identify optimization opportunities
   - Document performance characteristics
   
3. **User Training**
   - Train end users on new system
   - Collect initial feedback
   - Document common use cases


#### Phase 2: Optimization (Week 15-16)

1. **Performance Tuning**
   - Optimize based on real usage patterns
   - Fine-tune cache settings
   - Adjust resource allocations
   
2. **Cost Optimization**
   - Analyze LLM usage costs
   - Implement cost-saving strategies
   - Consider local LLM deployment
   
3. **Feature Enhancement**
   - Add multi-language support
   - Implement conversation context
   - Add advanced analytics


#### Phase 3: Scale & Extend (Week 17+)

1. **Integration Planning**
   - Plan integration with existing systems
   - Design API for other applications
   - Consider mobile app development
   
2. **Advanced Features**
   - Implement semantic caching
   - Add document summarization
   - Develop knowledge graph integration
   
3. **Enterprise Features**
   - Advanced user management
   - Department-specific customization
   - Audit and compliance features


### 12.4 Risk Mitigation Strategies

```mermaid
graph TB
    subgraph "ğŸ¯ High Priority Risks"
        R1[LLM API Costs<br/>Exceed Budget]
        R2[Performance Below<br/>SLA Requirements]
        R3[Security<br/>Vulnerabilities]
        R4[Low User<br/>Adoption]
    end
    
    subgraph "ğŸ›¡ï¸ Mitigation Strategies"
        M1[Local LLM Backup<br/>Cost Monitoring<br/>Usage Caps]
        M2[Performance Testing<br/>Caching Strategy<br/>Load Balancing]
        M3[Security Audit<br/>Penetration Testing<br/>Input Validation]
        M4[User Training<br/>Change Management<br/>Feedback Loop]
    end
    
    subgraph "ğŸ“Š Monitoring & Controls"
        C1[Real-time Cost<br/>Dashboards]
        C2[SLA Monitoring<br/>Alerting]
        C3[Security<br/>Scanning]
        C4[User Analytics<br/>Satisfaction Surveys]
    end
    
    R1 --> M1 --> C1
    R2 --> M2 --> C2
    R3 --> M3 --> C3
    R4 --> M4 --> C4
    
    classDef risk fill:#ffebee,stroke:#c62828,stroke-width:2px
    classDef mitigation fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef control fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    
    class R1,R2,R3,R4 risk
    class M1,M2,M3,M4 mitigation
    class C1,C2,C3,C4 control
```

### 12.5 Success Metrics & KPIs

#### ğŸ“ˆ **Technical KPIs**
| Metric | Target | Measurement |
|--------|--------|-------------|
| **Response Time** | < 60s (95th percentile) | Prometheus histogram |
| **Uptime** | 99.5% | Service monitoring |
| **Error Rate** | < 1% | Request success/failure ratio |
| **Cache Hit Rate** | > 70% | Cache statistics |
| **Concurrent Users** | 100+ without degradation | Load testing |

#### ğŸ’° **Business KPIs**
| Metric | Target | Measurement |
|--------|--------|-------------|
| **User Adoption** | 80% of target users in 3 months | User analytics |
| **Query Resolution** | 85% answered successfully | Response quality analysis |
| **Time Savings** | 50% reduction in info search time | User surveys |
| **Cost per Query** | < $0.05 average | Cost tracking |
| **User Satisfaction** | > 4.0/5.0 rating | Feedback surveys |

#### ğŸ” **Quality KPIs**
| Metric | Target | Measurement |
|--------|--------|-------------|
| **Answer Accuracy** | > 90% | Expert evaluation |
| **Citation Quality** | > 95% valid references | Automated checking |
| **Response Relevance** | > 85% rated as relevant | User feedback |
| **Language Quality** | Professional Vietnamese | Manual review |

### 12.6 Maintenance Schedule

#### ğŸ—“ï¸ **Daily Tasks**
- Monitor system health dashboards
- Review error logs and alerts
- Check resource utilization
- Backup verification

#### ğŸ“… **Weekly Tasks**
- Performance analysis and optimization
- Security log review
- User feedback analysis
- Cost analysis and optimization

#### ğŸ“† **Monthly Tasks**
- Comprehensive system review
- Security audit and updates
- Performance benchmarking
- User training sessions
- Documentation updates

#### ğŸ¯ **Quarterly Tasks**
- Major feature releases
- Infrastructure capacity planning
- Security penetration testing
- Business value assessment
- Technology stack updates

### 12.7 Team Responsibilities

#### ğŸ‘¨â€ğŸ’» **Development Team**
- Core engine development and maintenance
- Bug fixes and feature enhancements
- Code review and quality assurance
- Technical documentation

#### ğŸ—ï¸ **DevOps Team**
- Infrastructure management
- CI/CD pipeline maintenance
- Monitoring and alerting
- Security implementations

#### ğŸ“Š **Data Team**
- Data quality management
- Embedding model optimization
- Performance analysis
- Usage analytics

#### ğŸ¯ **Product Team**
- Requirements gathering
- User experience design
- Stakeholder communication
- Success metrics tracking

### 12.8 Technology Roadmap

#### ğŸ”® **Short-term (3-6 months)**
```mermaid
gantt
    title Technology Roadmap - Short Term
    dateFormat  YYYY-MM-DD
    section Core Features
    Multi-language Support    :2025-09-15, 30d
    Conversation Context      :2025-10-15, 45d
    Advanced Analytics        :2025-11-01, 30d
    
    section Performance
    Semantic Caching          :2025-09-20, 25d
    Local LLM Deployment      :2025-10-01, 60d
    Response Optimization     :2025-11-15, 30d
    
    section Integration
    API v2 Development        :2025-10-01, 45d
    Mobile App Planning       :2025-11-01, 30d
    SSO Integration           :2025-11-15, 30d
```

#### ğŸš€ **Long-term (6-12 months)**
- Knowledge graph integration
- Advanced AI capabilities (RAG + reasoning)
- Multi-modal support (images, documents)
- Enterprise-grade analytics
- AI model fine-tuning on company data

### 12.9 Final Recommendations

#### ğŸ¯ **Critical Success Factors**
1. **Start Simple**: Begin with core functionality, iterate based on user feedback
2. **Monitor Everything**: Comprehensive monitoring from day one
3. **Security First**: Never compromise on security, especially with LLM integrations
4. **User-Centric**: Regular user feedback and continuous improvement
5. **Cost Management**: Monitor LLM costs closely, implement safeguards

#### âš ï¸ **Key Warnings**
1. **LLM Costs Can Escalate Quickly**: Implement usage caps and monitoring
2. **Quality Control is Critical**: Bad answers hurt user trust permanently  
3. **Security is Paramount**: Any data leakage can be catastrophic
4. **Performance SLAs are Hard**: Plan for scale from the beginning
5. **Change Management**: Technical success â‰  business success without adoption

#### ğŸ“š **Recommended Reading**
- "Building LLM Applications for Production" - Chip Huyen
- "Designing Data-Intensive Applications" - Martin Kleppmann
- "Site Reliability Engineering" - Google SRE Team
- "The DevOps Handbook" - Gene Kim, et al.

---

## ğŸ‰ **KÃCH THÆ¯á»šC VÃ€ PHáº M VI Dá»° ÃN**

### Tá»•ng quan sá»‘ liá»‡u:
- **ğŸ“‹ TÃ i liá»‡u**: 50+ pages chi tiáº¿t specifications
- **â±ï¸ Timeline**: 12 tuáº§n implementation + 4 tuáº§n optimization  
- **ğŸ‘¥ Team size**: 4-6 developers (Backend, Frontend, DevOps, QA)
- **ğŸ’° Budget estimate**: 1.2 tá»· VNÄ total project cost
- **ğŸ¯ Success criteria**: 15+ KPIs across technical, business, and quality metrics

### Deliverables hoÃ n chá»‰nh:
âœ… **Kiáº¿n trÃºc chi tiáº¿t** vá»›i 6-layer architecture  
âœ… **Environment setup** vá»›i step-by-step instructions  
âœ… **Implementation roadmap** vá»›i 5 phases, 48 checkpoints  
âœ… **Testing strategy** vá»›i 80%+ code coverage target  
âœ… **Production deployment** vá»›i Docker + Kubernetes  
âœ… **Security framework** vá»›i monitoring vÃ  compliance  
âœ… **Monitoring & alerting** vá»›i Prometheus + Grafana  
âœ… **Error handling** vá»›i comprehensive fallback strategies  
âœ… **Performance optimization** vá»›i caching vÃ  load balancing  
âœ… **Maintenance procedures** vá»›i daily/weekly/monthly tasks  

**ğŸš€ Team ká»¹ thuáº­t giá» Ä‘Ã¢y cÃ³ thá»ƒ follow step-by-step guide nÃ y Ä‘á»ƒ implement FR-04.3 Generation Engine má»™t cÃ¡ch tá»‘i Æ°u vÃ  chuyÃªn nghiá»‡p!**
