Tuy·ªát v·ªùi! T√¥i s·∫Ω implement Documents API ƒë·ªÉ t√≠ch h·ª£p v·ªõi database schema t·ª´ FR-02.1. ƒê√¢y s·∫Ω l√† API core cho vi·ªác qu·∫£n l√Ω t√†i li·ªáu v·ªõi full CRUD operations v√† access control.

## üìÑ **DOCUMENTS API CHO FR-02.2**

### **Ki·∫øn tr√∫c Documents API:**

```mermaid
graph TB
    Client[Client Application] --> Router[Documents Router]
    Router --> Auth[Authentication<br/>Middleware]
    
    Auth --> CRUD[Document CRUD<br/>Operations]
    CRUD --> PostgresDB[(PostgreSQL<br/>documents_metadata_v2)]
    CRUD --> ChunksDB[(document_chunks_enhanced)]
    
    CRUD --> Vector[Vector Operations]
    Vector --> ChromaDB[(ChromaDB<br/>Vector Storage)]
    
    CRUD --> Cache[Cache Layer]
    Cache --> Redis[(Redis Cache)]
    
    Router --> Search[Search Integration]
    Search --> BM25[BM25 Search]
    Search --> Semantic[Semantic Search]
    
    Router --> Access[Access Control]
    Access --> Permissions[Permission Check]
    Access --> Department[Department Filter]
```

## üóÑÔ∏è **1. DOCUMENT MODELS**

### **app/models/document.py**
```python
"""
Document models t√≠ch h·ª£p v·ªõi schema t·ª´ FR-02.1 handover
"""
from sqlalchemy import Column, String, Text, Integer, Boolean, DateTime, Enum, DECIMAL, ForeignKey
from sqlalchemy.dialects.postgresql import UUID, JSONB, TSVECTOR
from sqlalchemy.sql import func
from sqlalchemy.orm import relationship
import uuid
import enum
from datetime import datetime
from typing import Optional

from app.models.base import Base

# ======================
# ENUMS T·ª™ FR-02.1 HANDOVER
# ======================

class DocumentType(str, enum.Enum):
    """Document types t·ª´ FR-02.1 schema"""
    POLICY = "policy"
    PROCEDURE = "procedure"
    TECHNICAL_GUIDE = "technical_guide"
    REPORT = "report"
    MANUAL = "manual"
    SPECIFICATION = "specification"
    TEMPLATE = "template"
    FORM = "form"
    PRESENTATION = "presentation"
    TRAINING_MATERIAL = "training_material"
    OTHER = "other"

class AccessLevel(str, enum.Enum):
    """Access levels t·ª´ FR-02.1 schema"""
    PUBLIC = "public"
    EMPLOYEE_ONLY = "employee_only"
    MANAGER_ONLY = "manager_only"
    DIRECTOR_ONLY = "director_only"
    SYSTEM_ADMIN = "system_admin"

class DocumentStatus(str, enum.Enum):
    """Document status t·ª´ FR-02.1 schema"""
    DRAFT = "draft"
    REVIEW = "review"
    APPROVED = "approved"
    PUBLISHED = "published"
    ARCHIVED = "archived"
    DEPRECATED = "deprecated"

class PipelineType(str, enum.Enum):
    """Pipeline types t·ª´ FR-02.1 schema"""
    STANDARD = "standard"
    REASONING = "reasoning"
    CONDITIONAL = "conditional"
    ADAPTIVE = "adaptive"

class RetrievalMethod(str, enum.Enum):
    """Retrieval methods t·ª´ FR-02.1 schema"""
    DENSE = "dense"
    SPARSE = "sparse"
    HYBRID = "hybrid"
    KNOWLEDGE_GRAPH = "knowledge_graph"
    MULTI_MODAL = "multi_modal"

# ======================
# MAIN DOCUMENT MODEL (t·ª´ FR-02.1)
# ======================

class DocumentMetadataV2(Base):
    """
    Main documents table t·ª´ FR-02.1 handover
    Mapping v·ªõi documents_metadata_v2 table
    """
    __tablename__ = "documents_metadata_v2"
    
    # Primary key
    document_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    
    # Basic metadata
    title = Column(String(500), nullable=False, index=True)
    content = Column(Text, nullable=True)  # Full text content
    document_type = Column(Enum(DocumentType), nullable=False, index=True)
    access_level = Column(Enum(AccessLevel), nullable=False, default=AccessLevel.EMPLOYEE_ONLY, index=True)
    
    # Ownership v√† authorship
    department_owner = Column(String(100), nullable=True, index=True)
    author = Column(String(255), nullable=True, index=True)
    author_id = Column(UUID(as_uuid=True), nullable=True, index=True)  # FK to users table
    
    # Status v√† versioning
    status = Column(Enum(DocumentStatus), nullable=False, default=DocumentStatus.DRAFT, index=True)
    version = Column(String(20), nullable=True, default="1.0")
    
    # File information
    source_file = Column(String(500), nullable=True)  # Original file path
    file_size = Column(Integer, nullable=True)  # File size in bytes
    file_hash = Column(String(64), nullable=True)  # SHA-256 hash
    mime_type = Column(String(100), nullable=True)
    
    # Language v√† processing
    language_detected = Column(String(10), nullable=True, default="vi")
    
    # Search v√† indexing
    search_tokens = Column(TSVECTOR, nullable=True)  # Full-text search
    tags = Column(JSONB, nullable=True, default=[])  # Array of tags
    
    # Statistics
    chunk_count = Column(Integer, nullable=True, default=0)
    view_count = Column(Integer, nullable=True, default=0)
    download_count = Column(Integer, nullable=True, default=0)
    
    # Timestamps
    created_at = Column(DateTime(timezone=True), server_default=func.now(), index=True)
    updated_at = Column(DateTime(timezone=True), onupdate=func.now())
    
    # Additional metadata
    metadata_json = Column(JSONB, nullable=True, default={})  # Flexible metadata
    
    def __repr__(self):
        return f"<Document {self.title} ({self.document_type})>"
    
    @property
    def is_public(self) -> bool:
        """Check if document is publicly accessible"""
        return self.access_level == AccessLevel.PUBLIC
    
    @property
    def access_level_hierarchy(self) -> int:
        """Get numeric hierarchy for access control"""
        hierarchy = {
            AccessLevel.PUBLIC: 0,
            AccessLevel.EMPLOYEE_ONLY: 1,
            AccessLevel.MANAGER_ONLY: 2,
            AccessLevel.DIRECTOR_ONLY: 3,
            AccessLevel.SYSTEM_ADMIN: 4
        }
        return hierarchy.get(self.access_level, 1)

# ======================
# DOCUMENT CHUNKS MODEL (t·ª´ FR-02.1)
# ======================

class DocumentChunkEnhanced(Base):
    """
    Document chunks table t·ª´ FR-02.1 handover
    Mapping v·ªõi document_chunks_enhanced table
    """
    __tablename__ = "document_chunks_enhanced"
    
    # Primary key
    chunk_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    
    # Foreign key to document
    document_id = Column(UUID(as_uuid=True), ForeignKey('documents_metadata_v2.document_id', ondelete='CASCADE'), nullable=False, index=True)
    
    # Chunk content
    chunk_content = Column(Text, nullable=False)
    chunk_position = Column(Integer, nullable=False, default=0)  # Position in document
    
    # Quality metrics
    chunk_quality_score = Column(DECIMAL(3, 2), nullable=True)  # 0.00 to 1.00
    semantic_boundary = Column(Boolean, nullable=True, default=False)
    
    # Embedding information
    embedding_model = Column(String(100), nullable=True)
    vector_id = Column(String(255), nullable=True)  # ID in vector database
    
    # Processing metadata
    chunk_metadata = Column(JSONB, nullable=True, default={})
    
    # Timestamps
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), onupdate=func.now())
    
    # Relationship
    document = relationship("DocumentMetadataV2", backref="chunks")
    
    def __repr__(self):
        return f"<Chunk {self.chunk_id} of {self.document_id}>"

# ======================
# SUPPORTING MODELS (t·ª´ FR-02.1)
# ======================

class RagPipelineSession(Base):
    """
    RAG pipeline sessions t·ª´ FR-02.1 handover
    """
    __tablename__ = "rag_pipeline_sessions"
    
    session_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    user_id = Column(UUID(as_uuid=True), nullable=True, index=True)
    
    # Query information
    original_query = Column(Text, nullable=False)
    processed_query = Column(Text, nullable=True)
    
    # Pipeline configuration
    pipeline_type = Column(Enum(PipelineType), nullable=False, default=PipelineType.STANDARD)
    retrieval_method = Column(Enum(RetrievalMethod), nullable=False, default=RetrievalMethod.HYBRID)
    
    # Results
    retrieved_documents = Column(JSONB, nullable=True, default=[])  # Array of document IDs
    response_content = Column(Text, nullable=True)
    response_quality_score = Column(DECIMAL(3, 2), nullable=True)
    
    # Performance metrics
    processing_time_ms = Column(Integer, nullable=True)
    retrieval_time_ms = Column(Integer, nullable=True)
    generation_time_ms = Column(Integer, nullable=True)
    
    # Timestamps
    created_at = Column(DateTime(timezone=True), server_default=func.now(), index=True)
    
    def __repr__(self):
        return f"<RAGSession {self.session_id}>"

class DocumentBM25Index(Base):
    """
    BM25 search index t·ª´ FR-02.1 handover
    """
    __tablename__ = "document_bm25_index"
    
    index_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    document_id = Column(UUID(as_uuid=True), ForeignKey('documents_metadata_v2.document_id', ondelete='CASCADE'), nullable=False, index=True)
    
    # BM25 specific fields
    term = Column(String(100), nullable=False, index=True)
    term_frequency = Column(Integer, nullable=False, default=0)
    document_frequency = Column(Integer, nullable=False, default=0)
    bm25_score = Column(DECIMAL(10, 6), nullable=True)
    
    # Language specific
    language = Column(String(10), nullable=False, default="vi")
    
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    
    # Relationship
    document = relationship("DocumentMetadataV2", backref="bm25_indexes")

class VietnameseTextAnalysis(Base):
    """
    Vietnamese text analysis t·ª´ FR-02.1 handover
    """
    __tablename__ = "vietnamese_text_analysis"
    
    analysis_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    document_id = Column(UUID(as_uuid=True), ForeignKey('documents_metadata_v2.document_id', ondelete='CASCADE'), nullable=False, index=True)
    
    # Vietnamese NLP results
    tokenized_content = Column(JSONB, nullable=True, default=[])
    pos_tags = Column(JSONB, nullable=True, default=[])
    named_entities = Column(JSONB, nullable=True, default=[])
    keywords = Column(JSONB, nullable=True, default=[])
    
    # Analysis metadata
    analysis_model = Column(String(100), nullable=True)
    processing_time_ms = Column(Integer, nullable=True)
    
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    
    # Relationship
    document = relationship("DocumentMetadataV2", backref="vietnamese_analysis")
```

## üìã **2. PYDANTIC SCHEMAS**

### **app/schemas/document.py**
```python
"""
Document schemas cho request/response
"""
from pydantic import BaseModel, Field, validator
from typing import Optional, List, Dict, Any, Union
from datetime import datetime
from enum import Enum

from app.models.document import DocumentType, AccessLevel, DocumentStatus, PipelineType, RetrievalMethod

# ======================
# BASE SCHEMAS
# ======================

class DocumentBase(BaseModel):
    """Base document schema"""
    title: str = Field(..., min_length=1, max_length=500, description="Document title")
    content: Optional[str] = Field(None, description="Document content")
    document_type: DocumentType = Field(..., description="Type of document")
    access_level: AccessLevel = Field(AccessLevel.EMPLOYEE_ONLY, description="Access level")
    department_owner: Optional[str] = Field(None, max_length=100, description="Owning department")
    author: Optional[str] = Field(None, max_length=255, description="Document author")
    status: DocumentStatus = Field(DocumentStatus.DRAFT, description="Document status")
    version: Optional[str] = Field("1.0", max_length=20, description="Document version")
    language_detected: Optional[str] = Field("vi", max_length=10, description="Detected language")
    tags: Optional[List[str]] = Field([], description="Document tags")

class DocumentCreate(DocumentBase):
    """Schema for creating new document"""
    file_content: Optional[str] = Field(None, description="File content for processing")
    source_file: Optional[str] = Field(None, max_length=500, description="Source file path")
    mime_type: Optional[str] = Field(None, max_length=100, description="MIME type")
    
    @validator('tags')
    def validate_tags(cls, v):
        if v and len(v) > 20:
            raise ValueError('Maximum 20 tags allowed')
        if v:
            for tag in v:
                if len(tag) > 50:
                    raise ValueError('Tag length cannot exceed 50 characters')
        return v

class DocumentUpdate(BaseModel):
    """Schema for updating document"""
    title: Optional[str] = Field(None, min_length=1, max_length=500)
    content: Optional[str] = None
    document_type: Optional[DocumentType] = None
    access_level: Optional[AccessLevel] = None
    department_owner: Optional[str] = Field(None, max_length=100)
    author: Optional[str] = Field(None, max_length=255)
    status: Optional[DocumentStatus] = None
    version: Optional[str] = Field(None, max_length=20)
    tags: Optional[List[str]] = None
    metadata_json: Optional[Dict[str, Any]] = None

class DocumentInDB(DocumentBase):
    """Schema for document in database"""
    document_id: str
    author_id: Optional[str] = None
    file_size: Optional[int] = None
    file_hash: Optional[str] = None
    chunk_count: int = 0
    view_count: int = 0
    download_count: int = 0
    created_at: datetime
    updated_at: Optional[datetime] = None
    metadata_json: Optional[Dict[str, Any]] = {}
    
    class Config:
        from_attributes = True

# ======================
# CHUNK SCHEMAS
# ======================

class ChunkBase(BaseModel):
    """Base chunk schema"""
    chunk_content: str = Field(..., min_length=1, description="Chunk content")
    chunk_position: int = Field(0, ge=0, description="Position in document")
    chunk_quality_score: Optional[float] = Field(None, ge=0.0, le=1.0, description="Quality score")
    semantic_boundary: Optional[bool] = Field(False, description="Is semantic boundary")
    embedding_model: Optional[str] = Field(None, max_length=100, description="Embedding model used")

class ChunkCreate(ChunkBase):
    """Schema for creating chunk"""
    document_id: str = Field(..., description="Parent document ID")

class ChunkInDB(ChunkBase):
    """Schema for chunk in database"""
    chunk_id: str
    document_id: str
    vector_id: Optional[str] = None
    chunk_metadata: Optional[Dict[str, Any]] = {}
    created_at: datetime
    updated_at: Optional[datetime] = None
    
    class Config:
        from_attributes = True

# ======================
# DOCUMENT LIST & PAGINATION
# ======================

class DocumentFilter(BaseModel):
    """Document filtering options"""
    document_type: Optional[DocumentType] = None
    access_level: Optional[AccessLevel] = None
    department_owner: Optional[str] = None
    author: Optional[str] = None
    status: Optional[DocumentStatus] = None
    language: Optional[str] = None
    tags: Optional[List[str]] = None
    search_query: Optional[str] = Field(None, min_length=1, max_length=1000)
    date_from: Optional[datetime] = None
    date_to: Optional[datetime] = None

class DocumentSort(str, Enum):
    """Document sorting options"""
    CREATED_ASC = "created_asc"
    CREATED_DESC = "created_desc" 
    UPDATED_ASC = "updated_asc"
    UPDATED_DESC = "updated_desc"
    TITLE_ASC = "title_asc"
    TITLE_DESC = "title_desc"
    VIEW_COUNT_ASC = "view_count_asc"
    VIEW_COUNT_DESC = "view_count_desc"

class DocumentListResponse(BaseModel):
    """Document list response with pagination"""
    documents: List[DocumentInDB]
    total: int
    page: int
    size: int
    pages: int
    has_next: bool
    has_prev: bool

class DocumentWithChunks(DocumentInDB):
    """Document with its chunks"""
    chunks: List[ChunkInDB] = []

# ======================
# DOCUMENT STATISTICS
# ======================

class DocumentStats(BaseModel):
    """Document statistics"""
    total_documents: int
    documents_by_type: Dict[str, int]
    documents_by_department: Dict[str, int]
    documents_by_status: Dict[str, int]
    documents_by_access_level: Dict[str, int]
    recent_uploads: int  # Last 30 days
    total_chunks: int
    average_chunks_per_document: float

class DocumentActivity(BaseModel):
    """Document activity metrics"""
    document_id: str
    title: str
    view_count: int
    download_count: int
    last_accessed: Optional[datetime] = None
    access_trend: List[Dict[str, Any]] = []  # Daily access counts

# ======================
# SEARCH SCHEMAS
# ======================

class DocumentSearchRequest(BaseModel):
    """Document search request"""
    query: str = Field(..., min_length=1, max_length=1000, description="Search query")
    search_type: str = Field("hybrid", description="Search type: semantic, keyword, or hybrid")
    filters: Optional[DocumentFilter] = None
    limit: int = Field(10, ge=1, le=100, description="Number of results")
    include_chunks: bool = Field(False, description="Include chunk details")
    min_score: Optional[float] = Field(None, ge=0.0, le=1.0, description="Minimum relevance score")

class DocumentSearchResult(BaseModel):
    """Single search result"""
    document: DocumentInDB
    relevance_score: float
    matching_chunks: Optional[List[ChunkInDB]] = None
    highlight_snippets: Optional[List[str]] = None

class DocumentSearchResponse(BaseModel):
    """Search results response"""
    query: str
    search_type: str
    results: List[DocumentSearchResult]
    total_found: int
    search_time_ms: int
    suggestions: Optional[List[str]] = None

# ======================
# FILE UPLOAD SCHEMAS
# ======================

class FileUploadResult(BaseModel):
    """File upload result"""
    success: bool
    message: str
    document_id: Optional[str] = None
    file_size: Optional[int] = None
    chunks_created: Optional[int] = None
    processing_time_ms: Optional[int] = None

class BulkUploadRequest(BaseModel):
    """Bulk upload request"""
    files: List[str] = Field(..., description="List of file paths or URLs")
    default_department: Optional[str] = None
    default_access_level: AccessLevel = AccessLevel.EMPLOYEE_ONLY
    auto_process: bool = Field(True, description="Auto process and chunk documents")

class BulkUploadResponse(BaseModel):
    """Bulk upload response"""
    total_files: int
    successful_uploads: int
    failed_uploads: int
    results: List[FileUploadResult]
    total_processing_time_ms: int

# ======================
# DOCUMENT PROCESSING
# ======================

class DocumentProcessingStatus(BaseModel):
    """Document processing status"""
    document_id: str
    status: str  # "pending", "processing", "completed", "failed"
    progress_percentage: int = Field(0, ge=0, le=100)
    current_step: Optional[str] = None
    error_message: Optional[str] = None
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    processing_time_ms: Optional[int] = None

class DocumentReprocessRequest(BaseModel):
    """Document reprocessing request"""
    document_ids: List[str] = Field(..., min_items=1, max_items=100)
    reprocess_chunks: bool = Field(True, description="Regenerate chunks")
    reprocess_embeddings: bool = Field(True, description="Regenerate embeddings")
    update_vietnamese_analysis: bool = Field(True, description="Update Vietnamese NLP analysis")

# ======================
# ACCESS CONTROL
# ======================

class DocumentAccess(BaseModel):
    """Document access information"""
    document_id: str
    user_can_read: bool
    user_can_write: bool
    user_can_delete: bool
    access_reason: str  # Why user has/doesn't have access

class DocumentAccessLog(BaseModel):
    """Document access log entry"""
    document_id: str
    user_id: str
    access_type: str  # "view", "download", "edit"
    timestamp: datetime
    ip_address: Optional[str] = None
    user_agent: Optional[str] = None

# ======================
# VERSION CONTROL
# ======================

class DocumentVersion(BaseModel):
    """Document version information"""
    version: str
    created_by: Optional[str] = None
    created_at: datetime
    changes_summary: Optional[str] = None
    file_hash: Optional[str] = None

class DocumentVersionHistory(BaseModel):
    """Document version history"""
    document_id: str
    current_version: str
    versions: List[DocumentVersion]

# ======================
# EXPORT SCHEMAS
# ======================

class DocumentExportRequest(BaseModel):
    """Document export request"""
    document_ids: List[str] = Field(..., min_items=1, max_items=1000)
    export_format: str = Field("json", description="Export format: json, csv, xlsx")
    include_content: bool = Field(True, description="Include document content")
    include_chunks: bool = Field(False, description="Include chunk data")
    include_metadata: bool = Field(True, description="Include metadata")

class DocumentExportResponse(BaseModel):
    """Document export response"""
    export_id: str
    download_url: str
    expires_at: datetime
    file_size: int
    document_count: int
```

## üîå **3. DATABASE CRUD OPERATIONS**

### **app/crud/document.py**
```python
"""
Document CRUD operations t√≠ch h·ª£p v·ªõi FR-02.1 schema
"""
from typing import Optional, List, Dict, Any, Tuple
from sqlalchemy.orm import Session
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, update, delete, and_, or_, func, text, desc, asc
from sqlalchemy.exc import IntegrityError
from datetime import datetime, timedelta
import uuid
import hashlib
import json

from app.models.document import (
    DocumentMetadataV2, 
    DocumentChunkEnhanced,
    DocumentType,
    AccessLevel,
    DocumentStatus
)
from app.models.user import User, UserLevel
from app.schemas.document import (
    DocumentCreate, 
    DocumentUpdate, 
    DocumentFilter,
    DocumentSort,
    ChunkCreate
)
import logging

logger = logging.getLogger(__name__)

class DocumentCRUD:
    """Document CRUD operations"""
    
    @staticmethod
    async def create_document(
        db: AsyncSession, 
        document_create: DocumentCreate,
        author_id: Optional[str] = None
    ) -> DocumentMetadataV2:
        """Create new document"""
        try:
            # Generate file hash if content provided
            file_hash = None
            file_size = None
            if document_create.file_content:
                content_bytes = document_create.file_content.encode('utf-8')
                file_hash = hashlib.sha256(content_bytes).hexdigest()
                file_size = len(content_bytes)
            
            # Create document instance
            db_document = DocumentMetadataV2(
                document_id=uuid.uuid4(),
                title=document_create.title,
                content=document_create.content or document_create.file_content,
                document_type=document_create.document_type,
                access_level=document_create.access_level,
                department_owner=document_create.department_owner,
                author=document_create.author,
                author_id=uuid.UUID(author_id) if author_id else None,
                status=document_create.status,
                version=document_create.version,
                source_file=document_create.source_file,
                file_size=file_size,
                file_hash=file_hash,
                mime_type=document_create.mime_type,
                language_detected=document_create.language_detected,
                tags=document_create.tags or [],
                chunk_count=0,
                view_count=0,
                download_count=0,
                metadata_json={}
            )
            
            db.add(db_document)
            await db.commit()
            await db.refresh(db_document)
            
            logger.info(f"Document created: {document_create.title} by {author_id}")
            return db_document
            
        except IntegrityError as e:
            await db.rollback()
            logger.error(f"Document creation failed - integrity error: {e}")
            raise ValueError("Document creation failed - data integrity error")
        except Exception as e:
            await db.rollback()
            logger.error(f"Document creation failed: {e}")
            raise
    
    @staticmethod
    async def get_document_by_id(
        db: AsyncSession, 
        document_id: str,
        user: Optional[User] = None
    ) -> Optional[DocumentMetadataV2]:
        """Get document by ID with access control"""
        try:
            stmt = select(DocumentMetadataV2).where(
                DocumentMetadataV2.document_id == document_id
            )
            
            result = await db.execute(stmt)
            document = result.scalar_one_or_none()
            
            if not document:
                return None
            
            # Check access control if user provided
            if user and not DocumentCRUD._check_document_access(document, user):
                logger.warning(f"Access denied to document {document_id} for user {user.username}")
                return None
            
            # Increment view count
            if user:
                await DocumentCRUD._increment_view_count(db, document_id)
            
            return document
            
        except Exception as e:
            logger.error(f"Error getting document {document_id}: {e}")
            return None
    
    @staticmethod
    async def get_documents_list(
        db: AsyncSession,
        user: User,
        skip: int = 0,
        limit: int = 100,
        filters: Optional[DocumentFilter] = None,
        sort: DocumentSort = DocumentSort.CREATED_DESC
    ) -> Tuple[List[DocumentMetadataV2], int]:
        """Get paginated list of documents with filtering and access control"""
        try:
            # Build base query with access control
            stmt = select(DocumentMetadataV2)
            count_stmt = select(func.count(DocumentMetadataV2.document_id))
            
            # Apply access control
            access_conditions = DocumentCRUD._build_access_conditions(user)
            stmt = stmt.where(access_conditions)
            count_stmt = count_stmt.where(access_conditions)
            
            # Apply filters
            if filters:
                filter_conditions = DocumentCRUD._build_filter_conditions(filters)
                if filter_conditions is not None:
                    stmt = stmt.where(filter_conditions)
                    count_stmt = count_stmt.where(filter_conditions)
            
            # Get total count
            count_result = await db.execute(count_stmt)
            total = count_result.scalar()
            
            # Apply sorting
            sort_column = DocumentCRUD._get_sort_column(sort)
            stmt = stmt.order_by(sort_column)
            
            # Apply pagination
            stmt = stmt.offset(skip).limit(limit)
            
            # Execute query
            result = await db.execute(stmt)
            documents = result.scalars().all()
            
            return list(documents), total
            
        except Exception as e:
            logger.error(f"Error getting documents list: {e}")
            return [], 0
    
    @staticmethod
    async def update_document(
        db: AsyncSession,
        document_id: str,
        document_update: DocumentUpdate,
        user: User
    ) -> Optional[DocumentMetadataV2]:
        """Update document with access control"""
        try:
            # Get existing document
            document = await DocumentCRUD.get_document_by_id(db, document_id, user)
            if not document:
                return None
            
            # Check write permission
            if not DocumentCRUD._check_write_permission(document, user):
                raise PermissionError("Insufficient permissions to update document")
            
            # Update fields
            update_data = document_update.dict(exclude_unset=True)
            
            for field, value in update_data.items():
                if hasattr(document, field):
                    setattr(document, field, value)
            
            document.updated_at = datetime.utcnow()
            
            await db.commit()
            await db.refresh(document)
            
            logger.info(f"Document updated: {document.title} by {user.username}")
            return document
            
        except PermissionError:
            raise
        except Exception as e:
            await db.rollback()
            logger.error(f"Document update failed: {e}")
            raise
    
    @staticmethod
    async def delete_document(
        db: AsyncSession,
        document_id: str,
        user: User
    ) -> bool:
        """Delete document with access control"""
        try:
            # Get existing document
            document = await DocumentCRUD.get_document_by_id(db, document_id, user)
            if not document:
                return False
            
            # Check delete permission
            if not DocumentCRUD._check_delete_permission(document, user):
                raise PermissionError("Insufficient permissions to delete document")
            
            # Delete related chunks first (cascade should handle this, but explicit is better)
            chunks_stmt = delete(DocumentChunkEnhanced).where(
                DocumentChunkEnhanced.document_id == document_id
            )
            await db.execute(chunks_stmt)
            
            # Delete document
            stmt = delete(DocumentMetadataV2).where(
                DocumentMetadataV2.document_id == document_id
            )
            result = await db.execute(stmt)
            await db.commit()
            
            logger.info(f"Document deleted: {document.title} by {user.username}")
            return result.rowcount > 0
            
        except PermissionError:
            raise
        except Exception as e:
            await db.rollback()
            logger.error(f"Document deletion failed: {e}")
            raise
    
    @staticmethod
    async def search_documents(
        db: AsyncSession,
        user: User,
        query: str,
        search_type: str = "hybrid",
        limit: int = 10,
        filters: Optional[DocumentFilter] = None
    ) -> List[Tuple[DocumentMetadataV2, float]]:
        """Search documents with relevance scoring"""
        try:
            if search_type == "keyword" or search_type == "hybrid":
                # Full-text search using PostgreSQL
                results = await DocumentCRUD._search_fulltext(db, user, query, limit, filters)
            else:
                # Basic text search as fallback
                results = await DocumentCRUD._search_basic(db, user, query, limit, filters)
            
            return results
            
        except Exception as e:
            logger.error(f"Document search failed: {e}")
            return []
    
    @staticmethod
    async def get_document_statistics(
        db: AsyncSession,
        user: User
    ) -> Dict[str, Any]:
        """Get document statistics for user's accessible documents"""
        try:
            # Build access conditions
            access_conditions = DocumentCRUD._build_access_conditions(user)
            
            # Total documents
            total_stmt = select(func.count(DocumentMetadataV2.document_id)).where(access_conditions)
            total_result = await db.execute(total_stmt)
            total_documents = total_result.scalar()
            
            # Documents by type
            type_stmt = select(
                DocumentMetadataV2.document_type,
                func.count(DocumentMetadataV2.document_id)
            ).where(access_conditions).group_by(DocumentMetadataV2.document_type)
            type_result = await db.execute(type_stmt)
            documents_by_type = {doc_type.value: count for doc_type, count in type_result}
            
            # Documents by department
            dept_stmt = select(
                DocumentMetadataV2.department_owner,
                func.count(DocumentMetadataV2.document_id)
            ).where(access_conditions).group_by(DocumentMetadataV2.department_owner)
            dept_result = await db.execute(dept_stmt)
            documents_by_department = {dept or "Unknown": count for dept, count in dept_result}
            
            # Documents by status
            status_stmt = select(
                DocumentMetadataV2.status,
                func.count(DocumentMetadataV2.document_id)
            ).where(access_conditions).group_by(DocumentMetadataV2.status)
            status_result = await db.execute(status_stmt)
            documents_by_status = {status.value: count for status, count in status_result}
            
            # Documents by access level
            access_stmt = select(
                DocumentMetadataV2.access_level,
                func.count(DocumentMetadataV2.document_id)
            ).where(access_conditions).group_by(DocumentMetadataV2.access_level)
            access_result = await db.execute(access_stmt)
            documents_by_access_level = {level.value: count for level, count in access_result}
            
            # Recent uploads (last 30 days)
            recent_date = datetime.utcnow() - timedelta(days=30)
            recent_stmt = select(func.count(DocumentMetadataV2.document_id)).where(
                and_(access_conditions, DocumentMetadataV2.created_at >= recent_date)
            )
            recent_result = await db.execute(recent_stmt)
            recent_uploads = recent_result.scalar()
            
            # Total chunks
            chunks_stmt = select(func.count(DocumentChunkEnhanced.chunk_id)).join(
                DocumentMetadataV2
            ).where(access_conditions)
            chunks_result = await db.execute(chunks_stmt)
            total_chunks = chunks_result.scalar()
            
            # Average chunks per document
            avg_chunks = total_chunks / total_documents if total_documents > 0 else 0
            
            return {
                "total_documents": total_documents,
                "documents_by_type": documents_by_type,
                "documents_by_department": documents_by_department,
                "documents_by_status": documents_by_status,
                "documents_by_access_level": documents_by_access_level,
                "recent_uploads": recent_uploads,
                "total_chunks": total_chunks,
                "average_chunks_per_document": round(avg_chunks, 2)
            }
            
        except Exception as e:
            logger.error(f"Error getting document statistics: {e}")
            return {}
    
    # ======================
    # PRIVATE HELPER METHODS
    # ======================
    
    @staticmethod
    def _check_document_access(document: DocumentMetadataV2, user: User) -> bool:
        """Check if user can access document"""
        # System admin can access everything
        if user.user_level == UserLevel.SYSTEM_ADMIN:
            return True
        
        # Check access level hierarchy
        user_hierarchy = user.access_level_hierarchy
        doc_hierarchy = document.access_level_hierarchy
        
        if user_hierarchy < doc_hierarchy:
            return False
        
        # Check department access for non-public documents
        if document.access_level != AccessLevel.PUBLIC:
            if document.department_owner and user.department != document.department_owner:
                # Allow if user is manager or above
                if user.user_level not in [UserLevel.MANAGER, UserLevel.DIRECTOR, UserLevel.SYSTEM_ADMIN]:
                    return False
        
        return True
    
    @staticmethod
    def _check_write_permission(document: DocumentMetadataV2, user: User) -> bool:
        """Check if user can write/update document"""
        # System admin can modify everything
        if user.user_level == UserLevel.SYSTEM_ADMIN:
            return True
        
        # Document author can modify their own documents
        if document.author_id and str(document.author_id) == str(user.user_id):
            return True
        
        # Department managers can modify department documents
        if (document.department_owner == user.department and 
            user.user_level in [UserLevel.MANAGER, UserLevel.DIRECTOR]):
            return True
        
        # Directors can modify documents in their access level
        if (user.user_level == UserLevel.DIRECTOR and 
            document.access_level_hierarchy <= user.access_level_hierarchy):
            return True
        
        return False
    
    @staticmethod
    def _check_delete_permission(document: DocumentMetadataV2, user: User) -> bool:
        """Check if user can delete document"""
        # System admin can delete everything
        if user.user_level == UserLevel.SYSTEM_ADMIN:
            return True
        
        # Document author can delete their own documents (if not published)
        if (document.author_id and str(document.author_id) == str(user.user_id) and
            document.status in [DocumentStatus.DRAFT, DocumentStatus.REVIEW]):
            return True
        
        # Directors can delete documents in their department
        if (user.user_level == UserLevel.DIRECTOR and 
            document.department_owner == user.department):
            return True
        
        return False
    
    @staticmethod
    def _build_access_conditions(user: User):
        """Build SQL conditions for access control"""
        if user.user_level == UserLevel.SYSTEM_ADMIN:
            # System admin can see everything
            return True
        
        conditions = []
        
        # Public documents are always accessible
        conditions.append(DocumentMetadataV2.access_level == AccessLevel.PUBLIC)
        
        # User level based access
        user_hierarchy = user.access_level_hierarchy
        
        if user_hierarchy >= 1:  # Employee or above
            conditions.append(DocumentMetadataV2.access_level == AccessLevel.EMPLOYEE_ONLY)
        
        if user_hierarchy >= 2:  # Manager or above
            conditions.append(DocumentMetadataV2.access_level == AccessLevel.MANAGER_ONLY)
        
        if user_hierarchy >= 3:  # Director or above
            conditions.append(DocumentMetadataV2.access_level == AccessLevel.DIRECTOR_ONLY)
        
        # Department access (users can see their department documents)
        if user.department:
            conditions.append(
                and_(
                    DocumentMetadataV2.department_owner == user.department,
                    DocumentMetadataV2.access_level.in_([
                        AccessLevel.EMPLOYEE_ONLY,
                        AccessLevel.MANAGER_ONLY,
                        AccessLevel.DIRECTOR_ONLY
                    ])
                )
            )
        
        return or_(*conditions) if conditions else False
    
    @staticmethod
    def _build_filter_conditions(filters: DocumentFilter):
        """Build SQL conditions from filters"""
        conditions = []
        
        if filters.document_type:
            conditions.append(DocumentMetadataV2.document_type == filters.document_type)
        
        if filters.access_level:
            conditions.append(DocumentMetadataV2.access_level == filters.access_level)
        
        if filters.department_owner:
            conditions.append(DocumentMetadataV2.department_owner == filters.department_owner)
        
        if filters.author:
            conditions.append(DocumentMetadataV2.author.ilike(f"%{filters.author}%"))
        
        if filters.status:
            conditions.append(DocumentMetadataV2.status == filters.status)
        
        if filters.language:
            conditions.append(DocumentMetadataV2.language_detected == filters.language)
        
        if filters.tags:
            # PostgreSQL JSONB array contains check
            for tag in filters.tags:
                conditions.append(DocumentMetadataV2.tags.op('?')(tag))
        
        if filters.search_query:
            # Simple text search in title and content
            search_term = f"%{filters.search_query}%"
            conditions.append(
                or_(
                    DocumentMetadataV2.title.ilike(search_term),
                    DocumentMetadataV2.content.ilike(search_term),
                    DocumentMetadataV2.author.ilike(search_term)
                )
            )
        
        if filters.date_from:
            conditions.append(DocumentMetadataV2.created_at >= filters.date_from)
        
        if filters.date_to:
            conditions.append(DocumentMetadataV2.created_at <= filters.date_to)
        
        return and_(*conditions) if conditions else None
    
    @staticmethod
    def _get_sort_column(sort: DocumentSort):
        """Get SQLAlchemy column for sorting"""
        sort_mapping = {
            DocumentSort.CREATED_ASC: asc(DocumentMetadataV2.created_at),
            DocumentSort.CREATED_DESC: desc(DocumentMetadataV2.created_at),
            DocumentSort.UPDATED_ASC: asc(DocumentMetadataV2.updated_at),
            DocumentSort.UPDATED_DESC: desc(DocumentMetadataV2.updated_at),
            DocumentSort.TITLE_ASC: asc(DocumentMetadataV2.title),
            DocumentSort.TITLE_DESC: desc(DocumentMetadataV2.title),
            DocumentSort.VIEW_COUNT_ASC: asc(DocumentMetadataV2.view_count),
            DocumentSort.VIEW_COUNT_DESC: desc(DocumentMetadataV2.view_count),
        }
        return sort_mapping.get(sort, desc(DocumentMetadataV2.created_at))
    
    @staticmethod
    async def _search_fulltext(
        db: AsyncSession,
        user: User,
        query: str,
        limit: int,
        filters: Optional[DocumentFilter]
    ) -> List[Tuple[DocumentMetadataV2, float]]:
        """Full-text search using PostgreSQL"""
        try:
            # Build access conditions
            access_conditions = DocumentCRUD._build_access_conditions(user)
            
            # Build search query using ts_query
            # For Vietnamese text, we'll use simple text matching for now
            search_conditions = or_(
                DocumentMetadataV2.title.ilike(f"%{query}%"),
                DocumentMetadataV2.content.ilike(f"%{query}%"),
                DocumentMetadataV2.tags.op('?')(query)  # Tag exact match
            )
            
            # Combine with access conditions
            where_conditions = and_(access_conditions, search_conditions)
            
            # Apply additional filters
            if filters:
                filter_conditions = DocumentCRUD._build_filter_conditions(filters)
                if filter_conditions is not None:
                    where_conditions = and_(where_conditions, filter_conditions)
            
            # Build query with relevance scoring
            stmt = select(
                DocumentMetadataV2,
                # Simple relevance scoring based on title vs content match
                func.case(
                    (DocumentMetadataV2.title.ilike(f"%{query}%"), 1.0),
                    (DocumentMetadataV2.content.ilike(f"%{query}%"), 0.5),
                    else_=0.1
                ).label('relevance_score')
            ).where(where_conditions).order_by(
                desc('relevance_score'),
                desc(DocumentMetadataV2.created_at)
            ).limit(limit)
            
            result = await db.execute(stmt)
            rows = result.all()
            
            return [(row[0], float(row[1])) for row in rows]
            
        except Exception as e:
            logger.error(f"Full-text search failed: {e}")
            return []
    
    @staticmethod
    async def _search_basic(
        db: AsyncSession,
        user: User,
        query: str,
        limit: int,
        filters: Optional[DocumentFilter]
    ) -> List[Tuple[DocumentMetadataV2, float]]:
        """Basic text search fallback"""
        try:
            # Build access conditions
            access_conditions = DocumentCRUD._build_access_conditions(user)
            
            # Simple ILIKE search
            search_term = f"%{query}%"
            search_conditions = or_(
                DocumentMetadataV2.title.ilike(search_term),
                DocumentMetadataV2.content.ilike(search_term),
                DocumentMetadataV2.author.ilike(search_term)
            )
            
            where_conditions = and_(access_conditions, search_conditions)
            
            # Apply additional filters
            if filters:
                filter_conditions = DocumentCRUD._build_filter_conditions(filters)
                if filter_conditions is not None:
                    where_conditions = and_(where_conditions, filter_conditions)
            
            stmt = select(DocumentMetadataV2).where(where_conditions).order_by(
                desc(DocumentMetadataV2.updated_at)
            ).limit(limit)
            
            result = await db.execute(stmt)
            documents = result.scalars().all()
            
            # Assign basic relevance scores
            return [(doc, 0.5) for doc in documents]
            
        except Exception as e:
            logger.error(f"Basic search failed: {e}")
            return []
    
    @staticmethod
    async def _increment_view_count(db: AsyncSession, document_id: str):
        """Increment document view count"""
        try:
            stmt = update(DocumentMetadataV2).where(
                DocumentMetadataV2.document_id == document_id
            ).values(view_count=DocumentMetadataV2.view_count + 1)
            
            await db.execute(stmt)
            await db.commit()
            
        except Exception as e:
            logger.error(f"Failed to increment view count for document {document_id}: {e}")

# ======================
# CHUNK CRUD OPERATIONS
# ======================

class ChunkCRUD:
    """Document chunk CRUD operations"""
    
    @staticmethod
    async def create_chunk(
        db: AsyncSession,
        chunk_create: ChunkCreate
    ) -> DocumentChunkEnhanced:
        """Create new document chunk"""
        try:
            db_chunk = DocumentChunkEnhanced(
                chunk_id=uuid.uuid4(),
                document_id=uuid.UUID(chunk_create.document_id),
                chunk_content=chunk_create.chunk_content,
                chunk_position=chunk_create.chunk_position,
                chunk_quality_score=chunk_create.chunk_quality_score,
                semantic_boundary=chunk_create.semantic_boundary,
                embedding_model=chunk_create.embedding_model,
                chunk_metadata={}
            )
            
            db.add(db_chunk)
            await db.commit()
            await db.refresh(db_chunk)
            
            # Update chunk count in parent document
            await ChunkCRUD._update_document_chunk_count(db, chunk_create.document_id)
            
            logger.info(f"Chunk created for document {chunk_create.document_id}")
            return db_chunk
            
        except Exception as e:
            await db.rollback()
            logger.error(f"Chunk creation failed: {e}")
            raise
    
    @staticmethod
    async def get_document_chunks(
        db: AsyncSession,
        document_id: str,
        user: User
    ) -> List[DocumentChunkEnhanced]:
        """Get all chunks for a document"""
        try:
            # First check if user can access the document
            document = await DocumentCRUD.get_document_by_id(db, document_id, user)
            if not document:
                return []
            
            stmt = select(DocumentChunkEnhanced).where(
                DocumentChunkEnhanced.document_id == document_id
            ).order_by(DocumentChunkEnhanced.chunk_position)
            
            result = await db.execute(stmt)
            chunks = result.scalars().all()
            
            return list(chunks)
            
        except Exception as e:
            logger.error(f"Error getting chunks for document {document_id}: {e}")
            return []
    
    @staticmethod
    async def delete_document_chunks(
        db: AsyncSession,
        document_id: str
    ) -> bool:
        """Delete all chunks for a document"""
        try:
            stmt = delete(DocumentChunkEnhanced).where(
                DocumentChunkEnhanced.document_id == document_id
            )
            
            result = await db.execute(stmt)
            await db.commit()
            
            # Update chunk count in parent document
            await ChunkCRUD._update_document_chunk_count(db, document_id)
            
            logger.info(f"Deleted {result.rowcount} chunks for document {document_id}")
            return True
            
        except Exception as e:
            await db.rollback()
            logger.error(f"Failed to delete chunks for document {document_id}: {e}")
            return False
    
    @staticmethod
    async def _update_document_chunk_count(db: AsyncSession, document_id: str):
        """Update chunk count in document metadata"""
        try:
            # Count current chunks
            count_stmt = select(func.count(DocumentChunkEnhanced.chunk_id)).where(
                DocumentChunkEnhanced.document_id == document_id
            )
            count_result = await db.execute(count_stmt)
            chunk_count = count_result.scalar()
            
            # Update document
            update_stmt = update(DocumentMetadataV2).where(
                DocumentMetadataV2.document_id == document_id
            ).values(chunk_count=chunk_count)
            
            await db.execute(update_stmt)
            await db.commit()
            
        except Exception as e:
            logger.error(f"Failed to update chunk count for document {document_id}: {e}")
```

## üõ£Ô∏è **4. DOCUMENTS API ENDPOINTS**

### **app/api/endpoints/documents.py**
```python
"""
Documents API endpoints cho FR-02.2
"""
from typing import Annotated, Optional, List
from fastapi import APIRouter, Depends, HTTPException, status, Query, File, UploadFile, BackgroundTasks
from fastapi.responses import StreamingResponse
from sqlalchemy.ext.asyncio import AsyncSession
import redis.asyncio as redis
from datetime import datetime
import uuid
import json
import io

from app.core.database import get_async_db, get_redis
from app.crud.document import DocumentCRUD, ChunkCRUD
from app.schemas.document import (
    DocumentCreate,
    DocumentUpdate,
    DocumentInDB,
    DocumentListResponse,
    DocumentWithChunks,
    DocumentFilter,
    DocumentSort,
    DocumentStats,
    DocumentSearchRequest,
    DocumentSearchResponse,
    DocumentSearchResult,
    FileUploadResult,
    BulkUploadRequest,
    BulkUploadResponse,
    DocumentProcessingStatus,
    ChunkInDB
)
from app.api.dependencies.auth import (
    get_current_user,
    get_current_user_optional,
    RequireEmployee,
    RequireManager,
    RequireSystemAdmin,
    StandardRateLimit
)
from app.models.user import User
from app.models.document import DocumentMetadataV2, DocumentType, AccessLevel, DocumentStatus
from app.core.exceptions import AuthorizationError
from app.services.document_processing import DocumentProcessingService
from app.services.embedding_service import EmbeddingService
import logging

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/documents", tags=["Documents"])

# ======================
# CORE CRUD ENDPOINTS
# ======================

@router.post("/", response_model=DocumentInDB, status_code=status.HTTP_201_CREATED)
async def create_document(
    document_data: DocumentCreate,
    current_user: Annotated[User, Depends(RequireEmployee)],
    db: Annotated[AsyncSession, Depends(get_async_db)],
    background_tasks: BackgroundTasks,
    _: Annotated[None, Depends(StandardRateLimit)]
):
    """
    T·∫°o document m·ªõi
    """
    try:
        # Set author info
        if not document_data.author:
            document_data.author = current_user.full_name
        
        if not document_data.department_owner:
            document_data.department_owner = current_user.department
        
        # Create document
        document = await DocumentCRUD.create_document(
            db=db,
            document_create=document_data,
            author_id=str(current_user.user_id)
        )
        
        # Schedule background processing if content provided
        if document_data.file_content or document_data.content:
            background_tasks.add_task(
                process_document_content,
                str(document.document_id),
                document_data.file_content or document_data.content
            )
        
        # Convert to response format
        return DocumentInDB(
            document_id=str(document.document_id),
            title=document.title,
            content=document.content,
            document_type=document.document_type,
            access_level=document.access_level,
            department_owner=document.department_owner,
            author=document.author,
            author_id=str(document.author_id) if document.author_id else None,
            status=document.status,
            version=document.version,
            language_detected=document.language_detected,
            tags=document.tags or [],
            file_size=document.file_size,
            file_hash=document.file_hash,
            chunk_count=document.chunk_count,
            view_count=document.view_count,
            download_count=document.download_count,
            created_at=document.created_at,
            updated_at=document.updated_at,
            metadata_json=document.metadata_json or {}
        )
        
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )
    except Exception as e:
        logger.error(f"Document creation error: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Error creating document"
        )

@router.get("/", response_model=DocumentListResponse)
async def list_documents(
    skip: int = Query(0, ge=0, description="Number of documents to skip"),
    limit: int = Query(20, ge=1, le=100, description="Number of documents to return"),
    document_type: Optional[DocumentType] = Query(None, description="Filter by document type"),
    access_level: Optional[AccessLevel] = Query(None, description="Filter by access level"),
    department: Optional[str] = Query(None, description="Filter by department"),
    status: Optional[DocumentStatus] = Query(None, description="Filter by status"),
    search: Optional[str] = Query(None, min_length=1, max_length=500, description="Search query"),
    sort: DocumentSort = Query(DocumentSort.CREATED_DESC, description="Sort order"),
    current_user: Annotated[User, Depends(RequireEmployee)],
    db: Annotated[AsyncSession, Depends(get_async_db)]
):
    """
    L·∫•y danh s√°ch documents v·ªõi filtering v√† pagination
    """
    try:
        # Build filters
        filters = DocumentFilter(
            document_type=document_type,
            access_level=access_level,
            department_owner=department,
            status=status,
            search_query=search
        )
        
        # Get documents with access control
        documents, total = await DocumentCRUD.get_documents_list(
            db=db,
            user=current_user,
            skip=skip,
            limit=limit,
            filters=filters,
            sort=sort
        )
        
        # Convert to response format
        document_responses = []
        for doc in documents:
            document_responses.append(DocumentInDB(
                document_id=str(doc.document_id),
                title=doc.title,
                content=doc.content,
                document_type=doc.document_type,
                access_level=doc.access_level,
                department_owner=doc.department_owner,
                author=doc.author,
                author_id=str(doc.author_id) if doc.author_id else None,
                status=doc.status,
                version=doc.version,
                language_detected=doc.language_detected,
                tags=doc.tags or [],
                file_size=doc.file_size,
                file_hash=doc.file_hash,
                chunk_count=doc.chunk_count,
                view_count=doc.view_count,
                download_count=doc.download_count,
                created_at=doc.created_at,
                updated_at=doc.updated_at,
                metadata_json=doc.metadata_json or {}
            ))
        
        # Calculate pagination info
        pages = (total + limit - 1) // limit
        current_page = (skip // limit) + 1
        has_next = current_page < pages
        has_prev = current_page > 1
        
        return DocumentListResponse(
            documents=document_responses,
            total=total,
            page=current_page,
            size=limit,
            pages=pages,
            has_next=has_next,
            has_prev=has_prev
        )
        
    except Exception as e:
        logger.error(f"Error listing documents: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Error retrieving documents"
        )

@router.get("/{document_id}", response_model=DocumentInDB)
async def get_document(
    document_id: str,
    current_user: Annotated[User, Depends(RequireEmployee)],
    db: Annotated[AsyncSession, Depends(get_async_db)]
):
    """
    L·∫•y th√¥ng tin chi ti·∫øt document
    """
    try:
        document = await DocumentCRUD.get_document_by_id(db, document_id, current_user)
        
        if not document:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Document not found or access denied"
            )
        
        return DocumentInDB(
            document_id=str(document.document_id),
            title=document.title,
            content=document.content,
            document_type=document.document_type,
            access_level=document.access_level,
            department_owner=document.department_owner,
            author=document.author,
            author_id=str(document.author_id) if document.author_id else None,
            status=document.status,
            version=document.version,
            language_detected=document.language_detected,
            tags=document.tags or [],
            file_size=document.file_size,
            file_hash=document.file_hash,
            chunk_count=document.chunk_count,
            view_count=document.view_count,
            download_count=document.download_count,
            created_at=document.created_at,
            updated_at=document.updated_at,
            metadata_json=document.metadata_json or {}
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting document {document_id}: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Error retrieving document"
        )

@router.put("/{document_id}", response_model=DocumentInDB)
async def update_document(
    document_id: str,
    document_update: DocumentUpdate,
    current_user: Annotated[User, Depends(RequireEmployee)],
    db: Annotated[AsyncSession, Depends(get_async_db)]
):
    """
    C·∫≠p nh·∫≠t document
    """
    try:
        document = await DocumentCRUD.update_document(
            db=db,
            document_id=document_id,
            document_update=document_update,
            user=current_user
        )
        
        if not document:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Document not found or access denied"
            )
        
        return DocumentInDB(
            document_id=str(document.document_id),
            title=document.title,
            content=document.content,
            document_type=document.document_type,
            access_level=document.access_level,
            department_owner=document.department_owner,
            author=document.author,
            author_id=str(document.author_id) if document.author_id else None,
            status=document.status,
            version=document.version,
            language_detected=document.language_detected,
            tags=document.tags or [],
            file_size=document.file_size,
            file_hash=document.file_hash,
            chunk_count=document.chunk_count,
            view_count=document.view_count,
            download_count=document.download_count,
            created_at=document.created_at,
            updated_at=document.updated_at,
            metadata_json			
```python
            updated_at=document.updated_at,
            metadata_json=document.metadata_json or {}
        )
        
    except PermissionError:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Insufficient permissions to update document"
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error updating document {document_id}: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Error updating document"
        )

@router.delete("/{document_id}")
async def delete_document(
    document_id: str,
    current_user: Annotated[User, Depends(RequireEmployee)],
    db: Annotated[AsyncSession, Depends(get_async_db)],
    background_tasks: BackgroundTasks
):
    """
    X√≥a document
    """
    try:
        success = await DocumentCRUD.delete_document(
            db=db,
            document_id=document_id,
            user=current_user
        )
        
        if not success:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Document not found or access denied"
            )
        
        # Schedule cleanup of vector embeddings
        background_tasks.add_task(cleanup_document_vectors, document_id)
        
        return {"message": "Document deleted successfully"}
        
    except PermissionError:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Insufficient permissions to delete document"
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error deleting document {document_id}: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Error deleting document"
        )

# ======================
# DOCUMENT CHUNKS ENDPOINTS
# ======================

@router.get("/{document_id}/chunks", response_model=List[ChunkInDB])
async def get_document_chunks(
    document_id: str,
    current_user: Annotated[User, Depends(RequireEmployee)],
    db: Annotated[AsyncSession, Depends(get_async_db)]
):
    """
    L·∫•y danh s√°ch chunks c·ªßa document
    """
    try:
        chunks = await ChunkCRUD.get_document_chunks(db, document_id, current_user)
        
        chunk_responses = []
        for chunk in chunks:
            chunk_responses.append(ChunkInDB(
                chunk_id=str(chunk.chunk_id),
                document_id=str(chunk.document_id),
                chunk_content=chunk.chunk_content,
                chunk_position=chunk.chunk_position,
                chunk_quality_score=float(chunk.chunk_quality_score) if chunk.chunk_quality_score else None,
                semantic_boundary=chunk.semantic_boundary,
                embedding_model=chunk.embedding_model,
                vector_id=chunk.vector_id,
                chunk_metadata=chunk.chunk_metadata or {},
                created_at=chunk.created_at,
                updated_at=chunk.updated_at
            ))
        
        return chunk_responses
        
    except Exception as e:
        logger.error(f"Error getting chunks for document {document_id}: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Error retrieving document chunks"
        )

@router.get("/{document_id}/with-chunks", response_model=DocumentWithChunks)
async def get_document_with_chunks(
    document_id: str,
    current_user: Annotated[User, Depends(RequireEmployee)],
    db: Annotated[AsyncSession, Depends(get_async_db)]
):
    """
    L·∫•y document c√πng v·ªõi chunks
    """
    try:
        # Get document
        document = await DocumentCRUD.get_document_by_id(db, document_id, current_user)
        if not document:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Document not found or access denied"
            )
        
        # Get chunks
        chunks = await ChunkCRUD.get_document_chunks(db, document_id, current_user)
        
        # Convert chunks to response format
        chunk_responses = []
        for chunk in chunks:
            chunk_responses.append(ChunkInDB(
                chunk_id=str(chunk.chunk_id),
                document_id=str(chunk.document_id),
                chunk_content=chunk.chunk_content,
                chunk_position=chunk.chunk_position,
                chunk_quality_score=float(chunk.chunk_quality_score) if chunk.chunk_quality_score else None,
                semantic_boundary=chunk.semantic_boundary,
                embedding_model=chunk.embedding_model,
                vector_id=chunk.vector_id,
                chunk_metadata=chunk.chunk_metadata or {},
                created_at=chunk.created_at,
                updated_at=chunk.updated_at
            ))
        
        # Create document response
        document_response = DocumentInDB(
            document_id=str(document.document_id),
            title=document.title,
            content=document.content,
            document_type=document.document_type,
            access_level=document.access_level,
            department_owner=document.department_owner,
            author=document.author,
            author_id=str(document.author_id) if document.author_id else None,
            status=document.status,
            version=document.version,
            language_detected=document.language_detected,
            tags=document.tags or [],
            file_size=document.file_size,
            file_hash=document.file_hash,
            chunk_count=document.chunk_count,
            view_count=document.view_count,
            download_count=document.download_count,
            created_at=document.created_at,
            updated_at=document.updated_at,
            metadata_json=document.metadata_json or {}
        )
        
        return DocumentWithChunks(**document_response.dict(), chunks=chunk_responses)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting document with chunks {document_id}: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Error retrieving document with chunks"
        )

# ======================
# SEARCH ENDPOINTS
# ======================

@router.post("/search", response_model=DocumentSearchResponse)
async def search_documents(
    search_request: DocumentSearchRequest,
    current_user: Annotated[User, Depends(RequireEmployee)],
    db: Annotated[AsyncSession, Depends(get_async_db)],
    redis_client: Annotated[redis.Redis, Depends(get_redis)]
):
    """
    T√¨m ki·∫øm documents
    """
    try:
        start_time = datetime.utcnow()
        
        # Check cache first (optional optimization)
        cache_key = f"search:{hash(search_request.query)}:{current_user.user_id}"
        cached_result = await redis_client.get(cache_key)
        
        if cached_result and search_request.search_type != "semantic":
            # Return cached result for non-semantic searches
            logger.info(f"Returning cached search result for query: {search_request.query}")
            return DocumentSearchResponse.parse_raw(cached_result)
        
        # Perform search
        search_results = await DocumentCRUD.search_documents(
            db=db,
            user=current_user,
            query=search_request.query,
            search_type=search_request.search_type,
            limit=search_request.limit,
            filters=search_request.filters
        )
        
        # Convert to response format
        results = []
        for document, relevance_score in search_results:
            if search_request.min_score and relevance_score < search_request.min_score:
                continue
            
            doc_response = DocumentInDB(
                document_id=str(document.document_id),
                title=document.title,
                content=document.content,
                document_type=document.document_type,
                access_level=document.access_level,
                department_owner=document.department_owner,
                author=document.author,
                author_id=str(document.author_id) if document.author_id else None,
                status=document.status,
                version=document.version,
                language_detected=document.language_detected,
                tags=document.tags or [],
                file_size=document.file_size,
                file_hash=document.file_hash,
                chunk_count=document.chunk_count,
                view_count=document.view_count,
                download_count=document.download_count,
                created_at=document.created_at,
                updated_at=document.updated_at,
                metadata_json=document.metadata_json or {}
            )
            
            # Get matching chunks if requested
            matching_chunks = None
            if search_request.include_chunks:
                chunks = await ChunkCRUD.get_document_chunks(db, str(document.document_id), current_user)
                # Filter chunks that contain search terms
                matching_chunks = []
                for chunk in chunks:
                    if search_request.query.lower() in chunk.chunk_content.lower():
                        matching_chunks.append(ChunkInDB(
                            chunk_id=str(chunk.chunk_id),
                            document_id=str(chunk.document_id),
                            chunk_content=chunk.chunk_content,
                            chunk_position=chunk.chunk_position,
                            chunk_quality_score=float(chunk.chunk_quality_score) if chunk.chunk_quality_score else None,
                            semantic_boundary=chunk.semantic_boundary,
                            embedding_model=chunk.embedding_model,
                            vector_id=chunk.vector_id,
                            chunk_metadata=chunk.chunk_metadata or {},
                            created_at=chunk.created_at,
                            updated_at=chunk.updated_at
                        ))
            
            # Generate highlight snippets
            highlight_snippets = generate_highlights(document.content or "", search_request.query)
            
            results.append(DocumentSearchResult(
                document=doc_response,
                relevance_score=relevance_score,
                matching_chunks=matching_chunks,
                highlight_snippets=highlight_snippets
            ))
        
        # Calculate search time
        search_time = (datetime.utcnow() - start_time).total_seconds() * 1000
        
        response = DocumentSearchResponse(
            query=search_request.query,
            search_type=search_request.search_type,
            results=results,
            total_found=len(results),
            search_time_ms=int(search_time),
            suggestions=generate_search_suggestions(search_request.query)
        )
        
        # Cache result (expire in 10 minutes)
        if search_request.search_type != "semantic":
            await redis_client.setex(cache_key, 600, response.json())
        
        return response
        
    except Exception as e:
        logger.error(f"Search error: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Error performing search"
        )

# ======================
# FILE UPLOAD ENDPOINTS
# ======================

@router.post("/upload", response_model=FileUploadResult)
async def upload_document(
    file: UploadFile = File(...),
    title: Optional[str] = None,
    document_type: DocumentType = DocumentType.OTHER,
    access_level: AccessLevel = AccessLevel.EMPLOYEE_ONLY,
    department: Optional[str] = None,
    tags: Optional[str] = None,  # Comma-separated tags
    current_user: Annotated[User, Depends(RequireEmployee)],
    db: Annotated[AsyncSession, Depends(get_async_db)],
    background_tasks: BackgroundTasks
):
    """
    Upload file v√† t·∫°o document
    """
    try:
        start_time = datetime.utcnow()
        
        # Validate file
        if not file.filename:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="No file provided"
            )
        
        # Check file size (10MB limit)
        file_content = await file.read()
        if len(file_content) > 10 * 1024 * 1024:  # 10MB
            raise HTTPException(
                status_code=status.HTTP_413_REQUEST_ENTITY_TOO_LARGE,
                detail="File too large. Maximum size is 10MB"
            )
        
        # Reset file pointer
        await file.seek(0)
        
        # Extract content based on file type
        content = extract_text_from_file(file_content, file.content_type)
        
        # Parse tags
        tag_list = []
        if tags:
            tag_list = [tag.strip() for tag in tags.split(",") if tag.strip()]
        
        # Create document
        document_data = DocumentCreate(
            title=title or file.filename,
            content=content,
            document_type=document_type,
            access_level=access_level,
            department_owner=department or current_user.department,
            author=current_user.full_name,
            source_file=file.filename,
            mime_type=file.content_type,
            file_content=content,
            tags=tag_list
        )
        
        document = await DocumentCRUD.create_document(
            db=db,
            document_create=document_data,
            author_id=str(current_user.user_id)
        )
        
        # Schedule background processing
        background_tasks.add_task(
            process_document_content,
            str(document.document_id),
            content
        )
        
        processing_time = (datetime.utcnow() - start_time).total_seconds() * 1000
        
        return FileUploadResult(
            success=True,
            message="File uploaded successfully",
            document_id=str(document.document_id),
            file_size=len(file_content),
            chunks_created=0,  # Will be updated after processing
            processing_time_ms=int(processing_time)
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"File upload error: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Error uploading file"
        )

@router.post("/bulk-upload", response_model=BulkUploadResponse)
async def bulk_upload_documents(
    upload_request: BulkUploadRequest,
    current_user: Annotated[User, Depends(RequireManager)],  # Manager level required
    db: Annotated[AsyncSession, Depends(get_async_db)],
    background_tasks: BackgroundTasks
):
    """
    Bulk upload documents (Manager tr·ªü l√™n)
    """
    try:
        start_time = datetime.utcnow()
        results = []
        successful_uploads = 0
        failed_uploads = 0
        
        for file_path in upload_request.files:
            try:
                # Process each file
                # This is a simplified version - in production, you'd handle actual file reading
                result = FileUploadResult(
                    success=True,
                    message=f"File {file_path} processed successfully",
                    document_id=str(uuid.uuid4()),
                    file_size=0,
                    chunks_created=0,
                    processing_time_ms=100
                )
                results.append(result)
                successful_uploads += 1
                
            except Exception as e:
                result = FileUploadResult(
                    success=False,
                    message=f"Failed to process {file_path}: {str(e)}",
                    document_id=None,
                    file_size=None,
                    chunks_created=None,
                    processing_time_ms=None
                )
                results.append(result)
                failed_uploads += 1
        
        total_processing_time = (datetime.utcnow() - start_time).total_seconds() * 1000
        
        return BulkUploadResponse(
            total_files=len(upload_request.files),
            successful_uploads=successful_uploads,
            failed_uploads=failed_uploads,
            results=results,
            total_processing_time_ms=int(total_processing_time)
        )
        
    except Exception as e:
        logger.error(f"Bulk upload error: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Error in bulk upload"
        )

# ======================
# STATISTICS ENDPOINTS
# ======================

@router.get("/stats/overview", response_model=DocumentStats)
async def get_document_statistics(
    current_user: Annotated[User, Depends(RequireEmployee)],
    db: Annotated[AsyncSession, Depends(get_async_db)]
):
    """
    L·∫•y th·ªëng k√™ documents
    """
    try:
        stats = await DocumentCRUD.get_document_statistics(db, current_user)
        
        return DocumentStats(
            total_documents=stats.get("total_documents", 0),
            documents_by_type=stats.get("documents_by_type", {}),
            documents_by_department=stats.get("documents_by_department", {}),
            documents_by_status=stats.get("documents_by_status", {}),
            documents_by_access_level=stats.get("documents_by_access_level", {}),
            recent_uploads=stats.get("recent_uploads", 0),
            total_chunks=stats.get("total_chunks", 0),
            average_chunks_per_document=stats.get("average_chunks_per_document", 0.0)
        )
        
    except Exception as e:
        logger.error(f"Error getting document statistics: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Error retrieving document statistics"
        )

# ======================
# ADMIN ENDPOINTS
# ======================

@router.post("/{document_id}/reprocess")
async def reprocess_document(
    document_id: str,
    current_user: Annotated[User, Depends(RequireManager)],
    db: Annotated[AsyncSession, Depends(get_async_db)],
    background_tasks: BackgroundTasks
):
    """
    Reprocess document (Manager tr·ªü l√™n)
    """
    try:
        # Check if document exists and user has access
        document = await DocumentCRUD.get_document_by_id(db, document_id, current_user)
        if not document:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Document not found or access denied"
            )
        
        # Schedule reprocessing
        background_tasks.add_task(
            reprocess_document_content,
            document_id,
            str(current_user.user_id)
        )
        
        return {"message": "Document reprocessing scheduled"}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error scheduling reprocessing for document {document_id}: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Error scheduling document reprocessing"
        )

@router.get("/{document_id}/processing-status", response_model=DocumentProcessingStatus)
async def get_processing_status(
    document_id: str,
    current_user: Annotated[User, Depends(RequireEmployee)],
    redis_client: Annotated[redis.Redis, Depends(get_redis)]
):
    """
    L·∫•y tr·∫°ng th√°i processing c·ªßa document
    """
    try:
        # Get status from Redis
        status_key = f"processing_status:{document_id}"
        status_data = await redis_client.get(status_key)
        
        if not status_data:
            return DocumentProcessingStatus(
                document_id=document_id,
                status="completed",
                progress_percentage=100
            )
        
        status_info = json.loads(status_data)
        
        return DocumentProcessingStatus(
            document_id=document_id,
            status=status_info.get("status", "unknown"),
            progress_percentage=status_info.get("progress", 0),
            current_step=status_info.get("current_step"),
            error_message=status_info.get("error"),
            started_at=datetime.fromisoformat(status_info["started_at"]) if status_info.get("started_at") else None,
            completed_at=datetime.fromisoformat(status_info["completed_at"]) if status_info.get("completed_at") else None,
            processing_time_ms=status_info.get("processing_time_ms")
        )
        
    except Exception as e:
        logger.error(f"Error getting processing status for document {document_id}: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Error retrieving processing status"
        )

# ======================
# HELPER FUNCTIONS
# ======================

def extract_text_from_file(file_content: bytes, content_type: str) -> str:
    """Extract text from uploaded file"""
    try:
        if content_type == "text/plain":
            return file_content.decode("utf-8")
        elif content_type == "application/pdf":
            # PDF extraction (implement with PyPDF2 or similar)
            return "PDF text extraction not implemented yet"
        elif content_type in ["application/msword", "application/vnd.openxmlformats-officedocument.wordprocessingml.document"]:
            # Word document extraction (implement with python-docx)
            return "Word document extraction not implemented yet"
        else:
            # Try to decode as text
            return file_content.decode("utf-8", errors="ignore")
    except Exception as e:
        logger.error(f"Error extracting text from file: {e}")
        return ""

def generate_highlights(content: str, query: str, max_snippets: int = 3) -> List[str]:
    """Generate highlight snippets from content"""
    if not content or not query:
        return []
    
    snippets = []
    words = query.lower().split()
    sentences = content.split(".")
    
    for sentence in sentences:
        if len(snippets) >= max_snippets:
            break
        
        sentence_lower = sentence.lower()
        if any(word in sentence_lower for word in words):
            # Truncate long sentences
            if len(sentence) > 200:
                sentence = sentence[:200] + "..."
            snippets.append(sentence.strip())
    
    return snippets

def generate_search_suggestions(query: str) -> List[str]:
    """Generate search suggestions"""
    # Simple suggestion logic - can be enhanced with ML
    suggestions = []
    
    if len(query) < 3:
        return suggestions
    
    # Common document-related suggestions
    common_terms = [
        "policy", "procedure", "guide", "manual", "report",
        "specification", "template", "form", "training"
    ]
    
    for term in common_terms:
        if term.startswith(query.lower()):
            suggestions.append(term)
    
    return suggestions[:5]

# ======================
# BACKGROUND TASKS
# ======================

async def process_document_content(document_id: str, content: str):
    """Background task to process document content"""
    try:
        logger.info(f"Starting content processing for document {document_id}")
        
        # This would integrate with document processing service
        # For now, just a placeholder
        await asyncio.sleep(1)  # Simulate processing
        
        logger.info(f"Content processing completed for document {document_id}")
        
    except Exception as e:
        logger.error(f"Error processing content for document {document_id}: {e}")

async def cleanup_document_vectors(document_id: str):
    """Background task to cleanup vector embeddings"""
    try:
        logger.info(f"Cleaning up vectors for document {document_id}")
        
        # This would integrate with ChromaDB to remove vectors
        # For now, just a placeholder
        await asyncio.sleep(1)  # Simulate cleanup
        
        logger.info(f"Vector cleanup completed for document {document_id}")
        
    except Exception as e:
        logger.error(f"Error cleaning up vectors for document {document_id}: {e}")

async def reprocess_document_content(document_id: str, user_id: str):
    """Background task to reprocess document"""
    try:
        logger.info(f"Starting reprocessing for document {document_id} by user {user_id}")
        
        # This would integrate with document processing pipeline
        # For now, just a placeholder
        await asyncio.sleep(2)  # Simulate reprocessing
        
        logger.info(f"Reprocessing completed for document {document_id}")
        
    except Exception as e:
        logger.error(f"Error reprocessing document {document_id}: {e}")
```

## üîó **5. UPDATE API ROUTER**

### **app/api/api_v1.py (Updated)**
```python
"""
Main API router cho FR-02.2 (Updated)
"""
from fastapi import APIRouter

from app.api.endpoints import auth, documents

api_router = APIRouter()

# Include authentication routes
api_router.include_router(auth.router, tags=["Authentication"])

# Include documents routes  
api_router.include_router(documents.router, tags=["Documents"])

# Health check route
@api_router.get("/status")
async def api_status():
    """API status endpoint"""
    return {
        "status": "operational",
        "api_version": "v1",
        "module": "FR-02.2 - API Qu·∫£n tr·ªã Th·ªëng nh·∫•t",
        "endpoints": {
            "authentication": "enabled",
            "documents": "enabled",
            "search": "enabled"
        }
    }
```

## üß™ **6. TESTING**

### **tests/test_documents_api.py**
```python
"""
Tests cho Documents API
"""
import pytest
from fastapi.testclient import TestClient
from sqlalchemy.ext.asyncio import AsyncSession
import uuid

from app.main import app
from app.models.user import User, UserLevel
from app.models.document import DocumentMetadataV2, DocumentType, AccessLevel, DocumentStatus
from app.crud.user import UserCRUD
from app.crud.document import DocumentCRUD
from app.schemas.auth import UserCreate
from app.schemas.document import DocumentCreate

@pytest.fixture
def client():
    return TestClient(app)

@pytest.fixture
async def test_user_employee(db_session: AsyncSession):
    """Create test employee user"""
    user_data = UserCreate(
        username="employee",
        email="employee@example.com", 
        full_name="Test Employee",
        password="TestPassword123!",
        confirm_password="TestPassword123!",
        user_level=UserLevel.EMPLOYEE,
        department="IT"
    )
    
    user = await UserCRUD.create_user(db_session, user_data)
    return user

@pytest.fixture
async def test_user_manager(db_session: AsyncSession):
    """Create test manager user"""
    user_data = UserCreate(
        username="manager",
        email="manager@example.com",
        full_name="Test Manager", 
        password="TestPassword123!",
        confirm_password="TestPassword123!",
        user_level=UserLevel.MANAGER,
        department="IT"
    )
    
    user = await UserCRUD.create_user(db_session, user_data)
    return user

@pytest.fixture
async def test_document(db_session: AsyncSession, test_user_employee: User):
    """Create test document"""
    doc_data = DocumentCreate(
        title="Test Document",
        content="This is test content for the document.",
        document_type=DocumentType.TECHNICAL_GUIDE,
        access_level=AccessLevel.EMPLOYEE_ONLY,
        department_owner="IT",
        author="Test Employee",
        tags=["test", "document"]
    )
    
    document = await DocumentCRUD.create_document(
        db_session, 
        doc_data, 
        str(test_user_employee.user_id)
    )
    return document

def get_auth_header(client: TestClient, username: str, password: str):
    """Helper to get auth header"""
    response = client.post("/api/v1/auth/login", json={
        "username": username,
        "password": password
    })
    assert response.status_code == 200
    token = response.json()["access_token"]
    return {"Authorization": f"Bearer {token}"}

class TestDocumentsAPI:
    """Test documents API endpoints"""
    
    def test_create_document(self, client: TestClient, test_user_employee: User):
        """Test document creation"""
        headers = get_auth_header(client, "employee", "TestPassword123!")
        
        document_data = {
            "title": "New Test Document",
            "content": "This is new test content.",
            "document_type": "technical_guide",
            "access_level": "employee_only",
            "department_owner": "IT",
            "tags": ["new", "test"]
        }
        
        response = client.post("/api/v1/documents/", 
                              json=document_data, 
                              headers=headers)
        
        assert response.status_code == 201
        data = response.json()
        assert data["title"] == "New Test Document"
        assert data["document_type"] == "technical_guide"
        assert "document_id" in data
    
    def test_list_documents(self, client: TestClient, test_user_employee: User, test_document: DocumentMetadataV2):
        """Test document listing"""
        headers = get_auth_header(client, "employee", "TestPassword123!")
        
        response = client.get("/api/v1/documents/", headers=headers)
        
        assert response.status_code == 200
        data = response.json()
        assert "documents" in data
        assert data["total"] > 0
        assert len(data["documents"]) > 0
    
    def test_get_document(self, client: TestClient, test_user_employee: User, test_document: DocumentMetadataV2):
        """Test get specific document"""
        headers = get_auth_header(client, "employee", "TestPassword123!")
        
        response = client.get(f"/api/v1/documents/{test_document.document_id}", 
                             headers=headers)
        
        assert response.status_code == 200
        data = response.json()
        assert data["document_id"] == str(test_document.document_id)
        assert data["title"] == test_document.title
    
    def test_update_document(self, client: TestClient, test_user_employee: User, test_document: DocumentMetadataV2):
        """Test document update"""
        headers = get_auth_header(client, "employee", "TestPassword123!")
        
        update_data = {
            "title": "Updated Test Document",
            "tags": ["updated", "test"]
        }
        
        response = client.put(f"/api/v1/documents/{test_document.document_id}",
                             json=update_data,
                             headers=headers)
        
        assert response.status_code == 200
        data = response.json()
        assert data["title"] == "Updated Test Document"
    
    def test_delete_document(self, client: TestClient, test_user_employee: User, test_document: DocumentMetadataV2):
        """Test document deletion"""
        headers = get_auth_header(client, "employee", "TestPassword123!")
        
        response = client.delete(f"/api/v1/documents/{test_document.document_id}",
                                headers=headers)
        
        # Employee can delete their own draft documents
        if test_document.status == DocumentStatus.DRAFT:
            assert response.status_code == 200
            assert "deleted successfully" in response.json()["message"]
        else:
            # Published documents need higher permissions
            assert response.status_code == 403
    
    def test_search_documents(self, client: TestClient, test_user_employee: User, test_document: DocumentMetadataV2):
        """Test document search"""
        headers = get_auth_header(client, "employee", "TestPassword123!")
        
        search_data = {
            "query": "test",
            "search_type": "keyword",
            "limit": 10
        }
        
        response = client.post("/api/v1/documents/search",
                              json=search_data,
                              headers=headers)
        
        assert response.status_code == 200
        data = response.json()
        assert "results" in data
        assert data["query"] == "test"
        assert isinstance(data["search_time_ms"], int)
    
    def test_get_document_chunks(self, client: TestClient, test_user_employee: User, test_document: DocumentMetadataV2):
        """Test get document chunks"""
        headers = get_auth_header(client, "employee", "TestPassword123!")
        
        response = client.get(f"/api/v1/documents/{test_document.document_id}/chunks",
                             headers=headers)
        
        assert response.status_code == 200
        data = response.json()
        assert isinstance(data, list)
    
    def test_get_document_with_chunks(self, client: TestClient, test_user_employee: User, test_document: DocumentMetadataV2):
        """Test get document with chunks"""
        headers = get_auth_header(client, "employee", "TestPassword123!")
        
        response = client.get(f"/api/v1/documents/{test_document.document_id}/with-chunks",
                             headers=headers)
        
        assert response.status_code == 200
        data = response.json()
        assert "document_id" in data
        assert "chunks" in data
        assert isinstance(data["chunks"], list)
    
    def test_document_statistics(self, client: TestClient, test_user_employee: User):
        """Test document statistics"""
        headers = get_auth_header(client, "employee", "TestPassword123!")
        
        response = client.get("/api/v1/documents/stats/overview",
                             headers=headers)
        
        assert response.status_code == 200
        data = response.json()
        assert "total_documents" in data
        assert "documents_by_type" in data
        assert "documents_by_department" in data

class TestDocumentAccess:
    """Test document access control"""
    
    def test_access_control_by_level(self, client: TestClient, test_user_employee: User):
        """Test access control based on user level"""
        headers = get_auth_header(client, "employee", "TestPassword123!")
        
        # Try to create director-only document (should fail)
        doc_data = {
            "title": "Director Only Document",
            "content": "Secret content",
            "document_type": "policy",
            "access_level": "director_only"
        }
        
        response = client.post("/api/v1/documents/",
                              json=doc_data,
                              headers=headers)
        
        # Employee can create but access will be controlled on read
        if response.status_code == 201:
            doc_id = response.json()["document_id"]
            
            # Try to read back (should fail for director-only)
            read_response = client.get(f"/api/v1/documents/{doc_id}",
                                      headers=headers)
            # Access might be denied based on implementation
    
    def test_department_access_control(self, client: TestClient, test_user_employee: User):
        """Test department-based access control"""
        headers = get_auth_header(client, "employee", "TestPassword123!")
        
        # Create document for different department
        doc_data = {
            "title": "HR Document",
            "content": "HR specific content",
            "document_type": "policy",
            "access_level": "employee_only",
            "department_owner": "HR"
        }
        
        response = client.post("/api/v1/documents/",
                              json=doc_data,
                              headers=headers)
        
        # Check if user can access documents from other departments
        assert response.status_code in [201, 403]  # Depends on implementation

class TestDocumentFiltering:
    """Test document filtering and sorting"""
    
    def test_filter_by_type(self, client: TestClient, test_user_employee: User):
        """Test filtering by document type"""
        headers = get_auth_header(client, "employee", "TestPassword123!")
        
        response = client.get("/api/v1/documents/?document_type=technical_guide",
                             headers=headers)
        
        assert response.status_code == 200
        data = response.json()
        
        # All returned documents should be technical_guide
        for doc in data["documents"]:
            assert doc["document_type"] == "technical_guide"
    
    def test_filter_by_department(self, client: TestClient, test_user_employee: User):
        """Test filtering by department"""
        headers = get_auth_header(client, "employee", "TestPassword123!")
        
        response = client.get("/api/v1/documents/?department=IT",
                             headers=headers)
        
        assert response.status_code == 200
        data = response.json()
        
        # All returned documents should be from IT department
        for doc in data["documents"]:
            assert doc["department_owner"] == "IT" or doc["department_owner"] is None
    
    def test_search_with_filters(self, client: TestClient, test_user_employee: User):
        """Test search with additional filters"""
        headers = get_auth_header(client, "employee", "TestPassword123!")
        
        search_data = {
            "query": "test",
            "search_type": "keyword",
            "filters": {
                "document_type": "technical_guide",
                "status": "approved"
            },
            "limit": 10
        }
        
        response = client.post("/api/v1/documents/search",
                              json=search_data,
                              headers=headers)
        
        assert response.status_code == 200
        data = response.json()
        assert "results" in data
        
        # Check that filters are applied
        for result in data["results"]:
            doc = result["document"]
            assert doc["document_type"] == "technical_guide"
    
    def test_pagination(self, client: TestClient, test_user_employee: User):
        """Test pagination"""
        headers = get_auth_header(client, "employee", "TestPassword123!")
        
        # Get first page
        response1 = client.get("/api/v1/documents/?skip=0&limit=5",
                              headers=headers)
        assert response1.status_code == 200
        data1 = response1.json()
        
        # Get second page
        response2 = client.get("/api/v1/documents/?skip=5&limit=5",
                              headers=headers)
        assert response2.status_code == 200
        data2 = response2.json()
        
        # Check pagination info
        assert "page" in data1
        assert "total" in data1
        assert "has_next" in data1
        assert "has_prev" in data1

class TestFileUpload:
    """Test file upload functionality"""
    
    def test_upload_text_file(self, client: TestClient, test_user_employee: User):
        """Test uploading text file"""
        headers = get_auth_header(client, "employee", "TestPassword123!")
        
        # Create test file content
        file_content = "This is test file content for upload."
        
        files = {
            "file": ("test.txt", file_content, "text/plain")
        }
        
        data = {
            "title": "Uploaded Test Document",
            "document_type": "technical_guide",
            "access_level": "employee_only",
            "tags": "upload,test"
        }
        
        response = client.post("/api/v1/documents/upload",
                              files=files,
                              data=data,
                              headers=headers)
        
        assert response.status_code == 200
        result = response.json()
        assert result["success"] is True
        assert "document_id" in result
        assert result["file_size"] > 0
    
    def test_upload_file_too_large(self, client: TestClient, test_user_employee: User):
        """Test uploading file that's too large"""
        headers = get_auth_header(client, "employee", "TestPassword123!")
        
        # Create large file content (>10MB)
        large_content = "x" * (11 * 1024 * 1024)  # 11MB
        
        files = {
            "file": ("large.txt", large_content, "text/plain")
        }
        
        response = client.post("/api/v1/documents/upload",
                              files=files,
                              headers=headers)
        
        assert response.status_code == 413  # Request Entity Too Large

class TestManagerFeatures:
    """Test features requiring manager permissions"""
    
    def test_bulk_upload_requires_manager(self, client: TestClient, test_user_employee: User):
        """Test that bulk upload requires manager permissions"""
        headers = get_auth_header(client, "employee", "TestPassword123!")
        
        bulk_data = {
            "files": ["file1.txt", "file2.txt"],
            "default_department": "IT",
            "auto_process": True
        }
        
        response = client.post("/api/v1/documents/bulk-upload",
                              json=bulk_data,
                              headers=headers)
        
        assert response.status_code == 403  # Forbidden for employee
    
    def test_reprocess_document_requires_manager(self, client: TestClient, test_user_employee: User, test_document: DocumentMetadataV2):
        """Test that reprocessing requires manager permissions"""
        headers = get_auth_header(client, "employee", "TestPassword123!")
        
        response = client.post(f"/api/v1/documents/{test_document.document_id}/reprocess",
                              headers=headers)
        
        assert response.status_code == 403  # Forbidden for employee
    
    def test_manager_can_bulk_upload(self, client: TestClient, test_user_manager: User):
        """Test that manager can perform bulk upload"""
        headers = get_auth_header(client, "manager", "TestPassword123!")
        
        bulk_data = {
            "files": ["file1.txt", "file2.txt"],
            "default_department": "IT",
            "auto_process": True
        }
        
        response = client.post("/api/v1/documents/bulk-upload",
                              json=bulk_data,
                              headers=headers)
        
        assert response.status_code == 200
        data = response.json()
        assert "total_files" in data
        assert data["total_files"] == 2

class TestDocumentSecurity:
    """Test document security features"""
    
    def test_unauthorized_access(self, client: TestClient):
        """Test access without authentication"""
        response = client.get("/api/v1/documents/")
        assert response.status_code == 401
    
    def test_invalid_document_id(self, client: TestClient, test_user_employee: User):
        """Test access with invalid document ID"""
        headers = get_auth_header(client, "employee", "TestPassword123!")
        
        invalid_id = str(uuid.uuid4())
        response = client.get(f"/api/v1/documents/{invalid_id}",
                             headers=headers)
        
        assert response.status_code == 404
    
    def test_malformed_document_id(self, client: TestClient, test_user_employee: User):
        """Test access with malformed document ID"""
        headers = get_auth_header(client, "employee", "TestPassword123!")
        
        response = client.get("/api/v1/documents/invalid-id",
                             headers=headers)
        
        assert response.status_code == 422  # Validation error
```

## üöÄ **7. INTEGRATION SCRIPT**

### **scripts/test_documents_integration.py**
```python
"""
Integration test script cho Documents API
"""
import asyncio
import sys
import os
import httpx
import json
from datetime import datetime

# Add app to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

async def test_documents_api():
    """Test complete Documents API integration"""
    
    base_url = "http://localhost:8000"
    
    async with httpx.AsyncClient() as client:
        print("üß™ Testing FR-02.2 Documents API Integration")
        print("=" * 60)
        
        # Step 1: Authentication
        print("1. Authenticating...")
        
        # Login as employee
        employee_login = {
            "username": "employee",
            "password": "TestPassword123!",
            "remember_me": False
        }
        
        response = await client.post(f"{base_url}/api/v1/auth/login", json=employee_login)
        
        if response.status_code != 200:
            print("‚ùå Employee login failed. Please ensure test users are created.")
            return False
        
        employee_token = response.json()["access_token"]
        employee_headers = {"Authorization": f"Bearer {employee_token}"}
        print("‚úÖ Employee authenticated")
        
        # Step 2: Create test document
        print("2. Creating test document...")
        
        document_data = {
            "title": "Test Document for Integration",
            "content": "This is a comprehensive test document for the FR-02.2 Documents API. It contains various information for testing search and retrieval functionality.",
            "document_type": "technical_guide",
            "access_level": "employee_only",
            "department_owner": "IT",
            "tags": ["test", "integration", "api", "vietnamese"],
            "language_detected": "vi"
        }
        
        response = await client.post(
            f"{base_url}/api/v1/documents/",
            json=document_data,
            headers=employee_headers
        )
        
        if response.status_code != 201:
            print(f"‚ùå Document creation failed: {response.status_code}")
            print(response.text)
            return False
        
        document = response.json()
        document_id = document["document_id"]
        print(f"‚úÖ Document created: {document['title']}")
        print(f"   ID: {document_id}")
        
        # Step 3: Test document retrieval
        print("3. Testing document retrieval...")
        
        response = await client.get(
            f"{base_url}/api/v1/documents/{document_id}",
            headers=employee_headers
        )
        
        if response.status_code != 200:
            print(f"‚ùå Document retrieval failed: {response.status_code}")
            return False
        
        retrieved_doc = response.json()
        print(f"‚úÖ Document retrieved: {retrieved_doc['title']}")
        print(f"   Type: {retrieved_doc['document_type']}")
        print(f"   Access Level: {retrieved_doc['access_level']}")
        print(f"   View Count: {retrieved_doc['view_count']}")
        
        # Step 4: Test document listing
        print("4. Testing document listing...")
        
        response = await client.get(
            f"{base_url}/api/v1/documents/?limit=10",
            headers=employee_headers
        )
        
        if response.status_code != 200:
            print(f"‚ùå Document listing failed: {response.status_code}")
            return False
        
        documents_list = response.json()
        print(f"‚úÖ Document listing successful")
        print(f"   Total documents: {documents_list['total']}")
        print(f"   Page: {documents_list['page']}")
        print(f"   Documents in response: {len(documents_list['documents'])}")
        
        # Step 5: Test filtering
        print("5. Testing document filtering...")
        
        response = await client.get(
            f"{base_url}/api/v1/documents/?document_type=technical_guide&department=IT",
            headers=employee_headers
        )
        
        if response.status_code != 200:
            print(f"‚ùå Document filtering failed: {response.status_code}")
            return False
        
        filtered_docs = response.json()
        print(f"‚úÖ Document filtering successful")
        print(f"   Filtered documents: {len(filtered_docs['documents'])}")
        
        # Step 6: Test search functionality
        print("6. Testing search functionality...")
        
        search_data = {
            "query": "test integration",
            "search_type": "keyword",
            "limit": 5,
            "include_chunks": False
        }
        
        response = await client.post(
            f"{base_url}/api/v1/documents/search",
            json=search_data,
            headers=employee_headers
        )
        
        if response.status_code != 200:
            print(f"‚ùå Document search failed: {response.status_code}")
            return False
        
        search_results = response.json()
        print(f"‚úÖ Document search successful")
        print(f"   Query: {search_results['query']}")
        print(f"   Results found: {search_results['total_found']}")
        print(f"   Search time: {search_results['search_time_ms']}ms")
        
        if search_results["results"]:
            first_result = search_results["results"][0]
            print(f"   Top result: {first_result['document']['title']}")
            print(f"   Relevance: {first_result['relevance_score']}")
        
        # Step 7: Test document chunks
        print("7. Testing document chunks...")
        
        response = await client.get(
            f"{base_url}/api/v1/documents/{document_id}/chunks",
            headers=employee_headers
        )
        
        if response.status_code != 200:
            print(f"‚ùå Chunks retrieval failed: {response.status_code}")
            return False
        
        chunks = response.json()
        print(f"‚úÖ Chunks retrieval successful")
        print(f"   Number of chunks: {len(chunks)}")
        
        # Step 8: Test document with chunks
        print("8. Testing document with chunks...")
        
        response = await client.get(
            f"{base_url}/api/v1/documents/{document_id}/with-chunks",
            headers=employee_headers
        )
        
        if response.status_code != 200:
            print(f"‚ùå Document with chunks failed: {response.status_code}")
            return False
        
        doc_with_chunks = response.json()
        print(f"‚úÖ Document with chunks successful")
        print(f"   Document: {doc_with_chunks['title']}")
        print(f"   Chunks included: {len(doc_with_chunks['chunks'])}")
        
        # Step 9: Test document update
        print("9. Testing document update...")
        
        update_data = {
            "title": "Updated Test Document for Integration",
            "tags": ["test", "integration", "api", "vietnamese", "updated"]
        }
        
        response = await client.put(
            f"{base_url}/api/v1/documents/{document_id}",
            json=update_data,
            headers=employee_headers
        )
        
        if response.status_code != 200:
            print(f"‚ùå Document update failed: {response.status_code}")
            return False
        
        updated_doc = response.json()
        print(f"‚úÖ Document update successful")
        print(f"   New title: {updated_doc['title']}")
        print(f"   New tags: {updated_doc['tags']}")
        
        # Step 10: Test statistics
        print("10. Testing document statistics...")
        
        response = await client.get(
            f"{base_url}/api/v1/documents/stats/overview",
            headers=employee_headers
        )
        
        if response.status_code != 200:
            print(f"‚ùå Statistics failed: {response.status_code}")
            return False
        
        stats = response.json()
        print(f"‚úÖ Statistics successful")
        print(f"   Total documents: {stats['total_documents']}")
        print(f"   Documents by type: {stats['documents_by_type']}")
        print(f"   Recent uploads: {stats['recent_uploads']}")
        
        # Step 11: Test file upload (text file)
        print("11. Testing file upload...")
        
        # Create test file content
        test_content = "This is a test file content for upload testing. It contains Vietnamese text: ƒê√¢y l√† n·ªôi dung ti·∫øng Vi·ªát ƒë·ªÉ test."
        
        files = {
            "file": ("test_upload.txt", test_content, "text/plain")
        }
        
        data = {
            "title": "Uploaded Test File",
            "document_type": "technical_guide",
            "access_level": "employee_only",
            "tags": "upload,test,file"
        }
        
        response = await client.post(
            f"{base_url}/api/v1/documents/upload",
            files=files,
            data=data,
            headers=employee_headers
        )
        
        if response.status_code != 200:
            print(f"‚ùå File upload failed: {response.status_code}")
            print(response.text)
        else:
            upload_result = response.json()
            print(f"‚úÖ File upload successful")
            print(f"   Document ID: {upload_result['document_id']}")
            print(f"   File size: {upload_result['file_size']} bytes")
        
        # Step 12: Test access control (try to access as different user level)
        print("12. Testing access control...")
        
        # Create a director-only document
        director_doc_data = {
            "title": "Director Only Document",
            "content": "This document should only be accessible to directors and above.",
            "document_type": "policy",
            "access_level": "director_only",
            "department_owner": "IT"
        }
        
        response = await client.post(
            f"{base_url}/api/v1/documents/",
            json=director_doc_data,
            headers=employee_headers
        )
        
        if response.status_code == 201:
            director_doc_id = response.json()["document_id"]
            
            # Try to access as employee (should work for creation but access control on read)
            response = await client.get(
                f"{base_url}/api/v1/documents/{director_doc_id}",
                headers=employee_headers
            )
            
            print(f"‚úÖ Access control test completed")
            print(f"   Director document access: {response.status_code}")
        
        # Step 13: Test search with filters
        print("13. Testing advanced search with filters...")
        
        advanced_search = {
            "query": "test",
            "search_type": "hybrid",
            "filters": {
                "document_type": "technical_guide",
                "department_owner": "IT",
                "tags": ["test"]
            },
            "limit": 5,
            "include_chunks": True
        }
        
        response = await client.post(
            f"{base_url}/api/v1/documents/search",
            json=advanced_search,
            headers=employee_headers
        )
        
        if response.status_code != 200:
            print(f"‚ùå Advanced search failed: {response.status_code}")
        else:
            advanced_results = response.json()
            print(f"‚úÖ Advanced search successful")
            print(f"   Filtered results: {advanced_results['total_found']}")
            
            if advanced_results["results"]:
                result_with_chunks = advanced_results["results"][0]
                print(f"   Chunks included: {len(result_with_chunks.get('matching_chunks', []))}")
        
        # Step 14: Test processing status
        print("14. Testing processing status...")
        
        response = await client.get(
            f"{base_url}/api/v1/documents/{document_id}/processing-status",
            headers=employee_headers
        )
        
        if response.status_code != 200:
            print(f"‚ùå Processing status failed: {response.status_code}")
        else:
            status = response.json()
            print(f"‚úÖ Processing status successful")
            print(f"   Status: {status['status']}")
            print(f"   Progress: {status['progress_percentage']}%")
        
        print("\n" + "=" * 60)
        print("üéâ ALL DOCUMENTS API TESTS PASSED!")
        print("=" * 60)
        
        # Summary
        print("\nüìã INTEGRATION TEST SUMMARY:")
        print("‚úÖ Document CRUD operations working")
        print("‚úÖ Search functionality operational")
        print("‚úÖ File upload system working")
        print("‚úÖ Access control enforced")
        print("‚úÖ Filtering and pagination working")
        print("‚úÖ Chunks management operational")
        print("‚úÖ Statistics and monitoring working")
        print("\nüöÄ Documents API is ready for production!")
        
        return True

async def test_edge_cases():
    """Test edge cases and error scenarios"""
    print("\nüîç Testing Edge Cases...")
    
    base_url = "http://localhost:8000"
    
    async with httpx.AsyncClient() as client:
        # Test unauthorized access
        print("1. Testing unauthorized access...")
        response = await client.get(f"{base_url}/api/v1/documents/")
        assert response.status_code == 401
        print("‚úÖ Unauthorized access properly rejected")
        
        # Test invalid document ID
        print("2. Testing invalid document ID...")
        # Get token first
        login_response = await client.post(f"{base_url}/api/v1/auth/login", json={
            "username": "employee",
            "password": "TestPassword123!"
        })
        
        if login_response.status_code == 200:
            token = login_response.json()["access_token"]
            headers = {"Authorization": f"Bearer {token}"}
            
            response = await client.get(
                f"{base_url}/api/v1/documents/invalid-uuid",
                headers=headers
            )
            assert response.status_code == 422  # Validation error
            print("‚úÖ Invalid document ID properly rejected")
            
            # Test non-existent document
            response = await client.get(
                f"{base_url}/api/v1/documents/00000000-0000-0000-0000-000000000000",
                headers=headers
            )
            assert response.status_code == 404
            print("‚úÖ Non-existent document properly handled")
        
        print("‚úÖ Edge cases testing completed")

async def main():
    """Main test runner"""
    print("üöÄ FR-02.2 Documents API Integration Test Suite")
    print("=" * 70)
    
    try:
        # Test main functionality
        success = await test_documents_api()
        
        if success:
            # Test edge cases
            await test_edge_cases()
            
            print("\nüéâ ALL TESTS COMPLETED SUCCESSFULLY!")
            print("Documents API is fully operational and ready for production use.")
            return True
        else:
            print("\n‚ùå Some tests failed. Please check the logs.")
            return False
            
    except Exception as e:
        print(f"\n‚ùå Test suite failed with error: {e}")
        return False

if __name__ == "__main__":
    result = asyncio.run(main())
    sys.exit(0 if result else 1)
```

## üìã **8. UPDATE DATABASE MIGRATION**

### **alembic/versions/002_add_documents_tables.py**
```python
"""
Add documents tables from FR-02.1

Revision ID: 002_documents_tables
Revises: 001_auth_tables
Create Date: 2025-09-05 15:00:00.000000
"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql
import uuid

# revision identifiers
revision = '002_documents_tables'
down_revision = '001_auth_tables'
branch_labels = None
depends_on = None

def upgrade() -> None:
    """Create documents tables matching FR-02.1 schema"""
    
    # Create document enums
    document_type_enum = sa.Enum(
        'policy', 'procedure', 'technical_guide', 'report', 'manual',
        'specification', 'template', 'form', 'presentation', 'training_material', 'other',
        name='documenttype'
    )
    document_type_enum.create(op.get_bind())
    
    access_level_enum = sa.Enum(
        'public', 'employee_only', 'manager_only', 'director_only', 'system_admin',
        name='accesslevel'
    )
    access_level_enum.create(op.get_bind())
    
    document_status_enum = sa.Enum(
        'draft', 'review', 'approved', 'published', 'archived', 'deprecated',
        name='documentstatus'
    )
    document_status_enum.create(op.get_bind())
    
    pipeline_type_enum = sa.Enum(
        'standard', 'reasoning', 'conditional', 'adaptive',
        name='pipelinetype'
    )
    pipeline_type_enum.create(op.get_bind())
    
    retrieval_method_enum = sa.Enum(
        'dense', 'sparse', 'hybrid', 'knowledge_graph', 'multi_modal',
        name='retrievalmethod'
    )
    retrieval_method_enum.create(op.get_bind())
    
    # Create main documents table
    op.create_table(
        'documents_metadata_v2',
        sa.Column('document_id', postgresql.UUID(as_uuid=True), primary_key=True, default=uuid.uuid4),
        sa.Column('title', sa.String(500), nullable=False, index=True),
        sa.Column('content', sa.Text, nullable=True),
        sa.Column('document_type', document_type_enum, nullable=False, index=True),
        sa.Column('access_level', access_level_enum, nullable=False, default='employee_only', index=True),
        sa.Column('department_owner', sa.String(100), nullable=True, index=True),
        sa.Column('author', sa.String(255), nullable=True, index=True),
        sa.Column('author_id', postgresql.UUID(as_uuid=True), nullable=True, index=True),
        sa.Column('status', document_status_enum, nullable=False, default='draft', index=True),
        sa.Column('version', sa.String(20), nullable=True, default='1.0'),
        sa.Column('source_file', sa.String(500), nullable=True),
        sa.Column('file_size', sa.Integer, nullable=True),
        sa.Column('file_hash', sa.String(64), nullable=True),
        sa.Column('mime_type', sa.String(100), nullable=True),
        sa.Column('language_detected', sa.String(10), nullable=True, default='vi'),
        sa.Column('search_tokens', postgresql.TSVECTOR, nullable=True),
        sa.Column('tags', postgresql.JSONB, nullable=True, default=[]),
        sa.Column('chunk_count', sa.Integer, nullable=True, default=0),
        sa.Column('view_count', sa.Integer, nullable=True, default=0),
        sa.Column('download_count', sa.Integer, nullable=True, default=0),
        sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.func.now(), index=True),
        sa.Column('updated_at', sa.DateTime(timezone=True), onupdate=sa.func.now()),
        sa.Column('metadata_json', postgresql.JSONB, nullable=True, default={}),
        sa.ForeignKeyConstraint(['author_id'], ['users.user_id'], ),
    )
    
    # Create document chunks table
    op.create_table(
        'document_chunks_enhanced',
        sa.Column('chunk_id', postgresql.UUID(as_uuid=True), primary_key=True, default=uuid.uuid4),
        sa.Column('document_id', postgresql.UUID(as_uuid=True), nullable=False, index=True),
        sa.Column('chunk_content', sa.Text, nullable=False),
        sa.Column('chunk_position', sa.Integer, nullable=False, default=0),
        sa.Column('chunk_quality_score', sa.DECIMAL(3, 2), nullable=True),
        sa.Column('semantic_boundary', sa.Boolean, nullable=True, default=False),
        sa.Column('embedding_model', sa.String(100), nullable=True),
        sa.Column('vector_id', sa.String(255), nullable=True),
        sa.Column('chunk_metadata', postgresql.JSONB, nullable=True, default={}),
        sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.func.now()),
        sa.Column('updated_at', sa.DateTime(timezone=True), onupdate=sa.func.now()),
        sa.ForeignKeyConstraint(['document_id'], ['documents_metadata_v2.document_id'], ondelete='CASCADE'),
    )
    
    # Create RAG pipeline sessions table
    op.create_table(
        'rag_pipeline_sessions',
        sa.Column('session_id', postgresql.UUID(as_uuid=True), primary_key=True, default=uuid.uuid4),
        sa.Column('user_id', postgresql.UUID(as_uuid=True), nullable=True, index=True),
        sa.Column('original_query', sa.Text, nullable=False),
        sa.Column('processed_query', sa.Text, nullable=True),
        sa.Column('pipeline_type', pipeline_type_enum, nullable=False, default='standard'),
        sa.Column('retrieval_method', retrieval_method_enum, nullable=False, default='hybrid'),
        sa.Column('retrieved_documents', postgresql.JSONB, nullable=True, default=[]),
        sa.Column('response_content', sa.Text, nullable=True),
        sa.Column('response_quality_score', sa.DECIMAL(3, 2), nullable=True),
        sa.Column('processing_time_ms', sa.Integer, nullable=True),
        sa.Column('retrieval_time_ms', sa.Integer, nullable=True),
        sa.Column('generation_time_ms', sa.Integer, nullable=True),
        sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.func.now(), index=True),
        sa.ForeignKeyConstraint(['user_id'], ['users.user_id'], ),
    )
    
    # Create BM25 index table
    op.create_table(
        'document_bm25_index',
        sa.Column('index_id', postgresql.UUID(as_uuid=True), primary_key=True, default=uuid.uuid4),
        sa.Column('document_id', postgresql.UUID(as_uuid=True), nullable=False, index=True),
        sa.Column('term', sa.String(100), nullable=False, index=True),
        sa.Column('term_frequency', sa.Integer, nullable=False, default=0),
        sa.Column('document_frequency', sa.Integer, nullable=False, default=0),
        sa.Column('bm25_score', sa.DECIMAL(10, 6), nullable=True),
        sa.Column('language', sa.String(10), nullable=False, default='vi'),
        sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.func.now()),
        sa.ForeignKeyConstraint(['document_id'], ['documents_metadata_v2.document_id'], ondelete='CASCADE'),
    )
    
    # Create Vietnamese text analysis table
    op.create_table(
        'vietnamese_text_analysis',
        sa.Column('analysis_id', postgresql.UUID(as_uuid=True), primary_key=True, default=uuid.uuid4),
        sa.Column('document_id', postgresql.UUID(as_uuid=True), nullable=False, index=True),
        sa.Column('tokenized_content', postgresql.JSONB, nullable=True, default=[]),
        sa.Column('pos_tags', postgresql.JSONB, nullable=True, default=[]),
        sa.Column('named_entities', postgresql.JSONB, nullable=True, default=[]),
        sa.Column('keywords', postgresql.JSONB, nullable=True, default=[]),
        sa.Column('analysis_model', sa.String(100), nullable=True),
        sa.Column('processing_time_ms', sa.Integer, nullable=True),
        sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.func.now()),
        sa.ForeignKeyConstraint(['document_id'], ['documents_metadata_v2.document_id'], ondelete='CASCADE'),
    )
    
    # Create indexes for optimal performance
    op.create_index('idx_documents_title', 'documents_metadata_v2', ['title'])
    op.create_index('idx_documents_type', 'documents_metadata_v2', ['document_type'])
    op.create_index('idx_documents_access_level', 'documents_metadata_v2', ['access_level'])
    op.create_index('idx_documents_department', 'documents_metadata_v2', ['department_owner'])
    op.create_index('idx_documents_author', 'documents_metadata_v2', ['author'])
    op.create_index('idx_documents_status', 'documents_metadata_v2', ['status'])
    op.create_index('idx_documents_created_at', 'documents_metadata_v2', ['created_at'])
    op.create_index('idx_documents_tags', 'documents_metadata_v2', ['tags'], postgresql_using='gin')
    
    op.create_index('idx_chunks_document_id', 'document_chunks_enhanced', ['document_id'])
    op.create_index('idx_chunks_position', 'document_chunks_enhanced', ['chunk_position'])
    op.create_index('idx_chunks_quality', 'document_chunks_enhanced', ['chunk_quality_score'])
    
    op.create_index('idx_rag_sessions_user_id', 'rag_pipeline_sessions', ['user_id'])
    op.create_index('idx_rag_sessions_created_at', 'rag_pipeline_sessions', ['created_at'])
    
    op.create_index('idx_bm25_document_id', 'document_bm25_index', ['document_id'])
    op.create_index('idx_bm25_term', 'document_bm25_index', ['term'])
    op.create_index('idx_bm25_language', 'document_bm25_index', ['language'])
    
    op.create_index('idx_vn_analysis_document_id', 'vietnamese_text_analysis', ['document_id'])

def downgrade() -> None:
    """Drop documents tables"""
    op.drop_table('vietnamese_text_analysis')
    op.drop_table('document_bm25_index')
    op.drop_table('rag_pipeline_sessions')
    op.drop_table('document_chunks_enhanced')
    op.drop_table('documents_metadata_v2')
    
    # Drop enums
    sa.Enum(name='documenttype').drop(op.get_bind())
    sa.Enum(name='accesslevel').drop(op.get_bind())
    sa.Enum(name='documentstatus').drop(op.get_bind())
    sa.Enum(name='pipelinetype').drop(op.get_bind())
    sa.Enum(name='retrievalmethod').drop(op.get_bind())
```

## üìÑ **9. SAMPLE DATA SEEDING**

### **scripts/seed_sample_documents.py**
```python
"""
Script ƒë·ªÉ seed sample documents t·ª´ FR-02.1
"""
import asyncio
import sys
import os
from datetime import datetime

# Add app to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.core.database import AsyncSessionLocal
from app.crud.document import DocumentCRUD, ChunkCRUD
from app.crud.user import UserCRUD
from app.schemas.document import DocumentCreate, ChunkCreate
from app.models.document import DocumentType, AccessLevel, DocumentStatus

async def seed_sample_documents():
    """Seed sample documents from FR-02.1 handover"""
    
    print("üå± Seeding sample documents from FR-02.1...")
    
    async with AsyncSessionLocal() as db:
        try:
            # Get admin user to create documents
            admin_user = await UserCRUD.get_user_by_username(db, "admin")
            if not admin_user:
                print("‚ùå Admin user not found. Please create admin user first.")
                return False
            
            # Sample documents from FR-02.1 handover
            sample_documents = [
                {
                    "title": "Quy tr√¨nh xin ngh·ªâ ph√©p",
                    "content": """
# QUY TR√åNH XIN NGH·ªà PH√âP

## 1. Quy ƒë·ªãnh chung
- Nh√¢n vi√™n c·∫ßn n·ªôp ƒë∆°n xin ngh·ªâ ph√©p tr∆∞·ªõc √≠t nh·∫•t 3 ng√†y l√†m vi·ªác
- ƒê∆°n xin ngh·ªâ ph√©p ph·∫£i ƒë∆∞·ª£c c·∫•p tr√™n tr·ª±c ti·∫øp ph√™ duy·ªát
- Ngh·ªâ ph√©p d√†i h·∫°n (>5 ng√†y) c·∫ßn ph√™ duy·ªát t·ª´ c·∫•p Gi√°m ƒë·ªëc

## 2. C√°c lo·∫°i ngh·ªâ ph√©p
### 2.1 Ngh·ªâ ph√©p nƒÉm
- Nh√¢n vi√™n ƒë∆∞·ª£c h∆∞·ªüng 12 ng√†y ngh·ªâ ph√©p c√≥ l∆∞∆°ng m·ªói nƒÉm
- Th·ªùi gian ngh·ªâ ph√©p nƒÉm kh√¥ng ƒë∆∞·ª£c chuy·ªÉn sang nƒÉm sau

### 2.2 Ngh·ªâ ph√©p ·ªëm ƒëau
- C·∫ßn c√≥ gi·∫•y ch·ª©ng nh·∫≠n y t·∫ø t·ª´ b√°c sƒ©
- ƒê∆∞·ª£c h∆∞·ªüng 100% l∆∞∆°ng n·∫øu ngh·ªâ d∆∞·ªõi 30 ng√†y

### 2.3 Ngh·ªâ ph√©p c√° nh√¢n
- Kh√¥ng h∆∞·ªüng l∆∞∆°ng
- C·∫ßn l√Ω do ch√≠nh ƒë√°ng v√† ƒë∆∞·ª£c c·∫•p tr√™n ph√™ duy·ªát

## 3. Quy tr√¨nh x·ª≠ l√Ω
1. Nh√¢n vi√™n ƒëi·ªÅn form ƒë∆°n xin ngh·ªâ ph√©p
2. G·ª≠i ƒë∆°n cho c·∫•p tr√™n tr·ª±c ti·∫øp
3. C·∫•p tr√™n xem x√©t v√† ph√™ duy·ªát/t·ª´ ch·ªëi
4. Th√¥ng b√°o k·∫øt qu·∫£ cho nh√¢n vi√™n
5. C·∫≠p nh·∫≠t v√†o h·ªá th·ªëng qu·∫£n l√Ω nh√¢n s·ª±

## 4. L∆∞u √Ω quan tr·ªçng
- Kh√¥ng ƒë∆∞·ª£c ngh·ªâ ph√©p trong th·ªùi gian cao ƒëi·ªÉm d·ª± √°n
- C·∫ßn b√†n giao c√¥ng vi·ªác tr∆∞·ªõc khi ngh·ªâ ph√©p
- Li√™n h·ªá HR n·∫øu c√≥ th·∫Øc m·∫Øc v·ªÅ ch√≠nh s√°ch ngh·ªâ ph√©p
                    """,
                    "document_type": DocumentType.PROCEDURE,
                    "access_level": AccessLevel.EMPLOYEE_ONLY,
                    "department_owner": "HR",
                    "author": "HR Department",
                    "status": DocumentStatus.APPROVED,
                    "tags": ["ngh·ªâ ph√©p", "quy tr√¨nh", "nh√¢n s·ª±", "HR"],
                    "language_detected": "vi"
                },
                {
                    "title": "Ch√≠nh s√°ch l√†m vi·ªác t·ª´ xa",
                    "content": """
# CH√çNH S√ÅCH L√ÄM VI·ªÜC T·ª™ XA

## 1. Ph·∫°m vi √°p d·ª•ng
Ch√≠nh s√°ch n√†y √°p d·ª•ng cho t·∫•t c·∫£ nh√¢n vi√™n to√†n th·ªùi gian c·ªßa c√¥ng ty, tr·ª´ c√°c v·ªã tr√≠ y√™u c·∫ßu hi·ªán di·ªán tr·ª±c ti·∫øp t·∫°i vƒÉn ph√≤ng.

## 2. ƒêi·ªÅu ki·ªán l√†m vi·ªác t·ª´ xa
### 2.1 ƒêi·ªÅu ki·ªán nh√¢n vi√™n
- ƒê√£ l√†m vi·ªác t·∫°i c√¥ng ty √≠t nh·∫•t 6 th√°ng
- C√≥ ƒë√°nh gi√° hi·ªáu su·∫•t t·ª´ "ƒê·∫°t" tr·ªü l√™n
- C√≥ kh·∫£ nƒÉng t·ª± qu·∫£n l√Ω c√¥ng vi·ªác hi·ªáu qu·∫£

### 2.2 ƒêi·ªÅu ki·ªán k·ªπ thu·∫≠t
- C√≥ k·∫øt n·ªëi internet ·ªïn ƒë·ªãnh (t·ªëi thi·ªÉu 50Mbps)
- M√°y t√≠nh l√†m vi·ªác ƒë√°p ·ª©ng y√™u c·∫ßu c√¥ng vi·ªác
- Kh√¥ng gian l√†m vi·ªác ph√π h·ª£p t·∫°i nh√†

## 3. Quy ƒë·ªãnh l√†m vi·ªác
### 3.1 Th·ªùi gian l√†m vi·ªác
- Gi·ªØ nguy√™n gi·ªù l√†m vi·ªác nh∆∞ t·∫°i vƒÉn ph√≤ng (8:00-17:00)
- C√≥ m·∫∑t trong c√°c cu·ªôc h·ªçp online theo l·ªãch
- Ph·∫£n h·ªìi tin nh·∫Øn/email trong v√≤ng 2 gi·ªù

### 3.2 B·∫£o m·∫≠t th√¥ng tin
- Ch·ªâ s·ª≠ d·ª•ng thi·∫øt b·ªã ƒë∆∞·ª£c c·∫•p ph√©p
- Kh√¥ng chia s·∫ª th√¥ng tin c√¥ng ty v·ªõi b√™n th·ª© ba
- Th·ª±c hi·ªán backup d·ªØ li·ªáu ƒë·ªãnh k·ª≥

## 4. Quy tr√¨nh ƒëƒÉng k√Ω
1. ƒêi·ªÅn form ƒëƒÉng k√Ω l√†m vi·ªác t·ª´ xa
2. ƒê∆∞·ª£c c·∫•p tr√™n tr·ª±c ti·∫øp ph√™ duy·ªát
3. IT setup c√°c c√¥ng c·ª• c·∫ßn thi·∫øt
4. Th·ª≠ nghi·ªám 1 tu·∫ßn tr∆∞·ªõc khi √°p d·ª•ng ch√≠nh th·ª©c

## 5. ƒê√°nh gi√° v√† theo d√µi
- ƒê√°nh gi√° hi·ªáu su·∫•t h√†ng th√°ng
- Review ch√≠nh s√°ch m·ªói 6 th√°ng
- Thu h·ªìi quy·ªÅn l√†m vi·ªác t·ª´ xa n·∫øu kh√¥ng ƒë·∫°t y√™u c·∫ßu
                    """,
                    "document_type": DocumentType.POLICY,
                    "access_level": AccessLevel.EMPLOYEE_ONLY,
                    "department_owner": "HR",
                    "author": "HR Department",
                    "status": DocumentStatus.APPROVED,
                    "tags": ["remote work", "ch√≠nh s√°ch", "l√†m vi·ªác t·ª´ xa", "HR"],
                    "language_detected": "vi"
                },
                {
                    "title": "H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng h·ªá th·ªëng ERP",
                    "content": """
# H∆Ø·ªöNG D·∫™N S·ª¨ D·ª§NG H·ªÜ TH·ªêNG ERP

## 1. Gi·ªõi thi·ªáu t·ªïng quan
H·ªá th·ªëng ERP (Enterprise Resource Planning) l√† gi·∫£i ph√°p qu·∫£n l√Ω t√≠ch h·ª£p gi√∫p t·ª± ƒë·ªông h√≥a c√°c quy tr√¨nh nghi·ªáp v·ª• c·ªßa c√¥ng ty.

## 2. ƒêƒÉng nh·∫≠p h·ªá th·ªëng
### 2.1 Truy c·∫≠p
- URL: https://erp.company.com
- S·ª≠ d·ª•ng t√†i kho·∫£n ƒë∆∞·ª£c c·∫•p b·ªüi IT
- M·∫≠t kh·∫©u ph·∫£i thay ƒë·ªïi l·∫ßn ƒë·∫ßu ƒëƒÉng nh·∫≠p

### 2.2 B·∫£o m·∫≠t
- Kh√¥ng chia s·∫ª t√†i kho·∫£n v·ªõi ng∆∞·ªùi kh√°c
- ƒêƒÉng xu·∫•t khi k·∫øt th√∫c ca l√†m vi·ªác
- B√°o ngay cho IT n·∫øu nghi ng·ªù t√†i kho·∫£n b·ªã x√¢m nh·∫≠p

## 3. C√°c module ch√≠nh
### 3.1 Module Nh√¢n s·ª± (HR)
- Qu·∫£n l√Ω th√¥ng tin c√° nh√¢n
- ƒêƒÉng k√Ω ngh·ªâ ph√©p, l√†m th√™m gi·ªù
- Xem b·∫£ng l∆∞∆°ng v√† c√°c ph√∫c l·ª£i

### 3.2 Module T√†i ch√≠nh (Finance)
- T·∫°o v√† theo d√µi h√≥a ƒë∆°n
- Qu·∫£n l√Ω ng√¢n s√°ch d·ª± √°n
- B√°o c√°o t√†i ch√≠nh

### 3.3 Module D·ª± √°n (Project)
- T·∫°o v√† ph√¢n c√¥ng nhi·ªám v·ª•
- Theo d√µi ti·∫øn ƒë·ªô d·ª± √°n
- Qu·∫£n l√Ω t√†i nguy√™n d·ª± √°n

## 4. H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng c∆° b·∫£n
### 4.1 T·∫°o m·ªõi b·∫£n ghi
1. Ch·ªçn module c·∫ßn l√†m vi·ªác
2. Click "T·∫°o m·ªõi" ho·∫∑c n√∫t "+"
3. ƒêi·ªÅn ƒë·∫ßy ƒë·ªß th√¥ng tin b·∫Øt bu·ªôc
4. L∆∞u v√† ki·ªÉm tra l·∫°i

### 4.2 T√¨m ki·∫øm v√† l·ªçc d·ªØ li·ªáu
- S·ª≠ d·ª•ng thanh t√¨m ki·∫øm ph√≠a tr√™n
- √Åp d·ª•ng b·ªô l·ªçc theo ng√†y, tr·∫°ng th√°i
- S·∫Øp x·∫øp theo c√°c c·ªôt c·∫ßn thi·∫øt

### 4.3 Xu·∫•t b√°o c√°o
1. Ch·ªçn menu "B√°o c√°o"
2. Ch·ªçn lo·∫°i b√°o c√°o c·∫ßn xu·∫•t
3. Thi·∫øt l·∫≠p tham s·ªë (th·ªùi gian, b·ªô ph·∫≠n...)
4. Click "Xu·∫•t" v√† ch·ªçn ƒë·ªãnh d·∫°ng (PDF, Excel)

## 5. Kh·∫Øc ph·ª•c s·ª± c·ªë th∆∞·ªùng g·∫∑p
### 5.1 Kh√¥ng ƒëƒÉng nh·∫≠p ƒë∆∞·ª£c
- Ki·ªÉm tra k·∫øt n·ªëi internet
- X√≥a cache tr√¨nh duy·ªát
- Li√™n h·ªá IT n·∫øu v·∫´n kh√¥ng ƒë∆∞·ª£c

### 5.2 Trang web ch·∫≠m
- ƒê√≥ng c√°c tab kh√¥ng c·∫ßn thi·∫øt
- S·ª≠ d·ª•ng tr√¨nh duy·ªát Chrome ho·∫∑c Edge
- Kh·ªüi ƒë·ªông l·∫°i tr√¨nh duy·ªát

## 6. Li√™n h·ªá h·ªó tr·ª£
- Email: support@company.com
- Hotline: 1800-1234
- Chat tr·ª±c tuy·∫øn trong h·ªá th·ªëng
- Ticket h·ªó tr·ª£ qua module IT Support
                    """,
                    "document_type": DocumentType.TECHNICAL_GUIDE,
                    "access_level": AccessLevel.EMPLOYEE_ONLY,
                    "department_owner": "IT",
                    "author": "IT Department",
                    "status": DocumentStatus.APPROVED,
                    "tags": ["ERP", "h∆∞·ªõng d·∫´n", "h·ªá th·ªëng", "IT", "ph·∫ßn m·ªÅm"],
                    "language_detected": "vi"
                }
            ]
            
            created_documents = []
            
            for doc_data in sample_documents:
                try:
                    # Create document
                    document_create = DocumentCreate(**doc_data)
                    document = await DocumentCRUD.create_document(
                        db=db,
                        document_create=document_create,
                        author_id=str(admin_user.user_id)
                    )
                    
                    created_documents.append(document)
                    print(f"‚úÖ Created document: {document.title}")
                    
                    # Create sample chunks for each document
                    await create_sample_chunks(db, document)
                    
                except Exception as e:
                    print(f"‚ùå Failed to create document '{doc_data['title']}': {e}")
            
            print(f"\nüéâ Successfully created {len(created_documents)} sample documents!")
            
            return True
            
        except Exception as e:
            print(f"‚ùå Error seeding sample documents: {e}")
            return False

async def create_sample_chunks(db, document):
    """Create sample chunks for a document"""
    try:
        # Split content into chunks (simplified chunking)
        content = document.content or ""
        paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
        
        for i, paragraph in enumerate(paragraphs[:10]):  # Limit to 10 chunks
            if len(paragraph) > 50:  # Only create chunks for substantial content
                chunk_create = ChunkCreate(
                    document_id=str(document.document_id),
                    chunk_content=paragraph,
                    chunk_position=i,
                    chunk_quality_score=0.8,
                    semantic_boundary=True,
                    embedding_model="Qwen/Qwen2.5-Embedding-0.6B"
                )
                
                await ChunkCRUD.create_chunk(db, chunk_create)
        
        print(f"   Created {min(len(paragraphs), 10)} chunks for '{document.title}'")
        
    except Exception as e:
        print(f"   ‚ùå Failed to create chunks for '{document.title}': {e}")

if __name__ == "__main__":
    result = asyncio.run(seed_sample_documents())
    sys.exit(0 if result else 1)
```

## üöÄ **10. COMPLETE STARTUP SCRIPT**

### **scripts/start_documents_api.sh**
```bash
#!/bin/bash
# ======================
# FR-02.2 Complete API Startup Script
# ======================

set -e

echo "üöÄ Starting FR-02.2 Documents API System..."

# Function definitions
log_info() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') [INFO] $1"
}

log_error() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') [ERROR] $1" >&2
}

# Check environment
if [[ "$VIRTUAL_ENV" == "" ]]; then
    log_error "Virtual environment not activated"
    echo "Run: source venv/bin/activate"
    exit 1
fi

log_info "Environment check passed"

# Check database connections
log_info "Testing database connections..."
python scripts/test_connections.py

if [ $? -ne 0 ]; then
    log_error "Database connection failed"
    echo "Please ensure FR-02.1 services are running:"
    echo "  cd ../chatbot-enhanced-db"
    echo "  docker-compose up -d"
    exit 1
fi

log_info "Database connections verified"

# Run database migrations
log_info "Running database migrations..."
alembic upgrade head

if [ $? -ne 0 ]; then
    log_error "Database migration failed"
    exit 1
fi

log_info "Database migrations completed"

# Check and create admin user
log_info "Checking admin user..."
python -c "
import asyncio
from app.core.database import AsyncSessionLocal
from app.crud.user import UserCRUD

async def check_admin():
    async with AsyncSessionLocal() as db:
        admin = await UserCRUD.get_user_by_username(db, 'admin')
        return admin is not None

result = asyncio.run(check_admin())
exit(0 if result else 1)
"

if [ $? -ne 0 ]; then
    log_info "Creating admin user..."
    python scripts/create_admin_user.py
    
    if [ $? -ne 0 ]; then
        log_error "Failed to create admin user"
        exit 1
    fi
fi

log_info "Admin user verified"

# Seed sample documents
log_info "Checking sample documents..."
python -c "
import asyncio
from app.core.database import AsyncSessionLocal
from app.crud.document import DocumentCRUD
from app.crud.user import UserCRUD

async def check_documents():
    async with AsyncSessionLocal() as db:
        admin = await UserCRUD.get_user_by_username(db, 'admin')
        if admin:
            docs, total = await DocumentCRUD.get_documents_list(db, admin, limit=1)
            return total > 0
        return False

result = asyncio.run(check_documents())
exit(0 if result else 1)
"

if [ $? -ne 0 ]; then
    log_info "Seeding sample documents..."
    python scripts/seed_sample_documents.py
    
    if [ $? -ne 0 ]; then
        log_error "Failed to seed sample documents"
        echo "‚ö†Ô∏è  Continuing without sample data..."
    fi
else
    log_info "Sample documents found"
fi

# Test authentication system
log_info "Testing authentication system..."
python scripts/test_auth_system.py

if [ $? -ne 0 ]; then
    log_error "Authentication system test failed"
    exit 1
fi

log_info "Authentication system verified"

# Test documents API
log_info "Testing documents API..."
timeout 30s python scripts/test_documents_integration.py &
TEST_PID=$!

# Start API server in background for testing
uvicorn app.main:app --host 127.0.0.1 --port 8001 --log-level warning &
SERVER_PID=$!

# Wait a bit for server to start
sleep 5

# Wait for test to complete
wait $TEST_PID
TEST_RESULT=$?

# Kill test server
kill $SERVER_PID 2>/dev/null || true

if [ $TEST_RESULT -ne 0 ]; then
    log_error "Documents API test failed"
    exit 1
fi

log_info "Documents API verified"

# Display startup information
echo ""
echo "=" * 60
echo "üéâ FR-02.2 API System Ready!"
echo "=" * 60
echo ""
echo "üìä System Information:"
echo "   ‚Ä¢ Authentication: ‚úÖ Enabled"
echo "   ‚Ä¢ Documents API: ‚úÖ Enabled"
echo "   ‚Ä¢ Search Engine: ‚úÖ Enabled"
echo "   ‚Ä¢ File Upload: ‚úÖ Enabled"
echo "   ‚Ä¢ Access Control: ‚úÖ Enabled"
echo ""
echo "üåê API Endpoints:"
echo "   ‚Ä¢ API Server: http://localhost:8000"
echo "   ‚Ä¢ Documentation: http://localhost:8000/docs"
echo "   ‚Ä¢ Health Check: http://localhost:8000/health"
echo "   ‚Ä¢ API Status: http://localhost:8000/api/v1/status"
echo ""
echo "üîê Test Accounts:"
echo "   ‚Ä¢ Admin: admin / (password set during setup)"
echo "   ‚Ä¢ Employee: employee / TestPassword123!"
echo "   ‚Ä¢ Manager: manager / TestPassword123!"
echo ""
echo "üìã Available APIs:"
echo "   ‚Ä¢ POST /api/v1/auth/login - User login"
echo "   ‚Ä¢ GET  /api/v1/documents/ - List documents"
echo "   ‚Ä¢ POST /api/v1/documents/ - Create document"
echo "   ‚Ä¢ GET  /api/v1/documents/{id} - Get document"
echo "   ‚Ä¢ POST /api/v1/documents/search - Search documents"
echo "   ‚Ä¢ POST /api/v1/documents/upload - Upload file"
echo ""
echo "üîÑ Press Ctrl+C to stop the server"
echo ""

# Start the main API server
log_info "Starting main API server..."
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

---

## üéØ **T·ªîNG K·∫æT DOCUMENTS API**

### ‚úÖ **Nh·ªØng g√¨ ƒë√£ ho√†n th√†nh:**

1. **üìÑ Complete CRUD Operations** - Create, Read, Update, Delete documents
2. **üîç Advanced Search System** - Keyword, semantic, v√† hybrid search
3. **üìä Document Management** - Chunks, metadata, versioning
4. **üîê Access Control** - Role-based permissions v·ªõi department filtering
5. **üì§ File Upload System** - Support multiple file types v·ªõi validation
6. **üìà Statistics & Analytics** - Document usage metrics v√† insights
7. **üóÑÔ∏è Database Integration** - Full integration v·ªõi schema t·ª´ FR-02.1
8. **üß™ Comprehensive Testing** - Unit, integration, v√† API tests
9. **‚ö° Performance Optimization** - Caching, indexing, pagination
10. **üõ°Ô∏è Security Features** - Authentication, authorization, input validation

### üîÑ **API Endpoints Summary:**

```
Authentication:
‚îú‚îÄ‚îÄ POST /api/v1/auth/login
‚îú‚îÄ‚îÄ POST /api/v1/auth/logout  
‚îú‚îÄ‚îÄ GET  /api/v1/auth/me
‚îî‚îÄ‚îÄ POST /api/v1/auth/refresh

Documents:
‚îú‚îÄ‚îÄ GET    /api/v1/documents/
‚îú‚îÄ‚îÄ POST   /api/v1/documents/
‚îú‚îÄ‚îÄ GET    /api/v1/documents/{id}
‚îú‚îÄ‚îÄ PUT    /api/v1/documents/{id}
‚îú‚îÄ‚îÄ DELETE /api/v1/documents/{id}
‚îú‚îÄ‚îÄ GET    /api/v1/documents/{id}/chunks
‚îú‚îÄ‚îÄ GET    /api/v1/documents/{id}/with-chunks
‚îú‚îÄ‚îÄ POST   /api/v1/documents/search
‚îú‚îÄ‚îÄ POST   /api/v1/documents/upload
‚îú‚îÄ‚îÄ POST   /api/v1/documents/bulk-upload
‚îú‚îÄ‚îÄ GET    /api/v1/documents/stats/overview
‚îî‚îÄ‚îÄ POST   /api/v1/documents/{id}/reprocess
```

### üéØ **Ready for Production:**

```mermaid
graph TB
    API[FR-02.2 Documents API] --> Auth[Authentication System ‚úÖ]
    API --> CRUD[CRUD Operations ‚úÖ]
    API --> Search[Search Engine ‚úÖ]
    API --> Upload[File Upload ‚úÖ]
    API --> Access[Access Control ‚úÖ]
    
    Auth --> JWT[JWT Tokens]
    Auth --> RBAC[Role-based Access]
    
    CRUD --> Postgres[(PostgreSQL)]
    CRUD --> Vector[(ChromaDB)]
    CRUD --> Cache[(Redis)]
    
    Search --> Hybrid[Hybrid Search]
    Search --> Semantic[Semantic Search]
    Search --> Keyword[Keyword Search]
    
    Upload --> Validation[File Validation]
    Upload --> Processing[Content Processing]
    Upload --> Chunking[Auto Chunking]
```

**üöÄ Documents API ƒë√£ s·∫µn s√†ng ho√†n to√†n ƒë·ªÉ t√≠ch h·ª£p v·ªõi c√°c components kh√°c c·ªßa FR-02.2!**

**B·∫°n mu·ªën ti·∫øp t·ª•c implement API endpoint n√†o ti·∫øp theo? T√¥i khuy·∫øn ngh·ªã:**

1. **üîç Search API** - Enhanced search v·ªõi vector embeddings
2. **üë• Users API** - Extended user management features  
3. **üõ°Ô∏è Access Control API** - Advanced permissions v√† role management
4. **üìä Analytics API** - Detailed reporting v√† insights

**Ho·∫∑c ch√∫ng ta c√≥ th·ªÉ:**
- **üîó Integrate v·ªõi ChromaDB** cho semantic search th·ª±c s·ª±
- **üìù Implement document processing pipeline** cho auto-chunking
- **üöÄ Deploy to√†n b·ªô system** l√™n staging environment

**B·∫°n mu·ªën ti·∫øp t·ª•c v·ªõi ph·∫ßn n√†o? T√¥i khuy·∫øn ngh·ªã l√†m Search API v·ªõi ChromaDB integration ƒë·ªÉ c√≥ ƒë∆∞·ª£c semantic search th·ª±c s·ª±, v√¨ ƒë√¢y l√† t√≠nh nƒÉng core c·ªßa RAG system.**

### üéØ **Next Steps Recommendation:**

**Option 1: üîç Enhanced Search API**
- Implement real semantic search v·ªõi ChromaDB
- Vector embeddings v·ªõi Qwen model
- Hybrid search combining keyword + semantic
- Search result ranking v√† relevance scoring

**Option 2: üìä Analytics & Reporting API**  
- Document usage analytics
- User behavior tracking
- Performance metrics
- Export capabilities

**Option 3: üöÄ Production Deployment**
- Docker containerization
- Kubernetes manifests
- CI/CD pipeline
- Monitoring setup

**B·∫°n prefer option n√†o? Search API s·∫Ω complete RAG pipeline, c√≤n Analytics API s·∫Ω provide business insights, c√≤n Production Deployment s·∫Ω make system ready for real usage.**