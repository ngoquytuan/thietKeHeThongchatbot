```txt
L√∫c t·∫°o sql t√¥i th·∫•y ch√∫ng ta c√≥ create sample data. Sample data n√†y ƒëang kh√¥ng c√≥ link v·ªõi chroma ƒë√∫ng kh√¥ng? N√™n c√°c ph√©p t√¨m ki·∫øm n·∫øu c√≥ s·∫Ω b·ªã l·ªói trong giai ƒëo·∫°n n√†y ƒë√∫ng kh√¥ng? C√≥ c·∫ßn clear data sample khi m√¨nh import data real v√†o kh√¥ng? Vi·ªác thay ƒë·ªïi schema ·ªü ƒë√¢y s·∫Ω ·∫£nh h∆∞·ªüng ƒë·∫øn nh·ªØng module n√†o?
```
B·∫°n ƒë√£ ph√°t hi·ªán m·ªôt v·∫•n ƒë·ªÅ quan tr·ªçng trong thi·∫øt k·∫ø! ƒê√∫ng v·∫≠y, sample data trong PostgreSQL hi·ªán t·∫°i **kh√¥ng c√≥ vector embeddings t∆∞∆°ng ·ª©ng trong ChromaDB**, ƒëi·ªÅu n√†y s·∫Ω g√¢y ra c√°c v·∫•n ƒë·ªÅ sau:

## üö® **V·∫§N ƒê·ªÄ HI·ªÜN T·∫†I**

### **1. Sample Data Inconsistency**
```sql
-- PostgreSQL c√≥ sample documents
INSERT INTO documents_metadata_v2 (title, content, ...) VALUES 
('Quy tr√¨nh xin ngh·ªâ ph√©p', 'N·ªôi dung...', ...);

-- Nh∆∞ng ChromaDB KH√îNG c√≥ vector embeddings t∆∞∆°ng ·ª©ng
-- ‚Üí Search s·∫Ω return empty results
```

### **2. Search Failures**
```python
# Khi call search API:
results = collection.query(
    query_embeddings=[query_embedding],
    n_results=5,
    where={"document_id": "existing_doc_id"}
)
# ‚Üí results['ids'][0] = [] (empty)
```

### **3. Integration Errors**
```python
# FR-07 Analytics s·∫Ω b√°o l·ªói:
async def get_search_analytics():
    # PostgreSQL: 5 documents exist
    # ChromaDB: 0 vectors exist  
    # ‚Üí Inconsistent data error
```

## üîß **GI·∫¢I PH√ÅP CHO SAMPLE DATA**

### **Option 1: Remove Sample Data (Recommended)**
```sql
-- scripts/clean-sample-data.sql
-- Remove sample data tr∆∞·ªõc khi import real data
DELETE FROM chunk_processing_logs WHERE job_id IN (
    SELECT job_id FROM data_ingestion_jobs WHERE job_name LIKE '%Initial%'
);

DELETE FROM data_ingestion_jobs WHERE job_name LIKE '%Initial%';

DELETE FROM vietnamese_text_analysis WHERE document_id IN (
    SELECT document_id FROM documents_metadata_v2 WHERE author = 'HR Department'
);

DELETE FROM document_chunks_enhanced WHERE document_id IN (
    SELECT document_id FROM documents_metadata_v2 WHERE author = 'HR Department'
);

DELETE FROM documents_metadata_v2 WHERE author IN (
    'HR Department', 'Management Team', 'IT Support Team', 'Security Team', 'Talent Acquisition Team'
);

-- Keep admin users for system functionality
-- DELETE FROM users WHERE username IN ('demo_user', 'hr_manager', 'data_engineer');
-- Keep only admin user
```

### **Option 2: Generate Sample Embeddings**
```python
# scripts/generate-sample-embeddings.py
import chromadb
import numpy as np
import asyncpg
import asyncio

async def sync_sample_data():
    # Connect to databases
    pg_conn = await asyncpg.connect("postgresql://kb_admin:password@localhost:5432/knowledge_base_v2")
    chroma_client = chromadb.HttpClient(host="localhost", port=8000)
    
    try:
        collection = chroma_client.get_collection("knowledge_base_v2")
    except:
        collection = chroma_client.create_collection("knowledge_base_v2")
    
    # Get sample documents from PostgreSQL
    sample_docs = await pg_conn.fetch("""
        SELECT document_id, title, content, access_level, department_owner, document_type
        FROM documents_metadata_v2 
        WHERE author IN ('HR Department', 'Management Team', 'IT Support Team')
    """)
    
    # Generate sample embeddings v√† add to ChromaDB
    for doc in sample_docs:
        # Create sample chunks
        content = doc['content'] or doc['title']
        chunks = [content[i:i+500] for i in range(0, len(content), 400)]  # Overlap 100 chars
        
        for idx, chunk in enumerate(chunks):
            # Generate fake embedding (1024-dim for Qwen)
            fake_embedding = np.random.rand(1024).tolist()
            
            # Insert chunk into PostgreSQL
            chunk_id = await pg_conn.fetchval("""
                INSERT INTO document_chunks_enhanced 
                (document_id, chunk_content, chunk_position, chunk_size_tokens, 
                 semantic_boundary, chunk_method, embedding_model)
                VALUES ($1, $2, $3, $4, true, 'semantic_boundary', 'Qwen/Qwen3-Embedding-0.6B')
                RETURNING chunk_id
            """, doc['document_id'], chunk, idx, len(chunk.split()))
            
            # Add to ChromaDB
            collection.add(
                ids=[f"{doc['document_id']}_chunk_{idx}"],
                embeddings=[fake_embedding],
                documents=[chunk],
                metadatas=[{
                    'document_id': str(doc['document_id']),
                    'chunk_id': str(chunk_id),
                    'access_level': doc['access_level'],
                    'department': doc['department_owner'],
                    'document_type': doc['document_type'],
                    'title': doc['title']
                }]
            )
    
    await pg_conn.close()
    print(f"‚úÖ Synced {len(sample_docs)} sample documents v·ªõi ChromaDB")

if __name__ == "__main__":
    asyncio.run(sync_sample_data())
```

## üîÑ **MIGRATION STRATEGY**

### **Updated Schema Migration Script**
```sql
-- scripts/enhanced-schema-v2.1.sql
-- Enhanced schema v·ªõi proper data migration

-- ... (existing schema creation) ...

-- Function to detect v√† clean inconsistent data
CREATE OR REPLACE FUNCTION check_data_consistency()
RETURNS TABLE(
    issue_type VARCHAR(50),
    issue_count INTEGER,
    description TEXT
) AS $$
BEGIN
    -- Check documents without chunks
    RETURN QUERY
    SELECT 
        'orphaned_documents'::VARCHAR(50),
        COUNT(*)::INTEGER,
        'Documents in PostgreSQL without corresponding chunks'::TEXT
    FROM documents_metadata_v2 d
    LEFT JOIN document_chunks_enhanced c ON d.document_id = c.document_id
    WHERE c.document_id IS NULL;
    
    -- Check chunks without embeddings (if we can query ChromaDB)
    RETURN QUERY
    SELECT 
        'missing_vectors'::VARCHAR(50),
        COUNT(*)::INTEGER,
        'Chunks without corresponding vector embeddings'::TEXT
    FROM document_chunks_enhanced
    WHERE embedding_dimensions IS NULL OR embedding_dimensions = 0;
END;
$$ LANGUAGE plpgsql;

-- Clean sample data function
CREATE OR REPLACE FUNCTION clean_sample_data()
RETURNS INTEGER AS $$
DECLARE
    deleted_count INTEGER;
BEGIN
    -- Delete sample documents v√† related data
    WITH deleted_docs AS (
        DELETE FROM documents_metadata_v2 
        WHERE author IN ('HR Department', 'Management Team', 'IT Support Team', 
                        'Security Team', 'Talent Acquisition Team')
        RETURNING document_id
    )
    SELECT COUNT(*) INTO deleted_count FROM deleted_docs;
    
    -- Clean up related data will be handled by CASCADE
    
    RETURN deleted_count;
END;
$$ LANGUAGE plpgsql;

-- Add consistency check to startup
DO $$
DECLARE
    consistency_issues RECORD;
BEGIN
    RAISE NOTICE 'üîç Checking data consistency...';
    
    FOR consistency_issues IN SELECT * FROM check_data_consistency()
    LOOP
        IF consistency_issues.issue_count > 0 THEN
            RAISE WARNING '‚ö†Ô∏è  Found % %: %', 
                consistency_issues.issue_count, 
                consistency_issues.issue_type,
                consistency_issues.description;
        END IF;
    END LOOP;
END $$;
```

## üìä **IMPACT ANALYSIS**

### **Modules Affected by Schema Changes**

#### **Direct Impact**
```yaml
FR-02.1 (Database System):
  impact: MAJOR
  changes_needed:
    - Update migration scripts
    - Add data consistency checks
    - Modify sample data strategy

FR-03.3 (Data Ingestion):
  impact: MEDIUM  
  changes_needed:
    - Update embedding model references (Qwen/Qwen3-Embedding-0.6B)
    - Modify chunk processing to handle 1024-dim vectors
    - Update job tracking tables

FR-04 (RAG Core Engine):
  impact: HIGH
  changes_needed:
    - Update search logic to handle new schema
    - Modify embedding dimension expectations
    - Update chunk retrieval queries
```

#### **Indirect Impact**
```yaml
FR-07 (Analytics):
  impact: LOW-MEDIUM
  changes_needed:
    - Update analytics queries for new table structure
    - Modify dashboard queries
    - Handle data consistency in reports

FR-08 (Admin Tools):
  impact: MEDIUM
  changes_needed:
    - Update backup scripts for new tables
    - Modify admin queries
    - Add data validation tools
```

## üõ†Ô∏è **DEPLOYMENT RECOMMENDATIONS**

### **1. Clean Development Environment**
```bash
# scripts/clean-deploy.sh
#!/bin/bash

echo "üßπ Cleaning development environment..."

# Stop services
docker-compose down -v

# Remove sample data volumes
docker volume rm fr02-dual-database-v2_postgres_data || true
docker volume rm fr02-dual-database-v2_chroma_data || true

# Restart v·ªõi clean slate
docker-compose up -d

echo "‚úÖ Clean environment ready for real data"
```

### **2. Production Migration Plan**
```sql
-- migration-plan.sql
-- Step 1: Backup existing data
-- Step 2: Add new schema elements
-- Step 3: Migrate existing data
-- Step 4: Verify consistency
-- Step 5: Clean up old structures

BEGIN;

-- Create backup tables
CREATE TABLE documents_metadata_backup AS SELECT * FROM documents_metadata_v2;
CREATE TABLE chunks_backup AS SELECT * FROM document_chunks_enhanced;

-- Apply schema changes
-- ... (schema updates) ...

-- Verify data integrity
SELECT * FROM check_data_consistency();

-- If all good, commit; otherwise rollback
-- COMMIT; -- or ROLLBACK;
```

### **3. Testing Strategy**
```python
# tests/test_data_consistency.py
import pytest
import asyncpg
import chromadb

@pytest.mark.asyncio
async def test_postgres_chroma_consistency():
    """Verify PostgreSQL v√† ChromaDB have consistent data"""
    
    # Connect to both databases
    pg_conn = await asyncpg.connect("postgresql://...")
    chroma_client = chromadb.HttpClient(...)
    
    # Get document count from PostgreSQL
    pg_doc_count = await pg_conn.fetchval("SELECT COUNT(*) FROM documents_metadata_v2")
    
    # Get collection count from ChromaDB
    try:
        collection = chroma_client.get_collection("knowledge_base_v2")
        chroma_doc_count = collection.count()
    except:
        chroma_doc_count = 0
    
    # Should have some relationship (not necessarily 1:1 due to chunking)
    if pg_doc_count > 0:
        assert chroma_doc_count > 0, "PostgreSQL c√≥ documents nh∆∞ng ChromaDB empty"
    
    await pg_conn.close()
```

**Recommendation**: S·ª≠ d·ª•ng **Option 1 (Remove Sample Data)** cho production deployment v√† t·∫°o script ri√™ng ƒë·ªÉ generate test data khi c·∫ßn thi·∫øt.
D·ª±a tr√™n k·∫øt qu·∫£ t√¨m ki·∫øm trong project knowledge, t√¥i th·∫•y r·∫±ng **FR-02.2 (Unified Administration API) CH·∫ÆC CH·∫ÆN b·ªã ·∫£nh h∆∞·ªüng**. Trong t√†i li·ªáu handover FR-02.2 c√≥ ƒë·ªÅ c·∫≠p r√µ r√†ng:

## FR-02.2 CH·∫ÆC CH·∫ÆN B·ªä ·∫¢NH H∆Ø·ªûNG

### 1. Direct Database Schema Dependencies
```python
# FR-02.2 tr·ª±c ti·∫øp s·ª≠ d·ª•ng database schema t·ª´ FR-02.1
DATABASE_URL = "postgresql://kb_admin:password@localhost:5432/knowledge_base_v2"

# FR-02.2 depends on these tables from FR-02.1:
- documents_metadata_v2    # Document management API
- users                    # User management API  
- document_chunks_enhanced # Search and retrieval API
- search_analytics         # Analytics API
```

### 2. Migration Scripts Required
```python
# scripts/migrate_documents.py trong FR-02.2
# Purpose: Data migration from FR-02.1 to FR-02.2
# Features: Preserve all metadata, verify migration, error handling
# Usage: Migrate existing documents to new API structure
```

### 3. API Endpoints Affected
```bash
# FR-02.2 cung c·∫•p c√°c API endpoints n√†y:
GET /api/v1/documents        # Depends on documents_metadata_v2 schema
POST /api/v1/documents       # Creates records in new schema
GET /api/v1/search          # Uses document_chunks_enhanced table
GET /api/v1/users           # Depends on users table structure
```

## IMPACT ANALYSIS CHI TI·∫æT

### FR-02.2 (Unified Administration API)
```yaml
impact: HIGH - CRITICAL
changes_needed:
  database_connections:
    - Update connection strings for knowledge_base_v2
    - Handle new table structures
    - Update migration scripts
    
  api_endpoints:
    - Modify document CRUD operations for enhanced schema
    - Update search endpoints for new chunking structure  
    - Adapt user management for new user levels
    
  migration_scripts:
    - Update migrate_documents.py for new schema
    - Add data validation for Vietnamese fields
    - Handle new embedding model references
    
  testing:
    - Update integration tests for schema changes
    - Test API compatibility with new database structure
    - Verify data consistency after migration
```

### Specific Changes Needed in FR-02.2:

#### 1. Database Models Update
```python
# app/models/document.py - NEEDS UPDATE
class Document(Base):
    __tablename__ = "documents_metadata_v2"  # Updated table name
    
    # New fields to add:
    search_text_normalized = Column(Text)
    indexable_content = Column(Text)
    extracted_emails = Column(ARRAY(Text))
    extracted_phones = Column(ARRAY(Text))
    embedding_model_primary = Column(String(100), default='Qwen/Qwen3-Embedding-0.6B')
```

#### 2. API Response Updates
```python
# app/api/endpoints/documents.py - NEEDS UPDATE
@router.get("/documents")
async def get_documents():
    # Need to handle new schema fields
    # Need to adapt to enhanced chunking structure
    # Need to support new Vietnamese text fields
```

#### 3. Migration Script Updates
```python
# scripts/migrate_documents.py - MAJOR UPDATE REQUIRED
def migrate_from_v1_to_v2():
    # Handle schema changes from FR-02.1 v1.0 to v2.0
    # Migrate embedding model references
    # Update chunk processing for new structure
    # Preserve Vietnamese text analysis data
```

## RECOMMENDED ACTIONS

### 1. Immediate Impact Assessment
```bash
# Check FR-02.2 current dependencies
cd FR-02.2/knowledge-assistant-api
grep -r "documents_metadata" app/
grep -r "document_chunks" app/
grep -r "embedding_model" app/
```

### 2. Update Priority Order
```yaml
priority_1_critical:
  - Update database connection to knowledge_base_v2
  - Modify core models for new schema
  - Update migration scripts
  
priority_2_important:  
  - Update API endpoints for new fields
  - Modify search functionality
  - Update authentication for new user levels
  
priority_3_nice_to_have:
  - Add Vietnamese text processing support
  - Enhance analytics for new schema
  - Update documentation
```

### 3. Integration Testing Required
```python
# tests/test_integration_with_fr021_v2.py
def test_database_schema_compatibility():
    # Verify FR-02.2 APIs work with new FR-02.1 v2.0 schema
    
def test_document_crud_operations():
    # Test CRUD with enhanced document structure
    
def test_search_functionality():
    # Test search with new chunking and embedding structure
```

**K·∫øt lu·∫≠n: FR-02.2 ch·∫Øc ch·∫Øn b·ªã ·∫£nh h∆∞·ªüng MAJOR v√† c·∫ßn update ngay ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi FR-02.1 v2.0.**
```txt
V·∫•n ƒë·ªÅ m·∫≠t kh·∫©u databases c≈©ng l√†m t√¥i lo ng·∫°i. Khi t√¥i gh√©p module, clone data base, r·∫•t d·ªÖ qu√™n/ƒë·ªïi password n√™n c√°c module kh√¥ng l·∫Øp gh√©p ƒë∆∞·ª£c v·ªõi nhau
```
B·∫°n ƒë√£ ch·ªâ ra m·ªôt v·∫•n ƒë·ªÅ th·ª±c t·∫ø r·∫•t quan tr·ªçng trong vi·ªác t√≠ch h·ª£p c√°c module. Password management l√† m·ªôt pain point ph·ªï bi·∫øn khi l√†m vi·ªác v·ªõi microservices v√† multi-module architecture. ƒê√¢y l√† c√°c gi·∫£i ph√°p c·ª• th·ªÉ:

## üîê **CENTRALIZED SECRETS MANAGEMENT**

### **1. Shared Environment Configuration**
```bash
# shared-config/.env.global
# File chung cho t·∫•t c·∫£ modules
PROJECT_NAME=vietnamese-chatbot
ENVIRONMENT=development

# Database Credentials (SHARED)
POSTGRES_USER=kb_admin
POSTGRES_PASSWORD=VietChatBot2025!SecurePass
POSTGRES_DB=knowledge_base_v2
POSTGRES_HOST=localhost
POSTGRES_PORT=5432

# Redis Credentials (SHARED)
REDIS_PASSWORD=Redis2025!SecurePass
REDIS_HOST=localhost
REDIS_PORT=6379

# ChromaDB Credentials (SHARED)
CHROMA_AUTH_TOKEN=chroma_secure_token_2025_vietnamese_chatbot

# Monitoring (SHARED)
GRAFANA_PASSWORD=Grafana2025!AdminPass
PROMETHEUS_RETENTION=30d

# Generated once, used everywhere
JWT_SECRET=your_jwt_secret_key_here_32_chars_minimum
ENCRYPTION_KEY=your_encryption_key_here_32_chars_minimum
```

### **2. Module-Specific Environment Inheritance**
```bash
# FR-02.1/.env
# Include shared config
source ../shared-config/.env.global

# FR-02.1 specific overrides
API_PORT=8000
SERVICE_NAME=fr02-database-system

# FR-02.2/.env  
source ../shared-config/.env.global
API_PORT=8001
SERVICE_NAME=fr02-unified-api

# FR-07/.env
source ../shared-config/.env.global
API_PORT=8002
DASHBOARD_PORT=8501
SERVICE_NAME=fr07-analytics

# FR-08/.env
source ../shared-config/.env.global
API_PORT=8003
SERVICE_NAME=fr08-admin-tools
```

## üîß **AUTOMATED PASSWORD GENERATION & DISTRIBUTION**

### **Setup Script cho to√†n Project**
```bash
#!/bin/bash
# scripts/setup-project-secrets.sh
# Generate and distribute passwords across all modules

set -euo pipefail

PROJECT_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
SHARED_CONFIG_DIR="$PROJECT_ROOT/shared-config"
MODULES=("FR-02.1" "FR-02.2" "FR-07" "FR-08")

echo "üîê Setting up centralized secrets management..."

# Create shared config directory
mkdir -p "$SHARED_CONFIG_DIR"

# Generate strong passwords
generate_password() {
    openssl rand -base64 32 | tr -d "=+/" | cut -c1-25
}

# Generate all passwords once
POSTGRES_PASSWORD=$(generate_password)
REDIS_PASSWORD=$(generate_password)
GRAFANA_PASSWORD=$(generate_password)
CHROMA_AUTH_TOKEN=$(openssl rand -hex 32)
JWT_SECRET=$(openssl rand -hex 32)
ENCRYPTION_KEY=$(openssl rand -hex 32)

# Create shared environment file
cat > "$SHARED_CONFIG_DIR/.env.global" << EOF
# Vietnamese Chatbot Project - Shared Configuration
# Generated on: $(date)
# DO NOT EDIT MANUALLY - Use setup-project-secrets.sh

# Project Configuration
PROJECT_NAME=vietnamese-chatbot
ENVIRONMENT=development

# Database Configuration (PostgreSQL)
POSTGRES_USER=kb_admin
POSTGRES_PASSWORD=$POSTGRES_PASSWORD
POSTGRES_DB=knowledge_base_v2
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
DATABASE_URL=postgresql://kb_admin:$POSTGRES_PASSWORD@localhost:5432/knowledge_base_v2

# Redis Configuration
REDIS_PASSWORD=$REDIS_PASSWORD
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_URL=redis://localhost:6379

# ChromaDB Configuration
CHROMA_AUTH_TOKEN=$CHROMA_AUTH_TOKEN
CHROMA_HOST=localhost
CHROMA_PORT=8000

# Monitoring Configuration
GRAFANA_PASSWORD=$GRAFANA_PASSWORD
PROMETHEUS_RETENTION=30d

# Security Configuration
JWT_SECRET=$JWT_SECRET
ENCRYPTION_KEY=$ENCRYPTION_KEY

# Vietnamese NLP Configuration
VIETNAMESE_NLP_MODEL=pyvi
EMBEDDING_MODEL=Qwen/Qwen3-Embedding-0.6B
EMBEDDING_DIMENSION=1024
EOF

echo "‚úÖ Shared configuration created: $SHARED_CONFIG_DIR/.env.global"

# Distribute to each module
for module in "${MODULES[@]}"; do
    module_dir="$PROJECT_ROOT/$module"
    if [ -d "$module_dir" ]; then
        echo "üì¶ Setting up $module..."
        
        # Create module-specific .env
        cat > "$module_dir/.env" << EOF
# $module Environment Configuration
# Auto-generated - Source from shared config

# Load shared configuration
$(cat "$SHARED_CONFIG_DIR/.env.global")

# Module-specific configuration
MODULE_NAME=$module
EOF

        # Add module-specific settings
        case $module in
            "FR-02.1")
                cat >> "$module_dir/.env" << EOF
API_PORT=8000
SERVICE_NAME=fr02-database-system
CONTAINER_PREFIX=fr02
EOF
                ;;
            "FR-02.2")
                cat >> "$module_dir/.env" << EOF
API_PORT=8001
SERVICE_NAME=fr02-unified-api
CONTAINER_PREFIX=fr02-api
EOF
                ;;
            "FR-07")
                cat >> "$module_dir/.env" << EOF
API_PORT=8002
DASHBOARD_PORT=8501
SERVICE_NAME=fr07-analytics
CONTAINER_PREFIX=fr07
EOF
                ;;
            "FR-08")
                cat >> "$module_dir/.env" << EOF
API_PORT=8003
SERVICE_NAME=fr08-admin-tools
CONTAINER_PREFIX=fr08
ADMIN_EMAIL=admin@company.com
EOF
                ;;
        esac
        
        echo "‚úÖ $module environment configured"
    else
        echo "‚ö†Ô∏è  Module directory not found: $module_dir"
    fi
done

# Create password reference file
cat > "$SHARED_CONFIG_DIR/passwords.txt" << EOF
Vietnamese Chatbot Project - Password Reference
Generated on: $(date)

‚ö†Ô∏è  KEEP THIS FILE SECURE - Contains all system passwords

Database (PostgreSQL):
  User: kb_admin
  Password: $POSTGRES_PASSWORD
  Connection: postgresql://kb_admin:$POSTGRES_PASSWORD@localhost:5432/knowledge_base_v2

Redis:
  Password: $REDIS_PASSWORD
  Connection: redis://localhost:6379

ChromaDB:
  Auth Token: $CHROMA_AUTH_TOKEN

Grafana Admin:
  Username: admin
  Password: $GRAFANA_PASSWORD
  URL: http://localhost:3000

Security Keys:
  JWT Secret: $JWT_SECRET
  Encryption Key: $ENCRYPTION_KEY

üîê To regenerate all passwords: ./scripts/setup-project-secrets.sh
EOF

chmod 600 "$SHARED_CONFIG_DIR/passwords.txt"

echo ""
echo "üéâ Project secrets setup completed!"
echo ""
echo "üìÅ Shared configuration: $SHARED_CONFIG_DIR/.env.global"
echo "üîë Password reference: $SHARED_CONFIG_DIR/passwords.txt"
echo ""
echo "üìã Next steps:"
echo "1. Review passwords in: $SHARED_CONFIG_DIR/passwords.txt"
echo "2. Start modules with: docker-compose up -d"
echo "3. All modules will use the same database credentials"
echo ""
echo "‚ö†Ô∏è  Important: Add shared-config/ to .gitignore to avoid committing secrets!"
```

## üê≥ **DOCKER COMPOSE NETWORKS**

### **Shared Network Configuration**
```yaml
# shared-config/docker-compose.shared.yml
# Shared infrastructure for all modules

version: '3.8'

networks:
  vietnamese-chatbot:
    driver: bridge
    ipam:
      config:
        - subnet: 172.30.0.0/16

services:
  # Shared PostgreSQL Database
  shared-postgres:
    image: postgres:15-alpine
    container_name: vietnamese-chatbot-postgres
    environment:
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    ports:
      - "${POSTGRES_PORT}:5432"
    volumes:
      - shared_postgres_data:/var/lib/postgresql/data
      - ../scripts/init-database.sql:/docker-entrypoint-initdb.d/init.sql
    networks:
      - vietnamese-chatbot
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Shared Redis Cache  
  shared-redis:
    image: redis:7-alpine
    container_name: vietnamese-chatbot-redis
    command: redis-server --requirepass ${REDIS_PASSWORD}
    ports:
      - "${REDIS_PORT}:6379"
    volumes:
      - shared_redis_data:/data
    networks:
      - vietnamese-chatbot
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "redis-cli -a ${REDIS_PASSWORD} ping | grep PONG"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Shared ChromaDB
  shared-chroma:
    image: chromadb/chroma:latest
    container_name: vietnamese-chatbot-chroma
    environment:
      - CHROMA_SERVER_AUTH_CREDENTIALS=${CHROMA_AUTH_TOKEN}
    ports:
      - "${CHROMA_PORT}:8000"
    volumes:
      - shared_chroma_data:/chroma/chroma
    networks:
      - vietnamese-chatbot
    restart: unless-stopped

volumes:
  shared_postgres_data:
    driver: local
  shared_redis_data:
    driver: local
  shared_chroma_data:
    driver: local

networks:
  vietnamese-chatbot:
    external: false
```

### **Module Docker Compose Integration**
```yaml
# FR-02.1/docker-compose.yml
version: '3.8'

services:
  fr02-api:
    build: .
    container_name: fr02-database-api
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=redis://:${REDIS_PASSWORD}@shared-redis:6379
      - CHROMA_URL=http://shared-chroma:8000
    ports:
      - "${API_PORT}:8000"
    depends_on:
      - shared-postgres
      - shared-redis
      - shared-chroma
    networks:
      - vietnamese-chatbot

networks:
  vietnamese-chatbot:
    external: true
    name: shared-config_vietnamese-chatbot

# Include shared services
include:
  - ../shared-config/docker-compose.shared.yml
```

## üöÄ **MASTER DEPLOYMENT SCRIPT**

### **One-Command Setup**
```bash
#!/bin/bash
# scripts/deploy-all-modules.sh
# Deploy entire Vietnamese Chatbot system with consistent configuration

PROJECT_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
MODULES=("FR-02.1" "FR-02.2" "FR-07" "FR-08")

echo "üöÄ Deploying Vietnamese Chatbot System..."

# Step 1: Generate shared secrets
echo "üîê Setting up project secrets..."
$PROJECT_ROOT/scripts/setup-project-secrets.sh

# Step 2: Start shared infrastructure
echo "üèóÔ∏è  Starting shared infrastructure..."
cd "$PROJECT_ROOT/shared-config"
docker-compose -f docker-compose.shared.yml up -d

# Wait for databases to be ready
echo "‚è≥ Waiting for databases to be ready..."
timeout 120 bash -c 'until docker exec vietnamese-chatbot-postgres pg_isready -U kb_admin -d knowledge_base_v2; do sleep 5; done'
timeout 120 bash -c 'until docker exec vietnamese-chatbot-redis redis-cli -a $REDIS_PASSWORD ping | grep PONG; do sleep 5; done'

# Step 3: Deploy each module
for module in "${MODULES[@]}"; do
    if [ -d "$PROJECT_ROOT/$module" ]; then
        echo "üì¶ Deploying $module..."
        cd "$PROJECT_ROOT/$module"
        
        # Load environment
        source .env
        
        # Deploy module
        docker-compose up -d
        
        # Health check
        sleep 10
        module_port=$(grep "API_PORT=" .env | cut -d'=' -f2)
        if curl -f "http://localhost:$module_port/health" > /dev/null 2>&1; then
            echo "‚úÖ $module deployed successfully"
        else
            echo "‚ö†Ô∏è  $module health check failed - check logs"
        fi
    fi
done

# Step 4: Verify system integration
echo "üîç Verifying system integration..."
echo "üìä Service Status:"
docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"

echo ""
echo "üåê Access URLs:"
echo "  FR-02.1 Database API: http://localhost:8000"
echo "  FR-02.2 Unified API: http://localhost:8001" 
echo "  FR-07 Analytics: http://localhost:8002"
echo "  FR-07 Dashboard: http://localhost:8501"
echo "  FR-08 Admin Tools: http://localhost:8003"
echo "  Grafana: http://localhost:3000 (admin / $GRAFANA_PASSWORD)"
echo ""
echo "üîë All passwords saved in: shared-config/passwords.txt"
echo ""
echo "üéâ Vietnamese Chatbot System deployed successfully!"
```

## üìã **BEST PRACTICES**

### **1. Version Control Exclusions**
```gitignore
# .gitignore (project root)
# Exclude all environment files
**/.env
**/.env.*
shared-config/.env.global
shared-config/passwords.txt

# But keep templates
!**/.env.example
!**/.env.template
```

### **2. Environment Validation**
```bash
#!/bin/bash
# scripts/validate-environment.sh
# Validate all modules have consistent configuration

check_module_env() {
    local module=$1
    local env_file="$module/.env"
    
    echo "üîç Validating $module environment..."
    
    if [ ! -f "$env_file" ]; then
        echo "‚ùå Missing .env file in $module"
        return 1
    fi
    
    # Check critical variables
    local required_vars=("POSTGRES_PASSWORD" "REDIS_PASSWORD" "DATABASE_URL")
    for var in "${required_vars[@]}"; do
        if ! grep -q "^$var=" "$env_file"; then
            echo "‚ùå Missing $var in $module/.env"
            return 1
        fi
    done
    
    echo "‚úÖ $module environment valid"
    return 0
}

# Validate all modules
for module in "${MODULES[@]}"; do
    check_module_env "$module" || exit 1
done

echo "üéâ All module environments validated!"
```

### **3. Password Rotation Script**
```bash
#!/bin/bash
# scripts/rotate-passwords.sh
# Rotate all system passwords safely

echo "üîÑ Rotating system passwords..."

# Stop all services
docker-compose -f shared-config/docker-compose.shared.yml down

# Generate new secrets
./scripts/setup-project-secrets.sh

# Update database with new passwords
# (implementation depends on specific requirements)

# Restart services
./scripts/deploy-all-modules.sh

echo "‚úÖ Password rotation completed!"
```

V·ªõi approach n√†y, b·∫°n ch·ªâ c·∫ßn ch·∫°y m·ªôt l·ªánh duy nh·∫•t `./scripts/deploy-all-modules.sh` v√† t·∫•t c·∫£ modules s·∫Ω t·ª± ƒë·ªông c√≥ c√πng credentials, tr√°nh ho√†n to√†n v·∫•n ƒë·ªÅ password mismatch.
