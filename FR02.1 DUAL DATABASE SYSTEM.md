# TÀI LIỆU THIẾT KẾ KỸ THUẬT FR-02.1
## HỆ THỐNG CƠ SỞ DỮ LIỆU KÉP (DUAL DATABASE SYSTEM)

---

**Phiên bản:** 1.0  
**Ngày:** 31/08/2025  
**Module:** FR-02.1 - Hệ thống CSDL kép  
**Mức độ ưu tiên:** CRITICAL  
**Người soạn thảo:** Technical Design Team  

---

## 📋 **TỔNG QUAN THIẾT KẾ**

### **Mục tiêu**
Thiết kế và triển khai hệ thống cơ sở dữ liệu kép bao gồm:
- **Vector Database**: Lưu trữ embeddings cho tìm kiếm ngữ nghĩa (≥100k chunks)
- **Relational Database**: Quản lý metadata, người dùng và phân quyền

### **Kiến trúc tổng quan**
```mermaid
graph TB
    subgraph "Application Layer"
        API[🌐 REST API Services]
        RAG[🤖 RAG Engine]
    end
    
    subgraph "🗄️ FR-02.1: DUAL DATABASE SYSTEM"
        subgraph "📊 Vector Database Layer"
            VectorDB[🔢 Vector Database<br/>Chroma/FAISS]
            VectorIndex[📇 Vector Index<br/>HNSW/IVF]
        end
        
        subgraph "🗃️ Relational Database Layer"
            PostgreSQL[(🐘 PostgreSQL<br/>Metadata + Users)]
            ConnPool[🔄 Connection Pool<br/>pgbouncer]
        end
        
        subgraph "⚡ Cache Layer"
            RedisCache[🔴 Redis<br/>Query Cache]
        end
    end
    
    subgraph "💾 Storage Layer"
        VectorStorage[📁 Vector Storage<br/>Persistent Volume]
        SQLStorage[📁 SQL Storage<br/>Persistent Volume]
    end
    
    API --> VectorDB
    API --> PostgreSQL
    RAG --> VectorDB
    RAG --> PostgreSQL
    
    VectorDB --> VectorIndex
    PostgreSQL --> ConnPool
    API -.-> RedisCache
    
    VectorDB --> VectorStorage
    PostgreSQL --> SQLStorage
    
    classDef app fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef vector fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef sql fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef cache fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef storage fill:#e0f2f1,stroke:#00796b,stroke-width:2px
    
    class API,RAG app
    class VectorDB,VectorIndex vector
    class PostgreSQL,ConnPool sql
    class RedisCache cache
    class VectorStorage,SQLStorage storage
```

---

## 🛠️ **CHUẨN BỊ MÔI TRƯỜNG PHÁT TRIỂN**

### **1. Yêu cầu hạ tầng**

#### **💻 Phần cứng tối thiểu**
- **CPU**: 8 cores (Intel i7 hoặc AMD Ryzen 7)
- **RAM**: 32GB (khuyến nghị 64GB)
- **Storage**: 
  - SSD 500GB cho OS và development tools
  - SSD 1TB cho databases và data storage
  - HDD 2TB cho backup (optional)
- **Network**: Gigabit Ethernet

#### **🖥️ Hệ điều hành**
- **Primary**: Ubuntu 22.04 LTS / CentOS 8
- **Alternative**: macOS 12+ / Windows 11 với WSL2

### **2. Phần mềm cần thiết**

#### **🐳 Container & Orchestration**
```bash
# Docker & Docker Compose
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
sudo curl -L "https://github.com/docker/compose/releases/download/v2.20.0/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose
```

#### **🐍 Python Development Stack**
```bash
# Python 3.11+ với virtual environment
sudo apt-get update
sudo apt-get install python3.11 python3.11-venv python3.11-dev
python3.11 -m venv venv-fr02
source venv-fr02/bin/activate
```

#### **🗄️ Database Tools**
```bash
# PostgreSQL Client Tools
sudo apt-get install postgresql-client-15
# GUI Tools (choose one)
# - pgAdmin 4 (Web-based)
# - DBeaver Community (Desktop)
# - TablePlus (macOS/Windows)
```

#### **📊 Monitoring & Development Tools**
```bash
# Essential development tools
sudo apt-get install git curl wget htop tree jq
# Monitoring tools
sudo apt-get install prometheus-node-exporter
```

---

## 🏗️ **KIẾN TRÚC CHI TIẾT**

### **1. Vector Database Design**

#### **🔢 Chroma Database Architecture**
```mermaid
graph TB
    subgraph "🔢 CHROMA VECTOR DATABASE"
        subgraph "API Layer"
            ChromaAPI[🌐 Chroma HTTP API<br/>Port: 8000]
            ChromaClient[📱 Chroma Client<br/>Python SDK]
        end
        
        subgraph "Storage Engine"
            Collections[📚 Collections<br/>Logical Groupings]
            Embeddings[🧮 Embeddings<br/>Vector Storage]
            Metadata[🏷️ Metadata<br/>JSON Documents]
            Indexes[📇 Vector Indexes<br/>HNSW Algorithm]
        end
        
        subgraph "Persistence Layer"
            SQLiteDB[(🗃️ SQLite<br/>Metadata Storage)]
            VectorFiles[📁 Vector Files<br/>Binary Storage)]
        end
    end
    
    ChromaAPI --> Collections
    ChromaClient --> ChromaAPI
    Collections --> Embeddings
    Collections --> Metadata
    Embeddings --> Indexes
    
    Collections --> SQLiteDB
    Embeddings --> VectorFiles
    Metadata --> SQLiteDB
    
    classDef api fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef engine fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef persistence fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    
    class ChromaAPI,ChromaClient api
    class Collections,Embeddings,Metadata,Indexes engine
    class SQLiteDB,VectorFiles persistence
```

#### **📊 Collection Schema Design**
```json
{
  "collection_name": "knowledge_base_v1",
  "embedding_dimension": 1536,
  "distance_metric": "cosine",
  "index_type": "HNSW",
  "max_elements": 100000,
  "ef_construction": 200,
  "M": 16
}
```

### **2. Relational Database Design**

#### **🐘 PostgreSQL Schema Architecture**
```mermaid
erDiagram
    USERS {
        uuid user_id PK
        string username UK
        string email UK
        string password_hash
        enum user_level
        string department
        boolean is_active
        timestamp created_at
        timestamp updated_at
        jsonb preferences
    }
    
    DOCUMENTS {
        uuid document_id PK
        string title
        string file_path
        string content_hash
        enum document_type
        enum access_level
        string department_owner
        uuid author_id FK
        string version
        integer chunk_count
        timestamp created_at
        timestamp updated_at
        jsonb metadata
    }
    
    DOCUMENT_CHUNKS {
        uuid chunk_id PK
        uuid document_id FK
        integer chunk_index
        text content
        string vector_id
        jsonb chunk_metadata
        timestamp created_at
    }
    
    USER_PERMISSIONS {
        uuid permission_id PK
        uuid user_id FK
        uuid document_id FK
        enum permission_type
        timestamp granted_at
        timestamp expires_at
        uuid granted_by FK
    }
    
    AUDIT_LOGS {
        uuid log_id PK
        uuid user_id FK
        string action
        string resource_type
        string resource_id
        jsonb action_details
        string ip_address
        string user_agent
        timestamp timestamp
    }
    
    CHAT_SESSIONS {
        uuid session_id PK
        uuid user_id FK
        string session_name
        timestamp created_at
        timestamp last_activity
        jsonb session_metadata
    }
    
    CHAT_MESSAGES {
        uuid message_id PK
        uuid session_id FK
        enum message_type
        text content
        jsonb response_metadata
        timestamp timestamp
    }
    
    USERS ||--o{ DOCUMENTS : creates
    USERS ||--o{ USER_PERMISSIONS : has
    DOCUMENTS ||--o{ DOCUMENT_CHUNKS : contains
    DOCUMENTS ||--o{ USER_PERMISSIONS : requires
    USERS ||--o{ AUDIT_LOGS : generates
    USERS ||--o{ CHAT_SESSIONS : owns
    CHAT_SESSIONS ||--o{ CHAT_MESSAGES : contains
    USERS ||--o{ USER_PERMISSIONS : grants
```

---

## 📋 **LỘ TRÌNH TRIỂN KHAI**

```mermaid
graph TB
    Start([🚀 Bắt đầu FR-02.1])
    
    subgraph "Phase 1: Environment Setup"
        P1_1[💻 Setup Development Environment]
        P1_2[🐳 Install Docker & Dependencies]
        P1_3[📁 Create Project Structure]
        P1_4[🔧 Configure Development Tools]
    end
    
    subgraph "Phase 2: PostgreSQL Setup"
        P2_1[🐘 Deploy PostgreSQL Container]
        P2_2[🗃️ Create Database Schema]
        P2_3[👤 Setup User Management]
        P2_4[🔄 Configure Connection Pool]
        P2_5[✅ Test PostgreSQL Setup]
        
        P2_Fail{❌ PostgreSQL<br/>Test Failed?}
        P2_Debug[🔍 Debug PostgreSQL Issues]
    end
    
    subgraph "Phase 3: Vector Database Setup"
        P3_1[🔢 Deploy Chroma Container]
        P3_2[📚 Create Vector Collections]
        P3_3[🧮 Configure Embedding Settings]
        P3_4[📇 Setup Vector Indexes]
        P3_5[✅ Test Vector Operations]
        
        P3_Fail{❌ Vector DB<br/>Test Failed?}
        P3_Debug[🔍 Debug Vector DB Issues]
    end
    
    subgraph "Phase 4: Integration & Testing"
        P4_1[🔗 Integrate Both Databases]
        P4_2[⚡ Setup Redis Cache]
        P4_3[📊 Configure Monitoring]
        P4_4[🧪 Run Integration Tests]
        P4_5[📈 Performance Testing]
        
        P4_Fail{❌ Integration<br/>Test Failed?}
        P4_Debug[🔍 Debug Integration Issues]
    end
    
    subgraph "Phase 5: Production Preparation"
        P5_1[🔒 Security Hardening]
        P5_2[💾 Setup Backup Strategy]
        P5_3[📋 Create Documentation]
        P5_4[✅ Final Acceptance Testing]
    end
    
    End([✅ FR-02.1 Complete])
    
    %% Main Flow
    Start --> P1_1
    P1_1 --> P1_2
    P1_2 --> P1_3
    P1_3 --> P1_4
    
    P1_4 --> P2_1
    P2_1 --> P2_2
    P2_2 --> P2_3
    P2_3 --> P2_4
    P2_4 --> P2_5
    
    P2_5 --> P2_Fail
    P2_Fail -->|Yes| P2_Debug
    P2_Debug --> P2_1
    P2_Fail -->|No| P3_1
    
    P3_1 --> P3_2
    P3_2 --> P3_3
    P3_3 --> P3_4
    P3_4 --> P3_5
    
    P3_5 --> P3_Fail
    P3_Fail -->|Yes| P3_Debug
    P3_Debug --> P3_1
    P3_Fail -->|No| P4_1
    
    P4_1 --> P4_2
    P4_2 --> P4_3
    P4_3 --> P4_4
    P4_4 --> P4_5
    
    P4_5 --> P4_Fail
    P4_Fail -->|Yes| P4_Debug
    P4_Debug --> P4_1
    P4_Fail -->|No| P5_1
    
    P5_1 --> P5_2
    P5_2 --> P5_3
    P5_3 --> P5_4
    P5_4 --> End
    
    %% Styling
    classDef phase1 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef phase2 fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef phase3 fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef phase4 fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef phase5 fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    classDef decision fill:#ffebee,stroke:#c62828,stroke-width:2px
    classDef debug fill:#fff3e0,stroke:#ff6f00,stroke-width:2px
    classDef startend fill:#e0f2f1,stroke:#00796b,stroke-width:3px
    
    class P1_1,P1_2,P1_3,P1_4 phase1
    class P2_1,P2_2,P2_3,P2_4,P2_5 phase2
    class P3_1,P3_2,P3_3,P3_4,P3_5 phase3
    class P4_1,P4_2,P4_3,P4_4,P4_5 phase4
    class P5_1,P5_2,P5_3,P5_4 phase5
    class P2_Fail,P3_Fail,P4_Fail decision
    class P2_Debug,P3_Debug,P4_Debug debug
    class Start,End startend
```

---

## 🔧 **CHI TIẾT TỪNG BƯỚC TRIỂN KHAI**

### **Phase 1: Environment Setup (Tuần 1)**

#### **Bước 1.1: Setup Development Environment** ⏱️ 4 giờ
```bash
# Tạo thư mục dự án
mkdir -p fr02-dual-database
cd fr02-dual-database

# Tạo cấu trúc thư mục
mkdir -p {config,docker,scripts,tests,docs,data}
mkdir -p data/{postgres,chroma,redis,backups}

# Setup Git repository
git init
echo "venv-fr02/" > .gitignore
echo "*.env" >> .gitignore
echo "data/" >> .gitignore
```

#### **Bước 1.2: Install Docker & Dependencies** ⏱️ 2 giờ
```bash
# Verify Docker installation
docker --version
docker-compose --version

# Pull required images
docker pull postgres:15
docker pull chromadb/chroma:latest
docker pull redis:7-alpine
```

#### **Bước 1.3: Create Project Structure** ⏱️ 2 giờ
```
fr02-dual-database/
├── config/
│   ├── postgres.conf
│   ├── chroma.yml
│   └── redis.conf
├── docker/
│   └── docker-compose.yml
├── scripts/
│   ├── setup.sh
│   ├── init-postgres.sql
│   └── test-connections.py
├── tests/
│   ├── test_postgres.py
│   ├── test_chroma.py
│   └── test_integration.py
├── docs/
│   └── README.md
└── data/
    ├── postgres/
    ├── chroma/
    ├── redis/
    └── backups/
```

#### **Bước 1.4: Configure Development Tools** ⏱️ 2 giờ
```python
# requirements.txt
psycopg2-binary==2.9.7
chromadb==0.4.10
redis==4.6.0
sqlalchemy==2.0.20
alembic==1.12.0
pytest==7.4.0
python-dotenv==1.0.0
```

### **Phase 2: PostgreSQL Setup (Tuần 1-2)**

#### **Bước 2.1: Deploy PostgreSQL Container** ⏱️ 3 giờ
```yaml
# docker/docker-compose.yml
version: '3.8'
services:
  postgres:
    image: postgres:15
    container_name: fr02-postgres
    environment:
      POSTGRES_DB: knowledge_base
      POSTGRES_USER: kb_admin
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    ports:
      - "5432:5432"
    volumes:
      - ./data/postgres:/var/lib/postgresql/data
      - ./config/postgres.conf:/etc/postgresql/postgresql.conf
      - ./scripts/init-postgres.sql:/docker-entrypoint-initdb.d/init.sql
    command: postgres -c config_file=/etc/postgresql/postgresql.conf
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U kb_admin -d knowledge_base"]
      interval: 30s
      timeout: 10s
      retries: 5
```

#### **Bước 2.2: Create Database Schema** ⏱️ 6 giờ
```sql
-- scripts/init-postgres.sql

-- Enable required extensions
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pg_trgm";
CREATE EXTENSION IF NOT EXISTS "btree_gin";

-- User levels enum
CREATE TYPE user_level AS ENUM ('guest', 'employee', 'manager', 'director', 'admin');

-- Document types enum
CREATE TYPE document_type AS ENUM ('policy', 'procedure', 'technical_guide', 'report', 'manual');

-- Access levels enum
CREATE TYPE access_level AS ENUM ('public', 'employee_only', 'manager_only', 'director_only', 'system_admin');

-- Users table
CREATE TABLE users (
    user_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    username VARCHAR(50) UNIQUE NOT NULL,
    email VARCHAR(255) UNIQUE NOT NULL,
    password_hash VARCHAR(255) NOT NULL,
    user_level user_level NOT NULL DEFAULT 'employee',
    department VARCHAR(100),
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    preferences JSONB DEFAULT '{}'::jsonb
);

-- Documents table
CREATE TABLE documents (
    document_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    title VARCHAR(500) NOT NULL,
    file_path VARCHAR(1000),
    content_hash VARCHAR(64) UNIQUE,
    document_type document_type NOT NULL,
    access_level access_level NOT NULL DEFAULT 'employee_only',
    department_owner VARCHAR(100),
    author_id UUID REFERENCES users(user_id),
    version VARCHAR(20) DEFAULT '1.0',
    chunk_count INTEGER DEFAULT 0,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    metadata JSONB DEFAULT '{}'::jsonb
);

-- Document chunks table
CREATE TABLE document_chunks (
    chunk_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    document_id UUID NOT NULL REFERENCES documents(document_id) ON DELETE CASCADE,
    chunk_index INTEGER NOT NULL,
    content TEXT NOT NULL,
    vector_id VARCHAR(100) UNIQUE, -- Reference to Chroma vector
    chunk_metadata JSONB DEFAULT '{}'::jsonb,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(document_id, chunk_index)
);

-- User permissions table
CREATE TABLE user_permissions (
    permission_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_id UUID NOT NULL REFERENCES users(user_id) ON DELETE CASCADE,
    document_id UUID NOT NULL REFERENCES documents(document_id) ON DELETE CASCADE,
    permission_type VARCHAR(20) NOT NULL DEFAULT 'read',
    granted_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    expires_at TIMESTAMP WITH TIME ZONE,
    granted_by UUID REFERENCES users(user_id),
    UNIQUE(user_id, document_id)
);

-- Audit logs table
CREATE TABLE audit_logs (
    log_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_id UUID REFERENCES users(user_id),
    action VARCHAR(50) NOT NULL,
    resource_type VARCHAR(50) NOT NULL,
    resource_id VARCHAR(100),
    action_details JSONB DEFAULT '{}'::jsonb,
    ip_address INET,
    user_agent TEXT,
    timestamp TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- Chat sessions table
CREATE TABLE chat_sessions (
    session_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_id UUID NOT NULL REFERENCES users(user_id) ON DELETE CASCADE,
    session_name VARCHAR(200),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    last_activity TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    session_metadata JSONB DEFAULT '{}'::jsonb
);

-- Chat messages table
CREATE TABLE chat_messages (
    message_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    session_id UUID NOT NULL REFERENCES chat_sessions(session_id) ON DELETE CASCADE,
    message_type VARCHAR(20) NOT NULL CHECK (message_type IN ('user', 'assistant', 'system')),
    content TEXT NOT NULL,
    response_metadata JSONB DEFAULT '{}'::jsonb,
    timestamp TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- Indexes for performance
CREATE INDEX idx_users_username ON users(username);
CREATE INDEX idx_users_email ON users(username);
CREATE INDEX idx_users_department ON users(department);
CREATE INDEX idx_users_level ON users(user_level);

CREATE INDEX idx_documents_title ON documents USING gin(to_tsvector('english', title));
CREATE INDEX idx_documents_content_hash ON documents(content_hash);
CREATE INDEX idx_documents_type ON documents(document_type);
CREATE INDEX idx_documents_access_level ON documents(access_level);
CREATE INDEX idx_documents_department ON documents(department_owner);
CREATE INDEX idx_documents_created_at ON documents(created_at DESC);

CREATE INDEX idx_chunks_document_id ON document_chunks(document_id);
CREATE INDEX idx_chunks_vector_id ON document_chunks(vector_id);
CREATE INDEX idx_chunks_content ON document_chunks USING gin(to_tsvector('english', content));

CREATE INDEX idx_permissions_user_id ON user_permissions(user_id);
CREATE INDEX idx_permissions_document_id ON user_permissions(document_id);

CREATE INDEX idx_audit_logs_user_id ON audit_logs(user_id);
CREATE INDEX idx_audit_logs_timestamp ON audit_logs(timestamp DESC);
CREATE INDEX idx_audit_logs_action ON audit_logs(action);

CREATE INDEX idx_chat_sessions_user_id ON chat_sessions(user_id);
CREATE INDEX idx_chat_sessions_last_activity ON chat_sessions(last_activity DESC);

CREATE INDEX idx_chat_messages_session_id ON chat_messages(session_id);
CREATE INDEX idx_chat_messages_timestamp ON chat_messages(timestamp DESC);

-- Updated_at trigger function
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = CURRENT_TIMESTAMP;
    RETURN NEW;
END;
$$ language 'plpgsql';

-- Apply updated_at triggers
CREATE TRIGGER update_users_updated_at BEFORE UPDATE ON users 
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();
CREATE TRIGGER update_documents_updated_at BEFORE UPDATE ON documents 
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();
```

#### **Bước 2.3: Setup User Management** ⏱️ 2 giờ
```sql
-- Create initial admin user
INSERT INTO users (username, email, password_hash, user_level, department) VALUES
('admin', 'admin@company.com', '$2b$12$encrypted_password_hash', 'admin', 'IT'),
('test_user', 'test@company.com', '$2b$12$encrypted_password_hash', 'employee', 'R&D');

-- Create sample documents for testing
INSERT INTO documents (title, document_type, access_level, department_owner, author_id) VALUES
('Company Policy Manual', 'policy', 'employee_only', 'HR', (SELECT user_id FROM users WHERE username = 'admin')),
('Technical Guidelines', 'technical_guide', 'employee_only', 'R&D', (SELECT user_id FROM users WHERE username = 'admin'));
```

#### **Bước 2.4: Configure Connection Pool** ⏱️ 3 giờ
```yaml
# Add pgbouncer to docker-compose.yml
  pgbouncer:
    image: pgbouncer/pgbouncer:latest
    container_name: fr02-pgbouncer
    environment:
      DATABASES_HOST: postgres
      DATABASES_PORT: 5432
      DATABASES_USER: kb_admin
      DATABASES_PASSWORD: ${POSTGRES_PASSWORD}
      DATABASES_DBNAME: knowledge_base
      POOL_MODE: transaction
      SERVER_RESET_QUERY: DISCARD ALL
      MAX_CLIENT_CONN: 1000
      DEFAULT_POOL_SIZE: 20
      MIN_POOL_SIZE: 5
    ports:
      - "6432:5432"
    depends_on:
      postgres:
        condition: service_healthy
```

#### **Bước 2.5: Test PostgreSQL Setup** ⏱️ 2 giờ
```python
# tests/test_postgres.py
import pytest
import psycopg2
from sqlalchemy import create_engine, text
import os

def test_postgres_connection():
    """Test basic PostgreSQL connection"""
    conn_string = f"postgresql://kb_admin:{os.getenv('POSTGRES_PASSWORD')}@localhost:5432/knowledge_base"
    engine = create_engine(conn_string)
    
    with engine.connect() as conn:
        result = conn.execute(text("SELECT version()"))
        version = result.fetchone()[0]
        assert "PostgreSQL 15" in version

def test_schema_creation():
    """Test that all tables are created"""
    conn_string = f"postgresql://kb_admin:{os.getenv('POSTGRES_PASSWORD')}@localhost:5432/knowledge_base"
    engine = create_engine(conn_string)
    
    expected_tables = ['users', 'documents', 'document_chunks', 'user_permissions', 'audit_logs', 'chat_sessions', 'chat_messages']
    
    with engine.connect() as conn:
        result = conn.execute(text("SELECT table_name FROM information_schema.tables WHERE table_schema = 'public'"))
        tables = [row[0] for row in result.fetchall()]
        
        for table in expected_tables:
            assert table in tables

def test_user_creation():
    """Test user CRUD operations"""
    conn_string = f"postgresql://kb_admin:{os.getenv('POSTGRES_PASSWORD')}@localhost:5432/knowledge_base"
    engine = create_engine(conn_string)
    
    with engine.connect() as conn:
        # Test insert
        result = conn.execute(text("""
            INSERT INTO users (username, email, password_hash, user_level) 
            VALUES ('test_crud', 'test_crud@test.com', 'hash123', 'employee') 
            RETURNING user_id
        """))
        user_id = result.fetchone()[0]
        conn.commit()
        
        # Test select
        result = conn.execute(text("SELECT username FROM users WHERE user_id = :user_id"), {"user_id": user_id})
        username = result.fetchone()[0]
        assert username == 'test_crud'
        
        # Test cleanup
        conn.execute(text("DELETE FROM users WHERE user_id = :user_id"), {"user_id": user_id})
        conn.commit()
```

### **Phase 3: Vector Database Setup (Tuần 2)**

#### **Bước 3.1: Deploy Chroma Container** ⏱️ 3 giờ
```yaml
# Add to docker-compose.yml
  chroma:
    image: chromadb/chroma:latest
    container_name: fr02-chroma
    ports:
      - "8000:8000"
    volumes:
      - ./data/chroma:/chroma/chroma
    environment:
      - CHROMA_SERVER_HOST=0.0.0.0
      - CHROMA_SERVER_PORT=8000
      - CHROMA_SERVER_CORS_ALLOW_ORIGINS=*
      - PERSIST_DIRECTORY=/chroma/chroma
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/api/v1/heartbeat || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
```

#### **Bước 3.2: Create Vector Collections** ⏱️ 4 giờ
```python
# scripts/setup_chroma.py
import chromadb
from chromadb.config import Settings
import uuid

def setup_chroma_collections():
    # Connect to Chroma
    client = chromadb.HttpClient(
        host="localhost",
        port=8000,
        settings=Settings(
            chroma_server_host="localhost",
            chroma_server_port=8000,
            chroma_server_cors_allow_origins="*"
        )
    )
    
    # Create main knowledge base collection
    collection_name = "knowledge_base_v1"
    
    try:
        # Delete existing collection if it exists
        try:
            client.delete_collection(collection_name)
        except:
            pass
        
        # Create new collection
        collection = client.create_collection(
            name=collection_name,
            metadata={
                "description": "Main knowledge base for chatbot",
                "embedding_dimension": 1536,  # OpenAI ada-002
                "distance_metric": "cosine",
                "max_elements": 100000,
                "created_at": str(datetime.now()),
                "version": "1.0"
            },
            embedding_function=None  # We'll provide embeddings manually
        )
        
        print(f"✅ Collection '{collection_name}' created successfully")
        
        # Create department-specific collections
        departments = ['HR', 'R&D', 'Sales', 'Finance', 'IT']
        for dept in departments:
            dept_collection_name = f"dept_{dept.lower()}_v1"
            try:
                client.delete_collection(dept_collection_name)
            except:
                pass
                
            dept_collection = client.create_collection(
                name=dept_collection_name,
                metadata={
                    "description": f"{dept} department knowledge base",
                    "department": dept,
                    "embedding_dimension": 1536,
                    "distance_metric": "cosine",
                    "parent_collection": collection_name
                }
            )
            print(f"✅ Department collection '{dept_collection_name}' created")
        
        return True
        
    except Exception as e:
        print(f"❌ Error creating collections: {e}")
        return False

if __name__ == "__main__":
    setup_chroma_collections()
```

#### **Bước 3.3: Configure Embedding Settings** ⏱️ 3 giờ
```python
# scripts/embedding_config.py
import openai
import numpy as np
from typing import List
import os

class EmbeddingConfig:
    def __init__(self):
        self.openai_api_key = os.getenv('OPENAI_API_KEY')
        self.model_name = "text-embedding-ada-002"
        self.dimension = 1536
        self.max_tokens = 8191
        
    def generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings for a list of texts"""
        try:
            response = openai.embeddings.create(
                model=self.model_name,
                input=texts
            )
            
            embeddings = []
            for data in response.data:
                embeddings.append(data.embedding)
                
            return embeddings
            
        except Exception as e:
            print(f"❌ Error generating embeddings: {e}")
            return []
    
    def validate_embedding(self, embedding: List[float]) -> bool:
        """Validate embedding format and dimension"""
        if not isinstance(embedding, list):
            return False
        if len(embedding) != self.dimension:
            return False
        if not all(isinstance(x, (int, float)) for x in embedding):
            return False
        return True
    
    def cosine_similarity(self, emb1: List[float], emb2: List[float]) -> float:
        """Calculate cosine similarity between two embeddings"""
        emb1 = np.array(emb1)
        emb2 = np.array(emb2)
        
        dot_product = np.dot(emb1, emb2)
        norm1 = np.linalg.norm(emb1)
        norm2 = np.linalg.norm(emb2)
        
        return dot_product / (norm1 * norm2)

# Test embedding configuration
def test_embedding_config():
    config = EmbeddingConfig()
    
    # Test embedding generation
    test_texts = [
        "This is a test document about company policies.",
        "Technical documentation for the chatbot system.",
        "User manual for the knowledge base."
    ]
    
    embeddings = config.generate_embeddings(test_texts)
    
    assert len(embeddings) == len(test_texts)
    for emb in embeddings:
        assert config.validate_embedding(emb)
    
    # Test similarity
    similarity = config.cosine_similarity(embeddings[0], embeddings[1])
    assert 0 <= similarity <= 1
    
    print("✅ Embedding configuration test passed")

if __name__ == "__main__":
    test_embedding_config()
```

#### **Bước 3.4: Setup Vector Indexes** ⏱️ 2 giờ
```python
# scripts/vector_operations.py
import chromadb
from chromadb.config import Settings
from typing import List, Dict, Any
import uuid

class VectorOperations:
    def __init__(self, host="localhost", port=8000):
        self.client = chromadb.HttpClient(
            host=host,
            port=port,
            settings=Settings(
                chroma_server_host=host,
                chroma_server_port=port,
                chroma_server_cors_allow_origins="*"
            )
        )
    
    def add_documents(self, collection_name: str, documents: List[Dict[str, Any]]) -> bool:
        """Add documents to vector collection"""
        try:
            collection = self.client.get_collection(collection_name)
            
            ids = []
            embeddings = []
            metadatas = []
            documents_text = []
            
            for doc in documents:
                # Generate unique ID if not provided
                doc_id = doc.get('id', str(uuid.uuid4()))
                ids.append(doc_id)
                
                # Extract embedding (should be pre-computed)
                embedding = doc['embedding']
                embeddings.append(embedding)
                
                # Extract metadata
                metadata = {
                    'document_id': doc.get('document_id'),
                    'chunk_index': doc.get('chunk_index'),
                    'access_level': doc.get('access_level', 'employee_only'),
                    'department': doc.get('department'),
                    'document_type': doc.get('document_type'),
                    'title': doc.get('title'),
                    'created_at': doc.get('created_at')
                }
                metadatas.append(metadata)
                
                # Document text
                documents_text.append(doc['content'])
            
            # Add to collection
            collection.add(
                ids=ids,
                embeddings=embeddings,
                metadatas=metadatas,
                documents=documents_text
            )
            
            print(f"✅ Added {len(documents)} documents to collection '{collection_name}'")
            return True
            
        except Exception as e:
            print(f"❌ Error adding documents: {e}")
            return False
    
    def search_documents(self, collection_name: str, query_embedding: List[float], 
                        filters: Dict[str, Any] = None, n_results: int = 5) -> Dict[str, Any]:
        """Search for similar documents"""
        try:
            collection = self.client.get_collection(collection_name)
            
            where_clause = {}
            if filters:
                # Build where clause for filtering
                if 'access_levels' in filters:
                    where_clause['access_level'] = {"$in": filters['access_levels']}
                if 'departments' in filters:
                    where_clause['department'] = {"$in": filters['departments']}
                if 'document_types' in filters:
                    where_clause['document_type'] = {"$in": filters['document_types']}
            
            results = collection.query(
                query_embeddings=[query_embedding],
                n_results=n_results,
                where=where_clause if where_clause else None,
                include=['documents', 'metadatas', 'distances']
            )
            
            return results
            
        except Exception as e:
            print(f"❌ Error searching documents: {e}")
            return {}
    
    def get_collection_stats(self, collection_name: str) -> Dict[str, Any]:
        """Get collection statistics"""
        try:
            collection = self.client.get_collection(collection_name)
            count = collection.count()
            
            return {
                'name': collection_name,
                'count': count,
                'metadata': collection.metadata
            }
            
        except Exception as e:
            print(f"❌ Error getting collection stats: {e}")
            return {}

# Test vector operations
def test_vector_operations():
    vector_ops = VectorOperations()
    
    # Test collection stats
    stats = vector_ops.get_collection_stats('knowledge_base_v1')
    print(f"Collection stats: {stats}")
    
    print("✅ Vector operations test passed")

if __name__ == "__main__":
    test_vector_operations()
```

#### **Bước 3.5: Test Vector Operations** ⏱️ 3 giờ
```python
# tests/test_chroma.py
import pytest
import chromadb
from chromadb.config import Settings
import numpy as np

def test_chroma_connection():
    """Test connection to Chroma server"""
    client = chromadb.HttpClient(host="localhost", port=8000)
    
    # Test heartbeat
    heartbeat = client.heartbeat()
    assert heartbeat is not None

def test_collection_operations():
    """Test collection CRUD operations"""
    client = chromadb.HttpClient(host="localhost", port=8000)
    
    test_collection_name = "test_collection"
    
    # Clean up if exists
    try:
        client.delete_collection(test_collection_name)
    except:
        pass
    
    # Create collection
    collection = client.create_collection(
        name=test_collection_name,
        metadata={"test": True}
    )
    
    assert collection.name == test_collection_name
    
    # List collections
    collections = client.list_collections()
    collection_names = [col.name for col in collections]
    assert test_collection_name in collection_names
    
    # Clean up
    client.delete_collection(test_collection_name)

def test_document_operations():
    """Test document add/search operations"""
    client = chromadb.HttpClient(host="localhost", port=8000)
    
    test_collection_name = "test_docs"
    
    # Clean up and create
    try:
        client.delete_collection(test_collection_name)
    except:
        pass
    
    collection = client.create_collection(test_collection_name)
    
    # Add test documents
    test_docs = [
        {
            'id': 'doc1',
            'content': 'This is about company policies and procedures.',
            'embedding': np.random.rand(1536).tolist(),
            'metadata': {'type': 'policy', 'department': 'HR'}
        },
        {
            'id': 'doc2', 
            'content': 'Technical documentation for software development.',
            'embedding': np.random.rand(1536).tolist(),
            'metadata': {'type': 'technical', 'department': 'R&D'}
        }
    ]
    
    collection.add(
        ids=[doc['id'] for doc in test_docs],
        embeddings=[doc['embedding'] for doc in test_docs],
        documents=[doc['content'] for doc in test_docs],
        metadatas=[doc['metadata'] for doc in test_docs]
    )
    
    # Test search
    query_embedding = np.random.rand(1536).tolist()
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=2
    )
    
    assert len(results['ids'][0]) == 2
    assert len(results['documents'][0]) == 2
    
    # Test filtered search
    filtered_results = collection.query(
        query_embeddings=[query_embedding],
        n_results=1,
        where={"department": "HR"}
    )
    
    assert len(filtered_results['ids'][0]) == 1
    assert filtered_results['metadatas'][0][0]['department'] == 'HR'
    
    # Clean up
    client.delete_collection(test_collection_name)

if __name__ == "__main__":
    test_chroma_connection()
    test_collection_operations()
    test_document_operations()
    print("✅ All Chroma tests passed")
```

### **Phase 4: Integration & Testing (Tuần 2-3)**

#### **Bước 4.1: Integrate Both Databases** ⏱️ 6 giờ
```python
# scripts/database_integration.py
import asyncio
from typing import List, Dict, Any, Optional
import asyncpg
import chromadb
from chromadb.config import Settings
import uuid
from datetime import datetime
import json

class DatabaseIntegration:
    def __init__(self, postgres_url: str, chroma_host: str = "localhost", chroma_port: int = 8000):
        self.postgres_url = postgres_url
        self.postgres_pool = None
        
        self.chroma_client = chromadb.HttpClient(
            host=chroma_host,
            port=chroma_port,
            settings=Settings(
                chroma_server_host=chroma_host,
                chroma_server_port=chroma_port,
                chroma_server_cors_allow_origins="*"
            )
        )
    
    async def init_postgres_pool(self):
        """Initialize PostgreSQL connection pool"""
        self.postgres_pool = await asyncpg.create_pool(
            self.postgres_url,
            min_size=5,
            max_size=20,
            command_timeout=60
        )
    
    async def close_postgres_pool(self):
        """Close PostgreSQL connection pool"""
        if self.postgres_pool:
            await self.postgres_pool.close()
    
    async def add_document_with_chunks(self, document_data: Dict[str, Any], 
                                     chunks_data: List[Dict[str, Any]]) -> bool:
        """Add document to PostgreSQL and chunks to Chroma in a coordinated way"""
        try:
            # Start PostgreSQL transaction
            async with self.postgres_pool.acquire() as conn:
                async with conn.transaction():
                    # Insert document
                    document_id = await conn.fetchval("""
                        INSERT INTO documents (title, file_path, content_hash, document_type, 
                                             access_level, department_owner, author_id, version, 
                                             chunk_count, metadata)
                        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                        RETURNING document_id
                    """, 
                    document_data['title'],
                    document_data.get('file_path'),
                    document_data.get('content_hash'),
                    document_data['document_type'],
                    document_data.get('access_level', 'employee_only'),
                    document_data.get('department_owner'),
                    document_data.get('author_id'),
                    document_data.get('version', '1.0'),
                    len(chunks_data),
                    json.dumps(document_data.get('metadata', {}))
                    )
                    
                    # Prepare chunks for PostgreSQL
                    chunk_records = []
                    chroma_documents = []
                    
                    for i, chunk in enumerate(chunks_data):
                        chunk_id = str(uuid.uuid4())
                        vector_id = f"{document_id}_{i}"
                        
                        # PostgreSQL chunk record
                        chunk_records.append({
                            'chunk_id': chunk_id,
                            'document_id': document_id,
                            'chunk_index': i,
                            'content': chunk['content'],
                            'vector_id': vector_id,
                            'chunk_metadata': json.dumps(chunk.get('metadata', {}))
                        })
                        
                        # Chroma document record
                        chroma_doc = {
                            'id': vector_id,
                            'content': chunk['content'],
                            'embedding': chunk['embedding'],
                            'metadata': {
                                'document_id': str(document_id),
                                'chunk_id': chunk_id,
                                'chunk_index': i,
                                'access_level': document_data.get('access_level', 'employee_only'),
                                'department': document_data.get('department_owner'),
                                'document_type': document_data['document_type'],
                                'title': document_data['title'],
                                'created_at': datetime.now().isoformat()
                            }
                        }
                        chroma_documents.append(chroma_doc)
                    
                    # Insert chunks into PostgreSQL
                    await conn.executemany("""
                        INSERT INTO document_chunks (chunk_id, document_id, chunk_index, 
                                                   content, vector_id, chunk_metadata)
                        VALUES ($1, $2, $3, $4, $5, $6)
                    """, 
                    [(r['chunk_id'], r['document_id'], r['chunk_index'], 
                      r['content'], r['vector_id'], r['chunk_metadata']) 
                     for r in chunk_records])
                    
                    # Add to Chroma (determine collection based on access level)
                    collection_name = self._get_collection_name(document_data)
                    collection = self.chroma_client.get_collection(collection_name)
                    
                    collection.add(
                        ids=[doc['id'] for doc in chroma_documents],
                        embeddings=[doc['embedding'] for doc in chroma_documents],
                        documents=[doc['content'] for doc in chroma_documents],
                        metadatas=[doc['metadata'] for doc in chroma_documents]
                    )
                    
                    print(f"✅ Successfully added document {document_id} with {len(chunks_data)} chunks")
                    return True
                    
        except Exception as e:
            print(f"❌ Error adding document with chunks: {e}")
            # TODO: Add cleanup logic for partial failures
            return False
    
    async def search_with_permissions(self, query_embedding: List[float], 
                                    user_id: str, n_results: int = 5) -> List[Dict[str, Any]]:
        """Search documents considering user permissions"""
        try:
            # Get user permissions from PostgreSQL
            async with self.postgres_pool.acquire() as conn:
                user_info = await conn.fetchrow("""
                    SELECT user_level, department FROM users WHERE user_id = $1
                """, uuid.UUID(user_id))
                
                if not user_info:
                    return []
                
                # Determine accessible access levels based on user level
                access_levels = self._get_accessible_levels(user_info['user_level'])
                
                # Search in Chroma with access level filters
                collection = self.chroma_client.get_collection('knowledge_base_v1')
                
                chroma_results = collection.query(
                    query_embeddings=[query_embedding],
                    n_results=n_results * 2,  # Get more to account for filtering
                    where={"access_level": {"$in": access_levels}},
                    include=['documents', 'metadatas', 'distances']
                )
                
                if not chroma_results['ids'][0]:
                    return []
                
                # Get additional document info from PostgreSQL
                document_ids = [metadata['document_id'] 
                              for metadata in chroma_results['metadatas'][0]]
                
                documents_info = await conn.fetch("""
                    SELECT document_id, title, document_type, access_level, department_owner
                    FROM documents 
                    WHERE document_id = ANY($1::uuid[])
                """, document_ids)
                
                doc_info_map = {str(row['document_id']): dict(row) for row in documents_info}
                
                # Combine results
                results = []
                for i, doc_id in enumerate(chroma_results['ids'][0][:n_results]):
                    metadata = chroma_results['metadatas'][0][i]
                    document_info = doc_info_map.get(metadata['document_id'], {})
                    
                    result = {
                        'chunk_id': doc_id,
                        'content': chroma_results['documents'][0][i],
                        'distance': chroma_results['distances'][0][i],
                        'metadata': metadata,
                        'document_info': document_info
                    }
                    results.append(result)
                
                return results
                
        except Exception as e:
            print(f"❌ Error searching with permissions: {e}")
            return []
    
    def _get_collection_name(self, document_data: Dict[str, Any]) -> str:
        """Determine Chroma collection name based on document properties"""
        access_level = document_data.get('access_level', 'employee_only')
        
        if access_level in ['director_only', 'system_admin']:
            return 'knowledge_base_secure_v1'
        elif document_data.get('department_owner'):
            dept = document_data['department_owner'].lower()
            return f'dept_{dept}_v1'
        else:
            return 'knowledge_base_v1'
    
    def _get_accessible_levels(self, user_level: str) -> List[str]:
        """Get list of access levels user can access"""
        level_hierarchy = {
            'guest': ['public'],
            'employee': ['public', 'employee_only'],
            'manager': ['public', 'employee_only', 'manager_only'],
            'director': ['public', 'employee_only', 'manager_only', 'director_only'],
            'admin': ['public', 'employee_only', 'manager_only', 'director_only', 'system_admin']
        }
        return level_hierarchy.get(user_level, ['public'])

# Integration test
async def test_integration():
    postgres_url = "postgresql://kb_admin:password@localhost:5432/knowledge_base"
    
    db_integration = DatabaseIntegration(postgres_url)
    await db_integration.init_postgres_pool()
    
    try:
        # Test document addition
        document_data = {
            'title': 'Test Integration Document',
            'document_type': 'technical_guide',
            'access_level': 'employee_only',
            'department_owner': 'IT',
            'metadata': {'test': True}
        }
        
        chunks_data = [
            {
                'content': 'This is the first chunk of the test document.',
                'embedding': np.random.rand(1536).tolist(),
                'metadata': {'section': 'introduction'}
            },
            {
                'content': 'This is the second chunk with different content.',
                'embedding': np.random.rand(1536).tolist(),
                'metadata': {'section': 'details'}
            }
        ]
        
        success = await db_integration.add_document_with_chunks(document_data, chunks_data)
        assert success
        
        print("✅ Integration test passed")
        
    finally:
        await db_integration.close_postgres_pool()

if __name__ == "__main__":
    import numpy as np
    asyncio.run(test_integration())
```

#### **Bước 4.2: Setup Redis Cache** ⏱️ 2 giờ
```yaml
# Add Redis to docker-compose.yml
  redis:
    image: redis:7-alpine
    container_name: fr02-redis
    ports:
      - "6379:6379"
    volumes:
      - ./data/redis:/data
      - ./config/redis.conf:/usr/local/etc/redis/redis.conf
    command: redis-server /usr/local/etc/redis/redis.conf
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "redis-cli ping | grep PONG"]
      interval: 30s
      timeout: 10s
      retries: 5
```

```python
# scripts/cache_manager.py
import redis
import json
import hashlib
from typing import Any, Optional, List, Dict
import pickle
from datetime import timedelta

class CacheManager:
    def __init__(self, redis_host: str = "localhost", redis_port: int = 6379, redis_db: int = 0):
        self.redis_client = redis.Redis(
            host=redis_host,
            port=redis_port,
            db=redis_db,
            decode_responses=False  # Keep binary for pickle
        )
        
        # Cache TTL settings (in seconds)
        self.ttl_settings = {
            'query_results': 300,     # 5 minutes for query results
            'user_permissions': 900,  # 15 minutes for user permissions
            'embeddings': 3600,       # 1 hour for embeddings
            'document_metadata': 600, # 10 minutes for document metadata
        }
    
    def _generate_cache_key(self, prefix: str, *args) -> str:
        """Generate cache key from prefix and arguments"""
        key_data = f"{prefix}:{':'.join(map(str, args))}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    def cache_query_results(self, query_embedding: List[float], user_id: str, 
                          results: List[Dict[str, Any]], filters: Dict[str, Any] = None) -> bool:
        """Cache search query results"""
        try:
            # Create cache key from query parameters
            query_hash = hashlib.md5(str(query_embedding).encode()).hexdigest()[:16]
            filters_hash = hashlib.md5(str(filters or {}).encode()).hexdigest()[:8]
            cache_key = f"query:{query_hash}:{user_id}:{filters_hash}"
            
            # Serialize results
            cached_data = {
                'results': results,
                'timestamp': datetime.now().isoformat(),
                'user_id': user_id,
                'filters': filters
            }
            
            # Store with TTL
            self.redis_client.setex(
                cache_key,
                timedelta(seconds=self.ttl_settings['query_results']),
                pickle.dumps(cached_data)
            )
            
            return True
            
        except Exception as e:
            print(f"❌ Error caching query results: {e}")
            return False
    
    def get_cached_query_results(self, query_embedding: List[float], user_id: str, 
                               filters: Dict[str, Any] = None) -> Optional[List[Dict[str, Any]]]:
        """Get cached query results"""
        try:
            query_hash = hashlib.md5(str(query_embedding).encode()).hexdigest()[:16]
            filters_hash = hashlib.md5(str(filters or {}).encode()).hexdigest()[:8]
            cache_key = f"query:{query_hash}:{user_id}:{filters_hash}"
            
            cached_data = self.redis_client.get(cache_key)
            if cached_data:
                data = pickle.loads(cached_data)
                return data['results']
            
            return None
            
        except Exception as e:
            print(f"❌ Error getting cached query results: {e}")
            return None
    
    def cache_user_permissions(self, user_id: str, permissions: Dict[str, Any]) -> bool:
        """Cache user permissions"""
        try:
            cache_key = f"permissions:{user_id}"
            
            self.redis_client.setex(
                cache_key,
                timedelta(seconds=self.ttl_settings['user_permissions']),
                json.dumps(permissions)
            )
            
            return True
            
        except Exception as e:
            print(f"❌ Error caching user permissions: {e}")
            return False
    
    def get_cached_user_permissions(self, user_id: str) -> Optional[Dict[str, Any]]:
        """Get cached user permissions"""
        try:
            cache_key = f"permissions:{user_id}"
            cached_data = self.redis_client.get(cache_key)
            
            if cached_data:
                return json.loads(cached_data.decode())
            
            return None
            
        except Exception as e:
            print(f"❌ Error getting cached user permissions: {e}")
            return None
    
    def invalidate_user_cache(self, user_id: str) -> bool:
        """Invalidate all cache entries for a user"""
        try:
            # Get all keys related to user
            patterns = [
                f"query:*:{user_id}:*",
                f"permissions:{user_id}",
                f"user_data:{user_id}"
            ]
            
            deleted_count = 0
            for pattern in patterns:
                keys = self.redis_client.keys(pattern)
                if keys:
                    deleted_count += self.redis_client.delete(*keys)
            
            print(f"✅ Invalidated {deleted_count} cache entries for user {user_id}")
            return True
            
        except Exception as e:
            print(f"❌ Error invalidating user cache: {e}")
            return False
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """Get cache statistics"""
        try:
            info = self.redis_client.info()
            stats = {
                'connected_clients': info['connected_clients'],
                'used_memory': info['used_memory_human'],
                'keyspace_hits': info['keyspace_hits'],
                'keyspace_misses': info['keyspace_misses'],
                'hit_rate': info['keyspace_hits'] / (info['keyspace_hits'] + info['keyspace_misses']) if (info['keyspace_hits'] + info['keyspace_misses']) > 0 else 0
            }
            
            # Count keys by type
            key_patterns = ['query:*', 'permissions:*', 'embeddings:*', 'document:*']
            for pattern in key_patterns:
                count = len(self.redis_client.keys(pattern))
                stats[f'{pattern.replace("*", "count")}'] = count
            
            return stats
            
        except Exception as e:
            print(f"❌ Error getting cache stats: {e}")
            return {}

# Test cache manager
def test_cache_manager():
    cache = CacheManager()
    
    # Test user permissions caching
    test_permissions = {
        'user_level': 'employee',
        'department': 'R&D',
        'accessible_levels': ['public', 'employee_only']
    }
    
    user_id = "test-user-123"
    
    # Cache permissions
    success = cache.cache_user_permissions(user_id, test_permissions)
    assert success
    
    # Retrieve permissions
    cached_permissions = cache.get_cached_user_permissions(user_id)
    assert cached_permissions == test_permissions
    
    # Test cache invalidation
    cache.invalidate_user_cache(user_id)
    cached_permissions = cache.get_cached_user_permissions(user_id)
    assert cached_permissions is None
    
    # Test cache stats
    stats = cache.get_cache_stats()
    assert 'connected_clients' in stats
    
    print("✅ Cache manager test passed")

if __name__ == "__main__":
    from datetime import datetime
    test_cache_manager()
```

#### **Bước 4.3: Configure Monitoring** ⏱️ 3 giờ
```yaml
# Add monitoring services to docker-compose.yml
  prometheus:
    image: prom/prometheus:latest
    container_name: fr02-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./config/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./data/prometheus:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      -
---
---
---
