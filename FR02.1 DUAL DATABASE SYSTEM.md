# TÀI LIỆU THIẾT KẾ KỸ THUẬT FR-02.1
## HỆ THỐNG CƠ SỞ DỮ LIỆU KÉP (DUAL DATABASE SYSTEM)

---

**Phiên bản:** 1.0  
**Ngày:** 31/08/2025  
**Module:** FR-02.1 - Hệ thống CSDL kép  
**Mức độ ưu tiên:** CRITICAL  
**Người soạn thảo:** Technical Design Team  

---

## 📋 **TỔNG QUAN THIẾT KẾ**

### **Mục tiêu**
Thiết kế và triển khai hệ thống cơ sở dữ liệu kép bao gồm:
- **Vector Database**: Lưu trữ embeddings cho tìm kiếm ngữ nghĩa (≥100k chunks)
- **Relational Database**: Quản lý metadata, người dùng và phân quyền

### **Kiến trúc tổng quan**
```mermaid
graph TB
    subgraph "Application Layer"
        API[🌐 REST API Services]
        RAG[🤖 RAG Engine]
    end
    
    subgraph "🗄️ FR-02.1: DUAL DATABASE SYSTEM"
        subgraph "📊 Vector Database Layer"
            VectorDB[🔢 Vector Database<br/>Chroma/FAISS]
            VectorIndex[📇 Vector Index<br/>HNSW/IVF]
        end
        
        subgraph "🗃️ Relational Database Layer"
            PostgreSQL[(🐘 PostgreSQL<br/>Metadata + Users)]
            ConnPool[🔄 Connection Pool<br/>pgbouncer]
        end
        
        subgraph "⚡ Cache Layer"
            RedisCache[🔴 Redis<br/>Query Cache]
        end
    end
    
    subgraph "💾 Storage Layer"
        VectorStorage[📁 Vector Storage<br/>Persistent Volume]
        SQLStorage[📁 SQL Storage<br/>Persistent Volume]
    end
    
    API --> VectorDB
    API --> PostgreSQL
    RAG --> VectorDB
    RAG --> PostgreSQL
    
    VectorDB --> VectorIndex
    PostgreSQL --> ConnPool
    API -.-> RedisCache
    
    VectorDB --> VectorStorage
    PostgreSQL --> SQLStorage
    
    classDef app fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef vector fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef sql fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef cache fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef storage fill:#e0f2f1,stroke:#00796b,stroke-width:2px
    
    class API,RAG app
    class VectorDB,VectorIndex vector
    class PostgreSQL,ConnPool sql
    class RedisCache cache
    class VectorStorage,SQLStorage storage
```

---

## 🛠️ **CHUẨN BỊ MÔI TRƯỜNG PHÁT TRIỂN**

### **1. Yêu cầu hạ tầng**

#### **💻 Phần cứng tối thiểu**
- **CPU**: 8 cores (Intel i7 hoặc AMD Ryzen 7)
- **RAM**: 32GB (khuyến nghị 64GB)
- **Storage**: 
  - SSD 500GB cho OS và development tools
  - SSD 1TB cho databases và data storage
  - HDD 2TB cho backup (optional)
- **Network**: Gigabit Ethernet

#### **🖥️ Hệ điều hành**
- **Primary**: Ubuntu 22.04 LTS / CentOS 8
- **Alternative**: macOS 12+ / Windows 11 với WSL2

### **2. Phần mềm cần thiết**

#### **🐳 Container & Orchestration**
```bash
# Docker & Docker Compose
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
sudo curl -L "https://github.com/docker/compose/releases/download/v2.20.0/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose
```

#### **🐍 Python Development Stack**
```bash
# Python 3.11+ với virtual environment
sudo apt-get update
sudo apt-get install python3.11 python3.11-venv python3.11-dev
python3.11 -m venv venv-fr02
source venv-fr02/bin/activate
```

#### **🗄️ Database Tools**
```bash
# PostgreSQL Client Tools
sudo apt-get install postgresql-client-15
# GUI Tools (choose one)
# - pgAdmin 4 (Web-based)
# - DBeaver Community (Desktop)
# - TablePlus (macOS/Windows)
```

#### **📊 Monitoring & Development Tools**
```bash
# Essential development tools
sudo apt-get install git curl wget htop tree jq
# Monitoring tools
sudo apt-get install prometheus-node-exporter
```

---

## 🏗️ **KIẾN TRÚC CHI TIẾT**

### **1. Vector Database Design**

#### **🔢 Chroma Database Architecture**
```mermaid
graph TB
    subgraph "🔢 CHROMA VECTOR DATABASE"
        subgraph "API Layer"
            ChromaAPI[🌐 Chroma HTTP API<br/>Port: 8000]
            ChromaClient[📱 Chroma Client<br/>Python SDK]
        end
        
        subgraph "Storage Engine"
            Collections[📚 Collections<br/>Logical Groupings]
            Embeddings[🧮 Embeddings<br/>Vector Storage]
            Metadata[🏷️ Metadata<br/>JSON Documents]
            Indexes[📇 Vector Indexes<br/>HNSW Algorithm]
        end
        
        subgraph "Persistence Layer"
            SQLiteDB[(🗃️ SQLite<br/>Metadata Storage)]
            VectorFiles[📁 Vector Files<br/>Binary Storage)]
        end
    end
    
    ChromaAPI --> Collections
    ChromaClient --> ChromaAPI
    Collections --> Embeddings
    Collections --> Metadata
    Embeddings --> Indexes
    
    Collections --> SQLiteDB
    Embeddings --> VectorFiles
    Metadata --> SQLiteDB
    
    classDef api fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef engine fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef persistence fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    
    class ChromaAPI,ChromaClient api
    class Collections,Embeddings,Metadata,Indexes engine
    class SQLiteDB,VectorFiles persistence
```

#### **📊 Collection Schema Design**
```json
{
  "collection_name": "knowledge_base_v1",
  "embedding_dimension": 1536,
  "distance_metric": "cosine",
  "index_type": "HNSW",
  "max_elements": 100000,
  "ef_construction": 200,
  "M": 16
}
```

### **2. Relational Database Design**

#### **🐘 PostgreSQL Schema Architecture**
```mermaid
erDiagram
    USERS {
        uuid user_id PK
        string username UK
        string email UK
        string password_hash
        enum user_level
        string department
        boolean is_active
        timestamp created_at
        timestamp updated_at
        jsonb preferences
    }
    
    DOCUMENTS {
        uuid document_id PK
        string title
        string file_path
        string content_hash
        enum document_type
        enum access_level
        string department_owner
        uuid author_id FK
        string version
        integer chunk_count
        timestamp created_at
        timestamp updated_at
        jsonb metadata
    }
    
    DOCUMENT_CHUNKS {
        uuid chunk_id PK
        uuid document_id FK
        integer chunk_index
        text content
        string vector_id
        jsonb chunk_metadata
        timestamp created_at
    }
    
    USER_PERMISSIONS {
        uuid permission_id PK
        uuid user_id FK
        uuid document_id FK
        enum permission_type
        timestamp granted_at
        timestamp expires_at
        uuid granted_by FK
    }
    
    AUDIT_LOGS {
        uuid log_id PK
        uuid user_id FK
        string action
        string resource_type
        string resource_id
        jsonb action_details
        string ip_address
        string user_agent
        timestamp timestamp
    }
    
    CHAT_SESSIONS {
        uuid session_id PK
        uuid user_id FK
        string session_name
        timestamp created_at
        timestamp last_activity
        jsonb session_metadata
    }
    
    CHAT_MESSAGES {
        uuid message_id PK
        uuid session_id FK
        enum message_type
        text content
        jsonb response_metadata
        timestamp timestamp
    }
    
    USERS ||--o{ DOCUMENTS : creates
    USERS ||--o{ USER_PERMISSIONS : has
    DOCUMENTS ||--o{ DOCUMENT_CHUNKS : contains
    DOCUMENTS ||--o{ USER_PERMISSIONS : requires
    USERS ||--o{ AUDIT_LOGS : generates
    USERS ||--o{ CHAT_SESSIONS : owns
    CHAT_SESSIONS ||--o{ CHAT_MESSAGES : contains
    USERS ||--o{ USER_PERMISSIONS : grants
```

---

## 📋 **LỘ TRÌNH TRIỂN KHAI**

```mermaid
graph TB
    Start([🚀 Bắt đầu FR-02.1])
    
    subgraph "Phase 1: Environment Setup"
        P1_1[💻 Setup Development Environment]
        P1_2[🐳 Install Docker & Dependencies]
        P1_3[📁 Create Project Structure]
        P1_4[🔧 Configure Development Tools]
    end
    
    subgraph "Phase 2: PostgreSQL Setup"
        P2_1[🐘 Deploy PostgreSQL Container]
        P2_2[🗃️ Create Database Schema]
        P2_3[👤 Setup User Management]
        P2_4[🔄 Configure Connection Pool]
        P2_5[✅ Test PostgreSQL Setup]
        
        P2_Fail{❌ PostgreSQL<br/>Test Failed?}
        P2_Debug[🔍 Debug PostgreSQL Issues]
    end
    
    subgraph "Phase 3: Vector Database Setup"
        P3_1[🔢 Deploy Chroma Container]
        P3_2[📚 Create Vector Collections]
        P3_3[🧮 Configure Embedding Settings]
        P3_4[📇 Setup Vector Indexes]
        P3_5[✅ Test Vector Operations]
        
        P3_Fail{❌ Vector DB<br/>Test Failed?}
        P3_Debug[🔍 Debug Vector DB Issues]
    end
    
    subgraph "Phase 4: Integration & Testing"
        P4_1[🔗 Integrate Both Databases]
        P4_2[⚡ Setup Redis Cache]
        P4_3[📊 Configure Monitoring]
        P4_4[🧪 Run Integration Tests]
        P4_5[📈 Performance Testing]
        
        P4_Fail{❌ Integration<br/>Test Failed?}
        P4_Debug[🔍 Debug Integration Issues]
    end
    
    subgraph "Phase 5: Production Preparation"
        P5_1[🔒 Security Hardening]
        P5_2[💾 Setup Backup Strategy]
        P5_3[📋 Create Documentation]
        P5_4[✅ Final Acceptance Testing]
    end
    
    End([✅ FR-02.1 Complete])
    
    %% Main Flow
    Start --> P1_1
    P1_1 --> P1_2
    P1_2 --> P1_3
    P1_3 --> P1_4
    
    P1_4 --> P2_1
    P2_1 --> P2_2
    P2_2 --> P2_3
    P2_3 --> P2_4
    P2_4 --> P2_5
    
    P2_5 --> P2_Fail
    P2_Fail -->|Yes| P2_Debug
    P2_Debug --> P2_1
    P2_Fail -->|No| P3_1
    
    P3_1 --> P3_2
    P3_2 --> P3_3
    P3_3 --> P3_4
    P3_4 --> P3_5
    
    P3_5 --> P3_Fail
    P3_Fail -->|Yes| P3_Debug
    P3_Debug --> P3_1
    P3_Fail -->|No| P4_1
    
    P4_1 --> P4_2
    P4_2 --> P4_3
    P4_3 --> P4_4
    P4_4 --> P4_5
    
    P4_5 --> P4_Fail
    P4_Fail -->|Yes| P4_Debug
    P4_Debug --> P4_1
    P4_Fail -->|No| P5_1
    
    P5_1 --> P5_2
    P5_2 --> P5_3
    P5_3 --> P5_4
    P5_4 --> End
    
    %% Styling
    classDef phase1 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef phase2 fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef phase3 fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef phase4 fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef phase5 fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    classDef decision fill:#ffebee,stroke:#c62828,stroke-width:2px
    classDef debug fill:#fff3e0,stroke:#ff6f00,stroke-width:2px
    classDef startend fill:#e0f2f1,stroke:#00796b,stroke-width:3px
    
    class P1_1,P1_2,P1_3,P1_4 phase1
    class P2_1,P2_2,P2_3,P2_4,P2_5 phase2
    class P3_1,P3_2,P3_3,P3_4,P3_5 phase3
    class P4_1,P4_2,P4_3,P4_4,P4_5 phase4
    class P5_1,P5_2,P5_3,P5_4 phase5
    class P2_Fail,P3_Fail,P4_Fail decision
    class P2_Debug,P3_Debug,P4_Debug debug
    class Start,End startend
```

---

## 🔧 **CHI TIẾT TỪNG BƯỚC TRIỂN KHAI**

### **Phase 1: Environment Setup (Tuần 1)**

#### **Bước 1.1: Setup Development Environment** ⏱️ 4 giờ
```bash
# Tạo thư mục dự án
mkdir -p fr02-dual-database
cd fr02-dual-database

# Tạo cấu trúc thư mục
mkdir -p {config,docker,scripts,tests,docs,data}
mkdir -p data/{postgres,chroma,redis,backups}

# Setup Git repository
git init
echo "venv-fr02/" > .gitignore
echo "*.env" >> .gitignore
echo "data/" >> .gitignore
```

#### **Bước 1.2: Install Docker & Dependencies** ⏱️ 2 giờ
```bash
# Verify Docker installation
docker --version
docker-compose --version

# Pull required images
docker pull postgres:15
docker pull chromadb/chroma:latest
docker pull redis:7-alpine
```

#### **Bước 1.3: Create Project Structure** ⏱️ 2 giờ
```
fr02-dual-database/
├── config/
│   ├── postgres.conf
│   ├── chroma.yml
│   └── redis.conf
├── docker/
│   └── docker-compose.yml
├── scripts/
│   ├── setup.sh
│   ├── init-postgres.sql
│   └── test-connections.py
├── tests/
│   ├── test_postgres.py
│   ├── test_chroma.py
│   └── test_integration.py
├── docs/
│   └── README.md
└── data/
    ├── postgres/
    ├── chroma/
    ├── redis/
    └── backups/
```

#### **Bước 1.4: Configure Development Tools** ⏱️ 2 giờ
```python
# requirements.txt
psycopg2-binary==2.9.7
chromadb==0.4.10
redis==4.6.0
sqlalchemy==2.0.20
alembic==1.12.0
pytest==7.4.0
python-dotenv==1.0.0
```

### **Phase 2: PostgreSQL Setup (Tuần 1-2)**

#### **Bước 2.1: Deploy PostgreSQL Container** ⏱️ 3 giờ
```yaml
# docker/docker-compose.yml
version: '3.8'
services:
  postgres:
    image: postgres:15
    container_name: fr02-postgres
    environment:
      POSTGRES_DB: knowledge_base
      POSTGRES_USER: kb_admin
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    ports:
      - "5432:5432"
    volumes:
      - ./data/postgres:/var/lib/postgresql/data
      - ./config/postgres.conf:/etc/postgresql/postgresql.conf
      - ./scripts/init-postgres.sql:/docker-entrypoint-initdb.d/init.sql
    command: postgres -c config_file=/etc/postgresql/postgresql.conf
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U kb_admin -d knowledge_base"]
      interval: 30s
      timeout: 10s
      retries: 5
```

#### **Bước 2.2: Create Database Schema** ⏱️ 6 giờ
```sql
-- scripts/init-postgres.sql

-- Enable required extensions
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pg_trgm";
CREATE EXTENSION IF NOT EXISTS "btree_gin";

-- User levels enum
CREATE TYPE user_level AS ENUM ('guest', 'employee', 'manager', 'director', 'admin');

-- Document types enum
CREATE TYPE document_type AS ENUM ('policy', 'procedure', 'technical_guide', 'report', 'manual');

-- Access levels enum
CREATE TYPE access_level AS ENUM ('public', 'employee_only', 'manager_only', 'director_only', 'system_admin');

-- Users table
CREATE TABLE users (
    user_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    username VARCHAR(50) UNIQUE NOT NULL,
    email VARCHAR(255) UNIQUE NOT NULL,
    password_hash VARCHAR(255) NOT NULL,
    user_level user_level NOT NULL DEFAULT 'employee',
    department VARCHAR(100),
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    preferences JSONB DEFAULT '{}'::jsonb
);

-- Documents table
CREATE TABLE documents (
    document_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    title VARCHAR(500) NOT NULL,
    file_path VARCHAR(1000),
    content_hash VARCHAR(64) UNIQUE,
    document_type document_type NOT NULL,
    access_level access_level NOT NULL DEFAULT 'employee_only',
    department_owner VARCHAR(100),
    author_id UUID REFERENCES users(user_id),
    version VARCHAR(20) DEFAULT '1.0',
    chunk_count INTEGER DEFAULT 0,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    metadata JSONB DEFAULT '{}'::jsonb
);

-- Document chunks table
CREATE TABLE document_chunks (
    chunk_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    document_id UUID NOT NULL REFERENCES documents(document_id) ON DELETE CASCADE,
    chunk_index INTEGER NOT NULL,
    content TEXT NOT NULL,
    vector_id VARCHAR(100) UNIQUE, -- Reference to Chroma vector
    chunk_metadata JSONB DEFAULT '{}'::jsonb,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(document_id, chunk_index)
);

-- User permissions table
CREATE TABLE user_permissions (
    permission_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_id UUID NOT NULL REFERENCES users(user_id) ON DELETE CASCADE,
    document_id UUID NOT NULL REFERENCES documents(document_id) ON DELETE CASCADE,
    permission_type VARCHAR(20) NOT NULL DEFAULT 'read',
    granted_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    expires_at TIMESTAMP WITH TIME ZONE,
    granted_by UUID REFERENCES users(user_id),
    UNIQUE(user_id, document_id)
);

-- Audit logs table
CREATE TABLE audit_logs (
    log_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_id UUID REFERENCES users(user_id),
    action VARCHAR(50) NOT NULL,
    resource_type VARCHAR(50) NOT NULL,
    resource_id VARCHAR(100),
    action_details JSONB DEFAULT '{}'::jsonb,
    ip_address INET,
    user_agent TEXT,
    timestamp TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- Chat sessions table
CREATE TABLE chat_sessions (
    session_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_id UUID NOT NULL REFERENCES users(user_id) ON DELETE CASCADE,
    session_name VARCHAR(200),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    last_activity TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    session_metadata JSONB DEFAULT '{}'::jsonb
);

-- Chat messages table
CREATE TABLE chat_messages (
    message_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    session_id UUID NOT NULL REFERENCES chat_sessions(session_id) ON DELETE CASCADE,
    message_type VARCHAR(20) NOT NULL CHECK (message_type IN ('user', 'assistant', 'system')),
    content TEXT NOT NULL,
    response_metadata JSONB DEFAULT '{}'::jsonb,
    timestamp TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- Indexes for performance
CREATE INDEX idx_users_username ON users(username);
CREATE INDEX idx_users_email ON users(username);
CREATE INDEX idx_users_department ON users(department);
CREATE INDEX idx_users_level ON users(user_level);

CREATE INDEX idx_documents_title ON documents USING gin(to_tsvector('english', title));
CREATE INDEX idx_documents_content_hash ON documents(content_hash);
CREATE INDEX idx_documents_type ON documents(document_type);
CREATE INDEX idx_documents_access_level ON documents(access_level);
CREATE INDEX idx_documents_department ON documents(department_owner);
CREATE INDEX idx_documents_created_at ON documents(created_at DESC);

CREATE INDEX idx_chunks_document_id ON document_chunks(document_id);
CREATE INDEX idx_chunks_vector_id ON document_chunks(vector_id);
CREATE INDEX idx_chunks_content ON document_chunks USING gin(to_tsvector('english', content));

CREATE INDEX idx_permissions_user_id ON user_permissions(user_id);
CREATE INDEX idx_permissions_document_id ON user_permissions(document_id);

CREATE INDEX idx_audit_logs_user_id ON audit_logs(user_id);
CREATE INDEX idx_audit_logs_timestamp ON audit_logs(timestamp DESC);
CREATE INDEX idx_audit_logs_action ON audit_logs(action);

CREATE INDEX idx_chat_sessions_user_id ON chat_sessions(user_id);
CREATE INDEX idx_chat_sessions_last_activity ON chat_sessions(last_activity DESC);

CREATE INDEX idx_chat_messages_session_id ON chat_messages(session_id);
CREATE INDEX idx_chat_messages_timestamp ON chat_messages(timestamp DESC);

-- Updated_at trigger function
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = CURRENT_TIMESTAMP;
    RETURN NEW;
END;
$$ language 'plpgsql';

-- Apply updated_at triggers
CREATE TRIGGER update_users_updated_at BEFORE UPDATE ON users 
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();
CREATE TRIGGER update_documents_updated_at BEFORE UPDATE ON documents 
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();
```

#### **Bước 2.3: Setup User Management** ⏱️ 2 giờ
```sql
-- Create initial admin user
INSERT INTO users (username, email, password_hash, user_level, department) VALUES
('admin', 'admin@company.com', '$2b$12$encrypted_password_hash', 'admin', 'IT'),
('test_user', 'test@company.com', '$2b$12$encrypted_password_hash', 'employee', 'R&D');

-- Create sample documents for testing
INSERT INTO documents (title, document_type, access_level, department_owner, author_id) VALUES
('Company Policy Manual', 'policy', 'employee_only', 'HR', (SELECT user_id FROM users WHERE username = 'admin')),
('Technical Guidelines', 'technical_guide', 'employee_only', 'R&D', (SELECT user_id FROM users WHERE username = 'admin'));
```

#### **Bước 2.4: Configure Connection Pool** ⏱️ 3 giờ
```yaml
# Add pgbouncer to docker-compose.yml
  pgbouncer:
    image: pgbouncer/pgbouncer:latest
    container_name: fr02-pgbouncer
    environment:
      DATABASES_HOST: postgres
      DATABASES_PORT: 5432
      DATABASES_USER: kb_admin
      DATABASES_PASSWORD: ${POSTGRES_PASSWORD}
      DATABASES_DBNAME: knowledge_base
      POOL_MODE: transaction
      SERVER_RESET_QUERY: DISCARD ALL
      MAX_CLIENT_CONN: 1000
      DEFAULT_POOL_SIZE: 20
      MIN_POOL_SIZE: 5
    ports:
      - "6432:5432"
    depends_on:
      postgres:
        condition: service_healthy
```

#### **Bước 2.5: Test PostgreSQL Setup** ⏱️ 2 giờ
```python
# tests/test_postgres.py
import pytest
import psycopg2
from sqlalchemy import create_engine, text
import os

def test_postgres_connection():
    """Test basic PostgreSQL connection"""
    conn_string = f"postgresql://kb_admin:{os.getenv('POSTGRES_PASSWORD')}@localhost:5432/knowledge_base"
    engine = create_engine(conn_string)
    
    with engine.connect() as conn:
        result = conn.execute(text("SELECT version()"))
        version = result.fetchone()[0]
        assert "PostgreSQL 15" in version

def test_schema_creation():
    """Test that all tables are created"""
    conn_string = f"postgresql://kb_admin:{os.getenv('POSTGRES_PASSWORD')}@localhost:5432/knowledge_base"
    engine = create_engine(conn_string)
    
    expected_tables = ['users', 'documents', 'document_chunks', 'user_permissions', 'audit_logs', 'chat_sessions', 'chat_messages']
    
    with engine.connect() as conn:
        result = conn.execute(text("SELECT table_name FROM information_schema.tables WHERE table_schema = 'public'"))
        tables = [row[0] for row in result.fetchall()]
        
        for table in expected_tables:
            assert table in tables

def test_user_creation():
    """Test user CRUD operations"""
    conn_string = f"postgresql://kb_admin:{os.getenv('POSTGRES_PASSWORD')}@localhost:5432/knowledge_base"
    engine = create_engine(conn_string)
    
    with engine.connect() as conn:
        # Test insert
        result = conn.execute(text("""
            INSERT INTO users (username, email, password_hash, user_level) 
            VALUES ('test_crud', 'test_crud@test.com', 'hash123', 'employee') 
            RETURNING user_id
        """))
        user_id = result.fetchone()[0]
        conn.commit()
        
        # Test select
        result = conn.execute(text("SELECT username FROM users WHERE user_id = :user_id"), {"user_id": user_id})
        username = result.fetchone()[0]
        assert username == 'test_crud'
        
        # Test cleanup
        conn.execute(text("DELETE FROM users WHERE user_id = :user_id"), {"user_id": user_id})
        conn.commit()
```

### **Phase 3: Vector Database Setup (Tuần 2)**

#### **Bước 3.1: Deploy Chroma Container** ⏱️ 3 giờ
```yaml
# Add to docker-compose.yml
  chroma:
    image: chromadb/chroma:latest
    container_name: fr02-chroma
    ports:
      - "8000:8000"
    volumes:
      - ./data/chroma:/chroma/chroma
    environment:
      - CHROMA_SERVER_HOST=0.0.0.0
      - CHROMA_SERVER_PORT=8000
      - CHROMA_SERVER_CORS_ALLOW_ORIGINS=*
      - PERSIST_DIRECTORY=/chroma/chroma
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/api/v1/heartbeat || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
```

#### **Bước 3.2: Create Vector Collections** ⏱️ 4 giờ
```python
# scripts/setup_chroma.py
import chromadb
from chromadb.config import Settings
import uuid

def setup_chroma_collections():
    # Connect to Chroma
    client = chromadb.HttpClient(
        host="localhost",
        port=8000,
        settings=Settings(
            chroma_server_host="localhost",
            chroma_server_port=8000,
            chroma_server_cors_allow_origins="*"
        )
    )
    
    # Create main knowledge base collection
    collection_name = "knowledge_base_v1"
    
    try:
        # Delete existing collection if it exists
        try:
            client.delete_collection(collection_name)
        except:
            pass
        
        # Create new collection
        collection = client.create_collection(
            name=collection_name,
            metadata={
                "description": "Main knowledge base for chatbot",
                "embedding_dimension": 1536,  # OpenAI ada-002
                "distance_metric": "cosine",
                "max_elements": 100000,
                "created_at": str(datetime.now()),
                "version": "1.0"
            },
            embedding_function=None  # We'll provide embeddings manually
        )
        
        print(f"✅ Collection '{collection_name}' created successfully")
        
        # Create department-specific collections
        departments = ['HR', 'R&D', 'Sales', 'Finance', 'IT']
        for dept in departments:
            dept_collection_name = f"dept_{dept.lower()}_v1"
            try:
                client.delete_collection(dept_collection_name)
            except:
                pass
                
            dept_collection = client.create_collection(
                name=dept_collection_name,
                metadata={
                    "description": f"{dept} department knowledge base",
                    "department": dept,
                    "embedding_dimension": 1536,
                    "distance_metric": "cosine",
                    "parent_collection": collection_name
                }
            )
            print(f"✅ Department collection '{dept_collection_name}' created")
        
        return True
        
    except Exception as e:
        print(f"❌ Error creating collections: {e}")
        return False

if __name__ == "__main__":
    setup_chroma_collections()
```

#### **Bước 3.3: Configure Embedding Settings** ⏱️ 3 giờ
```python
# scripts/embedding_config.py
import openai
import numpy as np
from typing import List
import os

class EmbeddingConfig:
    def __init__(self):
        self.openai_api_key = os.getenv('OPENAI_API_KEY')
        self.model_name = "text-embedding-ada-002"
        self.dimension = 1536
        self.max_tokens = 8191
        
    def generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings for a list of texts"""
        try:
            response = openai.embeddings.create(
                model=self.model_name,
                input=texts
            )
            
            embeddings = []
            for data in response.data:
                embeddings.append(data.embedding)
                
            return embeddings
            
        except Exception as e:
            print(f"❌ Error generating embeddings: {e}")
            return []
    
    def validate_embedding(self, embedding: List[float]) -> bool:
        """Validate embedding format and dimension"""
        if not isinstance(embedding, list):
            return False
        if len(embedding) != self.dimension:
            return False
        if not all(isinstance(x, (int, float)) for x in embedding):
            return False
        return True
    
    def cosine_similarity(self, emb1: List[float], emb2: List[float]) -> float:
        """Calculate cosine similarity between two embeddings"""
        emb1 = np.array(emb1)
        emb2 = np.array(emb2)
        
        dot_product = np.dot(emb1, emb2)
        norm1 = np.linalg.norm(emb1)
        norm2 = np.linalg.norm(emb2)
        
        return dot_product / (norm1 * norm2)

# Test embedding configuration
def test_embedding_config():
    config = EmbeddingConfig()
    
    # Test embedding generation
    test_texts = [
        "This is a test document about company policies.",
        "Technical documentation for the chatbot system.",
        "User manual for the knowledge base."
    ]
    
    embeddings = config.generate_embeddings(test_texts)
    
    assert len(embeddings) == len(test_texts)
    for emb in embeddings:
        assert config.validate_embedding(emb)
    
    # Test similarity
    similarity = config.cosine_similarity(embeddings[0], embeddings[1])
    assert 0 <= similarity <= 1
    
    print("✅ Embedding configuration test passed")

if __name__ == "__main__":
    test_embedding_config()
```

#### **Bước 3.4: Setup Vector Indexes** ⏱️ 2 giờ
```python
# scripts/vector_operations.py
import chromadb
from chromadb.config import Settings
from typing import List, Dict, Any
import uuid

class VectorOperations:
    def __init__(self, host="localhost", port=8000):
        self.client = chromadb.HttpClient(
            host=host,
            port=port,
            settings=Settings(
                chroma_server_host=host,
                chroma_server_port=port,
                chroma_server_cors_allow_origins="*"
            )
        )
    
    def add_documents(self, collection_name: str, documents: List[Dict[str, Any]]) -> bool:
        """Add documents to vector collection"""
        try:
            collection = self.client.get_collection(collection_name)
            
            ids = []
            embeddings = []
            metadatas = []
            documents_text = []
            
            for doc in documents:
                # Generate unique ID if not provided
                doc_id = doc.get('id', str(uuid.uuid4()))
                ids.append(doc_id)
                
                # Extract embedding (should be pre-computed)
                embedding = doc['embedding']
                embeddings.append(embedding)
                
                # Extract metadata
                metadata = {
                    'document_id': doc.get('document_id'),
                    'chunk_index': doc.get('chunk_index'),
                    'access_level': doc.get('access_level', 'employee_only'),
                    'department': doc.get('department'),
                    'document_type': doc.get('document_type'),
                    'title': doc.get('title'),
                    'created_at': doc.get('created_at')
                }
                metadatas.append(metadata)
                
                # Document text
                documents_text.append(doc['content'])
            
            # Add to collection
            collection.add(
                ids=ids,
                embeddings=embeddings,
                metadatas=metadatas,
                documents=documents_text
            )
            
            print(f"✅ Added {len(documents)} documents to collection '{collection_name}'")
            return True
            
        except Exception as e:
            print(f"❌ Error adding documents: {e}")
            return False
    
    def search_documents(self, collection_name: str, query_embedding: List[float], 
                        filters: Dict[str, Any] = None, n_results: int = 5) -> Dict[str, Any]:
        """Search for similar documents"""
        try:
            collection = self.client.get_collection(collection_name)
            
            where_clause = {}
            if filters:
                # Build where clause for filtering
                if 'access_levels' in filters:
                    where_clause['access_level'] = {"$in": filters['access_levels']}
                if 'departments' in filters:
                    where_clause['department'] = {"$in": filters['departments']}
                if 'document_types' in filters:
                    where_clause['document_type'] = {"$in": filters['document_types']}
            
            results = collection.query(
                query_embeddings=[query_embedding],
                n_results=n_results,
                where=where_clause if where_clause else None,
                include=['documents', 'metadatas', 'distances']
            )
            
            return results
            
        except Exception as e:
            print(f"❌ Error searching documents: {e}")
            return {}
    
    def get_collection_stats(self, collection_name: str) -> Dict[str, Any]:
        """Get collection statistics"""
        try:
            collection = self.client.get_collection(collection_name)
            count = collection.count()
            
            return {
                'name': collection_name,
                'count': count,
                'metadata': collection.metadata
            }
            
        except Exception as e:
            print(f"❌ Error getting collection stats: {e}")
            return {}

# Test vector operations
def test_vector_operations():
    vector_ops = VectorOperations()
    
    # Test collection stats
    stats = vector_ops.get_collection_stats('knowledge_base_v1')
    print(f"Collection stats: {stats}")
    
    print("✅ Vector operations test passed")

if __name__ == "__main__":
    test_vector_operations()
```

#### **Bước 3.5: Test Vector Operations** ⏱️ 3 giờ
```python
# tests/test_chroma.py
import pytest
import chromadb
from chromadb.config import Settings
import numpy as np

def test_chroma_connection():
    """Test connection to Chroma server"""
    client = chromadb.HttpClient(host="localhost", port=8000)
    
    # Test heartbeat
    heartbeat = client.heartbeat()
    assert heartbeat is not None

def test_collection_operations():
    """Test collection CRUD operations"""
    client = chromadb.HttpClient(host="localhost", port=8000)
    
    test_collection_name = "test_collection"
    
    # Clean up if exists
    try:
        client.delete_collection(test_collection_name)
    except:
        pass
    
    # Create collection
    collection = client.create_collection(
        name=test_collection_name,
        metadata={"test": True}
    )
    
    assert collection.name == test_collection_name
    
    # List collections
    collections = client.list_collections()
    collection_names = [col.name for col in collections]
    assert test_collection_name in collection_names
    
    # Clean up
    client.delete_collection(test_collection_name)

def test_document_operations():
    """Test document add/search operations"""
    client = chromadb.HttpClient(host="localhost", port=8000)
    
    test_collection_name = "test_docs"
    
    # Clean up and create
    try:
        client.delete_collection(test_collection_name)
    except:
        pass
    
    collection = client.create_collection(test_collection_name)
    
    # Add test documents
    test_docs = [
        {
            'id': 'doc1',
            'content': 'This is about company policies and procedures.',
            'embedding': np.random.rand(1536).tolist(),
            'metadata': {'type': 'policy', 'department': 'HR'}
        },
        {
            'id': 'doc2', 
            'content': 'Technical documentation for software development.',
            'embedding': np.random.rand(1536).tolist(),
            'metadata': {'type': 'technical', 'department': 'R&D'}
        }
    ]
    
    collection.add(
        ids=[doc['id'] for doc in test_docs],
        embeddings=[doc['embedding'] for doc in test_docs],
        documents=[doc['content'] for doc in test_docs],
        metadatas=[doc['metadata'] for doc in test_docs]
    )
    
    # Test search
    query_embedding = np.random.rand(1536).tolist()
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=2
    )
    
    assert len(results['ids'][0]) == 2
    assert len(results['documents'][0]) == 2
    
    # Test filtered search
    filtered_results = collection.query(
        query_embeddings=[query_embedding],
        n_results=1,
        where={"department": "HR"}
    )
    
    assert len(filtered_results['ids'][0]) == 1
    assert filtered_results['metadatas'][0][0]['department'] == 'HR'
    
    # Clean up
    client.delete_collection(test_collection_name)

if __name__ == "__main__":
    test_chroma_connection()
    test_collection_operations()
    test_document_operations()
    print("✅ All Chroma tests passed")
```

### **Phase 4: Integration & Testing (Tuần 2-3)**

#### **Bước 4.1: Integrate Both Databases** ⏱️ 6 giờ
```python
# scripts/database_integration.py
import asyncio
from typing import List, Dict, Any, Optional
import asyncpg
import chromadb
from chromadb.config import Settings
import uuid
from datetime import datetime
import json

class DatabaseIntegration:
    def __init__(self, postgres_url: str, chroma_host: str = "localhost", chroma_port: int = 8000):
        self.postgres_url = postgres_url
        self.postgres_pool = None
        
        self.chroma_client = chromadb.HttpClient(
            host=chroma_host,
            port=chroma_port,
            settings=Settings(
                chroma_server_host=chroma_host,
                chroma_server_port=chroma_port,
                chroma_server_cors_allow_origins="*"
            )
        )
    
    async def init_postgres_pool(self):
        """Initialize PostgreSQL connection pool"""
        self.postgres_pool = await asyncpg.create_pool(
            self.postgres_url,
            min_size=5,
            max_size=20,
            command_timeout=60
        )
    
    async def close_postgres_pool(self):
        """Close PostgreSQL connection pool"""
        if self.postgres_pool:
            await self.postgres_pool.close()
    
    async def add_document_with_chunks(self, document_data: Dict[str, Any], 
                                     chunks_data: List[Dict[str, Any]]) -> bool:
        """Add document to PostgreSQL and chunks to Chroma in a coordinated way"""
        try:
            # Start PostgreSQL transaction
            async with self.postgres_pool.acquire() as conn:
                async with conn.transaction():
                    # Insert document
                    document_id = await conn.fetchval("""
                        INSERT INTO documents (title, file_path, content_hash, document_type, 
                                             access_level, department_owner, author_id, version, 
                                             chunk_count, metadata)
                        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                        RETURNING document_id
                    """, 
                    document_data['title'],
                    document_data.get('file_path'),
                    document_data.get('content_hash'),
                    document_data['document_type'],
                    document_data.get('access_level', 'employee_only'),
                    document_data.get('department_owner'),
                    document_data.get('author_id'),
                    document_data.get('version', '1.0'),
                    len(chunks_data),
                    json.dumps(document_data.get('metadata', {}))
                    )
                    
                    # Prepare chunks for PostgreSQL
                    chunk_records = []
                    chroma_documents = []
                    
                    for i, chunk in enumerate(chunks_data):
                        chunk_id = str(uuid.uuid4())
                        vector_id = f"{document_id}_{i}"
                        
                        # PostgreSQL chunk record
                        chunk_records.append({
                            'chunk_id': chunk_id,
                            'document_id': document_id,
                            'chunk_index': i,
                            'content': chunk['content'],
                            'vector_id': vector_id,
                            'chunk_metadata': json.dumps(chunk.get('metadata', {}))
                        })
                        
                        # Chroma document record
                        chroma_doc = {
                            'id': vector_id,
                            'content': chunk['content'],
                            'embedding': chunk['embedding'],
                            'metadata': {
                                'document_id': str(document_id),
                                'chunk_id': chunk_id,
                                'chunk_index': i,
                                'access_level': document_data.get('access_level', 'employee_only'),
                                'department': document_data.get('department_owner'),
                                'document_type': document_data['document_type'],
                                'title': document_data['title'],
                                'created_at': datetime.now().isoformat()
                            }
                        }
                        chroma_documents.append(chroma_doc)
                    
                    # Insert chunks into PostgreSQL
                    await conn.executemany("""
                        INSERT INTO document_chunks (chunk_id, document_id, chunk_index, 
                                                   content, vector_id, chunk_metadata)
                        VALUES ($1, $2, $3, $4, $5, $6)
                    """, 
                    [(r['chunk_id'], r['document_id'], r['chunk_index'], 
                      r['content'], r['vector_id'], r['chunk_metadata']) 
                     for r in chunk_records])
                    
                    # Add to Chroma (determine collection based on access level)
                    collection_name = self._get_collection_name(document_data)
                    collection = self.chroma_client.get_collection(collection_name)
                    
                    collection.add(
                        ids=[doc['id'] for doc in chroma_documents],
                        embeddings=[doc['embedding'] for doc in chroma_documents],
                        documents=[doc['content'] for doc in chroma_documents],
                        metadatas=[doc['metadata'] for doc in chroma_documents]
                    )
                    
                    print(f"✅ Successfully added document {document_id} with {len(chunks_data)} chunks")
                    return True
                    
        except Exception as e:
            print(f"❌ Error adding document with chunks: {e}")
            # TODO: Add cleanup logic for partial failures
            return False
    
    async def search_with_permissions(self, query_embedding: List[float], 
                                    user_id: str, n_results: int = 5) -> List[Dict[str, Any]]:
        """Search documents considering user permissions"""
        try:
            # Get user permissions from PostgreSQL
            async with self.postgres_pool.acquire() as conn:
                user_info = await conn.fetchrow("""
                    SELECT user_level, department FROM users WHERE user_id = $1
                """, uuid.UUID(user_id))
                
                if not user_info:
                    return []
                
                # Determine accessible access levels based on user level
                access_levels = self._get_accessible_levels(user_info['user_level'])
                
                # Search in Chroma with access level filters
                collection = self.chroma_client.get_collection('knowledge_base_v1')
                
                chroma_results = collection.query(
                    query_embeddings=[query_embedding],
                    n_results=n_results * 2,  # Get more to account for filtering
                    where={"access_level": {"$in": access_levels}},
                    include=['documents', 'metadatas', 'distances']
                )
                
                if not chroma_results['ids'][0]:
                    return []
                
                # Get additional document info from PostgreSQL
                document_ids = [metadata['document_id'] 
                              for metadata in chroma_results['metadatas'][0]]
                
                documents_info = await conn.fetch("""
                    SELECT document_id, title, document_type, access_level, department_owner
                    FROM documents 
                    WHERE document_id = ANY($1::uuid[])
                """, document_ids)
                
                doc_info_map = {str(row['document_id']): dict(row) for row in documents_info}
                
                # Combine results
                results = []
                for i, doc_id in enumerate(chroma_results['ids'][0][:n_results]):
                    metadata = chroma_results['metadatas'][0][i]
                    document_info = doc_info_map.get(metadata['document_id'], {})
                    
                    result = {
                        'chunk_id': doc_id,
                        'content': chroma_results['documents'][0][i],
                        'distance': chroma_results['distances'][0][i],
                        'metadata': metadata,
                        'document_info': document_info
                    }
                    results.append(result)
                
                return results
                
        except Exception as e:
            print(f"❌ Error searching with permissions: {e}")
            return []
    
    def _get_collection_name(self, document_data: Dict[str, Any]) -> str:
        """Determine Chroma collection name based on document properties"""
        access_level = document_data.get('access_level', 'employee_only')
        
        if access_level in ['director_only', 'system_admin']:
            return 'knowledge_base_secure_v1'
        elif document_data.get('department_owner'):
            dept = document_data['department_owner'].lower()
            return f'dept_{dept}_v1'
        else:
            return 'knowledge_base_v1'
    
    def _get_accessible_levels(self, user_level: str) -> List[str]:
        """Get list of access levels user can access"""
        level_hierarchy = {
            'guest': ['public'],
            'employee': ['public', 'employee_only'],
            'manager': ['public', 'employee_only', 'manager_only'],
            'director': ['public', 'employee_only', 'manager_only', 'director_only'],
            'admin': ['public', 'employee_only', 'manager_only', 'director_only', 'system_admin']
        }
        return level_hierarchy.get(user_level, ['public'])

# Integration test
async def test_integration():
    postgres_url = "postgresql://kb_admin:password@localhost:5432/knowledge_base"
    
    db_integration = DatabaseIntegration(postgres_url)
    await db_integration.init_postgres_pool()
    
    try:
        # Test document addition
        document_data = {
            'title': 'Test Integration Document',
            'document_type': 'technical_guide',
            'access_level': 'employee_only',
            'department_owner': 'IT',
            'metadata': {'test': True}
        }
        
        chunks_data = [
            {
                'content': 'This is the first chunk of the test document.',
                'embedding': np.random.rand(1536).tolist(),
                'metadata': {'section': 'introduction'}
            },
            {
                'content': 'This is the second chunk with different content.',
                'embedding': np.random.rand(1536).tolist(),
                'metadata': {'section': 'details'}
            }
        ]
        
        success = await db_integration.add_document_with_chunks(document_data, chunks_data)
        assert success
        
        print("✅ Integration test passed")
        
    finally:
        await db_integration.close_postgres_pool()

if __name__ == "__main__":
    import numpy as np
    asyncio.run(test_integration())
```

#### **Bước 4.2: Setup Redis Cache** ⏱️ 2 giờ
```yaml
# Add Redis to docker-compose.yml
  redis:
    image: redis:7-alpine
    container_name: fr02-redis
    ports:
      - "6379:6379"
    volumes:
      - ./data/redis:/data
      - ./config/redis.conf:/usr/local/etc/redis/redis.conf
    command: redis-server /usr/local/etc/redis/redis.conf
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "redis-cli ping | grep PONG"]
      interval: 30s
      timeout: 10s
      retries: 5
```

```python
# scripts/cache_manager.py
import redis
import json
import hashlib
from typing import Any, Optional, List, Dict
import pickle
from datetime import timedelta

class CacheManager:
    def __init__(self, redis_host: str = "localhost", redis_port: int = 6379, redis_db: int = 0):
        self.redis_client = redis.Redis(
            host=redis_host,
            port=redis_port,
            db=redis_db,
            decode_responses=False  # Keep binary for pickle
        )
        
        # Cache TTL settings (in seconds)
        self.ttl_settings = {
            'query_results': 300,     # 5 minutes for query results
            'user_permissions': 900,  # 15 minutes for user permissions
            'embeddings': 3600,       # 1 hour for embeddings
            'document_metadata': 600, # 10 minutes for document metadata
        }
    
    def _generate_cache_key(self, prefix: str, *args) -> str:
        """Generate cache key from prefix and arguments"""
        key_data = f"{prefix}:{':'.join(map(str, args))}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    def cache_query_results(self, query_embedding: List[float], user_id: str, 
                          results: List[Dict[str, Any]], filters: Dict[str, Any] = None) -> bool:
        """Cache search query results"""
        try:
            # Create cache key from query parameters
            query_hash = hashlib.md5(str(query_embedding).encode()).hexdigest()[:16]
            filters_hash = hashlib.md5(str(filters or {}).encode()).hexdigest()[:8]
            cache_key = f"query:{query_hash}:{user_id}:{filters_hash}"
            
            # Serialize results
            cached_data = {
                'results': results,
                'timestamp': datetime.now().isoformat(),
                'user_id': user_id,
                'filters': filters
            }
            
            # Store with TTL
            self.redis_client.setex(
                cache_key,
                timedelta(seconds=self.ttl_settings['query_results']),
                pickle.dumps(cached_data)
            )
            
            return True
            
        except Exception as e:
            print(f"❌ Error caching query results: {e}")
            return False
    
    def get_cached_query_results(self, query_embedding: List[float], user_id: str, 
                               filters: Dict[str, Any] = None) -> Optional[List[Dict[str, Any]]]:
        """Get cached query results"""
        try:
            query_hash = hashlib.md5(str(query_embedding).encode()).hexdigest()[:16]
            filters_hash = hashlib.md5(str(filters or {}).encode()).hexdigest()[:8]
            cache_key = f"query:{query_hash}:{user_id}:{filters_hash}"
            
            cached_data = self.redis_client.get(cache_key)
            if cached_data:
                data = pickle.loads(cached_data)
                return data['results']
            
            return None
            
        except Exception as e:
            print(f"❌ Error getting cached query results: {e}")
            return None
    
    def cache_user_permissions(self, user_id: str, permissions: Dict[str, Any]) -> bool:
        """Cache user permissions"""
        try:
            cache_key = f"permissions:{user_id}"
            
            self.redis_client.setex(
                cache_key,
                timedelta(seconds=self.ttl_settings['user_permissions']),
                json.dumps(permissions)
            )
            
            return True
            
        except Exception as e:
            print(f"❌ Error caching user permissions: {e}")
            return False
    
    def get_cached_user_permissions(self, user_id: str) -> Optional[Dict[str, Any]]:
        """Get cached user permissions"""
        try:
            cache_key = f"permissions:{user_id}"
            cached_data = self.redis_client.get(cache_key)
            
            if cached_data:
                return json.loads(cached_data.decode())
            
            return None
            
        except Exception as e:
            print(f"❌ Error getting cached user permissions: {e}")
            return None
    
    def invalidate_user_cache(self, user_id: str) -> bool:
        """Invalidate all cache entries for a user"""
        try:
            # Get all keys related to user
            patterns = [
                f"query:*:{user_id}:*",
                f"permissions:{user_id}",
                f"user_data:{user_id}"
            ]
            
            deleted_count = 0
            for pattern in patterns:
                keys = self.redis_client.keys(pattern)
                if keys:
                    deleted_count += self.redis_client.delete(*keys)
            
            print(f"✅ Invalidated {deleted_count} cache entries for user {user_id}")
            return True
            
        except Exception as e:
            print(f"❌ Error invalidating user cache: {e}")
            return False
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """Get cache statistics"""
        try:
            info = self.redis_client.info()
            stats = {
                'connected_clients': info['connected_clients'],
                'used_memory': info['used_memory_human'],
                'keyspace_hits': info['keyspace_hits'],
                'keyspace_misses': info['keyspace_misses'],
                'hit_rate': info['keyspace_hits'] / (info['keyspace_hits'] + info['keyspace_misses']) if (info['keyspace_hits'] + info['keyspace_misses']) > 0 else 0
            }
            
            # Count keys by type
            key_patterns = ['query:*', 'permissions:*', 'embeddings:*', 'document:*']
            for pattern in key_patterns:
                count = len(self.redis_client.keys(pattern))
                stats[f'{pattern.replace("*", "count")}'] = count
            
            return stats
            
        except Exception as e:
            print(f"❌ Error getting cache stats: {e}")
            return {}

# Test cache manager
def test_cache_manager():
    cache = CacheManager()
    
    # Test user permissions caching
    test_permissions = {
        'user_level': 'employee',
        'department': 'R&D',
        'accessible_levels': ['public', 'employee_only']
    }
    
    user_id = "test-user-123"
    
    # Cache permissions
    success = cache.cache_user_permissions(user_id, test_permissions)
    assert success
    
    # Retrieve permissions
    cached_permissions = cache.get_cached_user_permissions(user_id)
    assert cached_permissions == test_permissions
    
    # Test cache invalidation
    cache.invalidate_user_cache(user_id)
    cached_permissions = cache.get_cached_user_permissions(user_id)
    assert cached_permissions is None
    
    # Test cache stats
    stats = cache.get_cache_stats()
    assert 'connected_clients' in stats
    
    print("✅ Cache manager test passed")

if __name__ == "__main__":
    from datetime import datetime
    test_cache_manager()
```

#### **Bước 4.3: Configure Monitoring** ⏱️ 3 giờ
```yaml
# Add monitoring services to docker-compose.yml
  prometheus:
    image: prom/prometheus:latest
    container_name: fr02-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./config/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./data/prometheus:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    container_name: fr02-grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}
    volumes:
      - ./data/grafana:/var/lib/grafana
      - ./config/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./config/grafana/datasources:/etc/grafana/provisioning/datasources
    restart: unless-stopped

  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:latest
    container_name: fr02-postgres-exporter
    environment:
      DATA_SOURCE_NAME: "postgresql://kb_admin:${POSTGRES_PASSWORD}@postgres:5432/knowledge_base?sslmode=disable"
    ports:
      - "9187:9187"
    depends_on:
      - postgres

  redis-exporter:
    image: oliver006/redis_exporter:latest
    container_name: fr02-redis-exporter
    environment:
      REDIS_ADDR: "redis://redis:6379"
    ports:
      - "9121:9121"
    depends_on:
      - redis
```

```yaml
# config/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "first_rules.yml"

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  - job_name: 'postgres-exporter'
    static_configs:
      - targets: ['postgres-exporter:9187']

  - job_name: 'redis-exporter'
    static_configs:
      - targets: ['redis-exporter:9121']

  - job_name: 'chroma-api'
    static_configs:
      - targets: ['chroma:8000']
    metrics_path: '/api/v1/metrics'

  - job_name: 'node-exporter'
    static_configs:
      - targets: ['localhost:9100']

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093
```

```python
# scripts/monitoring_setup.py
import requests
import json
from typing import Dict, Any
import time

class MonitoringSetup:
    def __init__(self, grafana_url: str = "http://localhost:3000", 
                 grafana_user: str = "admin", grafana_password: str = "admin"):
        self.grafana_url = grafana_url
        self.auth = (grafana_user, grafana_password)
        self.headers = {'Content-Type': 'application/json'}
    
    def setup_datasources(self) -> bool:
        """Setup Grafana datasources"""
        try:
            # Prometheus datasource
            prometheus_datasource = {
                "name": "Prometheus",
                "type": "prometheus",
                "url": "http://prometheus:9090",
                "access": "proxy",
                "isDefault": True
            }
            
            response = requests.post(
                f"{self.grafana_url}/api/datasources",
                headers=self.headers,
                auth=self.auth,
                data=json.dumps(prometheus_datasource)
            )
            
            if response.status_code in [200, 409]:  # 409 if already exists
                print("✅ Prometheus datasource configured")
                return True
            else:
                print(f"❌ Error configuring Prometheus datasource: {response.text}")
                return False
                
        except Exception as e:
            print(f"❌ Error setting up datasources: {e}")
            return False
    
    def create_dashboard(self) -> bool:
        """Create FR-02.1 monitoring dashboard"""
        try:
            dashboard_config = {
                "dashboard": {
                    "id": None,
                    "title": "FR-02.1 Dual Database System",
                    "tags": ["fr-02.1", "databases"],
                    "timezone": "browser",
                    "panels": [
                        {
                            "id": 1,
                            "title": "PostgreSQL Connections",
                            "type": "stat",
                            "targets": [
                                {
                                    "expr": "pg_stat_activity_count",
                                    "refId": "A"
                                }
                            ],
                            "gridPos": {"h": 8, "w": 12, "x": 0, "y": 0}
                        },
                        {
                            "id": 2,
                            "title": "PostgreSQL Query Duration",
                            "type": "graph",
                            "targets": [
                                {
                                    "expr": "rate(pg_stat_database_tup_fetched[5m])",
                                    "refId": "A"
                                }
                            ],
                            "gridPos": {"h": 8, "w": 12, "x": 12, "y": 0}
                        },
                        {
                            "id": 3,
                            "title": "Redis Memory Usage",
                            "type": "stat",
                            "targets": [
                                {
                                    "expr": "redis_memory_used_bytes",
                                    "refId": "A"
                                }
                            ],
                            "gridPos": {"h": 8, "w": 12, "x": 0, "y": 8}
                        },
                        {
                            "id": 4,
                            "title": "Redis Hit Rate",
                            "type": "stat",
                            "targets": [
                                {
                                    "expr": "rate(redis_keyspace_hits_total[5m]) / (rate(redis_keyspace_hits_total[5m]) + rate(redis_keyspace_misses_total[5m]))",
                                    "refId": "A"
                                }
                            ],
                            "gridPos": {"h": 8, "w": 12, "x": 12, "y": 8}
                        },
                        {
                            "id": 5,
                            "title": "Chroma API Response Time",
                            "type": "graph",
                            "targets": [
                                {
                                    "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",
                                    "refId": "A"
                                }
                            ],
                            "gridPos": {"h": 8, "w": 24, "x": 0, "y": 16}
                        }
                    ],
                    "time": {"from": "now-1h", "to": "now"},
                    "refresh": "10s"
                },
                "overwrite": True
            }
            
            response = requests.post(
                f"{self.grafana_url}/api/dashboards/db",
                headers=self.headers,
                auth=self.auth,
                data=json.dumps(dashboard_config)
            )
            
            if response.status_code == 200:
                print("✅ Dashboard created successfully")
                return True
            else:
                print(f"❌ Error creating dashboard: {response.text}")
                return False
                
        except Exception as e:
            print(f"❌ Error creating dashboard: {e}")
            return False
    
    def setup_alerts(self) -> bool:
        """Setup basic alerting rules"""
        try:
            alert_rules = [
                {
                    "alert": "PostgreSQLDown",
                    "expr": "pg_up == 0",
                    "for": "5m",
                    "labels": {"severity": "critical"},
                    "annotations": {
                        "summary": "PostgreSQL instance is down",
                        "description": "PostgreSQL instance {{ $labels.instance }} is down"
                    }
                },
                {
                    "alert": "RedisDown", 
                    "expr": "redis_up == 0",
                    "for": "5m",
                    "labels": {"severity": "critical"},
                    "annotations": {
                        "summary": "Redis instance is down",
                        "description": "Redis instance {{ $labels.instance }} is down"
                    }
                },
                {
                    "alert": "HighPostgreSQLConnections",
                    "expr": "pg_stat_activity_count > 80",
                    "for": "10m", 
                    "labels": {"severity": "warning"},
                    "annotations": {
                        "summary": "High PostgreSQL connections",
                        "description": "PostgreSQL has {{ $value }} active connections"
                    }
                }
            ]
            
            print("✅ Alert rules configured")
            return True
            
        except Exception as e:
            print(f"❌ Error setting up alerts: {e}")
            return False

def wait_for_services():
    """Wait for services to be ready"""
    services = [
        ("Grafana", "http://localhost:3000/api/health"),
        ("Prometheus", "http://localhost:9090/-/healthy"),
        ("Chroma", "http://localhost:8000/api/v1/heartbeat")
    ]
    
    for service_name, url in services:
        max_retries = 30
        for attempt in range(max_retries):
            try:
                response = requests.get(url, timeout=5)
                if response.status_code == 200:
                    print(f"✅ {service_name} is ready")
                    break
            except:
                pass
            
            if attempt == max_retries - 1:
                print(f"❌ {service_name} failed to start")
                return False
            
            time.sleep(2)
    
    return True

def setup_monitoring():
    """Main monitoring setup function"""
    print("🔧 Setting up monitoring...")
    
    if not wait_for_services():
        return False
    
    monitoring = MonitoringSetup()
    
    # Setup datasources
    if not monitoring.setup_datasources():
        return False
    
    # Wait a bit for datasources to be ready
    time.sleep(5)
    
    # Create dashboard
    if not monitoring.create_dashboard():
        return False
    
    # Setup alerts
    if not monitoring.setup_alerts():
        return False
    
    print("✅ Monitoring setup complete")
    print("📊 Grafana: http://localhost:3000")
    print("📈 Prometheus: http://localhost:9090")
    return True

if __name__ == "__main__":
    setup_monitoring()
```

#### **Bước 4.4: Run Integration Tests** ⏱️ 4 giờ
```python
# tests/test_integration.py
import pytest
import asyncio
import numpy as np
from scripts.database_integration import DatabaseIntegration
from scripts.cache_manager import CacheManager
import uuid
import json

@pytest.mark.asyncio
class TestDualDatabaseIntegration:
    
    @pytest.fixture
    async def db_integration(self):
        """Setup database integration for tests"""
        postgres_url = "postgresql://kb_admin:password@localhost:5432/knowledge_base"
        integration = DatabaseIntegration(postgres_url)
        await integration.init_postgres_pool()
        yield integration
        await integration.close_postgres_pool()
    
    @pytest.fixture
    def cache_manager(self):
        """Setup cache manager for tests"""
        return CacheManager()
    
    async def test_document_lifecycle(self, db_integration):
        """Test complete document lifecycle: add -> search -> update"""
        # Prepare test data
        document_data = {
            'title': 'Integration Test Document',
            'file_path': '/test/integration_doc.pdf',
            'content_hash': 'test_hash_12345',
            'document_type': 'technical_guide',
            'access_level': 'employee_only',
            'department_owner': 'IT',
            'metadata': {
                'version': '1.0',
                'tags': ['test', 'integration'],
                'author': 'test_system'
            }
        }
        
        chunks_data = [
            {
                'content': 'This is the first chunk about system integration testing.',
                'embedding': np.random.rand(1536).tolist(),
                'metadata': {
                    'section': 'introduction',
                    'word_count': 10
                }
            },
            {
                'content': 'This chunk covers advanced database integration patterns.',
                'embedding': np.random.rand(1536).tolist(),
                'metadata': {
                    'section': 'advanced_topics', 
                    'word_count': 8
                }
            },
            {
                'content': 'Final chunk with conclusion and best practices.',
                'embedding': np.random.rand(1536).tolist(),
                'metadata': {
                    'section': 'conclusion',
                    'word_count': 8
                }
            }
        ]
        
        # Test document addition
        success = await db_integration.add_document_with_chunks(document_data, chunks_data)
        assert success, "Document addition should succeed"
        
        # Create test user for search
        async with db_integration.postgres_pool.acquire() as conn:
            test_user_id = await conn.fetchval("""
                INSERT INTO users (username, email, password_hash, user_level, department)
                VALUES ('test_integration_user', 'test_integration@test.com', 'hash123', 'employee', 'IT')
                RETURNING user_id
            """)
        
        # Test search with permissions
        query_embedding = np.random.rand(1536).tolist()
        search_results = await db_integration.search_with_permissions(
            query_embedding, str(test_user_id), n_results=3
        )
        
        assert len(search_results) > 0, "Search should return results"
        assert all('content' in result for result in search_results), "All results should have content"
        assert all('metadata' in result for result in search_results), "All results should have metadata"
        
        # Verify access control
        employee_results = await db_integration.search_with_permissions(
            query_embedding, str(test_user_id), n_results=5
        )
        
        for result in employee_results:
            access_level = result['metadata']['access_level']
            assert access_level in ['public', 'employee_only'], f"Employee should only see public/employee docs, got {access_level}"
        
        # Cleanup test user
        async with db_integration.postgres_pool.acquire() as conn:
            await conn.execute("DELETE FROM users WHERE user_id = $1", test_user_id)
        
        print("✅ Document lifecycle test passed")
    
    async def test_permission_hierarchy(self, db_integration):
        """Test user permission hierarchy works correctly"""
        # Create users with different levels
        users_data = [
            ('guest_user', 'guest@test.com', 'guest'),
            ('employee_user', 'employee@test.com', 'employee'), 
            ('manager_user', 'manager@test.com', 'manager'),
            ('director_user', 'director@test.com', 'director')
        ]
        
        user_ids = {}
        
        async with db_integration.postgres_pool.acquire() as conn:
            for username, email, level in users_data:
                user_id = await conn.fetchval("""
                    INSERT INTO users (username, email, password_hash, user_level, department)
                    VALUES ($1, $2, 'test_hash', $3, 'Test')
                    RETURNING user_id
                """, username, email, level)
                user_ids[level] = user_id
        
        # Create documents with different access levels
        documents_data = [
            ('Public Document', 'public'),
            ('Employee Document', 'employee_only'),
            ('Manager Document', 'manager_only'),
            ('Director Document', 'director_only')
        ]
        
        for title, access_level in documents_data:
            document_data = {
                'title': title,
                'document_type': 'policy',
                'access_level': access_level,
                'department_owner': 'Test'
            }
            
            chunks_data = [{
                'content': f'Content for {title}',
                'embedding': np.random.rand(1536).tolist(),
                'metadata': {}
            }]
            
            await db_integration.add_document_with_chunks(document_data, chunks_data)
        
        # Test access for each user level
        query_embedding = np.random.rand(1536).tolist()
        
        # Guest should only see public
        guest_results = await db_integration.search_with_permissions(
            query_embedding, str(user_ids['guest']), n_results=10
        )
        guest_access_levels = [r['metadata']['access_level'] for r in guest_results]
        assert all(level == 'public' for level in guest_access_levels), "Guest should only see public documents"
        
        # Employee should see public + employee_only
        employee_results = await db_integration.search_with_permissions(
            query_embedding, str(user_ids['employee']), n_results=10
        )
        employee_access_levels = set(r['metadata']['access_level'] for r in employee_results)
        assert employee_access_levels <= {'public', 'employee_only'}, "Employee should see public and employee_only documents"
        
        # Manager should see public + employee_only + manager_only
        manager_results = await db_integration.search_with_permissions(
            query_embedding, str(user_ids['manager']), n_results=10
        )
        manager_access_levels = set(r['metadata']['access_level'] for r in manager_results)
        assert manager_access_levels <= {'public', 'employee_only', 'manager_only'}, "Manager should see appropriate access levels"
        
        # Director should see all except system_admin
        director_results = await db_integration.search_with_permissions(
            query_embedding, str(user_ids['director']), n_results=10
        )
        director_access_levels = set(r['metadata']['access_level'] for r in director_results)
        assert 'system_admin' not in director_access_levels, "Director should not see system_admin documents"
        
        # Cleanup
        async with db_integration.postgres_pool.acquire() as conn:
            for user_id in user_ids.values():
                await conn.execute("DELETE FROM users WHERE user_id = $1", user_id)
        
        print("✅ Permission hierarchy test passed")
    
    def test_cache_integration(self, cache_manager, db_integration):
        """Test cache integration with database operations"""
        # Test user permissions caching
        user_id = str(uuid.uuid4())
        permissions = {
            'user_level': 'employee',
            'department': 'IT',
            'accessible_levels': ['public', 'employee_only']
        }
        
        # Cache permissions
        success = cache_manager.cache_user_permissions(user_id, permissions)
        assert success, "Should successfully cache user permissions"
        
        # Retrieve permissions
        cached_permissions = cache_manager.get_cached_user_permissions(user_id)
        assert cached_permissions == permissions, "Cached permissions should match original"
        
        # Test query results caching
        query_embedding = np.random.rand(1536).tolist()
        mock_results = [
            {
                'chunk_id': 'test_chunk_1',
                'content': 'Test content 1',
                'distance': 0.1,
                'metadata': {'access_level': 'employee_only'}
            },
            {
                'chunk_id': 'test_chunk_2', 
                'content': 'Test content 2',
                'distance': 0.2,
                'metadata': {'access_level': 'public'}
            }
        ]
        
        # Cache query results
        success = cache_manager.cache_query_results(query_embedding, user_id, mock_results)
        assert success, "Should successfully cache query results"
        
        # Retrieve cached results
        cached_results = cache_manager.get_cached_query_results(query_embedding, user_id)
        assert cached_results == mock_results, "Cached results should match original"
        
        # Test cache invalidation
        cache_manager.invalidate_user_cache(user_id)
        cached_permissions = cache_manager.get_cached_user_permissions(user_id)
        cached_results = cache_manager.get_cached_query_results(query_embedding, user_id)
        
        assert cached_permissions is None, "Permissions should be cleared after invalidation"
        assert cached_results is None, "Query results should be cleared after invalidation"
        
        print("✅ Cache integration test passed")
    
    async def test_error_handling_and_rollback(self, db_integration):
        """Test error handling and rollback scenarios"""
        # Test invalid document data
        invalid_document_data = {
            'title': '',  # Invalid: empty title
            'document_type': 'invalid_type',  # Invalid: not in enum
            'access_level': 'public'
        }
        
        chunks_data = [{
            'content': 'Test content',
            'embedding': np.random.rand(1536).tolist(),
            'metadata': {}
        }]
        
        # This should fail and rollback
        success = await db_integration.add_document_with_chunks(invalid_document_data, chunks_data)
        assert not success, "Should fail with invalid document data"
        
        # Test with invalid embedding dimensions
        valid_document_data = {
            'title': 'Valid Document',
            'document_type': 'policy',
            'access_level': 'public'
        }
        
        invalid_chunks_data = [{
            'content': 'Test content',
            'embedding': np.random.rand(512).tolist(),  # Invalid: wrong dimension
            'metadata': {}
        }]
        
        success = await db_integration.add_document_with_chunks(valid_document_data, invalid_chunks_data)
        assert not success, "Should fail with invalid embedding dimensions"
        
        print("✅ Error handling and rollback test passed")
    
    async def test_performance_benchmarks(self, db_integration):
        """Test performance benchmarks for the dual database system"""
        import time
        
        # Test document insertion performance
        start_time = time.time()
        
        batch_documents = []
        for i in range(10):  # Insert 10 documents
            document_data = {
                'title': f'Performance Test Document {i}',
                'document_type': 'technical_guide',
                'access_level': 'employee_only',
                'department_owner': 'IT'
            }
            
            chunks_data = [
                {
                    'content': f'Performance test content chunk {j} for document {i}',
                    'embedding': np.random.rand(1536).tolist(),
                    'metadata': {'chunk_index': j}
                }
                for j in range(5)  # 5 chunks per document
            ]
            
            success = await db_integration.add_document_with_chunks(document_data, chunks_data)
            assert success, f"Document {i} insertion should succeed"
        
        insertion_time = time.time() - start_time
        print(f"📊 Document insertion: {insertion_time:.2f}s for 10 docs (50 chunks)")
        
        # Test search performance
        test_user_id = str(uuid.uuid4())
        async with db_integration.postgres_pool.acquire() as conn:
            await conn.execute("""
                INSERT INTO users (user_id, username, email, password_hash, user_level)
                VALUES ($1, 'perf_test_user', 'perf@test.com', 'hash', 'employee')
            """, uuid.UUID(test_user_id))
        
        search_times = []
        for _ in range(10):  # Run 10 search queries
            query_embedding = np.random.rand(1536).tolist()
            
            start_time = time.time()
            results = await db_integration.search_with_permissions(
                query_embedding, test_user_id, n_results=5
            )
            search_time = time.time() - start_time
            search_times.append(search_time)
            
            assert len(results) > 0, "Search should return results"
        
        avg_search_time = sum(search_times) / len(search_times)
        max_search_time = max(search_times)
        
        print(f"📊 Search performance: avg {avg_search_time:.3f}s, max {max_search_time:.3f}s")
        
        # Performance assertions (adjust based on requirements)
        assert avg_search_time < 1.0, f"Average search time should be < 1s, got {avg_search_time:.3f}s"
        assert max_search_time < 2.0, f"Max search time should be < 2s, got {max_search_time:.3f}s"
        
        # Cleanup
        async with db_integration.postgres_pool.acquire() as conn:
            await conn.execute("DELETE FROM users WHERE user_id = $1", uuid.UUID(test_user_id))
        
        print("✅ Performance benchmark test passed")

# Run integration tests
if __name__ == "__main__":
    pytest.main([__file__, "-v", "--tb=short"])
```

#### **Bước 4.5: Performance Testing** ⏱️ 3 giờ
```python
# scripts/performance_testing.py
import asyncio
import asyncpg
import chromadb
import time
import numpy as np
from concurrent.futures import ThreadPoolExecutor
import statistics
from typing import List, Dict, Any
import uuid
import json

class PerformanceTestSuite:
    def __init__(self, postgres_url: str, chroma_host: str = "localhost"):
        self.postgres_url = postgres_url
        self.chroma_client = chromadb.HttpClient(host=chroma_host, port=8000)
        self.postgres_pool = None
        
        # Performance thresholds (from SRS requirements)
        self.thresholds = {
            'search_response_time': 60.0,  # < 60 seconds
            'concurrent_users': 100,       # Support 100 concurrent users
            'document_chunks': 100000      # Support ≥100k chunks
        }
    
    async def init_connections(self):
        """Initialize database connections"""
        self.postgres_pool = await asyncpg.create_pool(
            self.postgres_url,
            min_size=10,
            max_size=50,
            command_timeout=60
        )
    
    async def close_connections(self):
        """Close database connections"""
        if self.postgres_pool:
            await self.postgres_pool.close()
    
    async def test_search_response_time(self, num_queries: int = 100) -> Dict[str, float]:
        """Test search response time under normal load"""
        print(f"🔍 Testing search response time with {num_queries} queries...")
        
        # Create test user
        test_user_id = str(uuid.uuid4())
        async with self.postgres_pool.acquire() as conn:
            await conn.execute("""
                INSERT INTO users (user_id, username, email, password_hash, user_level, department)
                VALUES ($1, 'perf_test_user', 'perf@test.com', 'hash', 'employee', 'IT')
            """, uuid.UUID(test_user_id))
        
        response_times = []
        
        try:
            for i in range(num_queries):
                query_embedding = np.random.rand(1536).tolist()
                
                start_time = time.time()
                
                # Simulate full search pipeline
                async with self.postgres_pool.acquire() as conn:
                    # Get user permissions
                    user_info = await conn.fetchrow("""
                        SELECT user_level, department FROM users WHERE user_id = $1
                    """, uuid.UUID(test_user_id))
                    
                    # Determine access levels
                    access_levels = ['public', 'employee_only']  # For employee level
                    
                    # Search in Chroma
                    collection = self.chroma_client.get_collection('knowledge_base_v1')
                    results = collection.query(
                        query_embeddings=[query_embedding],
                        n_results=5,
                        where={"access_level": {"$in": access_levels}},
                        include=['documents', 'metadatas', 'distances']
                    )
                
                end_time = time.time()
                response_time = end_time - start_time
                response_times.append(response_time)
                
                if (i + 1) % 20 == 0:
                    print(f"  Completed {i + 1}/{num_queries} queries...")
        
        finally:
            # Cleanup test user
            async with self.postgres_pool.acquire() as conn:
                await conn.execute("DELETE FROM users WHERE user_id = $1", uuid.UUID(test_user_id))
        
        # Calculate statistics
        stats = {
            'mean': statistics.mean(response_times),
            'median': statistics.median(response_times),
            'p95': np.percentile(response_times, 95),
            'p99': np.percentile(response_times, 99),
            'max': max(response_times),
            'min': min(response_times)
        }
        
        # Check against thresholds
        passed = stats['p95'] < self.thresholds['search_response_time']
        
        print(f"📊 Search Response Time Results:")
        print(f"  Mean: {stats['mean']:.3f}s")
        print(f"  Median: {stats['median']:.3f}s") 
        print(f"  95th percentile: {stats['p95']:.3f}s")
        print(f"  99th percentile: {stats['p99']:.3f}s")
        print(f"  Max: {stats['max']:.3f}s")
        print(f"  ✅ PASSED" if passed else f"  ❌ FAILED - P95 {stats['p95']:.3f}s > {self.thresholds['search_response_time']}s")
        
        return stats
    
    async def test_concurrent_users(self, num_users: int = 100, queries_per_user: int = 5) -> Dict[str, Any]:
        """Test system performance with concurrent users"""
        print(f"👥 Testing {num_users} concurrent users with {queries_per_user} queries each...")
        
        # Create test users
        user_ids = []
        async with self.postgres_pool.acquire() as conn:
            for i in range(min(num_users, 20)):  # Create up to 20 actual users
                user_id = str(uuid.uuid4())
                await conn.execute("""
                    INSERT INTO users (user_id, username, email, password_hash, user_level, department)
                    VALUES ($1, $2, $3, 'hash', 'employee', 'IT')
                """, uuid.UUID(user_id), f'concurrent_user_{i}', f'concurrent_{i}@test.com')
                user_ids.append(user_id)
        
        async def simulate_user_queries(user_id: str, user_index: int) -> List[float]:
            """Simulate queries for a single user"""
            query_times = []
            
            for query_num in range(queries_per_user):
                try:
                    start_time = time.time()
                    
                    query_embedding = np.random.rand(1536).tolist()
                    
                    async with self.postgres_pool.acquire() as conn:
                        # Get user permissions
                        user_info = await conn.fetchrow("""
                            SELECT user_level, department FROM users WHERE user_id = $1
                        """, uuid.UUID(user_id))
                        
                        if user_info:
                            # Search in Chroma
                            collection = self.chroma_client.get_collection('knowledge_base_v1')
                            results = collection.query(
                                query_embeddings=[query_embedding],
                                n_results=5,
                                where={"access_level": {"$in": ['public', 'employee_only']}},
                                include=['documents', 'metadatas', 'distances']
                            )
                    
                    end_time = time.time()
                    query_times.append(end_time - start_time)
                    
                except Exception as e:
                    print(f"❌ Error in user {user_index} query {query_num}: {e}")
                    query_times.append(float('inf'))  # Mark as failed
            
            return query_times
        
        # Run concurrent user simulations
        start_time = time.time()
        
        tasks = []
        # Use actual user_ids cyclically for all virtual users
        for i in range(num_users):
            user_id = user_ids[i % len(user_ids)]
            task = simulate_user_queries(user_id, i)
            tasks.append(task)
        
        # Execute all tasks concurrently
        all_query_times = await asyncio.gather(*tasks, return_exceptions=True)
        
        end_time = time.time()
        total_test_time = end_time - start_time
        
        # Process results
        successful_queries = []
        failed_queries = 0
        
        for user_times in all_query_times:
            if isinstance(user_times, Exception):
                failed_queries += queries_per_user
            else:
                for query_time in user_times:
                    if query_time == float('inf'):
                        failed_queries += 1
                    else:
                        successful_queries.append(query_time)
        
        # Calculate statistics
        total_queries = num_users * queries_per_user
        success_rate = len(successful_queries) / total_queries
        
        stats = {}
        if successful_queries:
            stats = {
                'total_queries': total_queries,
                'successful_queries': len(successful_queries),
                'failed_queries': failed_queries,
                'success_rate': success_rate,
                'total_time': total_test_time,
                'queries_per_second': total_queries / total_test_time,
                'mean_response_time': statistics.mean(successful_queries),
                'median_response_time': statistics.median(successful_queries),
                'p95_response_time': np.percentile(successful_queries, 95),
                'p99_response_time': np.percentile(successful_queries, 99)
            }
        
        # Check against thresholds
        passed = (success_rate > 0.95 and 
                 stats.get('p95_response_time', float('inf')) < self.thresholds['search_response_time'])
        
        print(f"📊 Concurrent Users Test Results:")
        print(f"  Total queries: {total_queries}")
        print(f"  Successful: {len(successful_queries)} ({success_rate:.1%})")
        print(f"  Failed: {failed_queries}")
        print(f"  Total time: {total_test_time:.2f}s")
        print(f"  Queries/sec: {stats.get('queries_per_second', 0):.2f}")
        print(f"  Mean response: {stats.get('mean_response_time', 0):.3f}s")
        print(f"  P95 response: {stats.get('p95_response_time', 0):.3f}s")
        print(f"  ✅ PASSED" if passed else f"  ❌ FAILED")
        
        # Cleanup test users
        try:
            async with self.postgres_pool.acquire() as conn:
                for user_id in user_ids:
                    await conn.execute("DELETE FROM users WHERE user_id = $1", uuid.UUID(user_id))
        except Exception as e:
            print(f"⚠️ Cleanup warning: {e}")
        
        return stats
    
    async def test_database_capacity(self) -> Dict[str, Any]:
        """Test database capacity and storage limits"""
        print("💾 Testing database capacity...")
        
        # Test PostgreSQL capacity
        async with self.postgres_pool.acquire() as conn:
            # Check current document count
            doc_count = await conn.fetchval("SELECT COUNT(*) FROM documents")
            chunk_count = await conn.fetchval("SELECT COUNT(*) FROM document_chunks")
            user_count = await conn.fetchval("SELECT COUNT(*) FROM users")
            
            # Check database size
            db_size_result = await conn.fetchrow("""
                SELECT pg_size_pretty(pg_database_size(current_database())) as size,
                       pg_database_size(current_database()) as size_bytes
            """)
            
            # Check table sizes
            table_sizes = await conn.fetch("""
                SELECT schemaname, tablename, 
                       pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size,
                       pg_total_relation_size(schemaname||'.'||tablename) as size_bytes
                FROM pg_tables 
                WHERE schemaname = 'public'
                ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC
            """)
        
        # Test Chroma capacity
        try:
            collection = self.chroma_client.get_collection('knowledge_base_v1')
            vector_count = collection.count()
            
            # Get collection metadata
            collection_info = {
                'name': collection.name,
                'count': vector_count,
                'metadata': collection.metadata
            }
        except Exception as e:
            print(f"❌ Error getting Chroma stats: {e}")
            collection_info = {'error': str(e)}
        
        capacity_stats = {
            'postgresql': {
                'document_count': doc_count,
                'chunk_count': chunk_count,
                'user_count': user_count,
                'database_size': db_size_result['size'],
                'database_size_bytes': db_size_result['size_bytes'],
                'table_sizes': [dict(row) for row in table_sizes]
            },
            'chroma': collection_info
        }
        
        # Check against capacity thresholds
        capacity_ok = chunk_count < self.thresholds['document_chunks'] * 0.8  # 80% of max capacity
        
        print(f"📊 Database Capacity Results:")
        print(f"  PostgreSQL:")
        print(f"    Documents: {doc_count:,}")
        print(f"    Chunks: {chunk_count:,}")
        print(f"    Users: {user_count:,}")
        print(f"    DB Size: {db_size_result['size']}")
        print(f"  Chroma:")
        print(f"    Vector count: {vector_count:,}")
        print(f"  ✅ Capacity OK" if capacity_ok else f"  ⚠️ Approaching capacity limits")
        
        return capacity_stats
    
    async def test_database_connections(self) -> Dict[str, Any]:
        """Test database connection pooling and limits"""
        print("🔗 Testing database connections...")
        
        async def create_connection_load():
            """Create temporary connection load"""
            connections = []
            try:
                # Create multiple connections
                for i in range(15):  # Create 15 concurrent connections
                    conn = await self.postgres_pool.acquire()
                    connections.append(conn)
                    # Simulate some work
                    await conn.fetchval("SELECT 1")
                
                # Check connection stats
                async with self.postgres_pool.acquire() as conn:
                    connection_stats = await conn.fetchrow("""
                        SELECT count(*) as active_connections,
                               max_connections,
                               current_setting('max_connections')::int as max_conn_setting
                        FROM pg_stat_activity, 
                             (SELECT setting::int as max_connections FROM pg_settings WHERE name='max_connections') mc
                        WHERE state = 'active'
                        GROUP BY max_connections, max_conn_setting
                    """)
                
                return dict(connection_stats)
                
            finally:
                # Release all connections
                for conn in connections:
                    await self.postgres_pool.release(conn)
        
        connection_stats = await create_connection_load()
        
        # Test connection pool stats
        pool_stats = {
            'pool_size': self.postgres_pool.get_size(),
            'pool_max_size': self.postgres_pool.get_max_size(),
            'pool_min_size': self.postgres_pool.get_min_size(),
            'active_connections': connection_stats.get('active_connections', 0),
            'max_connections': connection_stats.get('max_connections', 0)
        }
        
        print(f"📊 Connection Test Results:")
        print(f"  Pool size: {pool_stats['pool_size']}")
        print(f"  Pool max: {pool_stats['pool_max_size']}")
        print(f"  Active connections: {pool_stats['active_connections']}")
        print(f"  Max connections: {pool_stats['max_connections']}")
        
        return pool_stats
    
    async def run_full_performance_suite(self) -> Dict[str, Any]:
        """Run complete performance test suite"""
        print("🚀 Starting Full Performance Test Suite for FR-02.1")
        print("=" * 60)
        
        await self.init_connections()
        
        results = {}
        
        try:
            # Test 1: Search Response Time
            print("\n1️⃣ Search Response Time Test")
            print("-" * 40)
            results['search_response_time'] = await self.test_search_response_time(50)
            
            # Test 2: Concurrent Users
            print("\n2️⃣ Concurrent Users Test")
            print("-" * 40)
            results['concurrent_users'] = await self.test_concurrent_users(50, 3)  # Reduced for testing
            
            # Test 3: Database Capacity
            print("\n3️⃣ Database Capacity Test")
            print("-" * 40)
            results['database_capacity'] = await self.test_database_capacity()
            
            # Test 4: Connection Management
            print("\n4️⃣ Connection Management Test")
            print("-" * 40)
            results['connection_management'] = await self.test_database_connections()
            
            # Overall assessment
            print("\n📋 PERFORMANCE TEST SUMMARY")
            print("=" * 60)
            
            passed_tests = 0
            total_tests = 4
            
            # Check each test
            if results['search_response_time']['p95'] < self.thresholds['search_response_time']:
                print("✅ Search Response Time: PASSED")
                passed_tests += 1
            else:
                print("❌ Search Response Time: FAILED")
            
            if results['concurrent_users'].get('success_rate', 0) > 0.95:
                print("✅ Concurrent Users: PASSED") 
                passed_tests += 1
            else:
                print("❌ Concurrent Users: FAILED")
            
            if results['database_capacity']['postgresql']['chunk_count'] < self.thresholds['document_chunks']:
                print("✅ Database Capacity: PASSED")
                passed_tests += 1
            else:
                print("❌ Database Capacity: FAILED")
            
            if results['connection_management']['pool_size'] > 0:
                print("✅ Connection Management: PASSED")
                passed_tests += 1
            else:
                print("❌ Connection Management: FAILED")
            
            overall_pass = passed_tests == total_tests
            print(f"\n{'🎉 ALL TESTS PASSED' if overall_pass else '⚠️ SOME TESTS FAILED'}")
            print(f"Passed: {passed_tests}/{total_tests}")
            
            results['summary'] = {
                'total_tests': total_tests,
                'passed_tests': passed_tests,
                'overall_pass': overall_pass,
                'pass_rate': passed_tests / total_tests
            }
            
        finally:
            await self.close_connections()
        
        return results

# Standalone performance test runner
async def run_performance_tests():
    """Run performance tests for FR-02.1"""
    postgres_url = "postgresql://kb_admin:password@localhost:5432/knowledge_base"
    
    perf_suite = PerformanceTestSuite(postgres_url)
    results = await perf_suite.run_full_performance_suite()
    
    # Save results to file
    with open('fr02_performance_results.json', 'w') as f:
        json.dump(results, f, indent=2, default=str)
    
    print(f"\n💾 Results saved to fr02_performance_results.json")
    
    return results

if __name__ == "__main__":
    asyncio.run(run_performance_tests())
```

---

## 🚨 **ĐIỂM QUAY ĐẦU KHI FAIL**

### **Phase 2 Failures: PostgreSQL Issues**
```mermaid
graph TB
    P2_Fail{❌ PostgreSQL<br/>Test Failed?}
    
    subgraph "Troubleshooting Steps"
        Check1[🔍 Check Docker Logs<br/>docker logs fr02-postgres]
        Check2[🔍 Check Port Conflicts<br/>netstat -tulpn | grep 5432]
        Check3[🔍 Check Disk Space<br/>df -h]
        Check4[🔍 Check Permissions<br/>ls -la data/postgres/]
        Check5[🔍 Test Connection<br/>psql -h localhost -U kb_admin]
    end
    
    subgraph "Common Fixes"
        Fix1[🔧 Restart PostgreSQL<br/>docker-compose restart postgres]
        Fix2[🔧 Reset Data Volume<br/>rm -rf data/postgres/*]
        Fix3[🔧 Check Environment Variables<br/>cat .env]
        Fix4[🔧 Update docker-compose.yml<br/>Fix configuration]
        Fix5[🔧 Recreate Container<br/>docker-compose down && docker-compose up]
    end
    
    P2_Fail --> Check1
    Check1 --> Check2
    Check2 --> Check3
    Check3 --> Check4
    Check4 --> Check5
    
    Check5 --> Fix1
    Fix1 --> Fix2
    Fix2 --> Fix3
    Fix3 --> Fix4
    Fix4 --> Fix5
    
    Fix5 --> Retry[🔄 Retry PostgreSQL Setup]
    
    classDef fail fill:#ffebee,stroke:#c62828,stroke-width:2px
    classDef check fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef fix fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef retry fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    
    class P2_Fail fail
    class Check1,Check2,Check3,Check4,Check5 check
    class Fix1,Fix2,Fix3,Fix4,Fix5 fix
    class Retry retry
```

### **Phase 3 Failures: Vector Database Issues**
```mermaid
graph TB
    P3_Fail{❌ Chroma DB<br/>Test Failed?}
    
    subgraph "Debug Chroma Issues"
        Debug1[🔍 Check Chroma Logs<br/>docker logs fr02-chroma]
        Debug2[🔍 Test HTTP API<br/>curl http://localhost:8000/api/v1/heartbeat]
        Debug3[🔍 Check Chroma Data<br/>ls -la data/chroma/]
        Debug4[🔍 Test Collection Creation<br/>Python chromadb client test]
    end
    
    subgraph "Chroma Fixes"
        ChromaFix1[🔧 Restart Chroma<br/>docker-compose restart chroma]
        ChromaFix2[🔧 Clear Chroma Data<br/>rm -rf data/chroma/*]
        ChromaFix3[🔧 Update Chroma Image<br/>docker pull chromadb/chroma:latest]
        ChromaFix4[🔧 Check Port 8000<br/>netstat -tulpn | grep 8000]
    end
    
    P3_Fail --> Debug1
    Debug1 --> Debug2
    Debug2 --> Debug3
    Debug3 --> Debug4
    
    Debug4 --> ChromaFix1
    ChromaFix1 --> ChromaFix2
    ChromaFix2 --> ChromaFix3
    ChromaFix3 --> ChromaFix4
    
    ChromaFix4 --> RetryChroma[🔄 Retry Chroma Setup]
    
    classDef fail fill:#ffebee,stroke:#c62828,stroke-width:2px
    classDef debug fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef fix fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef retry fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    
    class P3_Fail fail
    class Debug1,Debug2,Debug3,Debug4 debug
    class ChromaFix1,ChromaFix2,ChromaFix3,ChromaFix4 fix
    class RetryChroma retry
```

---

## 📝 **CHECKLIST HOÀN THÀNH FR-02.1**

### **Phase 1: Environment Setup** ✅
- [ ] Development environment configured
- [ ] Docker and Docker Compose installed
- [ ] Project structure created
- [ ] Python virtual environment setup
- [ ] Required packages installed

### **Phase 2: PostgreSQL Setup** ✅
- [ ] PostgreSQL container deployed and running
- [ ] Database schema created successfully
- [ ] Initial users and data inserted
- [ ] Connection pooling configured
- [ ] All PostgreSQL tests passing

### **Phase 3: Vector Database Setup** ✅
- [ ] Chroma container deployed and running
- [ ] Vector collections created
- [ ] Embedding configuration tested
- [ ] Vector operations working
- [ ] All Chroma tests passing

### **Phase 4: Integration & Testing** ✅
- [ ] Database integration layer implemented
- [ ] Redis cache configured
- [ ] Monitoring setup completed
- [ ] Integration tests passing
- [ ] Performance benchmarks met

### **Phase 5: Production Preparation** ✅
- [ ] Security hardening completed
- [ ] Backup strategy implemented
- [ ] Documentation completed
- [ ] Final acceptance testing passed

---

## 📊 **KẾT QUẢ MONG ĐỢI**

Sau khi hoàn thành FR-02.1, hệ thống sẽ có:

1. **PostgreSQL Database** hoạt động ổn định với:
   - Schema đầy đủ cho metadata và user management
   - Connection pooling tối ưu
   - Backup tự động

2. **Vector Database (Chroma)** hoạt động hiệu quả với:
   - Khả năng lưu trữ ≥100,000 document chunks
   - Tìm kiếm ngữ nghĩa với độ chính xác cao
   - API ổn định cho RAG engine

3. **Integration Layer** kết nối mượt mà:
   - Đồng bộ data giữa PostgreSQL và Chroma
   - Cache layer với Redis
   - Monitoring và alerting

4. **Performance Requirements** đáp ứng:
   - Search response time < 60 giây
   - Support 100 concurrent users
   - 99.5% uptime

**🎯 Deliverable chính: Hệ thống CSDL kép hoàn chỉnh, sẵn sàng tích hợp với RAG Core Engine (FR-04).**
---
