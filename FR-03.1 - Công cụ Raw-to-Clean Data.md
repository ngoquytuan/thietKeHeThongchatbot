Nếu đã đủ thông tin hãy giúp tôi viết tài liệu thiết kế module dưới đây.
Tài liệu này cung cấp roadmap đầy đủ để implement FR-03.1 mà không cần code cụ thể nhưng 
có các bước làm cụ thể như chuẩn bị môi trường PC thế nào, phần mềm gì?
 để Team kỹ thuật có thể follow step-by-step guide này để thiết kế tối ưu cho hệ thống chatbot
 và có mermaidchart luồng công việc, điểm quay đầu khi bị fail.

3.3 Module Xử lý Dữ liệu (FR-03)
FR-03.1 - Công cụ Raw-to-Clean Data

Web form để nhập metadata cho tài liệu mới
Template chuẩn cho các loại tài liệu khác nhau
Validation dữ liệu đầu vào bắt buộc
Export tài liệu đã xử lý theo format chuẩn
---
# TÀI LIỆU THIẾT KẾ MODULE FR-03.1
## CÔNG CỤ RAW-TO-CLEAN DATA PROCESSING

---

**Module ID:** FR-03.1  
**Tên Module:** Raw-to-Clean Data Processing Tool  
**Phiên bản:** 1.0  
**Ngày:** 31/08/2025  
**Trạng thái:** Design Document  

---

## 1. TỔNG QUAN MODULE

### 1.1 Mục đích
Module FR-03.1 cung cấp công cụ web-based để chuyển đổi tài liệu thô (raw documents) thành dữ liệu sạch (clean data) có cấu trúc, metadata đầy đủ, sẵn sàng cho quá trình embedding và indexing trong hệ thống RAG.

### 1.2 Phạm vi chức năng
- **Web Form Interface**: Giao diện nhập metadata cho tài liệu mới
- **Template System**: Template chuẩn cho các loại tài liệu khác nhau  
- **Validation Engine**: Kiểm tra dữ liệu đầu vào bắt buộc
- **Export Functionality**: Xuất tài liệu đã xử lý theo format chuẩn

### 1.3 Đầu vào và Đầu ra

| **INPUT** | **OUTPUT** |
|-----------|------------|
| Raw documents (PDF, DOCX, TXT) | Clean structured documents |
| Manual metadata input | Standardized metadata JSON |
| Document templates | Validated document packages |
| Validation rules | Export-ready files |

---

## 2. KIẾN TRÚC MODULE

### 2.1 Component Architecture

```mermaid
graph TB
    subgraph "🌐 FRONTEND LAYER"
        WebUI[📱 Web Form Interface<br/>React + TypeScript]
        FileUpload[📁 File Upload Component<br/>Drag & Drop]
        MetadataForm[📝 Metadata Form<br/>Dynamic Forms]
        PreviewPane[👁️ Document Preview<br/>PDF/Text Viewer]
    end
    
    subgraph "⚙️ BACKEND API LAYER"
        APIGateway[🚪 API Gateway<br/>Express.js/FastAPI]
        
        subgraph "Core Services"
            UploadService[📤 Upload Service<br/>File Handling]
            ValidationService[✅ Validation Service<br/>Rules Engine]
            TemplateService[📋 Template Service<br/>Template Manager]
            ExportService[📦 Export Service<br/>File Generation]
        end
        
        subgraph "Processing Services"
            DocParser[📖 Document Parser<br/>PDF/DOCX/TXT]
            MetadataExtractor[🏷️ Metadata Extractor<br/>Auto-extraction]
            ContentCleaner[🧹 Content Cleaner<br/>Text Processing]
        end
    end
    
    subgraph "🗄️ STORAGE LAYER"
        TempStorage[(🔄 Temp Storage<br/>Processing Files)]
        TemplateStore[(📋 Template Store<br/>Document Templates)]
        ValidationRules[(✅ Validation Rules<br/>Rule Definitions)]
        ProcessedStorage[(✨ Processed Storage<br/>Clean Documents)]
    end
    
    %% Frontend connections
    WebUI --> FileUpload
    WebUI --> MetadataForm
    WebUI --> PreviewPane
    
    %% API connections
    FileUpload --> UploadService
    MetadataForm --> ValidationService
    PreviewPane --> TemplateService
    MetadataForm --> ExportService
    
    %% Service connections
    UploadService --> DocParser
    DocParser --> MetadataExtractor
    MetadataExtractor --> ContentCleaner
    ContentCleaner --> ValidationService
    ValidationService --> ExportService
    
    %% Storage connections
    UploadService --> TempStorage
    TemplateService --> TemplateStore
    ValidationService --> ValidationRules
    ExportService --> ProcessedStorage
    
    %% Styling
    classDef frontend fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef api fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef services fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef processing fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef storage fill:#e0f2f1,stroke:#00796b,stroke-width:2px
    
    class WebUI,FileUpload,MetadataForm,PreviewPane frontend
    class APIGateway api
    class UploadService,ValidationService,TemplateService,ExportService services
    class DocParser,MetadataExtractor,ContentCleaner processing
    class TempStorage,TemplateStore,ValidationRules,ProcessedStorage storage
```

### 2.2 Data Flow Architecture

```mermaid
graph LR
    subgraph "📥 INPUT STAGE"
        RawDoc[📄 Raw Document<br/>PDF/DOCX/TXT]
        UserInput[👤 User Metadata<br/>Manual Input]
    end
    
    subgraph "🔄 PROCESSING STAGE"
        Parse[📖 Document Parsing<br/>Extract Text + Structure]
        AutoMeta[🤖 Auto Metadata<br/>Extract from Content]
        Merge[🔄 Merge Metadata<br/>Auto + Manual]
        Validate[✅ Validation<br/>Rules Check]
        Clean[🧹 Content Cleaning<br/>Format + Normalize]
    end
    
    subgraph "📤 OUTPUT STAGE"
        Structure[📊 Structured Document<br/>JSON Format]
        Export[📦 Export Package<br/>Multiple Formats]
        Store[💾 Storage<br/>Ready for Ingestion]
    end
    
    %% Flow
    RawDoc --> Parse
    Parse --> AutoMeta
    UserInput --> Merge
    AutoMeta --> Merge
    Merge --> Validate
    Validate --> Clean
    Clean --> Structure
    Structure --> Export
    Export --> Store
    
    %% Error flows (dotted red)
    Validate -.->|Validation Failed| UserInput
    Parse -.->|Parse Error| RawDoc
    
    %% Styling
    classDef input fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef processing fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef output fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef error fill:#ffebee,stroke:#c62828,stroke-width:2px,stroke-dasharray: 5 5
    
    class RawDoc,UserInput input
    class Parse,AutoMeta,Merge,Validate,Clean processing
    class Structure,Export,Store output
```

---

## 3. CÀI ĐẶT MÔI TRƯỜNG PHÁT TRIỂN

### 3.1 Yêu cầu Hardware

| Component | Minimum | Recommended | Ghi chú |
|-----------|---------|-------------|---------|
| **CPU** | Intel i5 4 cores | Intel i7 8 cores | Cho document processing |
| **RAM** | 8GB | 16GB | PDF parsing memory intensive |
| **Storage** | 100GB SSD | 500GB SSD | Temporary file storage |
| **Network** | 100Mbps | 1Gbps | File upload performance |

### 3.2 Software Dependencies

#### 3.2.1 Development Environment
```bash
# Node.js Environment
Node.js: v18.17.0+ (LTS)
npm: v9.0.0+
yarn: v1.22.0+ (recommended)

# Python Environment (for document processing)
Python: v3.9+
pip: v23.0+
virtualenv: v20.0+

# Database
PostgreSQL: v14+
Redis: v7.0+

# Development Tools
Git: v2.40+
Docker: v24.0+
Docker Compose: v2.20+
```

#### 3.2.2 Frontend Stack
```json
{
  "dependencies": {
    "react": "^18.2.0",
    "typescript": "^5.0.0",
    "next.js": "^13.4.0",
    "tailwindcss": "^3.3.0",
    "react-hook-form": "^7.45.0",
    "react-dropzone": "^14.2.0",
    "react-pdf": "^7.3.0",
    "axios": "^1.4.0",
    "zod": "^3.22.0"
  },
  "devDependencies": {
    "@types/react": "^18.2.0",
    "eslint": "^8.44.0",
    "jest": "^29.6.0",
    "@testing-library/react": "^13.4.0"
  }
}
```

#### 3.2.3 Backend Stack
```python
# requirements.txt
fastapi==0.101.0
uvicorn==0.23.0
pydantic==2.1.0
sqlalchemy==2.0.19
alembic==1.11.0
redis==4.6.0
celery==5.3.0
pypdf2==3.0.1
python-docx==0.8.11
python-multipart==0.0.6
jinja2==3.1.2
aiofiles==23.1.0
```

### 3.3 Development Tools Setup

#### 3.3.1 IDE Configuration
```bash
# VS Code Extensions (recommended)
- ES7+ React/Redux/React-Native snippets
- TypeScript Importer
- Tailwind CSS IntelliSense
- Python extension pack
- Docker extension
- GitLens
- Prettier - Code formatter
- ESLint
```

#### 3.3.2 Git Configuration
```bash
# Git hooks setup
npm install husky --save-dev
npx husky add .husky/pre-commit "npm run lint && npm run test"
npx husky add .husky/commit-msg "npx commitlint --edit $1"

# Conventional commits
npm install @commitlint/config-conventional --save-dev
```

---

## 4. STEP-BY-STEP IMPLEMENTATION GUIDE

### 4.1 Phase 1: Project Setup & Environment (Week 1)

#### Step 1.1: Environment Initialization
```bash
# 1. Create project structure
mkdir raw-to-clean-processor
cd raw-to-clean-processor
mkdir {frontend,backend,docs,docker,scripts}

# 2. Initialize Git repository
git init
git remote add origin <repository-url>

# 3. Setup Docker development environment
docker-compose up -d postgres redis
```

#### Step 1.2: Frontend Setup
```bash
# Navigate to frontend directory
cd frontend

# Initialize Next.js project with TypeScript
npx create-next-app@latest . --typescript --tailwind --eslint --app

# Install additional dependencies
npm install react-hook-form zod @hookform/resolvers
npm install react-dropzone react-pdf
npm install axios @tanstack/react-query
npm install lucide-react @radix-ui/react-dialog

# Setup development server
npm run dev # Should run on http://localhost:3000
```

#### Step 1.3: Backend Setup
```bash
# Navigate to backend directory
cd ../backend

# Create Python virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Setup FastAPI project structure
mkdir {app,tests,migrations,config}
touch app/{__init__.py,main.py,models.py,schemas.py,crud.py,database.py}

# Initialize database
alembic init alembic
alembic revision --autogenerate -m "Initial migration"
alembic upgrade head

# Start development server
uvicorn app.main:app --reload # Should run on http://localhost:8000
```

### 4.2 Phase 2: Core Components Development (Week 2-3)

#### Step 2.1: Database Schema Design
```sql
-- Document Templates Table
CREATE TABLE document_templates (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name VARCHAR(100) NOT NULL,
    document_type VARCHAR(50) NOT NULL,
    fields JSONB NOT NULL,
    validation_rules JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Document Processing Sessions Table  
CREATE TABLE processing_sessions (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id VARCHAR(100),
    original_filename VARCHAR(255) NOT NULL,
    file_path VARCHAR(500),
    file_size INTEGER,
    mime_type VARCHAR(100),
    status VARCHAR(20) DEFAULT 'pending',
    template_id UUID REFERENCES document_templates(id),
    metadata JSONB,
    processed_content TEXT,
    error_message TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Validation Rules Table
CREATE TABLE validation_rules (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    rule_name VARCHAR(100) NOT NULL,
    field_name VARCHAR(100) NOT NULL,
    rule_type VARCHAR(50) NOT NULL, -- required, regex, length, custom
    rule_config JSONB NOT NULL,
    error_message VARCHAR(255),
    is_active BOOLEAN DEFAULT true,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

#### Step 2.2: Document Templates Configuration
```json
// Template: Technical Document
{
  "name": "Technical Document",
  "document_type": "technical_guide",
  "fields": {
    "title": {
      "type": "string",
      "required": true,
      "max_length": 200
    },
    "department_owner": {
      "type": "select",
      "required": true,
      "options": ["rd", "production", "qa", "sales", "admin"]
    },
    "author": {
      "type": "string", 
      "required": true,
      "max_length": 100
    },
    "version": {
      "type": "string",
      "required": true,
      "pattern": "^v\\d+\\.\\d+\\.\\d+$"
    },
    "access_level": {
      "type": "select",
      "required": true,
      "options": ["public", "employee_only", "manager_only", "director_only"]
    },
    "tags": {
      "type": "array",
      "items": "string",
      "max_items": 10
    },
    "summary": {
      "type": "text",
      "required": true,
      "max_length": 500
    },
    "effective_date": {
      "type": "date",
      "required": false
    },
    "expiry_date": {
      "type": "date", 
      "required": false
    }
  },
  "validation_rules": {
    "title": ["required", "max_length:200"],
    "version": ["required", "regex:^v\\d+\\.\\d+\\.\\d+$"],
    "access_level": ["required", "in:public,employee_only,manager_only,director_only"]
  }
}
```

#### Step 2.3: File Upload Component
```typescript
// components/FileUpload.tsx
import React, { useCallback, useState } from 'react';
import { useDropzone } from 'react-dropzone';
import { Upload, File, X } from 'lucide-react';

interface FileUploadProps {
  onFileSelect: (file: File) => void;
  acceptedFileTypes: string[];
  maxFileSize: number;
}

export const FileUpload: React.FC<FileUploadProps> = ({
  onFileSelect,
  acceptedFileTypes,
  maxFileSize
}) => {
  const [uploadedFile, setUploadedFile] = useState<File | null>(null);
  const [error, setError] = useState<string>('');

  const onDrop = useCallback((acceptedFiles: File[], rejectedFiles: any[]) => {
    if (rejectedFiles.length > 0) {
      setError(rejectedFiles[0].errors[0].message);
      return;
    }

    const file = acceptedFiles[0];
    setUploadedFile(file);
    setError('');
    onFileSelect(file);
  }, [onFileSelect]);

  const { getRootProps, getInputProps, isDragActive } = useDropzone({
    onDrop,
    accept: {
      'application/pdf': ['.pdf'],
      'application/vnd.openxmlformats-officedocument.wordprocessingml.document': ['.docx'],
      'text/plain': ['.txt']
    },
    maxSize: maxFileSize,
    multiple: false
  });

  return (
    <div className="w-full">
      <div
        {...getRootProps()}
        className={`
          border-2 border-dashed rounded-lg p-8 text-center cursor-pointer
          transition-colors duration-200
          ${isDragActive 
            ? 'border-blue-500 bg-blue-50' 
            : 'border-gray-300 hover:border-gray-400'
          }
        `}
      >
        <input {...getInputProps()} />
        <Upload className="mx-auto mb-4 h-12 w-12 text-gray-400" />
        <p className="text-lg font-medium text-gray-700">
          {isDragActive 
            ? 'Thả file vào đây...' 
            : 'Kéo thả file hoặc click để chọn'
          }
        </p>
        <p className="text-sm text-gray-500 mt-2">
          Hỗ trợ: PDF, DOCX, TXT (tối đa {Math.round(maxFileSize / 1024 / 1024)}MB)
        </p>
      </div>

      {uploadedFile && (
        <div className="mt-4 p-4 bg-green-50 border border-green-200 rounded-lg">
          <div className="flex items-center justify-between">
            <div className="flex items-center">
              <File className="h-5 w-5 text-green-600 mr-2" />
              <span className="text-green-800">{uploadedFile.name}</span>
              <span className="text-green-600 ml-2 text-sm">
                ({Math.round(uploadedFile.size / 1024)}KB)
              </span>
            </div>
            <button
              onClick={() => setUploadedFile(null)}
              className="text-red-500 hover:text-red-700"
            >
              <X className="h-5 w-5" />
            </button>
          </div>
        </div>
      )}

      {error && (
        <div className="mt-4 p-4 bg-red-50 border border-red-200 rounded-lg">
          <p className="text-red-800">{error}</p>
        </div>
      )}
    </div>
  );
};
```

### 4.3 Phase 3: Processing Logic Implementation (Week 3-4)

#### Step 3.1: Document Parser Service
```python
# app/services/document_parser.py
from abc import ABC, abstractmethod
import PyPDF2
import docx
from typing import Dict, Any, Optional

class DocumentParser(ABC):
    @abstractmethod
    def parse(self, file_path: str) -> Dict[str, Any]:
        pass

class PDFParser(DocumentParser):
    def parse(self, file_path: str) -> Dict[str, Any]:
        try:
            with open(file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                
                # Extract metadata
                metadata = pdf_reader.metadata or {}
                
                # Extract text content
                text_content = ""
                for page in pdf_reader.pages:
                    text_content += page.extract_text() + "\n"
                
                return {
                    "content": text_content.strip(),
                    "page_count": len(pdf_reader.pages),
                    "metadata": {
                        "title": metadata.get('/Title', ''),
                        "author": metadata.get('/Author', ''),
                        "creator": metadata.get('/Creator', ''),
                        "creation_date": str(metadata.get('/CreationDate', '')),
                        "modification_date": str(metadata.get('/ModDate', ''))
                    },
                    "format": "pdf"
                }
        except Exception as e:
            raise Exception(f"Error parsing PDF: {str(e)}")

class DOCXParser(DocumentParser):
    def parse(self, file_path: str) -> Dict[str, Any]:
        try:
            doc = docx.Document(file_path)
            
            # Extract text content
            text_content = ""
            for paragraph in doc.paragraphs:
                text_content += paragraph.text + "\n"
            
            # Extract metadata
            properties = doc.core_properties
            
            return {
                "content": text_content.strip(),
                "paragraph_count": len(doc.paragraphs),
                "metadata": {
                    "title": properties.title or '',
                    "author": properties.author or '',
                    "created": str(properties.created) if properties.created else '',
                    "modified": str(properties.modified) if properties.modified else '',
                    "subject": properties.subject or '',
                    "category": properties.category or ''
                },
                "format": "docx"
            }
        except Exception as e:
            raise Exception(f"Error parsing DOCX: {str(e)}")

class TXTParser(DocumentParser):
    def parse(self, file_path: str) -> Dict[str, Any]:
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                content = file.read()
            
            return {
                "content": content.strip(),
                "line_count": len(content.split('\n')),
                "metadata": {
                    "encoding": "utf-8"
                },
                "format": "txt"
            }
        except UnicodeDecodeError:
            # Try different encodings
            encodings = ['latin-1', 'cp1252', 'iso-8859-1']
            for encoding in encodings:
                try:
                    with open(file_path, 'r', encoding=encoding) as file:
                        content = file.read()
                    return {
                        "content": content.strip(),
                        "line_count": len(content.split('\n')),
                        "metadata": {
                            "encoding": encoding
                        },
                        "format": "txt"
                    }
                except UnicodeDecodeError:
                    continue
            raise Exception("Unable to decode text file with any common encoding")
        except Exception as e:
            raise Exception(f"Error parsing TXT: {str(e)}")

class ParserFactory:
    parsers = {
        'pdf': PDFParser,
        'docx': DOCXParser,
        'txt': TXTParser
    }
    
    @classmethod
    def get_parser(cls, file_extension: str) -> DocumentParser:
        parser_class = cls.parsers.get(file_extension.lower())
        if not parser_class:
            raise ValueError(f"Unsupported file format: {file_extension}")
        return parser_class()
```

#### Step 3.2: Metadata Extraction Service
```python
# app/services/metadata_extractor.py
import re
from typing import Dict, Any, List, Optional
from datetime import datetime

class MetadataExtractor:
    def __init__(self):
        self.patterns = {
            'version': [
                r'version\s*:?\s*v?(\d+\.\d+\.\d+)',
                r'phiên\s*bản\s*:?\s*v?(\d+\.\d+\.\d+)',
                r'v(\d+\.\d+\.\d+)'
            ],
            'date': [
                r'ngày\s*:?\s*(\d{1,2}[\/\-]\d{1,2}[\/\-]\d{2,4})',
                r'date\s*:?\s*(\d{1,2}[\/\-]\d{1,2}[\/\-]\d{2,4})',
                r'(\d{1,2}[\/\-]\d{1,2}[\/\-]\d{2,4})'
            ],
            'email': [
                r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})'
            ],
            'phone': [
                r'(\+?84|0)([1-9][0-9]{8,9})',
                r'(\d{3}[-.\s]?\d{3}[-.\s]?\d{4})'
            ]
        }
    
    def extract_from_content(self, content: str) -> Dict[str, Any]:
        """Extract metadata from document content using regex patterns"""
        extracted = {}
        
        # Extract version information
        versions = self._extract_pattern(content, 'version')
        if versions:
            extracted['version'] = versions[0]
        
        # Extract dates
        dates = self._extract_pattern(content, 'date')
        if dates:
            extracted['dates_found'] = dates[:3]  # Limit to 3 dates
        
        # Extract email addresses
        emails = self._extract_pattern(content, 'email')
        if emails:
            extracted['emails'] = emails[:5]  # Limit to 5 emails
        
        # Extract phone numbers
        phones = self._extract_pattern(content, 'phone')
        if phones:
            extracted['phones'] = phones[:3]  # Limit to 3 phones
        
        # Extract document structure info
        extracted['structure'] = self._analyze_structure(content)
        
        return extracted
    
    def _extract_pattern(self, content: str, pattern_type: str) -> List[str]:
        """Extract matches for a specific pattern type"""
        matches = []
        patterns = self.patterns.get(pattern_type, [])
        
        for pattern in patterns:
            found = re.findall(pattern, content, re.IGNORECASE | re.MULTILINE)
            matches.extend(found)
        
        return list(set(matches))  # Remove duplicates
    
    def _analyze_structure(self, content: str) -> Dict[str, Any]:
        """Analyze document structure"""
        lines = content.split('\n')
        
        return {
            'total_lines': len(lines),
            'non_empty_lines': len([line for line in lines if line.strip()]),
            'word_count': len(content.split()),
            'character_count': len(content),
            'has_headers': self._detect_headers(content),
            'has_tables': self._detect_tables(content),
            'has_lists': self._detect_lists(content)
        }
    
    def _detect_headers(self, content: str) -> bool:
        """Detect if content has headers"""
        header_patterns = [
            r'^#{1,6}\s+.+$',  # Markdown headers
            r'^[A-Z\s]{2,}$',  # All caps headers
            r'^\d+\.\s+.+$'    # Numbered headers
        ]
        
        lines = content.split('\n')
        for line in lines[:20]:  # Check first 20 lines
            for pattern in header_patterns:
                if re.match(pattern, line.strip()):
                    return True
        return False
    
    def _detect_tables(self, content: str) -> bool:
        """Detect if content has table-like structures"""
        table_indicators = ['|', '\t', '  +  ']
        return any(indicator in content for indicator in table_indicators)
    
    def _detect_lists(self, content: str) -> bool:
        """Detect if content has list structures"""
        list_patterns = [
            r'^\s*[-\*\+]\s+',  # Bullet lists
            r'^\s*\d+\.\s+',    # Numbered lists
            r'^\s*[a-zA-Z]\)\s+' # Lettered lists
        ]
        
        lines = content.split('\n')
        for line in lines:
            for pattern in list_patterns:
                if re.match(pattern, line):
                    return True
        return False

    def suggest_document_type(self, content: str, filename: str) -> str:
        """Suggest document type based on content analysis"""
        content_lower = content.lower()
        filename_lower = filename.lower()
        
        # Policy documents
        if any(keyword in content_lower for keyword in 
               ['chính sách', 'policy', 'quy định', 'regulation']):
            return 'policy'
        
        # Technical documents
        elif any(keyword in content_lower for keyword in 
                ['kỹ thuật', 'technical', 'hướng dẫn', 'guide', 'manual']):
            return 'technical_guide'
        
        # Procedures
        elif any(keyword in content_lower for keyword in 
                ['quy trình', 'procedure', 'process', 'workflow']):
            return 'procedure'
        
        # Reports
        elif any(keyword in content_lower for keyword in 
                ['báo cáo', 'report', 'analysis', 'phân tích']):
            return 'report'
        
        # Default
        else:
            return 'general'
```

### 4.4 Phase 4: Validation & Export System (Week 4-5)

#### Step 4.1: Validation Engine
```python
# app/services/validation_service.py
from typing import Dict, Any, List, Optional
from pydantic import BaseModel, ValidationError
import re
from datetime import datetime

class ValidationResult(BaseModel):
    is_valid: bool
    errors: List[str] = []
    warnings: List[str] = []

class FieldValidator:
    def __init__(self):
        self.validators = {
            'required': self._validate_required,
            'max_length': self._validate_max_length,
            'min_length': self._validate_min_length,
            'regex': self._validate_regex,
            'email': self._validate_email,
            'date': self._validate_date,
            'in': self._validate_in_list,
            'numeric': self._validate_numeric,
            'url': self._validate_url
        }
    
    def validate_field(self, 
                      field_name: str, 
                      value: Any, 
                      rules: List[str]) -> ValidationResult:
        """Validate a single field against multiple rules"""
        result = ValidationResult(is_valid=True)
        
        for rule in rules:
            rule_result = self._apply_rule(field_name, value, rule)
            if not rule_result.is_valid:
                result.is_valid = False
                result.errors.extend(rule_result.errors)
            result.warnings.extend(rule_result.warnings)
        
        return result
    
    def _apply_rule(self, field_name: str, value: Any, rule: str) -> ValidationResult:
        """Apply a single validation rule"""
        if ':' in rule:
            rule_name, rule_param = rule.split(':', 1)
        else:
            rule_name, rule_param = rule, None
        
        validator = self.validators.get(rule_name)
        if not validator:
            return ValidationResult(
                is_valid=False, 
                errors=[f"Unknown validation rule: {rule_name}"]
            )
        
        return validator(field_name, value, rule_param)
    
    def _validate_required(self, field_name: str, value: Any, param: str) -> ValidationResult:
        """Validate required field"""
        if value is None or (isinstance(value, str) and not value.strip()):
            return ValidationResult(
                is_valid=False,
                errors=[f"{field_name} là trường bắt buộc"]
            )
        return ValidationResult(is_valid=True)
    
    def _validate_max_length(self, field_name: str, value: Any, param: str) -> ValidationResult:
        """Validate maximum length"""
        if value is None:
            return ValidationResult(is_valid=True)
        
        max_len = int(param)
        if len(str(value)) > max_len:
            return ValidationResult(
                is_valid=False,
                errors=[f"{field_name} không được vượt quá {max_len} ký tự"]
            )
        return ValidationResult(is_valid=True)
    
    def _validate_min_length(self, field_name: str, value: Any, param: str) -> ValidationResult:
        """Validate minimum length"""
        if value is None:
            return ValidationResult(is_valid=True)
        
        min_len = int(param)
        if len(str(value)) < min_len:
            return ValidationResult(
                is_valid=False,
                errors=[f"{field_name} phải có ít nhất {min_len} ký tự"]
            )
        return ValidationResult(is_valid=True)
    
    def _validate_regex(self, field_name: str, value: Any, param: str) -> ValidationResult:
        """Validate regex pattern"""
        if value is None:
            return ValidationResult(is_valid=True)
        
        if not re.match(param, str(value)):
            return ValidationResult(
                is_valid=False,
                errors=[f"{field_name} không đúng định dạng yêu cầu"]
            )
        return ValidationResult(is_valid=True)
    
    def _validate_email(self, field_name: str, value: Any, param: str) -> ValidationResult:
        """Validate email format"""
        if value is None:
            return ValidationResult(is_valid=True)
        
        email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
        if not re.match(email_pattern, str(value)):
            return ValidationResult(
                is_valid=False,
                errors=[f"{field_name} không đúng định dạng email"]
            )
        return ValidationResult(is_valid=True)
    
    def _validate_date(self, field_name: str, value: Any, param: str) -> ValidationResult:
        """Validate date format"""
        if value is None:
            return ValidationResult(is_valid=True)
        
        try:
            if isinstance(value, str):
                datetime.strptime(value, '%Y-%m-%d')
            return ValidationResult(is_valid=True)
        except ValueError:
            return ValidationResult(
                is_valid=False,
                errors=[f"{field_name} phải có định dạng YYYY-MM-DD"]
            )
    
    def _validate_in_list(self, field_name: str, value: Any, param: str) -> ValidationResult:
        """Validate value is in allowed list"""
        if value is None:
            return ValidationResult(is_valid=True)
        
        allowed_values = [v.strip() for v in param.split(',')]
        if str(value) not in allowed_values:
            return ValidationResult(
                is_valid=False,
                errors=[f"{field_name} phải là một trong các giá trị: {', '.join(allowed_values)}"]
            )
        return ValidationResult(is_valid=True)
    
    def _validate_numeric(self, field_name: str, value: Any, param: str) -> ValidationResult:
        """Validate numeric value"""
        if value is None:
            return ValidationResult(is_valid=True)
        
        try:
            float(value)
            return ValidationResult(is_valid=True)
        except (ValueError, TypeError):
            return ValidationResult(
                is_valid=False,
                errors=[f"{field_name} phải là số"]
            )
    
    def _validate_url(self, field_name: str, value: Any, param: str) -> ValidationResult:
        """Validate URL format"""
        if value is None:
            return ValidationResult(is_valid=True)
        
        url_pattern = r'^https?://(?:[-\w.])+(?:[:\d]+)?(?:/(?:[\w/_.])*(?:\?(?:[\w&=%.])*)?(?:#(?:\w)*)?)?$'
        if not re.match(url_pattern, str(value)):
            return ValidationResult(
                is_valid=False,
                errors=[f"{field_name} không đúng định dạng URL"]
            )
        return ValidationResult(is_valid=True)

class DocumentValidator:
    def __init__(self):
        self.field_validator = FieldValidator()
    
    def validate_document(self, 
                         metadata: Dict[str, Any], 
                         template: Dict[str, Any]) -> ValidationResult:
        """Validate entire document against template"""
        overall_result = ValidationResult(is_valid=True)
        
        # Get validation rules from template
        validation_rules = template.get('validation_rules', {})
        fields_config = template.get('fields', {})
        
        # Validate each field
        for field_name, field_config in fields_config.items():
            field_value = metadata.get(field_name)
            field_rules = validation_rules.get(field_name, [])
            
            # Add implicit rules from field config
            if field_config.get('required', False):
                field_rules.append('required')
            
            if 'max_length' in field_config:
                field_rules.append(f"max_length:{field_config['max_length']}")
            
            if 'pattern' in field_config:
                field_rules.append(f"regex:{field_config['pattern']}")
            
            if 'options' in field_config:
                field_rules.append(f"in:{','.join(field_config['options'])}")
            
            # Validate field
            field_result = self.field_validator.validate_field(
                field_name, field_value, field_rules
            )
            
            if not field_result.is_valid:
                overall_result.is_valid = False
                overall_result.errors.extend(field_result.errors)
            
            overall_result.warnings.extend(field_result.warnings)
        
        return overall_result
```

#### Step 4.2: Export Service
```python
# app/services/export_service.py
import json
import csv
import xml.etree.ElementTree as ET
from typing import Dict, Any, List
from datetime import datetime
import os
import zipfile

class ExportService:
    def __init__(self):
        self.export_formats = {
            'json': self._export_json,
            'xml': self._export_xml,
            'csv': self._export_csv,
            'package': self._export_package
        }
    
    def export_document(self, 
                       document_data: Dict[str, Any], 
                       export_format: str,
                       output_path: str) -> str:
        """Export document in specified format"""
        
        if export_format not in self.export_formats:
            raise ValueError(f"Unsupported export format: {export_format}")
        
        export_func = self.export_formats[export_format]
        return export_func(document_data, output_path)
    
    def _export_json(self, document_data: Dict[str, Any], output_path: str) -> str:
        """Export as JSON format"""
        export_data = self._prepare_export_data(document_data)
        
        filename = f"{output_path}/document_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)
        
        return filename
    
    def _export_xml(self, document_data: Dict[str, Any], output_path: str) -> str:
        """Export as XML format"""
        export_data = self._prepare_export_data(document_data)
        
        # Create XML structure
        root = ET.Element("document")
        
        # Add metadata
        metadata_elem = ET.SubElement(root, "metadata")
        for key, value in export_data['metadata'].items():
            elem = ET.SubElement(metadata_elem, key)
            elem.text = str(value) if value is not None else ""
        
        # Add content
        content_elem = ET.SubElement(root, "content")
        content_elem.text = export_data['content']
        
        # Add processing info
        processing_elem = ET.SubElement(root, "processing_info")
        for key, value in export_data['processing_info'].items():
            elem = ET.SubElement(processing_elem, key)
            elem.text = str(value) if value is not None else ""
        
        # Write to file
        filename = f"{output_path}/document_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xml"
        
        tree = ET.ElementTree(root)
        tree.write(filename, encoding='utf-8', xml_declaration=True)
        
        return filename
    
    def _export_csv(self, document_data: Dict[str, Any], output_path: str) -> str:
        """Export metadata as CSV format"""
        export_data = self._prepare_export_data(document_data)
        
        filename = f"{output_path}/document_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        
        # Flatten metadata for CSV
        flattened_data = []
        metadata = export_data['metadata']
        processing_info = export_data['processing_info']
        
        # Combine all data
        all_data = {**metadata, **processing_info}
        all_data['content_preview'] = export_data['content'][:500] + "..." if len(export_data['content']) > 500 else export_data['content']
        
        flattened_data.append(all_data)
        
        with open(filename, 'w', newline='', encoding='utf-8') as f:
            if flattened_data:
                writer = csv.DictWriter(f, fieldnames=flattened_data[0].keys())
                writer.writeheader()
                writer.writerows(flattened_data)
        
        return filename
    
    def _export_package(self, document_data: Dict[str, Any], output_path: str) -> str:
        """Export as complete package (ZIP with multiple formats)"""
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        package_name = f"document_package_{timestamp}"
        package_path = f"{output_path}/{package_name}"
        
        # Create temporary directory
        os.makedirs(package_path, exist_ok=True)
        
        # Export in multiple formats
        json_file = self._export_json(document_data, package_path)
        xml_file = self._export_xml(document_data, package_path)
        csv_file = self._export_csv(document_data, package_path)
        
        # Create README file
        readme_path = f"{package_path}/README.txt"
        with open(readme_path, 'w', encoding='utf-8') as f:
            f.write(f"""Document Processing Package
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

Files included:
- {os.path.basename(json_file)}: Complete document data in JSON format
- {os.path.basename(xml_file)}: Document data in XML format  
- {os.path.basename(csv_file)}: Metadata in CSV format
- README.txt: This file

Original filename: {document_data.get('original_filename', 'Unknown')}
Processing status: {document_data.get('status', 'Unknown')}
""")
        
        # Create ZIP package
        zip_filename = f"{output_path}/{package_name}.zip"
        with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for root, dirs, files in os.walk(package_path):
                for file in files:
                    file_path = os.path.join(root, file)
                    arcname = os.path.relpath(file_path, package_path)
                    zipf.write(file_path, arcname)
        
        # Clean up temporary directory
        import shutil
        shutil.rmtree(package_path)
        
        return zip_filename
    
    def _prepare_export_data(self, document_data: Dict[str, Any]) -> Dict[str, Any]:
        """Prepare data for export"""
        return {
            "metadata": document_data.get('metadata', {}),
            "content": document_data.get('processed_content', ''),
            "processing_info": {
                "id": document_data.get('id'),
                "original_filename": document_data.get('original_filename'),
                "file_size": document_data.get('file_size'),
                "mime_type": document_data.get('mime_type'),
                "status": document_data.get('status'),
                "template_id": document_data.get('template_id'),
                "processed_at": document_data.get('updated_at'),
                "created_at": document_data.get('created_at')
            }
        }

    def get_supported_formats(self) -> List[str]:
        """Get list of supported export formats"""
        return list(self.export_formats.keys())
```

---

## 5. WORKFLOW VÀ ERROR HANDLING

### 5.1 Complete Processing Workflow

```mermaid
graph TD
    Start([🚀 Start Processing]) --> UploadFile[📤 Upload File]
    
    UploadFile --> ValidateFile{📋 Validate File}
    ValidateFile -->|❌ Invalid| FileError[🚫 File Validation Error]
    ValidateFile -->|✅ Valid| ParseDoc[📖 Parse Document]
    
    ParseDoc --> ParseSuccess{✅ Parse Success?}
    ParseSuccess -->|❌ Failed| ParseError[🚫 Parse Error]
    ParseSuccess -->|✅ Success| ExtractMeta[🏷️ Extract Metadata]
    
    ExtractMeta --> SelectTemplate[📋 Select Template]
    SelectTemplate --> ShowForm[📝 Show Metadata Form]
    
    ShowForm --> UserInput[👤 User Input Metadata]
    UserInput --> ValidateMeta{✅ Validate Metadata}
    
    ValidateMeta -->|❌ Invalid| ValidationError[🚫 Validation Error]
    ValidateMeta -->|✅ Valid| CleanContent[🧹 Clean Content]
    
    CleanContent --> GenerateOutput[📊 Generate Structured Output]
    GenerateOutput --> SelectExport[📦 Select Export Format]
    
    SelectExport --> ExportFile[📤 Export File]
    ExportFile --> Success([✅ Process Complete])
    
    %% Error Handling & Retry Paths
    FileError --> RetryUpload{🔄 Retry Upload?}
    RetryUpload -->|Yes| UploadFile
    RetryUpload -->|No| Abort([❌ Process Aborted])
    
    ParseError --> RetryParse{🔄 Retry with Different Parser?}
    RetryParse -->|Yes| ParseDoc
    RetryParse -->|No| ManualInput[✍️ Manual Content Input]
    ManualInput --> SelectTemplate
    
    ValidationError --> FixErrors[🔧 Fix Validation Errors]
    FixErrors --> UserInput
    
    %% Additional Error Paths
    ExportFile --> ExportSuccess{✅ Export Success?}
    ExportSuccess -->|❌ Failed| ExportError[🚫 Export Error]
    ExportSuccess -->|✅ Success| Success
    
    ExportError --> RetryExport{🔄 Retry Export?}
    RetryExport -->|Yes| SelectExport
    RetryExport -->|No| PartialSuccess([⚠️ Partial Success])
    
    %% Rollback Points
    ValidationError -.->|Rollback| ShowForm
    ParseError -.->|Rollback| UploadFile
    ExportError -.->|Rollback| GenerateOutput
    
    %% Styling
    classDef startEnd fill:#e8f5e8,stroke:#388e3c,stroke-width:3px
    classDef process fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef decision fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef error fill:#ffebee,stroke:#c62828,stroke-width:2px
    classDef retry fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef rollback fill:#e0f2f1,stroke:#00796b,stroke-width:2px,stroke-dasharray: 5 5
    
    class Start,Success,Abort,PartialSuccess startEnd
    class UploadFile,ParseDoc,ExtractMeta,SelectTemplate,ShowForm,UserInput,CleanContent,GenerateOutput,SelectExport,ExportFile,ManualInput,FixErrors process
    class ValidateFile,ParseSuccess,ValidateMeta,RetryUpload,RetryParse,ExportSuccess,RetryExport decision
    class FileError,ParseError,ValidationError,ExportError error
    class RetryUpload,RetryParse,RetryExport retry
```

### 5.2 Error Handling Strategy

#### Step 5.1: Error Classification & Recovery
```python
# app/services/error_handler.py
from enum import Enum
from typing import Dict, Any, Optional, List
from dataclasses import dataclass
import logging

class ErrorType(Enum):
    FILE_VALIDATION = "file_validation"
    PARSE_ERROR = "parse_error"
    VALIDATION_ERROR = "validation_error"
    PROCESSING_ERROR = "processing_error"
    EXPORT_ERROR = "export_error"
    SYSTEM_ERROR = "system_error"

class ErrorSeverity(Enum):
    LOW = "low"          # Warning, process can continue
    MEDIUM = "medium"    # Error, but recoverable
    HIGH = "high"        # Critical error, process must stop
    CRITICAL = "critical" # System-level error

@dataclass
class ProcessingError:
    error_type: ErrorType
    severity: ErrorSeverity
    message: str
    details: Optional[Dict[str, Any]] = None
    recovery_suggestions: Optional[List[str]] = None
    rollback_point: Optional[str] = None

class ErrorHandler:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.error_handlers = {
            ErrorType.FILE_VALIDATION: self._handle_file_error,
            ErrorType.PARSE_ERROR: self._handle_parse_error,
            ErrorType.VALIDATION_ERROR: self._handle_validation_error,
            ErrorType.PROCESSING_ERROR: self._handle_processing_error,
            ErrorType.EXPORT_ERROR: self._handle_export_error,
            ErrorType.SYSTEM_ERROR: self._handle_system_error
        }
    
    def handle_error(self, error: ProcessingError) -> Dict[str, Any]:
        """Handle processing error and return recovery options"""
        
        # Log error
        self.logger.error(f"Processing error: {error.error_type.value} - {error.message}")
        if error.details:
            self.logger.error(f"Error details: {error.details}")
        
        # Get specific handler
        handler = self.error_handlers.get(error.error_type, self._handle_generic_error)
        
        return handler(error)
    
    def _handle_file_error(self, error: ProcessingError) -> Dict[str, Any]:
        """Handle file validation errors"""
        return {
            "error_type": error.error_type.value,
            "message": error.message,
            "severity": error.severity.value,
            "recovery_options": [
                "retry_upload",
                "check_file_format",
                "reduce_file_size",
                "convert_file_format"
            ],
            "user_actions": [
                "Kiểm tra lại định dạng file (PDF, DOCX, TXT)",
                "Đảm bảo kích thước file < 50MB",
                "Thử upload file khác",
                "Liên hệ support nếu vấn đề tiếp tục"
            ],
            "rollback_to": "file_upload"
        }
    
    def _handle_parse_error(self, error: ProcessingError) -> Dict[str, Any]:
        """Handle document parsing errors"""
        return {
            "error_type": error.error_type.value,
            "message": error.message,
            "severity": error.severity.value,
            "recovery_options": [
                "retry_with_different_parser",
                "manual_content_input",
                "file_conversion",
                "skip_auto_extraction"
            ],
            "user_actions": [
                "Thử chuyển đổi file sang format khác",
                "Nhập nội dung thủ công nếu cần",
                "Kiểm tra file có bị lỗi/corrupt không",
                "Sử dụng OCR cho file scan"
            ],
            "rollback_to": "document_upload"
        }
    
    def _handle_validation_error(self, error: ProcessingError) -> Dict[str, Any]:
        """Handle metadata validation errors"""
        return {
            "error_type": error.error_type.value,
            "message": error.message,
            "severity": error.severity.value,
            "recovery_options": [
                "fix_validation_errors",
                "use_different_template",
                "skip_optional_fields",
                "request_admin_review"
            ],
            "user_actions": [
                "Sửa các lỗi validation được highlight",
                "Kiểm tra format của các trường bắt buộc",
                "Chọn template phù hợp hơn",
                "Liên hệ admin nếu rule không hợp lý"
            ],
            "validation_errors": error.details.get("errors", []) if error.details else [],
            "rollback_to": "metadata_form"
        }
    
    def _handle_processing_error(self, error: ProcessingError) -> Dict[str, Any]:
        """Handle content processing errors"""
        return {
            "error_type": error.error_type.value,
            "message": error.message,
            "severity": error.severity.value,
            "recovery_options": [
                "retry_processing",
                "skip_problematic_sections",
                "use_alternative_processing",
                "manual_review"
            ],
            "user_actions": [
                "Thử lại quá trình xử lý",
                "Kiểm tra nội dung có ký tự đặc biệt không",
                "Simplify document structure",
                "Contact technical support"
            ],
            "rollback_to": "content_processing"
        }
    
    def _handle_export_error(self, error: ProcessingError) -> Dict[str, Any]:
        """Handle export errors"""
        return {
            "error_type": error.error_type.value,
            "message": error.message,
            "severity": error.severity.value,
            "recovery_options": [
                "retry_export",
                "change_export_format",
                "reduce_export_content",
                "save_partial_results"
            ],
            "user_actions": [
                "Thử export format khác (JSON, XML, CSV)",
                "Kiểm tra dung lượng ổ đĩa",
                "Giảm số lượng field export",
                "Lưu kết quả đã xử lý trước"
            ],
            "rollback_to": "export_selection"
        }
    
    def _handle_system_error(self, error: ProcessingError) -> Dict[str, Any]:
        """Handle system-level errors"""
        return {
            "error_type": error.error_type.value,
            "message": "Lỗi hệ thống. Vui lòng thử lại sau.",
            "severity": error.severity.value,
            "recovery_options": [
                "retry_after_delay",
                "contact_support",
                "check_system_status"
            ],
            "user_actions": [
                "Đợi 5-10 phút rồi thử lại",
                "Kiểm tra kết nối mạng",
                "Liên hệ IT support",
                "Báo cáo lỗi với screenshot"
            ],
            "rollback_to": "start"
        }
    
    def _handle_generic_error(self, error: ProcessingError) -> Dict[str, Any]:
        """Handle unknown errors"""
        return {
            "error_type": "unknown_error",
            "message": "Lỗi không xác định",
            "severity": "medium",
            "recovery_options": ["retry", "contact_support"],
            "user_actions": [
                "Thử lại từ bước trước",
                "Liên hệ technical support"
            ],
            "rollback_to": "previous_step"
        }
```

---

## 6. TESTING STRATEGY

### 6.1 Unit Testing Setup
```python
# tests/test_document_parser.py
import pytest
import tempfile
import os
from app.services.document_parser import PDFParser, DOCXParser, TXTParser, ParserFactory

class TestDocumentParser:
    
    def test_pdf_parser_success(self):
        """Test PDF parsing with valid file"""
        # Create test PDF file here or use mock
        parser = PDFParser()
        # Add actual test implementation
        pass
    
    def test_pdf_parser_invalid_file(self):
        """Test PDF parser with invalid file"""
        parser = PDFParser()
        with pytest.raises(Exception):
            parser.parse("nonexistent_file.pdf")
    
    def test_docx_parser_success(self):
        """Test DOCX parsing with valid file"""
        parser = DOCXParser()
        # Add test implementation
        pass
    
    def test_txt_parser_utf8(self):
        """Test TXT parser with UTF-8 file"""
        parser = TXTParser()
        
        # Create temp file
        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt', encoding='utf-8') as f:
            f.write("Test content with UTF-8: áêíôû")
            temp_path = f.name
        
        try:
            result = parser.parse(temp_path)
            assert result['format'] == 'txt'
            assert "Test content" in result['content']
            assert result['metadata']['encoding'] == 'utf-8'
        finally:
            os.unlink(temp_path)
    
    def test_parser_factory(self):
        """Test parser factory method"""
        pdf_parser = ParserFactory.get_parser('pdf')
        assert isinstance(pdf_parser, PDFParser)
        
        docx_parser = ParserFactory.get_parser('docx')
        assert isinstance(docx_parser, DOCXParser)
        
        txt_parser = ParserFactory.get_parser('txt')
        assert isinstance(txt_parser, TXTParser)
        
        with pytest.raises(ValueError):
            ParserFactory.get_parser('unsupported')

# tests/test_validation_service.py
import pytest
from app.services.validation_service import FieldValidator, DocumentValidator, ValidationResult

class TestValidationService:
    
    def test_required_field_validation(self):
        validator = FieldValidator()
        
        # Test valid required field
        result = validator.validate_field("title", "Test Title", ["required"])
        assert result.is_valid == True
        
        # Test missing required field
        result = validator.validate_field("title", "", ["required"])
        assert result.is_valid == False
        assert len(result.errors) > 0
        
        # Test None required field
        result = validator.validate_field("title", None, ["required"])
        assert result.is_valid == False
    
    def test_max_length_validation(self):
        validator = FieldValidator()
        
        # Test valid length
        result = validator.validate_field("title", "Short", ["max_length:10"])
        assert result.is_valid == True
        
        # Test invalid length
        result = validator.validate_field("title", "This is a very long title", ["max_length:10"])
        assert result.is_valid == False
    
    def test_email_validation(self):
        validator = FieldValidator()
        
        # Test valid email
        result = validator.validate_field("email", "test@example.com", ["email"])
        assert result.is_valid == True
        
        # Test invalid email
        result = validator.validate_field("email", "invalid-email", ["email"])
        assert result.is_valid == False
    
    def test_document_validation(self):
        validator = DocumentValidator()
        
        template = {
            "fields": {
                "title": {"required": True, "max_length": 100},
                "author": {"required": True},
                "version": {"pattern": "^v\\d+\\.\\d+\\.\\d+$"}
            },
            "validation_rules": {
                "title": ["required", "max_length:100"],
                "author": ["required"],
                "version": ["regex:^v\\d+\\.\\d+\\.\\d+$"]
            }
        }
        
        # Test valid document
        metadata = {
            "title": "Test Document",
            "author": "John Doe", 
            "version": "v1.0.0"
        }
        
        result = validator.validate_document(metadata, template)
        assert result.is_valid == True
        
        # Test invalid document
        invalid_metadata = {
            "title": "",  # Required field empty
            "author": "John Doe",
            "version": "1.0.0"  # Invalid version format
        }
        
        result = validator.validate_document(invalid_metadata, template)
        assert result.is_valid == False
        assert len(result.errors) >= 2
```

### 6.2 Integration Testing
```python
# tests/test_integration.py
import pytest
import tempfile
import json
from fastapi.testclient import TestClient
from app.main import app

client = TestClient(app)

class TestIntegrationWorkflow:
    
    def test_complete_processing_workflow(self):
        """Test complete document processing workflow"""
        
        # Step 1: Upload file
        with tempfile.NamedTemporaryFile(suffix='.txt') as tmp_file:
            tmp_file.write(b"Test document content")
            tmp_file.seek(0)
            
            response = client.post(
                "/api/upload",
                files={"file": ("test.txt", tmp_file, "text/plain")}
            )
            
            assert response.status_code == 200
            upload_result = response.json()
            session_id = upload_result["session_id"]
        
        # Step 2: Get available templates
        response = client.get("/api/templates")
        assert response.status_code == 200
        templates = response.json()
        template_id = templates[0]["id"]  # Use first template
        
        # Step 3: Submit metadata
        metadata = {
            "title": "Test Integration Document",
            "author": "Test Author",
            "department_owner": "rd",
            "access_level": "employee_only",
            "version": "v1.0.0",
            "tags": ["test", "integration"],
            "summary": "This is a test document for integration testing"
        }
        
        response = client.post(
            f"/api/sessions/{session_id}/metadata",
            json={
                "template_id": template_id,
                "metadata": metadata
            }
        )
        
        assert response.status_code == 200
        validation_result = response.json()
        assert validation_result["is_valid"] == True
        
        # Step 4: Process document
        response = client.post(f"/api/sessions/{session_id}/process")
        assert response.status_code == 200
        process_result = response.json()
        assert process_result["status"] == "completed"
        
        # Step 5: Export document
        response = client.post(
            f"/api/sessions/{session_id}/export",
            json={"format": "json"}
        )
        
        assert response.status_code == 200
        export_result = response.json()
        assert "download_url" in export_result
    
    def test_error_handling_workflow(self):
        """Test error handling and recovery"""
        
        # Test with invalid file type
        response = client.post(
            "/api/upload",
            files={"file": ("test.exe", b"invalid content", "application/x-executable")}
        )
        
        assert response.status_code == 400
        error_result = response.json()
        assert error_result["error_type"] == "file_validation"
        assert "recovery_options" in error_result
    
    def test_validation_error_recovery(self):
        """Test validation error handling"""
        
        # Upload valid file first
        with tempfile.NamedTemporaryFile(suffix='.txt') as tmp_file:
            tmp_file.write(b"Test content")
            tmp_file.seek(0)
            
            response = client.post(
                "/api/upload",
                files={"file": ("test.txt", tmp_file, "text/plain")}
            )
            session_id = response.json()["session_id"]
        
        # Submit invalid metadata
        invalid_metadata = {
            "title": "",  # Required field empty
            "author": "Test Author",
            "version": "invalid-version",  # Invalid format
            "access_level": "invalid_level"  # Invalid option
        }
        
        response = client.post(
            f"/api/sessions/{session_id}/metadata",
            json={
                "template_id": "template-uuid",
                "metadata": invalid_metadata
            }
        )
        
        assert response.status_code == 400
        validation_error = response.json()
        assert validation_error["error_type"] == "validation_error"
        assert len(validation_error["validation_errors"]) >= 3
        assert validation_error["rollback_to"] == "metadata_form"
```

---

## 7. DEPLOYMENT & PRODUCTION SETUP

### 7.1 Docker Configuration

#### Frontend Dockerfile
```dockerfile
# frontend/Dockerfile
FROM node:18-alpine as builder

WORKDIR /app
COPY package*.json ./
RUN npm ci --only=production

COPY . .
RUN npm run build

FROM nginx:alpine
COPY --from=builder /app/out /usr/share/nginx/html
COPY nginx.conf /etc/nginx/nginx.conf

EXPOSE 80
CMD ["nginx", "-g", "daemon off;"]
```

#### Backend Dockerfile  
```dockerfile
# backend/Dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Create non-root user
RUN useradd --create-home --shell /bin/bash app
RUN chown -R app:app /app
USER app

EXPOSE 8000

CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

#### Docker Compose Configuration
```yaml
# docker-compose.yml
version: '3.8'

services:
  frontend:
    build: ./frontend
    ports:
      - "3000:80"
    depends_on:
      - backend
    environment:
      - NEXT_PUBLIC_API_URL=http://localhost:8000
    networks:
      - app-network

  backend:
    build: ./backend
    ports:
      - "8000:8000"
    depends_on:
      - postgres
      - redis
    environment:
      - DATABASE_URL=postgresql://rawclean:password@postgres:5432/rawclean_db
      - REDIS_URL=redis://redis:6379
      - SECRET_KEY=your-secret-key-here
      - UPLOAD_PATH=/app/uploads
      - EXPORT_PATH=/app/exports
    volumes:
      - ./uploads:/app/uploads
      - ./exports:/app/exports
    networks:
      - app-network

  postgres:
    image: postgres:14
    environment:
      - POSTGRES_USER=rawclean
      - POSTGRES_PASSWORD=password
      - POSTGRES_DB=rawclean_db
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5432:5432"
    networks:
      - app-network

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - app-network

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./nginx/ssl:/etc/nginx/ssl
    depends_on:
      - frontend
      - backend
    networks:
      - app-network

volumes:
  postgres_data:
  redis_data:

networks:
  app-network:
    driver: bridge
```

### 7.2 Production Environment Configuration

#### Environment Variables
```bash
# .env.production
# Database
DATABASE_URL=postgresql://user:password@localhost:5432/production_db
REDIS_URL=redis://localhost:6379

# Security
SECRET_KEY=your-super-secret-key-for-production
JWT_SECRET_KEY=your-jwt-secret-key
ALLOWED_HOSTS=yourdomain.com,www.yourdomain.com

# File Storage
UPLOAD_PATH=/var/app/uploads
EXPORT_PATH=/var/app/exports
MAX_FILE_SIZE=52428800  # 50MB
ALLOWED_FILE_TYPES=pdf,docx,txt

# External Services
LLM_API_KEY=your-llm-api-key
EMBEDDING_API_KEY=your-embedding-api-key

# Monitoring
LOG_LEVEL=INFO
SENTRY_DSN=your-sentry-dsn

# Performance
WORKER_PROCESSES=4
MAX_CONCURRENT_UPLOADS=10
REDIS_MAX_CONNECTIONS=20
```

#### Nginx Production Configuration
```nginx
# nginx/nginx.conf
upstream frontend {
    server frontend:80;
}

upstream backend {
    server backend:8000;
}

server {
    listen 80;
    server_name yourdomain.com www.yourdomain.com;
    return 301 https://$server_name$request_uri;
}

server {
    listen 443 ssl http2;
    server_name yourdomain.com www.yourdomain.com;

    ssl_certificate /etc/nginx/ssl/cert.pem;
    ssl_certificate_key /etc/nginx/ssl/key.pem;
    
    # Security headers
    add_header X-Frame-Options DENY;
    add_header X-Content-Type-Options nosniff;
    add_header X-XSS-Protection "1; mode=block";
    add_header Strict-Transport-Security "max-age=31536000; includeSubDomains" always;

    # Frontend
    location / {
        proxy_pass http://frontend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }

    # Backend API
    location /api/ {
        proxy_pass http://backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # File upload settings
        client_max_body_size 50M;
        proxy_connect_timeout 60s;
        proxy_send_timeout 60s;
        proxy_read_timeout 60s;
    }

    # Static files
    location /static/ {
        alias /var/app/static/;
        expires 1y;
        add_header Cache-Control "public, immutable";
    }
}
```

---

## 8. MONITORING & MAINTENANCE

### 8.1 Health Checks & Monitoring
```python
# app/monitoring/health_check.py
from fastapi import APIRouter, Depends
from sqlalchemy.orm import Session
from app.database import get_db
import redis
import psutil
from datetime import datetime
from typing import Dict, Any

router = APIRouter()

@router.get("/health")
async def health_check():
    """Basic health check endpoint"""
    return {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "service": "raw-to-clean-processor"
    }

@router.get("/health/detailed")
async def detailed_health_check(db: Session = Depends(get_db)):
    """Detailed health check with dependencies"""
    
    health_status = {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "checks": {}
    }
    
    # Database check
    try:
        db.execute("SELECT 1")
        health_status["checks"]["database"] = {"status": "healthy"}
    except Exception as e:
        health_status["checks"]["database"] = {
            "status": "unhealthy",
            "error": str(e)
        }
        health_status["status"] = "unhealthy"
    
    # Redis check
    try:
        r = redis.Redis.from_url("redis://localhost:6379")
        r.ping()
        health_status["checks"]["redis"] = {"status": "healthy"}
    except Exception as e:
        health_status["checks"]["redis"] = {
            "status": "unhealthy", 
            "error": str(e)
        }
        health_status["status"] = "unhealthy"
    
    # System resources
    health_status["checks"]["system"] = {
        "cpu_percent": psutil.cpu_percent(),
        "memory_percent": psutil.virtual_memory().percent,
        "disk_percent": psutil.disk_usage('/').percent
    }
    
    return health_status

@router.get("/metrics")
async def get_metrics():
    """Application metrics endpoint"""
    
    # Add application-specific metrics here
    return {
        "processed_documents_total": 0,  # Implement counter
        "active_sessions": 0,            # Implement counter  
        "error_rate": 0.0,              # Implement error tracking
        "avg_processing_time": 0.0,     # Implement timing metrics
        "storage_usage": {
            "uploads_size_mb": 0,
            "exports_size_mb": 0
        }
    }
```

### 8.2 Logging Configuration
```python
# app/logging_config.py
import logging
import sys
from pathlib import Path

def setup_logging(log_level: str = "INFO"):
    """Configure application logging"""
    
    # Create logs directory
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)
    
    # Configure root logger
    logging.basicConfig(
        level=getattr(logging, log_level.upper()),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_dir / 'app.log'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    # Configure specific loggers
    logging.getLogger("uvicorn.error").setLevel(logging.INFO)
    logging.getLogger("uvicorn.access").setLevel(logging.INFO)
    logging.getLogger("sqlalchemy.engine").setLevel(logging.WARNING)
```

---

## 9. PERFORMANCE OPTIMIZATION

### 9.1 Caching Strategy
```python
# app/services/cache_service.py
import redis
import json
import hashlib
from typing import Any, Optional
from functools import wraps

class CacheService:
    def __init__(self, redis_url: str):
        self.redis_client = redis.from_url(redis_url)
        self.default_ttl = 3600  # 1 hour
    
    def get_cache_key(self, prefix: str, *args) -> str:
        """Generate cache key from arguments"""
        key_data = f"{prefix}:{':'.join(str(arg) for arg in args)}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    def get(self, key: str) -> Optional[Any]:
        """Get value from cache"""
        try:
            data = self.redis_client.get(key)
            return json.loads(data) if data else None
        except Exception:
            return None
    
    def set(self, key: str, value: Any, ttl: int = None) -> bool:
        """Set value in cache"""
        try:
            ttl = ttl or self.default_ttl
            serialized = json.dumps(value, default=str)
            return self.redis_client.setex(key, ttl, serialized)
        except Exception:
            return False
    
    def delete(self, key: str) -> bool:
        """Delete key from cache"""
        try:
            return bool(self.redis_client.delete(key))
        except Exception:
            return False

def cache_result(prefix: str, ttl: int = 3600):
    """Decorator to cache function results"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            cache_service = CacheService("redis://localhost:6379")
            
            # Generate cache key
            cache_key = cache_service.get_cache_key(
                prefix, 
                *args, 
                *[f"{k}:{v}" for k, v in sorted(kwargs.items())]
            )
            
            # Try to get from cache
            cached_result = cache_service.get(cache_key)
            if cached_result is not None:
                return cached_result
            
            # Execute function and cache result
            result = await func(*args, **kwargs)
            cache_service.set(cache_key, result, ttl)
            
            return result
        return wrapper
    return decorator
```

### 9.2 Async Processing
```python
# app/services/async_processor.py
import asyncio
from typing import List, Dict, Any
from concurrent.futures import ThreadPoolExecutor
import aiofiles

class AsyncDocumentProcessor:
    def __init__(self, max_workers: int = 4):
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
    
    async def process_multiple_documents(self, 
                                       file_paths: List[str]) -> List[Dict[str, Any]]:
        """Process multiple documents concurrently"""
        
        tasks = []
        for file_path in file_paths:
            task = asyncio.create_task(
                self.process_single_document(file_path)
            )
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Handle exceptions
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                processed_results.append({
                    "file_path": file_paths[i],
                    "status": "error",
                    "error": str(result)
                })
            else:
                processed_results.append(result)
        
        return processed_results
    
    async def process_single_document(self, file_path: str) -> Dict[str, Any]:
        """Process single document asynchronously"""
        
        loop = asyncio.get_event_loop()
        
        # Run CPU-intensive parsing in thread pool
        result = await loop.run_in_executor(
            self.executor,
            self._parse_document_sync,
            file_path
        )
        
        return result
    
    def _parse_document_sync(self, file_path: str) -> Dict[str, Any]:
        """Synchronous document parsing (runs in thread)"""
        # Implementation of parsing logic
        pass
    
    async def save_processed_document(self, 
                                    document_data: Dict[str, Any],
                                    output_path: str) -> str:
        """Save processed document asynchronously"""
        
        async with aiofiles.open(output_path, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(document_data, indent=2))
        
        return output_path
```

---

## 10. SECURITY CONSIDERATIONS

### 10.1 File Upload Security
```python
# app/security/file_validator.py
import magic
import hashlib
from pathlib import Path
from typing import List, Dict, Any

class FileSecurityValidator:
    def __init__(self):
        self.allowed_mime_types = {
            'application/pdf',
            'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
            'text/plain'
        }
        
        self.allowed_extensions = {'.pdf', '.docx', '.txt'}
        self.max_file_size = 50 * 1024 * 1024  # 50MB
        
        # Malicious patterns to check
        self.malicious_patterns = [
            b'<script',
            b'javascript:',
            b'<?php',
            b'<%',
            b'eval(',
            b'exec(',
            b'system(',
            b'shell_exec('
        ]
    
    def validate_file(self, file_path: str, original_filename: str) -> Dict[str, Any]:
        """Comprehensive file validation"""
        
        validation_result = {
            "is_valid": True,
            "errors": [],
            "warnings": [],
            "file_info": {}
        }
        
        file_path_obj = Path(file_path)
        
        # Check file exists
        if not file_path_obj.exists():
            validation_result["is_valid"] = False
            validation_result["errors"].append("File does not exist")
            return validation_result
        
        # Check file size
        file_size = file_path_obj.stat().st_size
        validation_result["file_info"]["size"] = file_size
        
        if file_size > self.max_file_size:
            validation_result["is_valid"] = False
            validation_result["errors"].append(f"File size exceeds {self.max_file_size} bytes")
        
        if file_size == 0:
            validation_result["is_valid"] = False
            validation_result["errors"].append("File is empty")
        
        # Check file extension
        file_extension = file_path_obj.suffix.lower()
        validation_result["file_info"]["extension"] = file_extension
        
        if file_extension not in self.allowed_extensions:
            validation_result["is_valid"] = False
            validation_result["errors"].append(f"File extension {file_extension} not allowed")
        
        # Check MIME type using python-magic
        try:
            mime_type = magic.from_file(file_path, mime=True)
            validation_result["file_info"]["mime_type"] = mime_type
            
            if mime_type not in self.allowed_mime_types:
                validation_result["is_valid"] = False
                validation_result["errors"].append(f"MIME type {mime_type} not allowed")
        except Exception as e:
            validation_result["warnings"].append(f"Could not determine MIME type: {e}")
        
        # Check for malicious content
        malicious_found = self._check_malicious_content(file_path)
        if malicious_found:
            validation_result["is_valid"] = False
            validation_result["errors"].append("Potentially malicious content detected")
        
        # Calculate file hash
        validation_result["file_info"]["sha256"] = self._calculate_file_hash(file_path)
        
        return validation_result
    
    def _check_malicious_content(self, file_path: str) -> bool:
        """Check for malicious patterns in file content"""
        try:
            with open(file_path, 'rb') as f:
                # Read first 1MB to check for malicious patterns
                content = f.read(1024 * 1024)
                
                for pattern in self.malicious_patterns:
                    if pattern in content.lower():
                        return True
            return False
        except Exception:
            return False
    
    def _calculate_file_hash(self, file_path: str) -> str:
        """Calculate SHA256 hash of file"""
        hash_sha256 = hashlib.sha256()
        
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_sha256.update(chunk)
        
        return hash_sha256.hexdigest()
```

---

## 11. FINAL CHECKLIST & GO-LIVE

### 11.1 Pre-Production Checklist

```markdown
## Development Environment Setup ✅
- [ ] Node.js 18+ installed
- [ ] Python 3.9+ installed  
- [ ] PostgreSQL 14+ running
- [ ] Redis 7+ running
- [ ] Docker & Docker Compose installed
- [ ] Git repository initialized
- [ ] Development dependencies installed

## Core Functionality ✅
- [ ] File upload component working
- [ ] Document parsing (PDF, DOCX, TXT) implemented
- [ ] Metadata extraction working
- [ ] Validation engine functional
- [ ] Export system operational
- [ ] Error handling implemented
- [ ] Caching system working

## Security ✅
- [ ] File validation implemented
- [ ] Input sanitization working
- [ ] Authentication ready
- [ ] HTTPS configured
- [ ] Security headers implemented
- [ ] Rate limiting active

## Testing ✅
- [ ] Unit tests written (80%+ coverage)
- [ ] Integration tests passing
- [ ] Error handling tests passing
- [ ] Performance tests completed
- [ ] Security tests passed

## Production Deployment ✅
- [ ] Docker images built
- [ ] Environment variables configured
- [ ] Database migrations ready
- [ ] Nginx configuration tested
- [ ] SSL certificates installed
- [ ] Monitoring configured
- [ ] Backup system ready

## Documentation ✅
- [ ] API documentation complete
- [ ] User guide written
- [ ] Deployment guide ready
- [ ] Troubleshooting guide available
```

### 11.2 Go-Live Steps
```bash
# 1. Build and deploy
docker-compose -f docker-compose.prod.yml build
docker-compose -f docker-compose.prod.yml up -d

# 2. Run database migrations
docker-compose exec backend alembic upgrade head

# 3. Load initial data (templates, validation rules)
docker-compose exec backend python scripts/load_initial_data.py

# 4. Verify deployment
curl https://yourdomain.com/api/health
curl https://yourdomain.com/api/health/detailed

# 5. Run smoke tests
docker-compose exec backend python -m pytest tests/smoke_tests/

# 6. Monitor logs
docker-compose logs -f backend
```

---

## 12. SUPPORT & MAINTENANCE

### 12.1 Common Issues & Solutions

| Issue | Symptoms | Solution |
|-------|----------|----------|
| **File Upload Fails** | 413 Request Entity Too Large | Increase nginx `client_max_body_size` |
| **Parsing Timeout** | 504 Gateway Timeout | Increase `proxy_read_timeout` in nginx |
| **Memory Issues** | High memory usage | Implement file streaming, reduce batch sizes |
| **Database Locks** | Slow queries | Add database indexes, optimize queries |
| **Redis Connection** | Cache misses | Check Redis connectivity, increase connection pool |

### 12.2 Maintenance Schedule
```markdown
## Daily ✅
- [ ] Check application logs for errors
- [ ] Monitor system resources (CPU, RAM, Disk)
- [ ] Verify backup completion

## Weekly ✅  
- [ ] Review error rates and performance metrics
- [ ] Clean up temporary files
- [ ] Update security patches

## Monthly ✅
- [ ] Performance optimization review
- [ ] Database maintenance (VACUUM, REINDEX)
- [ ] Security audit
- [ ] Documentation updates
```

---

**Đây là bản dự thảo đầu tiên của tài liệu thiết kế module FR-03.1. Mời bạn xem qua và cho tôi biết nếu có bất kỳ điểm nào cần chỉnh sửa, làm rõ hoặc bổ sung thêm nhé.**
