Náº¿u Ä‘Ã£ Ä‘á»§ thÃ´ng tin hÃ£y giÃºp tÃ´i viáº¿t tÃ i liá»‡u thiáº¿t káº¿ module dÆ°á»›i Ä‘Ã¢y.
TÃ i liá»‡u nÃ y cung cáº¥p roadmap Ä‘áº§y Ä‘á»§ Ä‘á»ƒ implement FR-03.1 mÃ  khÃ´ng cáº§n code cá»¥ thá»ƒ nhÆ°ng 
cÃ³ cÃ¡c bÆ°á»›c lÃ m cá»¥ thá»ƒ nhÆ° chuáº©n bá»‹ mÃ´i trÆ°á»ng PC tháº¿ nÃ o, pháº§n má»m gÃ¬?
 Ä‘á»ƒ Team ká»¹ thuáº­t cÃ³ thá»ƒ follow step-by-step guide nÃ y Ä‘á»ƒ thiáº¿t káº¿ tá»‘i Æ°u cho há»‡ thá»‘ng chatbot
 vÃ  cÃ³ mermaidchart luá»“ng cÃ´ng viá»‡c, Ä‘iá»ƒm quay Ä‘áº§u khi bá»‹ fail.

3.3 Module Xá»­ lÃ½ Dá»¯ liá»‡u (FR-03)
FR-03.1 - CÃ´ng cá»¥ Raw-to-Clean Data

Web form Ä‘á»ƒ nháº­p metadata cho tÃ i liá»‡u má»›i
Template chuáº©n cho cÃ¡c loáº¡i tÃ i liá»‡u khÃ¡c nhau
Validation dá»¯ liá»‡u Ä‘áº§u vÃ o báº¯t buá»™c
Export tÃ i liá»‡u Ä‘Ã£ xá»­ lÃ½ theo format chuáº©n
---
# TÃ€I LIá»†U THIáº¾T Káº¾ MODULE FR-03.1
## CÃ”NG Cá»¤ RAW-TO-CLEAN DATA PROCESSING

---

**Module ID:** FR-03.1  
**TÃªn Module:** Raw-to-Clean Data Processing Tool  
**PhiÃªn báº£n:** 1.0  
**NgÃ y:** 31/08/2025  
**Tráº¡ng thÃ¡i:** Design Document  

---

## 1. Tá»”NG QUAN MODULE

### 1.1 Má»¥c Ä‘Ã­ch
Module FR-03.1 cung cáº¥p cÃ´ng cá»¥ web-based Ä‘á»ƒ chuyá»ƒn Ä‘á»•i tÃ i liá»‡u thÃ´ (raw documents) thÃ nh dá»¯ liá»‡u sáº¡ch (clean data) cÃ³ cáº¥u trÃºc, metadata Ä‘áº§y Ä‘á»§, sáºµn sÃ ng cho quÃ¡ trÃ¬nh embedding vÃ  indexing trong há»‡ thá»‘ng RAG.

### 1.2 Pháº¡m vi chá»©c nÄƒng
- **Web Form Interface**: Giao diá»‡n nháº­p metadata cho tÃ i liá»‡u má»›i
- **Template System**: Template chuáº©n cho cÃ¡c loáº¡i tÃ i liá»‡u khÃ¡c nhau  
- **Validation Engine**: Kiá»ƒm tra dá»¯ liá»‡u Ä‘áº§u vÃ o báº¯t buá»™c
- **Export Functionality**: Xuáº¥t tÃ i liá»‡u Ä‘Ã£ xá»­ lÃ½ theo format chuáº©n

### 1.3 Äáº§u vÃ o vÃ  Äáº§u ra

| **INPUT** | **OUTPUT** |
|-----------|------------|
| Raw documents (PDF, DOCX, TXT) | Clean structured documents |
| Manual metadata input | Standardized metadata JSON |
| Document templates | Validated document packages |
| Validation rules | Export-ready files |

---

## 2. KIáº¾N TRÃšC MODULE

### 2.1 Component Architecture

```mermaid
graph TB
    subgraph "ğŸŒ FRONTEND LAYER"
        WebUI[ğŸ“± Web Form Interface<br/>React + TypeScript]
        FileUpload[ğŸ“ File Upload Component<br/>Drag & Drop]
        MetadataForm[ğŸ“ Metadata Form<br/>Dynamic Forms]
        PreviewPane[ğŸ‘ï¸ Document Preview<br/>PDF/Text Viewer]
    end
    
    subgraph "âš™ï¸ BACKEND API LAYER"
        APIGateway[ğŸšª API Gateway<br/>Express.js/FastAPI]
        
        subgraph "Core Services"
            UploadService[ğŸ“¤ Upload Service<br/>File Handling]
            ValidationService[âœ… Validation Service<br/>Rules Engine]
            TemplateService[ğŸ“‹ Template Service<br/>Template Manager]
            ExportService[ğŸ“¦ Export Service<br/>File Generation]
        end
        
        subgraph "Processing Services"
            DocParser[ğŸ“– Document Parser<br/>PDF/DOCX/TXT]
            MetadataExtractor[ğŸ·ï¸ Metadata Extractor<br/>Auto-extraction]
            ContentCleaner[ğŸ§¹ Content Cleaner<br/>Text Processing]
        end
    end
    
    subgraph "ğŸ—„ï¸ STORAGE LAYER"
        TempStorage[(ğŸ”„ Temp Storage<br/>Processing Files)]
        TemplateStore[(ğŸ“‹ Template Store<br/>Document Templates)]
        ValidationRules[(âœ… Validation Rules<br/>Rule Definitions)]
        ProcessedStorage[(âœ¨ Processed Storage<br/>Clean Documents)]
    end
    
    %% Frontend connections
    WebUI --> FileUpload
    WebUI --> MetadataForm
    WebUI --> PreviewPane
    
    %% API connections
    FileUpload --> UploadService
    MetadataForm --> ValidationService
    PreviewPane --> TemplateService
    MetadataForm --> ExportService
    
    %% Service connections
    UploadService --> DocParser
    DocParser --> MetadataExtractor
    MetadataExtractor --> ContentCleaner
    ContentCleaner --> ValidationService
    ValidationService --> ExportService
    
    %% Storage connections
    UploadService --> TempStorage
    TemplateService --> TemplateStore
    ValidationService --> ValidationRules
    ExportService --> ProcessedStorage
    
    %% Styling
    classDef frontend fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef api fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef services fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef processing fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef storage fill:#e0f2f1,stroke:#00796b,stroke-width:2px
    
    class WebUI,FileUpload,MetadataForm,PreviewPane frontend
    class APIGateway api
    class UploadService,ValidationService,TemplateService,ExportService services
    class DocParser,MetadataExtractor,ContentCleaner processing
    class TempStorage,TemplateStore,ValidationRules,ProcessedStorage storage
```

### 2.2 Data Flow Architecture

```mermaid
graph LR
    subgraph "ğŸ“¥ INPUT STAGE"
        RawDoc[ğŸ“„ Raw Document<br/>PDF/DOCX/TXT]
        UserInput[ğŸ‘¤ User Metadata<br/>Manual Input]
    end
    
    subgraph "ğŸ”„ PROCESSING STAGE"
        Parse[ğŸ“– Document Parsing<br/>Extract Text + Structure]
        AutoMeta[ğŸ¤– Auto Metadata<br/>Extract from Content]
        Merge[ğŸ”„ Merge Metadata<br/>Auto + Manual]
        Validate[âœ… Validation<br/>Rules Check]
        Clean[ğŸ§¹ Content Cleaning<br/>Format + Normalize]
    end
    
    subgraph "ğŸ“¤ OUTPUT STAGE"
        Structure[ğŸ“Š Structured Document<br/>JSON Format]
        Export[ğŸ“¦ Export Package<br/>Multiple Formats]
        Store[ğŸ’¾ Storage<br/>Ready for Ingestion]
    end
    
    %% Flow
    RawDoc --> Parse
    Parse --> AutoMeta
    UserInput --> Merge
    AutoMeta --> Merge
    Merge --> Validate
    Validate --> Clean
    Clean --> Structure
    Structure --> Export
    Export --> Store
    
    %% Error flows (dotted red)
    Validate -.->|Validation Failed| UserInput
    Parse -.->|Parse Error| RawDoc
    
    %% Styling
    classDef input fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef processing fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef output fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef error fill:#ffebee,stroke:#c62828,stroke-width:2px,stroke-dasharray: 5 5
    
    class RawDoc,UserInput input
    class Parse,AutoMeta,Merge,Validate,Clean processing
    class Structure,Export,Store output
```

---

## 3. CÃ€I Äáº¶T MÃ”I TRÆ¯á»œNG PHÃT TRIá»‚N

### 3.1 YÃªu cáº§u Hardware

| Component | Minimum | Recommended | Ghi chÃº |
|-----------|---------|-------------|---------|
| **CPU** | Intel i5 4 cores | Intel i7 8 cores | Cho document processing |
| **RAM** | 8GB | 16GB | PDF parsing memory intensive |
| **Storage** | 100GB SSD | 500GB SSD | Temporary file storage |
| **Network** | 100Mbps | 1Gbps | File upload performance |

### 3.2 Software Dependencies

#### 3.2.1 Development Environment
```bash
# Node.js Environment
Node.js: v18.17.0+ (LTS)
npm: v9.0.0+
yarn: v1.22.0+ (recommended)

# Python Environment (for document processing)
Python: v3.9+
pip: v23.0+
virtualenv: v20.0+

# Database
PostgreSQL: v14+
Redis: v7.0+

# Development Tools
Git: v2.40+
Docker: v24.0+
Docker Compose: v2.20+
```

#### 3.2.2 Frontend Stack
```json
{
  "dependencies": {
    "react": "^18.2.0",
    "typescript": "^5.0.0",
    "next.js": "^13.4.0",
    "tailwindcss": "^3.3.0",
    "react-hook-form": "^7.45.0",
    "react-dropzone": "^14.2.0",
    "react-pdf": "^7.3.0",
    "axios": "^1.4.0",
    "zod": "^3.22.0"
  },
  "devDependencies": {
    "@types/react": "^18.2.0",
    "eslint": "^8.44.0",
    "jest": "^29.6.0",
    "@testing-library/react": "^13.4.0"
  }
}
```

#### 3.2.3 Backend Stack
```python
# requirements.txt
fastapi==0.101.0
uvicorn==0.23.0
pydantic==2.1.0
sqlalchemy==2.0.19
alembic==1.11.0
redis==4.6.0
celery==5.3.0
pypdf2==3.0.1
python-docx==0.8.11
python-multipart==0.0.6
jinja2==3.1.2
aiofiles==23.1.0
```

### 3.3 Development Tools Setup

#### 3.3.1 IDE Configuration
```bash
# VS Code Extensions (recommended)
- ES7+ React/Redux/React-Native snippets
- TypeScript Importer
- Tailwind CSS IntelliSense
- Python extension pack
- Docker extension
- GitLens
- Prettier - Code formatter
- ESLint
```

#### 3.3.2 Git Configuration
```bash
# Git hooks setup
npm install husky --save-dev
npx husky add .husky/pre-commit "npm run lint && npm run test"
npx husky add .husky/commit-msg "npx commitlint --edit $1"

# Conventional commits
npm install @commitlint/config-conventional --save-dev
```

---

## 4. STEP-BY-STEP IMPLEMENTATION GUIDE

### 4.1 Phase 1: Project Setup & Environment (Week 1)

#### Step 1.1: Environment Initialization
```bash
# 1. Create project structure
mkdir raw-to-clean-processor
cd raw-to-clean-processor
mkdir {frontend,backend,docs,docker,scripts}

# 2. Initialize Git repository
git init
git remote add origin <repository-url>

# 3. Setup Docker development environment
docker-compose up -d postgres redis
```

#### Step 1.2: Frontend Setup
```bash
# Navigate to frontend directory
cd frontend

# Initialize Next.js project with TypeScript
npx create-next-app@latest . --typescript --tailwind --eslint --app

# Install additional dependencies
npm install react-hook-form zod @hookform/resolvers
npm install react-dropzone react-pdf
npm install axios @tanstack/react-query
npm install lucide-react @radix-ui/react-dialog

# Setup development server
npm run dev # Should run on http://localhost:3000
```

#### Step 1.3: Backend Setup
```bash
# Navigate to backend directory
cd ../backend

# Create Python virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Setup FastAPI project structure
mkdir {app,tests,migrations,config}
touch app/{__init__.py,main.py,models.py,schemas.py,crud.py,database.py}

# Initialize database
alembic init alembic
alembic revision --autogenerate -m "Initial migration"
alembic upgrade head

# Start development server
uvicorn app.main:app --reload # Should run on http://localhost:8000
```

### 4.2 Phase 2: Core Components Development (Week 2-3)

#### Step 2.1: Database Schema Design
```sql
-- Document Templates Table
CREATE TABLE document_templates (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name VARCHAR(100) NOT NULL,
    document_type VARCHAR(50) NOT NULL,
    fields JSONB NOT NULL,
    validation_rules JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Document Processing Sessions Table  
CREATE TABLE processing_sessions (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id VARCHAR(100),
    original_filename VARCHAR(255) NOT NULL,
    file_path VARCHAR(500),
    file_size INTEGER,
    mime_type VARCHAR(100),
    status VARCHAR(20) DEFAULT 'pending',
    template_id UUID REFERENCES document_templates(id),
    metadata JSONB,
    processed_content TEXT,
    error_message TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Validation Rules Table
CREATE TABLE validation_rules (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    rule_name VARCHAR(100) NOT NULL,
    field_name VARCHAR(100) NOT NULL,
    rule_type VARCHAR(50) NOT NULL, -- required, regex, length, custom
    rule_config JSONB NOT NULL,
    error_message VARCHAR(255),
    is_active BOOLEAN DEFAULT true,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

#### Step 2.2: Document Templates Configuration
```json
// Template: Technical Document
{
  "name": "Technical Document",
  "document_type": "technical_guide",
  "fields": {
    "title": {
      "type": "string",
      "required": true,
      "max_length": 200
    },
    "department_owner": {
      "type": "select",
      "required": true,
      "options": ["rd", "production", "qa", "sales", "admin"]
    },
    "author": {
      "type": "string", 
      "required": true,
      "max_length": 100
    },
    "version": {
      "type": "string",
      "required": true,
      "pattern": "^v\\d+\\.\\d+\\.\\d+$"
    },
    "access_level": {
      "type": "select",
      "required": true,
      "options": ["public", "employee_only", "manager_only", "director_only"]
    },
    "tags": {
      "type": "array",
      "items": "string",
      "max_items": 10
    },
    "summary": {
      "type": "text",
      "required": true,
      "max_length": 500
    },
    "effective_date": {
      "type": "date",
      "required": false
    },
    "expiry_date": {
      "type": "date", 
      "required": false
    }
  },
  "validation_rules": {
    "title": ["required", "max_length:200"],
    "version": ["required", "regex:^v\\d+\\.\\d+\\.\\d+$"],
    "access_level": ["required", "in:public,employee_only,manager_only,director_only"]
  }
}
```

#### Step 2.3: File Upload Component
```typescript
// components/FileUpload.tsx
import React, { useCallback, useState } from 'react';
import { useDropzone } from 'react-dropzone';
import { Upload, File, X } from 'lucide-react';

interface FileUploadProps {
  onFileSelect: (file: File) => void;
  acceptedFileTypes: string[];
  maxFileSize: number;
}

export const FileUpload: React.FC<FileUploadProps> = ({
  onFileSelect,
  acceptedFileTypes,
  maxFileSize
}) => {
  const [uploadedFile, setUploadedFile] = useState<File | null>(null);
  const [error, setError] = useState<string>('');

  const onDrop = useCallback((acceptedFiles: File[], rejectedFiles: any[]) => {
    if (rejectedFiles.length > 0) {
      setError(rejectedFiles[0].errors[0].message);
      return;
    }

    const file = acceptedFiles[0];
    setUploadedFile(file);
    setError('');
    onFileSelect(file);
  }, [onFileSelect]);

  const { getRootProps, getInputProps, isDragActive } = useDropzone({
    onDrop,
    accept: {
      'application/pdf': ['.pdf'],
      'application/vnd.openxmlformats-officedocument.wordprocessingml.document': ['.docx'],
      'text/plain': ['.txt']
    },
    maxSize: maxFileSize,
    multiple: false
  });

  return (
    <div className="w-full">
      <div
        {...getRootProps()}
        className={`
          border-2 border-dashed rounded-lg p-8 text-center cursor-pointer
          transition-colors duration-200
          ${isDragActive 
            ? 'border-blue-500 bg-blue-50' 
            : 'border-gray-300 hover:border-gray-400'
          }
        `}
      >
        <input {...getInputProps()} />
        <Upload className="mx-auto mb-4 h-12 w-12 text-gray-400" />
        <p className="text-lg font-medium text-gray-700">
          {isDragActive 
            ? 'Tháº£ file vÃ o Ä‘Ã¢y...' 
            : 'KÃ©o tháº£ file hoáº·c click Ä‘á»ƒ chá»n'
          }
        </p>
        <p className="text-sm text-gray-500 mt-2">
          Há»— trá»£: PDF, DOCX, TXT (tá»‘i Ä‘a {Math.round(maxFileSize / 1024 / 1024)}MB)
        </p>
      </div>

      {uploadedFile && (
        <div className="mt-4 p-4 bg-green-50 border border-green-200 rounded-lg">
          <div className="flex items-center justify-between">
            <div className="flex items-center">
              <File className="h-5 w-5 text-green-600 mr-2" />
              <span className="text-green-800">{uploadedFile.name}</span>
              <span className="text-green-600 ml-2 text-sm">
                ({Math.round(uploadedFile.size / 1024)}KB)
              </span>
            </div>
            <button
              onClick={() => setUploadedFile(null)}
              className="text-red-500 hover:text-red-700"
            >
              <X className="h-5 w-5" />
            </button>
          </div>
        </div>
      )}

      {error && (
        <div className="mt-4 p-4 bg-red-50 border border-red-200 rounded-lg">
          <p className="text-red-800">{error}</p>
        </div>
      )}
    </div>
  );
};
```

### 4.3 Phase 3: Processing Logic Implementation (Week 3-4)

#### Step 3.1: Document Parser Service
```python
# app/services/document_parser.py
from abc import ABC, abstractmethod
import PyPDF2
import docx
from typing import Dict, Any, Optional

class DocumentParser(ABC):
    @abstractmethod
    def parse(self, file_path: str) -> Dict[str, Any]:
        pass

class PDFParser(DocumentParser):
    def parse(self, file_path: str) -> Dict[str, Any]:
        try:
            with open(file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                
                # Extract metadata
                metadata = pdf_reader.metadata or {}
                
                # Extract text content
                text_content = ""
                for page in pdf_reader.pages:
                    text_content += page.extract_text() + "\n"
                
                return {
                    "content": text_content.strip(),
                    "page_count": len(pdf_reader.pages),
                    "metadata": {
                        "title": metadata.get('/Title', ''),
                        "author": metadata.get('/Author', ''),
                        "creator": metadata.get('/Creator', ''),
                        "creation_date": str(metadata.get('/CreationDate', '')),
                        "modification_date": str(metadata.get('/ModDate', ''))
                    },
                    "format": "pdf"
                }
        except Exception as e:
            raise Exception(f"Error parsing PDF: {str(e)}")

class DOCXParser(DocumentParser):
    def parse(self, file_path: str) -> Dict[str, Any]:
        try:
            doc = docx.Document(file_path)
            
            # Extract text content
            text_content = ""
            for paragraph in doc.paragraphs:
                text_content += paragraph.text + "\n"
            
            # Extract metadata
            properties = doc.core_properties
            
            return {
                "content": text_content.strip(),
                "paragraph_count": len(doc.paragraphs),
                "metadata": {
                    "title": properties.title or '',
                    "author": properties.author or '',
                    "created": str(properties.created) if properties.created else '',
                    "modified": str(properties.modified) if properties.modified else '',
                    "subject": properties.subject or '',
                    "category": properties.category or ''
                },
                "format": "docx"
            }
        except Exception as e:
            raise Exception(f"Error parsing DOCX: {str(e)}")

class TXTParser(DocumentParser):
    def parse(self, file_path: str) -> Dict[str, Any]:
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                content = file.read()
            
            return {
                "content": content.strip(),
                "line_count": len(content.split('\n')),
                "metadata": {
                    "encoding": "utf-8"
                },
                "format": "txt"
            }
        except UnicodeDecodeError:
            # Try different encodings
            encodings = ['latin-1', 'cp1252', 'iso-8859-1']
            for encoding in encodings:
                try:
                    with open(file_path, 'r', encoding=encoding) as file:
                        content = file.read()
                    return {
                        "content": content.strip(),
                        "line_count": len(content.split('\n')),
                        "metadata": {
                            "encoding": encoding
                        },
                        "format": "txt"
                    }
                except UnicodeDecodeError:
                    continue
            raise Exception("Unable to decode text file with any common encoding")
        except Exception as e:
            raise Exception(f"Error parsing TXT: {str(e)}")

class ParserFactory:
    parsers = {
        'pdf': PDFParser,
        'docx': DOCXParser,
        'txt': TXTParser
    }
    
    @classmethod
    def get_parser(cls, file_extension: str) -> DocumentParser:
        parser_class = cls.parsers.get(file_extension.lower())
        if not parser_class:
            raise ValueError(f"Unsupported file format: {file_extension}")
        return parser_class()
```

#### Step 3.2: Metadata Extraction Service
```python
# app/services/metadata_extractor.py
import re
from typing import Dict, Any, List, Optional
from datetime import datetime

class MetadataExtractor:
    def __init__(self):
        self.patterns = {
            'version': [
                r'version\s*:?\s*v?(\d+\.\d+\.\d+)',
                r'phiÃªn\s*báº£n\s*:?\s*v?(\d+\.\d+\.\d+)',
                r'v(\d+\.\d+\.\d+)'
            ],
            'date': [
                r'ngÃ y\s*:?\s*(\d{1,2}[\/\-]\d{1,2}[\/\-]\d{2,4})',
                r'date\s*:?\s*(\d{1,2}[\/\-]\d{1,2}[\/\-]\d{2,4})',
                r'(\d{1,2}[\/\-]\d{1,2}[\/\-]\d{2,4})'
            ],
            'email': [
                r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})'
            ],
            'phone': [
                r'(\+?84|0)([1-9][0-9]{8,9})',
                r'(\d{3}[-.\s]?\d{3}[-.\s]?\d{4})'
            ]
        }
    
    def extract_from_content(self, content: str) -> Dict[str, Any]:
        """Extract metadata from document content using regex patterns"""
        extracted = {}
        
        # Extract version information
        versions = self._extract_pattern(content, 'version')
        if versions:
            extracted['version'] = versions[0]
        
        # Extract dates
        dates = self._extract_pattern(content, 'date')
        if dates:
            extracted['dates_found'] = dates[:3]  # Limit to 3 dates
        
        # Extract email addresses
        emails = self._extract_pattern(content, 'email')
        if emails:
            extracted['emails'] = emails[:5]  # Limit to 5 emails
        
        # Extract phone numbers
        phones = self._extract_pattern(content, 'phone')
        if phones:
            extracted['phones'] = phones[:3]  # Limit to 3 phones
        
        # Extract document structure info
        extracted['structure'] = self._analyze_structure(content)
        
        return extracted
    
    def _extract_pattern(self, content: str, pattern_type: str) -> List[str]:
        """Extract matches for a specific pattern type"""
        matches = []
        patterns = self.patterns.get(pattern_type, [])
        
        for pattern in patterns:
            found = re.findall(pattern, content, re.IGNORECASE | re.MULTILINE)
            matches.extend(found)
        
        return list(set(matches))  # Remove duplicates
    
    def _analyze_structure(self, content: str) -> Dict[str, Any]:
        """Analyze document structure"""
        lines = content.split('\n')
        
        return {
            'total_lines': len(lines),
            'non_empty_lines': len([line for line in lines if line.strip()]),
            'word_count': len(content.split()),
            'character_count': len(content),
            'has_headers': self._detect_headers(content),
            'has_tables': self._detect_tables(content),
            'has_lists': self._detect_lists(content)
        }
    
    def _detect_headers(self, content: str) -> bool:
        """Detect if content has headers"""
        header_patterns = [
            r'^#{1,6}\s+.+$',  # Markdown headers
            r'^[A-Z\s]{2,}$',  # All caps headers
            r'^\d+\.\s+.+$'    # Numbered headers
        ]
        
        lines = content.split('\n')
        for line in lines[:20]:  # Check first 20 lines
            for pattern in header_patterns:
                if re.match(pattern, line.strip()):
                    return True
        return False
    
    def _detect_tables(self, content: str) -> bool:
        """Detect if content has table-like structures"""
        table_indicators = ['|', '\t', '  +  ']
        return any(indicator in content for indicator in table_indicators)
    
    def _detect_lists(self, content: str) -> bool:
        """Detect if content has list structures"""
        list_patterns = [
            r'^\s*[-\*\+]\s+',  # Bullet lists
            r'^\s*\d+\.\s+',    # Numbered lists
            r'^\s*[a-zA-Z]\)\s+' # Lettered lists
        ]
        
        lines = content.split('\n')
        for line in lines:
            for pattern in list_patterns:
                if re.match(pattern, line):
                    return True
        return False

    def suggest_document_type(self, content: str, filename: str) -> str:
        """Suggest document type based on content analysis"""
        content_lower = content.lower()
        filename_lower = filename.lower()
        
        # Policy documents
        if any(keyword in content_lower for keyword in 
               ['chÃ­nh sÃ¡ch', 'policy', 'quy Ä‘á»‹nh', 'regulation']):
            return 'policy'
        
        # Technical documents
        elif any(keyword in content_lower for keyword in 
                ['ká»¹ thuáº­t', 'technical', 'hÆ°á»›ng dáº«n', 'guide', 'manual']):
            return 'technical_guide'
        
        # Procedures
        elif any(keyword in content_lower for keyword in 
                ['quy trÃ¬nh', 'procedure', 'process', 'workflow']):
            return 'procedure'
        
        # Reports
        elif any(keyword in content_lower for keyword in 
                ['bÃ¡o cÃ¡o', 'report', 'analysis', 'phÃ¢n tÃ­ch']):
            return 'report'
        
        # Default
        else:
            return 'general'
```

### 4.4 Phase 4: Validation & Export System (Week 4-5)

#### Step 4.1: Validation Engine
```python
# app/services/validation_service.py
from typing import Dict, Any, List, Optional
from pydantic import BaseModel, ValidationError
import re
from datetime import datetime

class ValidationResult(BaseModel):
    is_valid: bool
    errors: List[str] = []
    warnings: List[str] = []

class FieldValidator:
    def __init__(self):
        self.validators = {
            'required': self._validate_required,
            'max_length': self._validate_max_length,
            'min_length': self._validate_min_length,
            'regex': self._validate_regex,
            'email': self._validate_email,
            'date': self._validate_date,
            'in': self._validate_in_list,
            'numeric': self._validate_numeric,
            'url': self._validate_url
        }
    
    def validate_field(self, 
                      field_name: str, 
                      value: Any, 
                      rules: List[str]) -> ValidationResult:
        """Validate a single field against multiple rules"""
        result = ValidationResult(is_valid=True)
        
        for rule in rules:
            rule_result = self._apply_rule(field_name, value, rule)
            if not rule_result.is_valid:
                result.is_valid = False
                result.errors.extend(rule_result.errors)
            result.warnings.extend(rule_result.warnings)
        
        return result
    
    def _apply_rule(self, field_name: str, value: Any, rule: str) -> ValidationResult:
        """Apply a single validation rule"""
        if ':' in rule:
            rule_name, rule_param = rule.split(':', 1)
        else:
            rule_name, rule_param = rule, None
        
        validator = self.validators.get(rule_name)
        if not validator:
            return ValidationResult(
                is_valid=False, 
                errors=[f"Unknown validation rule: {rule_name}"]
            )
        
        return validator(field_name, value, rule_param)
    
    def _validate_required(self, field_name: str, value: Any, param: str) -> ValidationResult:
        """Validate required field"""
        if value is None or (isinstance(value, str) and not value.strip()):
            return ValidationResult(
                is_valid=False,
                errors=[f"{field_name} lÃ  trÆ°á»ng báº¯t buá»™c"]
            )
        return ValidationResult(is_valid=True)
    
    def _validate_max_length(self, field_name: str, value: Any, param: str) -> ValidationResult:
        """Validate maximum length"""
        if value is None:
            return ValidationResult(is_valid=True)
        
        max_len = int(param)
        if len(str(value)) > max_len:
            return ValidationResult(
                is_valid=False,
                errors=[f"{field_name} khÃ´ng Ä‘Æ°á»£c vÆ°á»£t quÃ¡ {max_len} kÃ½ tá»±"]
            )
        return ValidationResult(is_valid=True)
    
    def _validate_min_length(self, field_name: str, value: Any, param: str) -> ValidationResult:
        """Validate minimum length"""
        if value is None:
            return ValidationResult(is_valid=True)
        
        min_len = int(param)
        if len(str(value)) < min_len:
            return ValidationResult(
                is_valid=False,
                errors=[f"{field_name} pháº£i cÃ³ Ã­t nháº¥t {min_len} kÃ½ tá»±"]
            )
        return ValidationResult(is_valid=True)
    
    def _validate_regex(self, field_name: str, value: Any, param: str) -> ValidationResult:
        """Validate regex pattern"""
        if value is None:
            return ValidationResult(is_valid=True)
        
        if not re.match(param, str(value)):
            return ValidationResult(
                is_valid=False,
                errors=[f"{field_name} khÃ´ng Ä‘Ãºng Ä‘á»‹nh dáº¡ng yÃªu cáº§u"]
            )
        return ValidationResult(is_valid=True)
    
    def _validate_email(self, field_name: str, value: Any, param: str) -> ValidationResult:
        """Validate email format"""
        if value is None:
            return ValidationResult(is_valid=True)
        
        email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
        if not re.match(email_pattern, str(value)):
            return ValidationResult(
                is_valid=False,
                errors=[f"{field_name} khÃ´ng Ä‘Ãºng Ä‘á»‹nh dáº¡ng email"]
            )
        return ValidationResult(is_valid=True)
    
    def _validate_date(self, field_name: str, value: Any, param: str) -> ValidationResult:
        """Validate date format"""
        if value is None:
            return ValidationResult(is_valid=True)
        
        try:
            if isinstance(value, str):
                datetime.strptime(value, '%Y-%m-%d')
            return ValidationResult(is_valid=True)
        except ValueError:
            return ValidationResult(
                is_valid=False,
                errors=[f"{field_name} pháº£i cÃ³ Ä‘á»‹nh dáº¡ng YYYY-MM-DD"]
            )
    
    def _validate_in_list(self, field_name: str, value: Any, param: str) -> ValidationResult:
        """Validate value is in allowed list"""
        if value is None:
            return ValidationResult(is_valid=True)
        
        allowed_values = [v.strip() for v in param.split(',')]
        if str(value) not in allowed_values:
            return ValidationResult(
                is_valid=False,
                errors=[f"{field_name} pháº£i lÃ  má»™t trong cÃ¡c giÃ¡ trá»‹: {', '.join(allowed_values)}"]
            )
        return ValidationResult(is_valid=True)
    
    def _validate_numeric(self, field_name: str, value: Any, param: str) -> ValidationResult:
        """Validate numeric value"""
        if value is None:
            return ValidationResult(is_valid=True)
        
        try:
            float(value)
            return ValidationResult(is_valid=True)
        except (ValueError, TypeError):
            return ValidationResult(
                is_valid=False,
                errors=[f"{field_name} pháº£i lÃ  sá»‘"]
            )
    
    def _validate_url(self, field_name: str, value: Any, param: str) -> ValidationResult:
        """Validate URL format"""
        if value is None:
            return ValidationResult(is_valid=True)
        
        url_pattern = r'^https?://(?:[-\w.])+(?:[:\d]+)?(?:/(?:[\w/_.])*(?:\?(?:[\w&=%.])*)?(?:#(?:\w)*)?)?$'
        if not re.match(url_pattern, str(value)):
            return ValidationResult(
                is_valid=False,
                errors=[f"{field_name} khÃ´ng Ä‘Ãºng Ä‘á»‹nh dáº¡ng URL"]
            )
        return ValidationResult(is_valid=True)

class DocumentValidator:
    def __init__(self):
        self.field_validator = FieldValidator()
    
    def validate_document(self, 
                         metadata: Dict[str, Any], 
                         template: Dict[str, Any]) -> ValidationResult:
        """Validate entire document against template"""
        overall_result = ValidationResult(is_valid=True)
        
        # Get validation rules from template
        validation_rules = template.get('validation_rules', {})
        fields_config = template.get('fields', {})
        
        # Validate each field
        for field_name, field_config in fields_config.items():
            field_value = metadata.get(field_name)
            field_rules = validation_rules.get(field_name, [])
            
            # Add implicit rules from field config
            if field_config.get('required', False):
                field_rules.append('required')
            
            if 'max_length' in field_config:
                field_rules.append(f"max_length:{field_config['max_length']}")
            
            if 'pattern' in field_config:
                field_rules.append(f"regex:{field_config['pattern']}")
            
            if 'options' in field_config:
                field_rules.append(f"in:{','.join(field_config['options'])}")
            
            # Validate field
            field_result = self.field_validator.validate_field(
                field_name, field_value, field_rules
            )
            
            if not field_result.is_valid:
                overall_result.is_valid = False
                overall_result.errors.extend(field_result.errors)
            
            overall_result.warnings.extend(field_result.warnings)
        
        return overall_result
```

#### Step 4.2: Export Service
```python
# app/services/export_service.py
import json
import csv
import xml.etree.ElementTree as ET
from typing import Dict, Any, List
from datetime import datetime
import os
import zipfile

class ExportService:
    def __init__(self):
        self.export_formats = {
            'json': self._export_json,
            'xml': self._export_xml,
            'csv': self._export_csv,
            'package': self._export_package
        }
    
    def export_document(self, 
                       document_data: Dict[str, Any], 
                       export_format: str,
                       output_path: str) -> str:
        """Export document in specified format"""
        
        if export_format not in self.export_formats:
            raise ValueError(f"Unsupported export format: {export_format}")
        
        export_func = self.export_formats[export_format]
        return export_func(document_data, output_path)
    
    def _export_json(self, document_data: Dict[str, Any], output_path: str) -> str:
        """Export as JSON format"""
        export_data = self._prepare_export_data(document_data)
        
        filename = f"{output_path}/document_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)
        
        return filename
    
    def _export_xml(self, document_data: Dict[str, Any], output_path: str) -> str:
        """Export as XML format"""
        export_data = self._prepare_export_data(document_data)
        
        # Create XML structure
        root = ET.Element("document")
        
        # Add metadata
        metadata_elem = ET.SubElement(root, "metadata")
        for key, value in export_data['metadata'].items():
            elem = ET.SubElement(metadata_elem, key)
            elem.text = str(value) if value is not None else ""
        
        # Add content
        content_elem = ET.SubElement(root, "content")
        content_elem.text = export_data['content']
        
        # Add processing info
        processing_elem = ET.SubElement(root, "processing_info")
        for key, value in export_data['processing_info'].items():
            elem = ET.SubElement(processing_elem, key)
            elem.text = str(value) if value is not None else ""
        
        # Write to file
        filename = f"{output_path}/document_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xml"
        
        tree = ET.ElementTree(root)
        tree.write(filename, encoding='utf-8', xml_declaration=True)
        
        return filename
    
    def _export_csv(self, document_data: Dict[str, Any], output_path: str) -> str:
        """Export metadata as CSV format"""
        export_data = self._prepare_export_data(document_data)
        
        filename = f"{output_path}/document_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        
        # Flatten metadata for CSV
        flattened_data = []
        metadata = export_data['metadata']
        processing_info = export_data['processing_info']
        
        # Combine all data
        all_data = {**metadata, **processing_info}
        all_data['content_preview'] = export_data['content'][:500] + "..." if len(export_data['content']) > 500 else export_data['content']
        
        flattened_data.append(all_data)
        
        with open(filename, 'w', newline='', encoding='utf-8') as f:
            if flattened_data:
                writer = csv.DictWriter(f, fieldnames=flattened_data[0].keys())
                writer.writeheader()
                writer.writerows(flattened_data)
        
        return filename
    
    def _export_package(self, document_data: Dict[str, Any], output_path: str) -> str:
        """Export as complete package (ZIP with multiple formats)"""
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        package_name = f"document_package_{timestamp}"
        package_path = f"{output_path}/{package_name}"
        
        # Create temporary directory
        os.makedirs(package_path, exist_ok=True)
        
        # Export in multiple formats
        json_file = self._export_json(document_data, package_path)
        xml_file = self._export_xml(document_data, package_path)
        csv_file = self._export_csv(document_data, package_path)
        
        # Create README file
        readme_path = f"{package_path}/README.txt"
        with open(readme_path, 'w', encoding='utf-8') as f:
            f.write(f"""Document Processing Package
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

Files included:
- {os.path.basename(json_file)}: Complete document data in JSON format
- {os.path.basename(xml_file)}: Document data in XML format  
- {os.path.basename(csv_file)}: Metadata in CSV format
- README.txt: This file

Original filename: {document_data.get('original_filename', 'Unknown')}
Processing status: {document_data.get('status', 'Unknown')}
""")
        
        # Create ZIP package
        zip_filename = f"{output_path}/{package_name}.zip"
        with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for root, dirs, files in os.walk(package_path):
                for file in files:
                    file_path = os.path.join(root, file)
                    arcname = os.path.relpath(file_path, package_path)
                    zipf.write(file_path, arcname)
        
        # Clean up temporary directory
        import shutil
        shutil.rmtree(package_path)
        
        return zip_filename
    
    def _prepare_export_data(self, document_data: Dict[str, Any]) -> Dict[str, Any]:
        """Prepare data for export"""
        return {
            "metadata": document_data.get('metadata', {}),
            "content": document_data.get('processed_content', ''),
            "processing_info": {
                "id": document_data.get('id'),
                "original_filename": document_data.get('original_filename'),
                "file_size": document_data.get('file_size'),
                "mime_type": document_data.get('mime_type'),
                "status": document_data.get('status'),
                "template_id": document_data.get('template_id'),
                "processed_at": document_data.get('updated_at'),
                "created_at": document_data.get('created_at')
            }
        }

    def get_supported_formats(self) -> List[str]:
        """Get list of supported export formats"""
        return list(self.export_formats.keys())
```

---

## 5. WORKFLOW VÃ€ ERROR HANDLING

### 5.1 Complete Processing Workflow

```mermaid
graph TD
    Start([ğŸš€ Start Processing]) --> UploadFile[ğŸ“¤ Upload File]
    
    UploadFile --> ValidateFile{ğŸ“‹ Validate File}
    ValidateFile -->|âŒ Invalid| FileError[ğŸš« File Validation Error]
    ValidateFile -->|âœ… Valid| ParseDoc[ğŸ“– Parse Document]
    
    ParseDoc --> ParseSuccess{âœ… Parse Success?}
    ParseSuccess -->|âŒ Failed| ParseError[ğŸš« Parse Error]
    ParseSuccess -->|âœ… Success| ExtractMeta[ğŸ·ï¸ Extract Metadata]
    
    ExtractMeta --> SelectTemplate[ğŸ“‹ Select Template]
    SelectTemplate --> ShowForm[ğŸ“ Show Metadata Form]
    
    ShowForm --> UserInput[ğŸ‘¤ User Input Metadata]
    UserInput --> ValidateMeta{âœ… Validate Metadata}
    
    ValidateMeta -->|âŒ Invalid| ValidationError[ğŸš« Validation Error]
    ValidateMeta -->|âœ… Valid| CleanContent[ğŸ§¹ Clean Content]
    
    CleanContent --> GenerateOutput[ğŸ“Š Generate Structured Output]
    GenerateOutput --> SelectExport[ğŸ“¦ Select Export Format]
    
    SelectExport --> ExportFile[ğŸ“¤ Export File]
    ExportFile --> Success([âœ… Process Complete])
    
    %% Error Handling & Retry Paths
    FileError --> RetryUpload{ğŸ”„ Retry Upload?}
    RetryUpload -->|Yes| UploadFile
    RetryUpload -->|No| Abort([âŒ Process Aborted])
    
    ParseError --> RetryParse{ğŸ”„ Retry with Different Parser?}
    RetryParse -->|Yes| ParseDoc
    RetryParse -->|No| ManualInput[âœï¸ Manual Content Input]
    ManualInput --> SelectTemplate
    
    ValidationError --> FixErrors[ğŸ”§ Fix Validation Errors]
    FixErrors --> UserInput
    
    %% Additional Error Paths
    ExportFile --> ExportSuccess{âœ… Export Success?}
    ExportSuccess -->|âŒ Failed| ExportError[ğŸš« Export Error]
    ExportSuccess -->|âœ… Success| Success
    
    ExportError --> RetryExport{ğŸ”„ Retry Export?}
    RetryExport -->|Yes| SelectExport
    RetryExport -->|No| PartialSuccess([âš ï¸ Partial Success])
    
    %% Rollback Points
    ValidationError -.->|Rollback| ShowForm
    ParseError -.->|Rollback| UploadFile
    ExportError -.->|Rollback| GenerateOutput
    
    %% Styling
    classDef startEnd fill:#e8f5e8,stroke:#388e3c,stroke-width:3px
    classDef process fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef decision fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef error fill:#ffebee,stroke:#c62828,stroke-width:2px
    classDef retry fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef rollback fill:#e0f2f1,stroke:#00796b,stroke-width:2px,stroke-dasharray: 5 5
    
    class Start,Success,Abort,PartialSuccess startEnd
    class UploadFile,ParseDoc,ExtractMeta,SelectTemplate,ShowForm,UserInput,CleanContent,GenerateOutput,SelectExport,ExportFile,ManualInput,FixErrors process
    class ValidateFile,ParseSuccess,ValidateMeta,RetryUpload,RetryParse,ExportSuccess,RetryExport decision
    class FileError,ParseError,ValidationError,ExportError error
    class RetryUpload,RetryParse,RetryExport retry
```

### 5.2 Error Handling Strategy

#### Step 5.1: Error Classification & Recovery
```python
# app/services/error_handler.py
from enum import Enum
from typing import Dict, Any, Optional, List
from dataclasses import dataclass
import logging

class ErrorType(Enum):
    FILE_VALIDATION = "file_validation"
    PARSE_ERROR = "parse_error"
    VALIDATION_ERROR = "validation_error"
    PROCESSING_ERROR = "processing_error"
    EXPORT_ERROR = "export_error"
    SYSTEM_ERROR = "system_error"

class ErrorSeverity(Enum):
    LOW = "low"          # Warning, process can continue
    MEDIUM = "medium"    # Error, but recoverable
    HIGH = "high"        # Critical error, process must stop
    CRITICAL = "critical" # System-level error

@dataclass
class ProcessingError:
    error_type: ErrorType
    severity: ErrorSeverity
    message: str
    details: Optional[Dict[str, Any]] = None
    recovery_suggestions: Optional[List[str]] = None
    rollback_point: Optional[str] = None

class ErrorHandler:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.error_handlers = {
            ErrorType.FILE_VALIDATION: self._handle_file_error,
            ErrorType.PARSE_ERROR: self._handle_parse_error,
            ErrorType.VALIDATION_ERROR: self._handle_validation_error,
            ErrorType.PROCESSING_ERROR: self._handle_processing_error,
            ErrorType.EXPORT_ERROR: self._handle_export_error,
            ErrorType.SYSTEM_ERROR: self._handle_system_error
        }
    
    def handle_error(self, error: ProcessingError) -> Dict[str, Any]:
        """Handle processing error and return recovery options"""
        
        # Log error
        self.logger.error(f"Processing error: {error.error_type.value} - {error.message}")
        if error.details:
            self.logger.error(f"Error details: {error.details}")
        
        # Get specific handler
        handler = self.error_handlers.get(error.error_type, self._handle_generic_error)
        
        return handler(error)
    
    def _handle_file_error(self, error: ProcessingError) -> Dict[str, Any]:
        """Handle file validation errors"""
        return {
            "error_type": error.error_type.value,
            "message": error.message,
            "severity": error.severity.value,
            "recovery_options": [
                "retry_upload",
                "check_file_format",
                "reduce_file_size",
                "convert_file_format"
            ],
            "user_actions": [
                "Kiá»ƒm tra láº¡i Ä‘á»‹nh dáº¡ng file (PDF, DOCX, TXT)",
                "Äáº£m báº£o kÃ­ch thÆ°á»›c file < 50MB",
                "Thá»­ upload file khÃ¡c",
                "LiÃªn há»‡ support náº¿u váº¥n Ä‘á» tiáº¿p tá»¥c"
            ],
            "rollback_to": "file_upload"
        }
    
    def _handle_parse_error(self, error: ProcessingError) -> Dict[str, Any]:
        """Handle document parsing errors"""
        return {
            "error_type": error.error_type.value,
            "message": error.message,
            "severity": error.severity.value,
            "recovery_options": [
                "retry_with_different_parser",
                "manual_content_input",
                "file_conversion",
                "skip_auto_extraction"
            ],
            "user_actions": [
                "Thá»­ chuyá»ƒn Ä‘á»•i file sang format khÃ¡c",
                "Nháº­p ná»™i dung thá»§ cÃ´ng náº¿u cáº§n",
                "Kiá»ƒm tra file cÃ³ bá»‹ lá»—i/corrupt khÃ´ng",
                "Sá»­ dá»¥ng OCR cho file scan"
            ],
            "rollback_to": "document_upload"
        }
    
    def _handle_validation_error(self, error: ProcessingError) -> Dict[str, Any]:
        """Handle metadata validation errors"""
        return {
            "error_type": error.error_type.value,
            "message": error.message,
            "severity": error.severity.value,
            "recovery_options": [
                "fix_validation_errors",
                "use_different_template",
                "skip_optional_fields",
                "request_admin_review"
            ],
            "user_actions": [
                "Sá»­a cÃ¡c lá»—i validation Ä‘Æ°á»£c highlight",
                "Kiá»ƒm tra format cá»§a cÃ¡c trÆ°á»ng báº¯t buá»™c",
                "Chá»n template phÃ¹ há»£p hÆ¡n",
                "LiÃªn há»‡ admin náº¿u rule khÃ´ng há»£p lÃ½"
            ],
            "validation_errors": error.details.get("errors", []) if error.details else [],
            "rollback_to": "metadata_form"
        }
    
    def _handle_processing_error(self, error: ProcessingError) -> Dict[str, Any]:
        """Handle content processing errors"""
        return {
            "error_type": error.error_type.value,
            "message": error.message,
            "severity": error.severity.value,
            "recovery_options": [
                "retry_processing",
                "skip_problematic_sections",
                "use_alternative_processing",
                "manual_review"
            ],
            "user_actions": [
                "Thá»­ láº¡i quÃ¡ trÃ¬nh xá»­ lÃ½",
                "Kiá»ƒm tra ná»™i dung cÃ³ kÃ½ tá»± Ä‘áº·c biá»‡t khÃ´ng",
                "Simplify document structure",
                "Contact technical support"
            ],
            "rollback_to": "content_processing"
        }
    
    def _handle_export_error(self, error: ProcessingError) -> Dict[str, Any]:
        """Handle export errors"""
        return {
            "error_type": error.error_type.value,
            "message": error.message,
            "severity": error.severity.value,
            "recovery_options": [
                "retry_export",
                "change_export_format",
                "reduce_export_content",
                "save_partial_results"
            ],
            "user_actions": [
                "Thá»­ export format khÃ¡c (JSON, XML, CSV)",
                "Kiá»ƒm tra dung lÆ°á»£ng á»• Ä‘Ä©a",
                "Giáº£m sá»‘ lÆ°á»£ng field export",
                "LÆ°u káº¿t quáº£ Ä‘Ã£ xá»­ lÃ½ trÆ°á»›c"
            ],
            "rollback_to": "export_selection"
        }
    
    def _handle_system_error(self, error: ProcessingError) -> Dict[str, Any]:
        """Handle system-level errors"""
        return {
            "error_type": error.error_type.value,
            "message": "Lá»—i há»‡ thá»‘ng. Vui lÃ²ng thá»­ láº¡i sau.",
            "severity": error.severity.value,
            "recovery_options": [
                "retry_after_delay",
                "contact_support",
                "check_system_status"
            ],
            "user_actions": [
                "Äá»£i 5-10 phÃºt rá»“i thá»­ láº¡i",
                "Kiá»ƒm tra káº¿t ná»‘i máº¡ng",
                "LiÃªn há»‡ IT support",
                "BÃ¡o cÃ¡o lá»—i vá»›i screenshot"
            ],
            "rollback_to": "start"
        }
    
    def _handle_generic_error(self, error: ProcessingError) -> Dict[str, Any]:
        """Handle unknown errors"""
        return {
            "error_type": "unknown_error",
            "message": "Lá»—i khÃ´ng xÃ¡c Ä‘á»‹nh",
            "severity": "medium",
            "recovery_options": ["retry", "contact_support"],
            "user_actions": [
                "Thá»­ láº¡i tá»« bÆ°á»›c trÆ°á»›c",
                "LiÃªn há»‡ technical support"
            ],
            "rollback_to": "previous_step"
        }
```

---

## 6. TESTING STRATEGY

### 6.1 Unit Testing Setup
```python
# tests/test_document_parser.py
import pytest
import tempfile
import os
from app.services.document_parser import PDFParser, DOCXParser, TXTParser, ParserFactory

class TestDocumentParser:
    
    def test_pdf_parser_success(self):
        """Test PDF parsing with valid file"""
        # Create test PDF file here or use mock
        parser = PDFParser()
        # Add actual test implementation
        pass
    
    def test_pdf_parser_invalid_file(self):
        """Test PDF parser with invalid file"""
        parser = PDFParser()
        with pytest.raises(Exception):
            parser.parse("nonexistent_file.pdf")
    
    def test_docx_parser_success(self):
        """Test DOCX parsing with valid file"""
        parser = DOCXParser()
        # Add test implementation
        pass
    
    def test_txt_parser_utf8(self):
        """Test TXT parser with UTF-8 file"""
        parser = TXTParser()
        
        # Create temp file
        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt', encoding='utf-8') as f:
            f.write("Test content with UTF-8: Ã¡ÃªÃ­Ã´Ã»")
            temp_path = f.name
        
        try:
            result = parser.parse(temp_path)
            assert result['format'] == 'txt'
            assert "Test content" in result['content']
            assert result['metadata']['encoding'] == 'utf-8'
        finally:
            os.unlink(temp_path)
    
    def test_parser_factory(self):
        """Test parser factory method"""
        pdf_parser = ParserFactory.get_parser('pdf')
        assert isinstance(pdf_parser, PDFParser)
        
        docx_parser = ParserFactory.get_parser('docx')
        assert isinstance(docx_parser, DOCXParser)
        
        txt_parser = ParserFactory.get_parser('txt')
        assert isinstance(txt_parser, TXTParser)
        
        with pytest.raises(ValueError):
            ParserFactory.get_parser('unsupported')

# tests/test_validation_service.py
import pytest
from app.services.validation_service import FieldValidator, DocumentValidator, ValidationResult

class TestValidationService:
    
    def test_required_field_validation(self):
        validator = FieldValidator()
        
        # Test valid required field
        result = validator.validate_field("title", "Test Title", ["required"])
        assert result.is_valid == True
        
        # Test missing required field
        result = validator.validate_field("title", "", ["required"])
        assert result.is_valid == False
        assert len(result.errors) > 0
        
        # Test None required field
        result = validator.validate_field("title", None, ["required"])
        assert result.is_valid == False
    
    def test_max_length_validation(self):
        validator = FieldValidator()
        
        # Test valid length
        result = validator.validate_field("title", "Short", ["max_length:10"])
        assert result.is_valid == True
        
        # Test invalid length
        result = validator.validate_field("title", "This is a very long title", ["max_length:10"])
        assert result.is_valid == False
    
    def test_email_validation(self):
        validator = FieldValidator()
        
        # Test valid email
        result = validator.validate_field("email", "test@example.com", ["email"])
        assert result.is_valid == True
        
        # Test invalid email
        result = validator.validate_field("email", "invalid-email", ["email"])
        assert result.is_valid == False
    
    def test_document_validation(self):
        validator = DocumentValidator()
        
        template = {
            "fields": {
                "title": {"required": True, "max_length": 100},
                "author": {"required": True},
                "version": {"pattern": "^v\\d+\\.\\d+\\.\\d+$"}
            },
            "validation_rules": {
                "title": ["required", "max_length:100"],
                "author": ["required"],
                "version": ["regex:^v\\d+\\.\\d+\\.\\d+$"]
            }
        }
        
        # Test valid document
        metadata = {
            "title": "Test Document",
            "author": "John Doe", 
            "version": "v1.0.0"
        }
        
        result = validator.validate_document(metadata, template)
        assert result.is_valid == True
        
        # Test invalid document
        invalid_metadata = {
            "title": "",  # Required field empty
            "author": "John Doe",
            "version": "1.0.0"  # Invalid version format
        }
        
        result = validator.validate_document(invalid_metadata, template)
        assert result.is_valid == False
        assert len(result.errors) >= 2
```

### 6.2 Integration Testing
```python
# tests/test_integration.py
import pytest
import tempfile
import json
from fastapi.testclient import TestClient
from app.main import app

client = TestClient(app)

class TestIntegrationWorkflow:
    
    def test_complete_processing_workflow(self):
        """Test complete document processing workflow"""
        
        # Step 1: Upload file
        with tempfile.NamedTemporaryFile(suffix='.txt') as tmp_file:
            tmp_file.write(b"Test document content")
            tmp_file.seek(0)
            
            response = client.post(
                "/api/upload",
                files={"file": ("test.txt", tmp_file, "text/plain")}
            )
            
            assert response.status_code == 200
            upload_result = response.json()
            session_id = upload_result["session_id"]
        
        # Step 2: Get available templates
        response = client.get("/api/templates")
        assert response.status_code == 200
        templates = response.json()
        template_id = templates[0]["id"]  # Use first template
        
        # Step 3: Submit metadata
        metadata = {
            "title": "Test Integration Document",
            "author": "Test Author",
            "department_owner": "rd",
            "access_level": "employee_only",
            "version": "v1.0.0",
            "tags": ["test", "integration"],
            "summary": "This is a test document for integration testing"
        }
        
        response = client.post(
            f"/api/sessions/{session_id}/metadata",
            json={
                "template_id": template_id,
                "metadata": metadata
            }
        )
        
        assert response.status_code == 200
        validation_result = response.json()
        assert validation_result["is_valid"] == True
        
        # Step 4: Process document
        response = client.post(f"/api/sessions/{session_id}/process")
        assert response.status_code == 200
        process_result = response.json()
        assert process_result["status"] == "completed"
        
        # Step 5: Export document
        response = client.post(
            f"/api/sessions/{session_id}/export",
            json={"format": "json"}
        )
        
        assert response.status_code == 200
        export_result = response.json()
        assert "download_url" in export_result
    
    def test_error_handling_workflow(self):
        """Test error handling and recovery"""
        
        # Test with invalid file type
        response = client.post(
            "/api/upload",
            files={"file": ("test.exe", b"invalid content", "application/x-executable")}
        )
        
        assert response.status_code == 400
        error_result = response.json()
        assert error_result["error_type"] == "file_validation"
        assert "recovery_options" in error_result
    
    def test_validation_error_recovery(self):
        """Test validation error handling"""
        
        # Upload valid file first
        with tempfile.NamedTemporaryFile(suffix='.txt') as tmp_file:
            tmp_file.write(b"Test content")
            tmp_file.seek(0)
            
            response = client.post(
                "/api/upload",
                files={"file": ("test.txt", tmp_file, "text/plain")}
            )
            session_id = response.json()["session_id"]
        
        # Submit invalid metadata
        invalid_metadata = {
            "title": "",  # Required field empty
            "author": "Test Author",
            "version": "invalid-version",  # Invalid format
            "access_level": "invalid_level"  # Invalid option
        }
        
        response = client.post(
            f"/api/sessions/{session_id}/metadata",
            json={
                "template_id": "template-uuid",
                "metadata": invalid_metadata
            }
        )
        
        assert response.status_code == 400
        validation_error = response.json()
        assert validation_error["error_type"] == "validation_error"
        assert len(validation_error["validation_errors"]) >= 3
        assert validation_error["rollback_to"] == "metadata_form"
```

---

## 7. DEPLOYMENT & PRODUCTION SETUP

### 7.1 Docker Configuration

#### Frontend Dockerfile
```dockerfile
# frontend/Dockerfile
FROM node:18-alpine as builder

WORKDIR /app
COPY package*.json ./
RUN npm ci --only=production

COPY . .
RUN npm run build

FROM nginx:alpine
COPY --from=builder /app/out /usr/share/nginx/html
COPY nginx.conf /etc/nginx/nginx.conf

EXPOSE 80
CMD ["nginx", "-g", "daemon off;"]
```

#### Backend Dockerfile  
```dockerfile
# backend/Dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Create non-root user
RUN useradd --create-home --shell /bin/bash app
RUN chown -R app:app /app
USER app

EXPOSE 8000

CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

#### Docker Compose Configuration
```yaml
# docker-compose.yml
version: '3.8'

services:
  frontend:
    build: ./frontend
    ports:
      - "3000:80"
    depends_on:
      - backend
    environment:
      - NEXT_PUBLIC_API_URL=http://localhost:8000
    networks:
      - app-network

  backend:
    build: ./backend
    ports:
      - "8000:8000"
    depends_on:
      - postgres
      - redis
    environment:
      - DATABASE_URL=postgresql://rawclean:password@postgres:5432/rawclean_db
      - REDIS_URL=redis://redis:6379
      - SECRET_KEY=your-secret-key-here
      - UPLOAD_PATH=/app/uploads
      - EXPORT_PATH=/app/exports
    volumes:
      - ./uploads:/app/uploads
      - ./exports:/app/exports
    networks:
      - app-network

  postgres:
    image: postgres:14
    environment:
      - POSTGRES_USER=rawclean
      - POSTGRES_PASSWORD=password
      - POSTGRES_DB=rawclean_db
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5432:5432"
    networks:
      - app-network

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - app-network

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./nginx/ssl:/etc/nginx/ssl
    depends_on:
      - frontend
      - backend
    networks:
      - app-network

volumes:
  postgres_data:
  redis_data:

networks:
  app-network:
    driver: bridge
```

### 7.2 Production Environment Configuration

#### Environment Variables
```bash
# .env.production
# Database
DATABASE_URL=postgresql://user:password@localhost:5432/production_db
REDIS_URL=redis://localhost:6379

# Security
SECRET_KEY=your-super-secret-key-for-production
JWT_SECRET_KEY=your-jwt-secret-key
ALLOWED_HOSTS=yourdomain.com,www.yourdomain.com

# File Storage
UPLOAD_PATH=/var/app/uploads
EXPORT_PATH=/var/app/exports
MAX_FILE_SIZE=52428800  # 50MB
ALLOWED_FILE_TYPES=pdf,docx,txt

# External Services
LLM_API_KEY=your-llm-api-key
EMBEDDING_API_KEY=your-embedding-api-key

# Monitoring
LOG_LEVEL=INFO
SENTRY_DSN=your-sentry-dsn

# Performance
WORKER_PROCESSES=4
MAX_CONCURRENT_UPLOADS=10
REDIS_MAX_CONNECTIONS=20
```

#### Nginx Production Configuration
```nginx
# nginx/nginx.conf
upstream frontend {
    server frontend:80;
}

upstream backend {
    server backend:8000;
}

server {
    listen 80;
    server_name yourdomain.com www.yourdomain.com;
    return 301 https://$server_name$request_uri;
}

server {
    listen 443 ssl http2;
    server_name yourdomain.com www.yourdomain.com;

    ssl_certificate /etc/nginx/ssl/cert.pem;
    ssl_certificate_key /etc/nginx/ssl/key.pem;
    
    # Security headers
    add_header X-Frame-Options DENY;
    add_header X-Content-Type-Options nosniff;
    add_header X-XSS-Protection "1; mode=block";
    add_header Strict-Transport-Security "max-age=31536000; includeSubDomains" always;

    # Frontend
    location / {
        proxy_pass http://frontend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }

    # Backend API
    location /api/ {
        proxy_pass http://backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # File upload settings
        client_max_body_size 50M;
        proxy_connect_timeout 60s;
        proxy_send_timeout 60s;
        proxy_read_timeout 60s;
    }

    # Static files
    location /static/ {
        alias /var/app/static/;
        expires 1y;
        add_header Cache-Control "public, immutable";
    }
}
```

---

## 8. MONITORING & MAINTENANCE

### 8.1 Health Checks & Monitoring
```python
# app/monitoring/health_check.py
from fastapi import APIRouter, Depends
from sqlalchemy.orm import Session
from app.database import get_db
import redis
import psutil
from datetime import datetime
from typing import Dict, Any

router = APIRouter()

@router.get("/health")
async def health_check():
    """Basic health check endpoint"""
    return {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "service": "raw-to-clean-processor"
    }

@router.get("/health/detailed")
async def detailed_health_check(db: Session = Depends(get_db)):
    """Detailed health check with dependencies"""
    
    health_status = {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "checks": {}
    }
    
    # Database check
    try:
        db.execute("SELECT 1")
        health_status["checks"]["database"] = {"status": "healthy"}
    except Exception as e:
        health_status["checks"]["database"] = {
            "status": "unhealthy",
            "error": str(e)
        }
        health_status["status"] = "unhealthy"
    
    # Redis check
    try:
        r = redis.Redis.from_url("redis://localhost:6379")
        r.ping()
        health_status["checks"]["redis"] = {"status": "healthy"}
    except Exception as e:
        health_status["checks"]["redis"] = {
            "status": "unhealthy", 
            "error": str(e)
        }
        health_status["status"] = "unhealthy"
    
    # System resources
    health_status["checks"]["system"] = {
        "cpu_percent": psutil.cpu_percent(),
        "memory_percent": psutil.virtual_memory().percent,
        "disk_percent": psutil.disk_usage('/').percent
    }
    
    return health_status

@router.get("/metrics")
async def get_metrics():
    """Application metrics endpoint"""
    
    # Add application-specific metrics here
    return {
        "processed_documents_total": 0,  # Implement counter
        "active_sessions": 0,            # Implement counter  
        "error_rate": 0.0,              # Implement error tracking
        "avg_processing_time": 0.0,     # Implement timing metrics
        "storage_usage": {
            "uploads_size_mb": 0,
            "exports_size_mb": 0
        }
    }
```

### 8.2 Logging Configuration
```python
# app/logging_config.py
import logging
import sys
from pathlib import Path

def setup_logging(log_level: str = "INFO"):
    """Configure application logging"""
    
    # Create logs directory
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)
    
    # Configure root logger
    logging.basicConfig(
        level=getattr(logging, log_level.upper()),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_dir / 'app.log'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    # Configure specific loggers
    logging.getLogger("uvicorn.error").setLevel(logging.INFO)
    logging.getLogger("uvicorn.access").setLevel(logging.INFO)
    logging.getLogger("sqlalchemy.engine").setLevel(logging.WARNING)
```

---

## 9. PERFORMANCE OPTIMIZATION

### 9.1 Caching Strategy
```python
# app/services/cache_service.py
import redis
import json
import hashlib
from typing import Any, Optional
from functools import wraps

class CacheService:
    def __init__(self, redis_url: str):
        self.redis_client = redis.from_url(redis_url)
        self.default_ttl = 3600  # 1 hour
    
    def get_cache_key(self, prefix: str, *args) -> str:
        """Generate cache key from arguments"""
        key_data = f"{prefix}:{':'.join(str(arg) for arg in args)}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    def get(self, key: str) -> Optional[Any]:
        """Get value from cache"""
        try:
            data = self.redis_client.get(key)
            return json.loads(data) if data else None
        except Exception:
            return None
    
    def set(self, key: str, value: Any, ttl: int = None) -> bool:
        """Set value in cache"""
        try:
            ttl = ttl or self.default_ttl
            serialized = json.dumps(value, default=str)
            return self.redis_client.setex(key, ttl, serialized)
        except Exception:
            return False
    
    def delete(self, key: str) -> bool:
        """Delete key from cache"""
        try:
            return bool(self.redis_client.delete(key))
        except Exception:
            return False

def cache_result(prefix: str, ttl: int = 3600):
    """Decorator to cache function results"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            cache_service = CacheService("redis://localhost:6379")
            
            # Generate cache key
            cache_key = cache_service.get_cache_key(
                prefix, 
                *args, 
                *[f"{k}:{v}" for k, v in sorted(kwargs.items())]
            )
            
            # Try to get from cache
            cached_result = cache_service.get(cache_key)
            if cached_result is not None:
                return cached_result
            
            # Execute function and cache result
            result = await func(*args, **kwargs)
            cache_service.set(cache_key, result, ttl)
            
            return result
        return wrapper
    return decorator
```

### 9.2 Async Processing
```python
# app/services/async_processor.py
import asyncio
from typing import List, Dict, Any
from concurrent.futures import ThreadPoolExecutor
import aiofiles

class AsyncDocumentProcessor:
    def __init__(self, max_workers: int = 4):
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
    
    async def process_multiple_documents(self, 
                                       file_paths: List[str]) -> List[Dict[str, Any]]:
        """Process multiple documents concurrently"""
        
        tasks = []
        for file_path in file_paths:
            task = asyncio.create_task(
                self.process_single_document(file_path)
            )
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Handle exceptions
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                processed_results.append({
                    "file_path": file_paths[i],
                    "status": "error",
                    "error": str(result)
                })
            else:
                processed_results.append(result)
        
        return processed_results
    
    async def process_single_document(self, file_path: str) -> Dict[str, Any]:
        """Process single document asynchronously"""
        
        loop = asyncio.get_event_loop()
        
        # Run CPU-intensive parsing in thread pool
        result = await loop.run_in_executor(
            self.executor,
            self._parse_document_sync,
            file_path
        )
        
        return result
    
    def _parse_document_sync(self, file_path: str) -> Dict[str, Any]:
        """Synchronous document parsing (runs in thread)"""
        # Implementation of parsing logic
        pass
    
    async def save_processed_document(self, 
                                    document_data: Dict[str, Any],
                                    output_path: str) -> str:
        """Save processed document asynchronously"""
        
        async with aiofiles.open(output_path, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(document_data, indent=2))
        
        return output_path
```

---

## 10. SECURITY CONSIDERATIONS

### 10.1 File Upload Security
```python
# app/security/file_validator.py
import magic
import hashlib
from pathlib import Path
from typing import List, Dict, Any

class FileSecurityValidator:
    def __init__(self):
        self.allowed_mime_types = {
            'application/pdf',
            'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
            'text/plain'
        }
        
        self.allowed_extensions = {'.pdf', '.docx', '.txt'}
        self.max_file_size = 50 * 1024 * 1024  # 50MB
        
        # Malicious patterns to check
        self.malicious_patterns = [
            b'<script',
            b'javascript:',
            b'<?php',
            b'<%',
            b'eval(',
            b'exec(',
            b'system(',
            b'shell_exec('
        ]
    
    def validate_file(self, file_path: str, original_filename: str) -> Dict[str, Any]:
        """Comprehensive file validation"""
        
        validation_result = {
            "is_valid": True,
            "errors": [],
            "warnings": [],
            "file_info": {}
        }
        
        file_path_obj = Path(file_path)
        
        # Check file exists
        if not file_path_obj.exists():
            validation_result["is_valid"] = False
            validation_result["errors"].append("File does not exist")
            return validation_result
        
        # Check file size
        file_size = file_path_obj.stat().st_size
        validation_result["file_info"]["size"] = file_size
        
        if file_size > self.max_file_size:
            validation_result["is_valid"] = False
            validation_result["errors"].append(f"File size exceeds {self.max_file_size} bytes")
        
        if file_size == 0:
            validation_result["is_valid"] = False
            validation_result["errors"].append("File is empty")
        
        # Check file extension
        file_extension = file_path_obj.suffix.lower()
        validation_result["file_info"]["extension"] = file_extension
        
        if file_extension not in self.allowed_extensions:
            validation_result["is_valid"] = False
            validation_result["errors"].append(f"File extension {file_extension} not allowed")
        
        # Check MIME type using python-magic
        try:
            mime_type = magic.from_file(file_path, mime=True)
            validation_result["file_info"]["mime_type"] = mime_type
            
            if mime_type not in self.allowed_mime_types:
                validation_result["is_valid"] = False
                validation_result["errors"].append(f"MIME type {mime_type} not allowed")
        except Exception as e:
            validation_result["warnings"].append(f"Could not determine MIME type: {e}")
        
        # Check for malicious content
        malicious_found = self._check_malicious_content(file_path)
        if malicious_found:
            validation_result["is_valid"] = False
            validation_result["errors"].append("Potentially malicious content detected")
        
        # Calculate file hash
        validation_result["file_info"]["sha256"] = self._calculate_file_hash(file_path)
        
        return validation_result
    
    def _check_malicious_content(self, file_path: str) -> bool:
        """Check for malicious patterns in file content"""
        try:
            with open(file_path, 'rb') as f:
                # Read first 1MB to check for malicious patterns
                content = f.read(1024 * 1024)
                
                for pattern in self.malicious_patterns:
                    if pattern in content.lower():
                        return True
            return False
        except Exception:
            return False
    
    def _calculate_file_hash(self, file_path: str) -> str:
        """Calculate SHA256 hash of file"""
        hash_sha256 = hashlib.sha256()
        
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_sha256.update(chunk)
        
        return hash_sha256.hexdigest()
```

---

## 11. FINAL CHECKLIST & GO-LIVE

### 11.1 Pre-Production Checklist

```markdown
## Development Environment Setup âœ…
- [ ] Node.js 18+ installed
- [ ] Python 3.9+ installed  
- [ ] PostgreSQL 14+ running
- [ ] Redis 7+ running
- [ ] Docker & Docker Compose installed
- [ ] Git repository initialized
- [ ] Development dependencies installed

## Core Functionality âœ…
- [ ] File upload component working
- [ ] Document parsing (PDF, DOCX, TXT) implemented
- [ ] Metadata extraction working
- [ ] Validation engine functional
- [ ] Export system operational
- [ ] Error handling implemented
- [ ] Caching system working

## Security âœ…
- [ ] File validation implemented
- [ ] Input sanitization working
- [ ] Authentication ready
- [ ] HTTPS configured
- [ ] Security headers implemented
- [ ] Rate limiting active

## Testing âœ…
- [ ] Unit tests written (80%+ coverage)
- [ ] Integration tests passing
- [ ] Error handling tests passing
- [ ] Performance tests completed
- [ ] Security tests passed

## Production Deployment âœ…
- [ ] Docker images built
- [ ] Environment variables configured
- [ ] Database migrations ready
- [ ] Nginx configuration tested
- [ ] SSL certificates installed
- [ ] Monitoring configured
- [ ] Backup system ready

## Documentation âœ…
- [ ] API documentation complete
- [ ] User guide written
- [ ] Deployment guide ready
- [ ] Troubleshooting guide available
```

### 11.2 Go-Live Steps
```bash
# 1. Build and deploy
docker-compose -f docker-compose.prod.yml build
docker-compose -f docker-compose.prod.yml up -d

# 2. Run database migrations
docker-compose exec backend alembic upgrade head

# 3. Load initial data (templates, validation rules)
docker-compose exec backend python scripts/load_initial_data.py

# 4. Verify deployment
curl https://yourdomain.com/api/health
curl https://yourdomain.com/api/health/detailed

# 5. Run smoke tests
docker-compose exec backend python -m pytest tests/smoke_tests/

# 6. Monitor logs
docker-compose logs -f backend
```

---

## 12. SUPPORT & MAINTENANCE

### 12.1 Common Issues & Solutions

| Issue | Symptoms | Solution |
|-------|----------|----------|
| **File Upload Fails** | 413 Request Entity Too Large | Increase nginx `client_max_body_size` |
| **Parsing Timeout** | 504 Gateway Timeout | Increase `proxy_read_timeout` in nginx |
| **Memory Issues** | High memory usage | Implement file streaming, reduce batch sizes |
| **Database Locks** | Slow queries | Add database indexes, optimize queries |
| **Redis Connection** | Cache misses | Check Redis connectivity, increase connection pool |

### 12.2 Maintenance Schedule
```markdown
## Daily âœ…
- [ ] Check application logs for errors
- [ ] Monitor system resources (CPU, RAM, Disk)
- [ ] Verify backup completion

## Weekly âœ…  
- [ ] Review error rates and performance metrics
- [ ] Clean up temporary files
- [ ] Update security patches

## Monthly âœ…
- [ ] Performance optimization review
- [ ] Database maintenance (VACUUM, REINDEX)
- [ ] Security audit
- [ ] Documentation updates
```

---

**ÄÃ¢y lÃ  báº£n dá»± tháº£o Ä‘áº§u tiÃªn cá»§a tÃ i liá»‡u thiáº¿t káº¿ module FR-03.1. Má»i báº¡n xem qua vÃ  cho tÃ´i biáº¿t náº¿u cÃ³ báº¥t ká»³ Ä‘iá»ƒm nÃ o cáº§n chá»‰nh sá»­a, lÃ m rÃµ hoáº·c bá»• sung thÃªm nhÃ©.**
