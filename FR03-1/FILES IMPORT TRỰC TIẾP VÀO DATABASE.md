Dá»±a trÃªn cáº¥u trÃºc export FR03.1 vÃ  schema FR-02.1 v2.0, Ä‘Ã¢y lÃ  phÃ¢n loáº¡i xá»­ lÃ½ tá»«ng loáº¡i file:

## ğŸ“¥ **FILES IMPORT TRá»°C TIáº¾P VÃ€O DATABASE**

### **PostgreSQL Direct Import**
```bash
# Import ngay vÃ o báº£ng mÃ  khÃ´ng cáº§n xá»­ lÃ½ thÃªm
FOR_DATABASE/document_metadata_v2.json     â†’ documents_metadata_v2 table
FOR_DATABASE/ingestion_job_metadata.json   â†’ data_ingestion_jobs table
FOR_DATABASE/database_ready_check.json     â†’ chunk_processing_logs table (status)
```

**SQL Import Example:**
```sql
-- Direct JSON import
INSERT INTO documents_metadata_v2 
SELECT * FROM json_populate_recordset(null::documents_metadata_v2, 
    pg_read_file('/import/document_metadata_v2.json')::json);
```

## âš™ï¸ **FILES Cáº¦N Xá»¬ LÃ TRÆ¯á»šC KHI IMPORT**

### **1. Enhanced Processing Required**
```bash
# Cáº§n enrich thÃªm data trÆ°á»›c khi insert
FOR_DATABASE/chunks_enhanced_v2.jsonl     â†’ document_chunks_enhanced table
FOR_DATABASE/vietnamese_analysis_v2.json  â†’ vietnamese_text_analysis table  
FOR_SEARCH/search_document.json          â†’ Update search_tokens fields
```

**Processing Pipeline:**
```python
# chunks_enhanced_v2.jsonl processing
def process_chunks_for_database(chunks_file):
    chunks = read_jsonl(chunks_file)
    
    for chunk in chunks:
        # Generate BM25 tokens
        chunk['bm25_tokens'] = generate_tsvector(chunk['chunk_content'])
        
        # Calculate overlap references
        chunk['overlap_source_prev'] = find_overlap_source(chunk, 'prev')
        chunk['overlap_source_next'] = find_overlap_source(chunk, 'next')
        
        # Validate embedding dimensions
        if chunk['embedding_dimensions'] != 1024:
            chunk['embedding_dimensions'] = 1024  # Qwen default
    
    return chunks

# vietnamese_analysis processing  
def process_vietnamese_analysis(analysis_file):
    analysis = read_json(analysis_file)
    
    # Normalize quality scores to database format
    analysis['language_quality_score'] = round(analysis['vietnamese_specific']['language_purity'] * 10, 1)
    analysis['diacritics_density'] = analysis['vietnamese_specific']['diacritics_density'] 
    analysis['token_diversity'] = calculate_token_diversity(analysis['word_segmentation'])
    
    return analysis
```

### **2. Vector Database Processing**
```bash
# Xá»­ lÃ½ Ä‘á»ƒ táº¡o embeddings vÃ  insert vÃ o ChromaDB  
FOR_VECTOR_DB/embeddings_preparation.json â†’ ChromaDB collections
FOR_VECTOR_DB/similarity_features.json   â†’ Deduplication processing
```

**ChromaDB Processing:**
```python
def process_for_chroma(embeddings_prep_file):
    data = read_json(embeddings_prep_file)
    
    # Generate embeddings using Qwen model
    embedding_model = SentenceTransformer('Qwen/Qwen3-Embedding-0.6B')
    
    chroma_docs = []
    for item in data['chunks']:
        # Generate 1024-dim embeddings
        embedding = embedding_model.encode(item['content']).tolist()
        
        chroma_doc = {
            'id': item['chunk_id'],
            'embedding': embedding,
            'document': item['content'],
            'metadata': {
                'document_id': item['document_id'],
                'access_level': item['access_level'],
                'department': item['department'],
                'language': 'vi',
                'quality_score': item.get('chunk_quality_score', 0.8)
            }
        }
        chroma_docs.append(chroma_doc)
    
    return chroma_docs
```

## ğŸš« **FILES KHÃ”NG ÄÆ¯á»¢C Xá»¬ LÃ/IMPORT**

### **Reference Only Files**
```bash
# Chá»‰ Ä‘á»ƒ tham kháº£o, khÃ´ng import vÃ o database
manifest.json                    # Package summary only
user_info.json                   # Creator context only  
processed/document.md             # Human-readable format only
signatures/file_fingerprints.json # File integrity checking only
signatures/content_signatures.json # Content deduplication only
validation/processing_stats.json   # Processing metrics only
FOR_SEARCH/search_config.json     # Elasticsearch config only
```

### **Archive Files** 
```bash
# LÆ°u trá»¯ tham chiáº¿u, khÃ´ng active processing
signatures/semantic_features.json # Semantic analysis archive
validation/quality_score.json    # Quality assessment archive (used v2 instead)
```

## ğŸ“ **STRATEGIC FILE STORAGE**

### **1. Database Storage (PostgreSQL)**
```sql
-- File gá»‘c reference trong database
ALTER TABLE documents_metadata_v2 ADD COLUMN file_storage_info JSONB DEFAULT '{}';

-- Example storage info
{
  "original_file": {
    "path": "/storage/original/HR_POLICY_20250912_143022/original/employee_handbook.pdf",
    "size_bytes": 2847392,
    "checksum": "sha256:abc123...",
    "storage_tier": "warm"
  },
  "processed_files": {
    "export_package": "/storage/processed/HR_POLICY_20250912_143022.zip",
    "extraction_timestamp": "2025-09-12T14:30:22Z"
  }
}
```

### **2. File System Storage Strategy**

#### **Hot Storage (SSD) - Active Access**
```bash
/opt/chatbot-data/
â”œâ”€â”€ processed/           # Processed packages ready for import
â”‚   â”œâ”€â”€ pending/         # Awaiting import
â”‚   â”œâ”€â”€ processing/      # Currently being imported  
â”‚   â””â”€â”€ completed/       # Successfully imported
â”œâ”€â”€ temp/               # Temporary processing space
â””â”€â”€ failed/             # Failed import packages
```

#### **Warm Storage (HDD) - Archive Access**
```bash
/opt/chatbot-archive/
â”œâ”€â”€ original/                    # Original uploaded files
â”‚   â””â”€â”€ {DEPT}/{YYYY-MM}/       # Organized by department and date
â”‚       â””â”€â”€ {DEPT}_{TYPE}_{TIMESTAMP}/
â”‚           â””â”€â”€ original/        # Original file preservation
â””â”€â”€ export-packages/            # Complete FR03.1 export packages
    â””â”€â”€ {DEPT}_{TYPE}_{TIMESTAMP}.zip
```

#### **Cold Storage (Object Storage) - Long-term Archive**
```bash
# AWS S3 or equivalent
s3://chatbot-archive/
â”œâ”€â”€ original-files/
â”‚   â””â”€â”€ {year}/{month}/{DEPT}_{TYPE}_{TIMESTAMP}/
â””â”€â”€ export-packages/
    â””â”€â”€ {year}/{month}/{DEPT}_{TYPE}_{TIMESTAMP}.zip
```

### **3. Storage Lifecycle Policy**
```python
STORAGE_POLICY = {
    "hot_storage": {
        "location": "/opt/chatbot-data/",
        "retention_days": 30,
        "purpose": "active processing and recent access"
    },
    "warm_storage": {
        "location": "/opt/chatbot-archive/", 
        "retention_months": 12,
        "purpose": "reference and compliance"
    },
    "cold_storage": {
        "location": "s3://chatbot-archive/",
        "retention_years": 7,
        "purpose": "legal compliance and disaster recovery"
    }
}
```

## ğŸ”„ **IMPORT WORKFLOW SEQUENCE**

### **1. Pre-Import Validation**
```python
def validate_export_package(zip_path):
    checks = {
        "manifest_exists": check_file_exists(zip_path, "manifest.json"),
        "database_files_complete": check_database_files(zip_path),
        "quality_thresholds_met": validate_quality_scores(zip_path),
        "vietnamese_processing_done": check_vietnamese_analysis(zip_path),
        "chunk_integrity": validate_chunk_consistency(zip_path)
    }
    return all(checks.values()), checks
```

### **2. Import Sequence**
```bash
# Step 1: Extract and validate package
extract_package() â†’ /opt/chatbot-data/processing/{job_id}/

# Step 2: Direct database imports
import_job_metadata() â†’ data_ingestion_jobs table
import_document_metadata() â†’ documents_metadata_v2 table  

# Step 3: Processed imports  
process_and_import_chunks() â†’ document_chunks_enhanced table
process_and_import_analysis() â†’ vietnamese_text_analysis table

# Step 4: Vector database import
generate_embeddings_and_import() â†’ ChromaDB collections

# Step 5: Search index update
update_search_vectors() â†’ PostgreSQL TSVECTOR fields

# Step 6: Move to completed storage
archive_package() â†’ /opt/chatbot-data/completed/
```

### **3. Error Handling & Rollback**
```python
def rollback_import(job_id):
    # Remove database entries
    delete_from_tables(['documents_metadata_v2', 'document_chunks_enhanced'], 
                      where={'ingestion_job_id': job_id})
    
    # Remove from ChromaDB
    chroma_collection.delete(where={"job_id": job_id})
    
    # Move package to failed directory
    move_package(f"/opt/chatbot-data/processing/{job_id}/", 
                f"/opt/chatbot-data/failed/{job_id}/")
```

Cáº¥u trÃºc nÃ y Ä‘áº£m báº£o hiá»‡u quáº£ xá»­ lÃ½, kháº£ nÄƒng trace back Ä‘áº§y Ä‘á»§ vÃ  tuÃ¢n thá»§ compliance requirements trong khi tá»‘i Æ°u storage costs.
