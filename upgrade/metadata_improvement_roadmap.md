# L·ªò TR√åNH C·∫¢I THI·ªÜN METADATA ‚Äî T·ª™ G·ªêC R·ªÑ ƒê·∫æN ƒê·ªòT PH√Å
## ATTECH RAG Knowledge Assistant ‚Äî Gi·∫£i quy·∫øt "n·ª£ k·ªπ thu·∫≠t" metadata

**Ng√†y l·∫≠p:** 12/02/2026  
**Ng∆∞·ªùi th·ª±c hi·ªán:** Technical Lead  
**Th·ªùi gian t·ªïng:** 6 tu·∫ßn (chia 4 giai ƒëo·∫°n)  

---

## 1. CH·∫®N ƒêO√ÅN: V·∫§N ƒê·ªÄ TH·ª∞C S·ª∞ L√Ä G√å?

### V√≤ng l·∫∑p b·∫ø t·∫Øc hi·ªán t·∫°i

```
Upload docs ‚Üí metadata tr·ªëng/sai ‚Üí s·ª≠a th·ªß c√¥ng (nh∆∞ng kh√¥ng k·ªãp)
     ‚Üì                                        ‚Üì
Graph RAG tr·ªëng (0 edges)              Metadata Search v√¥ d·ª•ng
     ‚Üì                                        ‚Üì
Kh√¥ng c√≥ cross-reference              Hybrid Search thi·∫øu 1 ch√¢n
     ‚Üì                                        ‚Üì
RAG ch·ªâ d·ª±a v√†o Semantic Search ‚Üí accuracy b·ªã gi·ªõi h·∫°n ~75%
```

### B·∫£ng hi·ªán tr·∫°ng metadata (d·ª±a tr√™n project knowledge)

| Field trong JSONB | C√≥ d·ªØ li·ªáu? | Ch·∫•t l∆∞·ª£ng | ·∫¢nh h∆∞·ªüng |
|---|---|---|---|
| `identification.document_number` | ~60% docs | Trung b√¨nh | Substring/Metadata Search |
| `identification.document_type` | ~70% docs | T·ªët | Filter, routing |
| `identification.issue_date` | ~30% docs | K√©m ‚Äî nhi·ªÅu NULL | Timeline, freshness ranking |
| `authority.organization` | ~20% docs | R·∫•t k√©m ‚Äî h·∫ßu h·∫øt "General" | Graph RAG, filter |
| `authority.department` | ~10% docs | R·∫•t k√©m | RBAC, department filter |
| `domain.category` | ~50% docs | Trung b√¨nh | Graph RAG edges, routing |
| `domain.keywords` | ~30% docs | K√©m ‚Äî nhi·ªÅu empty arrays | Graph RAG edges, search boost |
| `relationships.based_on` | ~15% docs | R·∫•t k√©m ‚Äî ch·ª©a text thay v√¨ law_id | Graph RAG ‚Äî BLOCKED |
| `relationships.relates_to` | ~5% docs | G·∫ßn nh∆∞ tr·ªëng | Graph RAG ‚Äî BLOCKED |
| `hierarchy.parent_id` | ~10% docs | Sai format (string thay v√¨ UUID) | Parent-child links ‚Äî BROKEN |
| `financial.budget` | ~40% docs | Trung b√¨nh | Filter (√≠t d√πng) |

### G·ªëc r·ªÖ: Metadata k√©m KH√îNG ph·∫£i v√¨ thi·∫øu schema

Schema metadata JSONB ƒë√£ thi·∫øt k·∫ø t·ªët (v3.0/v3.1 v·ªõi identification, authority, domain, 
relationships, financial, content_stats...). V·∫•n ƒë·ªÅ n·∫±m ·ªü 3 ch·ªó:

1. **FR-03.1 (Document Processing)** ‚Äî document.json ƒë∆∞·ª£c t·∫°o v·ªõi metadata s∆° s√†i. 
   Extraction logic d·ª±a v√†o regex ƒë∆°n gi·∫£n, kh√¥ng ƒë·ªß cho Vietnamese legal docs.

2. **Kh√¥ng c√≥ validation gate** ‚Äî Documents ƒëi th·∫≥ng t·ª´ FR-03.1 ‚Üí FR-03.3 (import) 
   m√† kh√¥ng ki·ªÉm tra metadata completeness. FR-03.2 Quality Control l√† mock service.

3. **Kh√¥ng c√≥ enrichment pipeline** ‚Äî Sau khi import, kh√¥ng c√≥ c∆° ch·∫ø t·ª± ƒë·ªông b·ªï sung 
   metadata t·ª´ content analysis.

---

## 2. NGUY√äN T·∫ÆC TI·∫æP C·∫¨N

### Kh√¥ng c·ªë s·ª≠a h·∫øt metadata c√πng l√∫c

V·ªõi 42 docs hi·ªán t·∫°i th√¨ s·ª≠a tay ƒë∆∞·ª£c. Nh∆∞ng target 100K docs th√¨ b·∫Øt bu·ªôc ph·∫£i t·ª± ƒë·ªông.
L·ªô tr√¨nh ƒëi t·ª´ "manual fix cho data hi·ªán c√≥" ‚Üí "semi-auto cho data m·ªõi" ‚Üí "full-auto pipeline".

### ∆Øu ti√™n theo impact l√™n search quality

Kh√¥ng ph·∫£i field n√†o c≈©ng quan tr·ªçng ngang nhau. Th·ª© t·ª± impact:

```
CRITICAL (tr·ª±c ti·∫øp ·∫£nh h∆∞·ªüng search):
  1. document_number (law_id) ‚Äî Substring Search, Metadata Search
  2. keywords                 ‚Äî Graph RAG edges, search boosting
  3. category                 ‚Äî Graph RAG edges, query routing
  4. issue_date               ‚Äî Freshness ranking, timeline queries
  5. relationships.based_on   ‚Äî Graph RAG cross-reference

HIGH (·∫£nh h∆∞·ªüng filtering/RBAC):
  6. organization             ‚Äî Authority filter, Graph RAG
  7. department               ‚Äî RBAC filtering
  8. document_type            ‚Äî Query routing

MEDIUM (nice-to-have):
  9. parent_id hierarchy      ‚Äî Parent-child navigation
  10. financial data          ‚Äî Specialized queries
```

---

## 3. L·ªò TR√åNH 4 GIAI ƒêO·∫†N

```
Tu·∫ßn 1-2          Tu·∫ßn 3           Tu·∫ßn 4           Tu·∫ßn 5-6
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Gƒê 1     ‚îÇ    ‚îÇ Gƒê 2     ‚îÇ    ‚îÇ Gƒê 3     ‚îÇ    ‚îÇ Gƒê 4     ‚îÇ
‚îÇ Audit &  ‚îÇ ‚Üí  ‚îÇ Auto     ‚îÇ ‚Üí  ‚îÇ T√≠ch h·ª£p ‚îÇ ‚Üí  ‚îÇ Graph    ‚îÇ
‚îÇ Manual   ‚îÇ    ‚îÇ Enrichment‚îÇ    ‚îÇ v√†o Search‚îÇ    ‚îÇ RAG Live ‚îÇ
‚îÇ Fix      ‚îÇ    ‚îÇ Pipeline  ‚îÇ    ‚îÇ Algorithms‚îÇ    ‚îÇ          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
  N·ªÅn t·∫£ng        T·ª± ƒë·ªông h√≥a     Khai th√°c        ƒê·ªôt ph√°
```

---

### GIAI ƒêO·∫†N 1: AUDIT & MANUAL FIX (Tu·∫ßn 1-2)

**M·ª•c ti√™u:** Bi·∫øt ch√≠nh x√°c hi·ªán tr·∫°ng, s·ª≠a data hi·ªán c√≥, t·∫°o "ground truth" cho testing

#### B∆∞·ªõc 1.1: Ch·∫°y Metadata Audit Script (Ng√†y 1)

T·∫°o script ƒë√°nh gi√° completeness cho to√†n b·ªô database:

```python
# scripts/audit_metadata_completeness.py
"""
Audit to√†n b·ªô metadata trong documents_metadata_v2.
Output: B√°o c√°o chi ti·∫øt t·ª´ng document, t·ªïng h·ª£p theo field.
"""

import asyncio
import asyncpg
import json
from collections import defaultdict

CRITICAL_FIELDS = {
    # path trong JSONB ‚Üí t√™n hi·ªÉn th·ªã ‚Üí validator function
    ("identification", "document_number"): {
        "name": "S·ªë hi·ªáu vƒÉn b·∫£n",
        "validate": lambda v: bool(v) and len(str(v).strip()) > 2,
        "impact": "CRITICAL"
    },
    ("identification", "document_type"): {
        "name": "Lo·∫°i vƒÉn b·∫£n", 
        "validate": lambda v: bool(v) and v not in ("unknown", "other", ""),
        "impact": "HIGH"
    },
    ("identification", "issue_date"): {
        "name": "Ng√†y ban h√†nh",
        "validate": lambda v: bool(v) and v != "null" and len(str(v)) >= 8,
        "impact": "CRITICAL"
    },
    ("authority", "organization"): {
        "name": "C∆° quan ban h√†nh",
        "validate": lambda v: bool(v) and v.lower() not in ("general", "unknown", ""),
        "impact": "HIGH"
    },
    ("authority", "department"): {
        "name": "Ph√≤ng ban",
        "validate": lambda v: bool(v) and v.lower() not in ("general", "unknown", ""),
        "impact": "HIGH"
    },
    ("domain", "category"): {
        "name": "Lƒ©nh v·ª±c",
        "validate": lambda v: bool(v) and v not in ("unknown", "other", ""),
        "impact": "CRITICAL"
    },
    ("domain", "keywords"): {
        "name": "T·ª´ kh√≥a",
        "validate": lambda v: isinstance(v, list) and len(v) >= 2,
        "impact": "CRITICAL"
    },
    ("relationships", "based_on"): {
        "name": "CƒÉn c·ª© ph√°p l√Ω",
        "validate": lambda v: isinstance(v, list) and len(v) > 0,
        "impact": "CRITICAL"
    },
    ("relationships", "relates_to"): {
        "name": "VƒÉn b·∫£n li√™n quan",
        "validate": lambda v: isinstance(v, list) and len(v) > 0,
        "impact": "MEDIUM"
    },
}

async def audit_metadata():
    conn = await asyncpg.connect(
        host="192.168.1.70", port=5432,
        database="knowledge_base_v2",
        user="kb_admin", password="1234567890"
    )
    
    rows = await conn.fetch("""
        SELECT document_id, title, metadata 
        FROM documents_metadata_v2 
        ORDER BY title
    """)
    
    # Per-document report
    doc_reports = []
    # Aggregate stats
    field_stats = defaultdict(lambda: {"total": 0, "valid": 0, "invalid_docs": []})
    
    for row in rows:
        meta = json.loads(row["metadata"]) if row["metadata"] else {}
        doc_report = {
            "document_id": str(row["document_id"]),
            "title": row["title"][:80],
            "missing_fields": [],
            "invalid_fields": [],
            "completeness_score": 0
        }
        
        valid_count = 0
        total_fields = len(CRITICAL_FIELDS)
        
        for (section, field), config in CRITICAL_FIELDS.items():
            field_key = f"{section}.{field}"
            stat = field_stats[field_key]
            stat["total"] += 1
            
            # Extract value
            value = None
            if section in meta and isinstance(meta[section], dict):
                value = meta[section].get(field)
            
            # Validate
            is_valid = config["validate"](value)
            if is_valid:
                stat["valid"] += 1
                valid_count += 1
            else:
                stat["invalid_docs"].append(row["title"][:50])
                if value is None or value == "" or value == []:
                    doc_report["missing_fields"].append(config["name"])
                else:
                    doc_report["invalid_fields"].append(
                        f"{config['name']}: '{value}'"
                    )
        
        doc_report["completeness_score"] = round(
            valid_count / total_fields * 100, 1
        )
        doc_reports.append(doc_report)
    
    await conn.close()
    
    # Print report
    print("=" * 80)
    print("METADATA COMPLETENESS AUDIT REPORT")
    print(f"Total documents: {len(rows)}")
    print("=" * 80)
    
    # Field-level summary
    print("\nüìä FIELD COMPLETENESS SUMMARY:\n")
    print(f"{'Field':<35} {'Valid':<8} {'Total':<8} {'Rate':<8} {'Impact'}")
    print("-" * 75)
    
    for (section, field), config in CRITICAL_FIELDS.items():
        key = f"{section}.{field}"
        stat = field_stats[key]
        rate = stat["valid"] / max(stat["total"], 1) * 100
        status = "‚úÖ" if rate >= 80 else "‚ö†Ô∏è" if rate >= 50 else "‚ùå"
        print(
            f"{status} {config['name']:<32} "
            f"{stat['valid']:<8} {stat['total']:<8} "
            f"{rate:>5.1f}%   {config['impact']}"
        )
    
    # Per-document detail
    print(f"\nüìã DOCUMENTS BY COMPLETENESS:\n")
    doc_reports.sort(key=lambda x: x["completeness_score"])
    
    for doc in doc_reports:
        score = doc["completeness_score"]
        status = "‚úÖ" if score >= 80 else "‚ö†Ô∏è" if score >= 50 else "‚ùå"
        print(f"{status} [{score:>5.1f}%] {doc['title']}")
        if doc["missing_fields"]:
            print(f"         Missing: {', '.join(doc['missing_fields'])}")
        if doc["invalid_fields"]:
            print(f"         Invalid: {', '.join(doc['invalid_fields'][:3])}")
    
    # Overall score
    avg_score = sum(d["completeness_score"] for d in doc_reports) / len(doc_reports)
    print(f"\n{'=' * 80}")
    print(f"OVERALL METADATA COMPLETENESS: {avg_score:.1f}%")
    print(f"Target: ‚â•80%")
    print(f"{'=' * 80}")
    
    return doc_reports, field_stats

if __name__ == "__main__":
    asyncio.run(audit_metadata())
```

**Output mong ƒë·ª£i:** Bi·∫øt ch√≠nh x√°c field n√†o thi·∫øu bao nhi√™u %, document n√†o t·ªá nh·∫•t.

#### B∆∞·ªõc 1.2: Manual Fix cho 42 documents hi·ªán c√≥ (Ng√†y 2-5)

V√¨ ch·ªâ c√≥ 42 docs, manual fix l√† feasible v√† t·∫°o ra "gold standard" dataset.

**Ph∆∞∆°ng ph√°p: D√πng Metadata Editor ƒë√£ c√≥** (port 8005) k·∫øt h·ª£p SQL scripts.

```sql
-- FIX 1: Department ‚Äî thay "General" b·∫±ng gi√° tr·ªã ƒë√∫ng
-- Tr√≠ch t·ª´ authority.organization ho·∫∑c content analysis
UPDATE documents_metadata_v2
SET metadata = jsonb_set(
    metadata,
    '{authority,department}',
    '"Ph√≤ng Nghi√™n c·ª©u Ph√°t tri·ªÉn"'::jsonb
)
WHERE title ILIKE '%DTCT%' OR title ILIKE '%nghi√™n c·ª©u%';

-- FIX 2: Organization ‚Äî thay "General" 
UPDATE documents_metadata_v2
SET metadata = jsonb_set(
    metadata,
    '{authority,organization}',
    '"C√¥ng ty ATTECH"'::jsonb
)
WHERE metadata->'authority'->>'organization' IN ('General', '', NULL);

-- FIX 3: Issue date ‚Äî tr√≠ch t·ª´ document_number pattern
-- V√≠ d·ª•: "265/2025/Nƒê-CP" ‚Üí year 2025
UPDATE documents_metadata_v2
SET metadata = jsonb_set(
    metadata,
    '{identification,issue_date}',
    ('"2025-01-01"')::jsonb
)
WHERE metadata->'identification'->>'document_number' LIKE '%/2025/%'
  AND (metadata->'identification'->>'issue_date') IS NULL;

-- FIX 4: Relationships ‚Äî fix governing_laws ch·ª©a text thay v√¨ law_id
-- Ph·∫£i l√†m t·ª´ng doc v√¨ m·ªói doc c√≥ based_on kh√°c nhau
-- D√πng Metadata Editor UI cho ph·∫ßn n√†y

-- FIX 5: Parent ID ‚Äî convert string ‚Üí UUID
UPDATE documents_metadata_v2 d1
SET metadata = jsonb_set(
    d1.metadata,
    '{hierarchy,parent_id}',
    to_jsonb(d2.document_id::text)
)
FROM documents_metadata_v2 d2
WHERE d1.metadata->'hierarchy'->>'parent_id' = 
      d2.metadata->'identification'->>'document_number'
  AND d1.metadata->'hierarchy'->>'parent_id' IS NOT NULL;
```

**Checklist cho m·ªói document:**
- [ ] document_number ch√≠nh x√°c (VD: "265/2025/Nƒê-CP")
- [ ] document_type ƒë√∫ng (nghi_dinh, quyet_dinh, thong_tu, luat...)
- [ ] issue_date c√≥ (YYYY-MM-DD format)
- [ ] organization l√† t√™n c∆° quan th·ª±c (kh√¥ng ph·∫£i "General")
- [ ] category ƒë√∫ng lƒ©nh v·ª±c (tai_chinh, lao_dong, hanh_chinh...)
- [ ] keywords c√≥ √≠t nh·∫•t 3-5 t·ª´ kh√≥a relevant
- [ ] based_on ch·ª©a law_id (kh√¥ng ph·∫£i text snippet)

#### B∆∞·ªõc 1.3: Ch·∫°y l·∫°i Graph RAG sau khi fix (Ng√†y 5-6)

```bash
# Sau khi metadata ƒë√£ clean:
python populate_graph_correct.py    # Sync docs ‚Üí graph_documents
python create_semantic_links.py     # T·∫°o edges d·ª±a tr√™n metadata m·ªõi
python validate_graph_links.py      # Verify

# K·ª≥ v·ªçng: t·ª´ 0 edges ‚Üí 300-500 edges (g·∫ßn target 507)
```

#### B∆∞·ªõc 1.4: T·∫°o test dataset (Ng√†y 6-7)

```python
# scripts/create_metadata_test_dataset.py
"""
T·∫°o 20 test queries + expected metadata filters ƒë·ªÉ ƒëo impact.
"""

TEST_QUERIES = [
    {
        "query": "quy ƒë·ªãnh ngh·ªâ ph√©p nƒÉm",
        "expected_metadata": {
            "category": "lao_dong",
            "document_type": "luat",
            "keywords_should_contain": ["ngh·ªâ ph√©p", "lao ƒë·ªông"]
        },
        "expected_top3_law_ids": ["45/2019/QH14"]  # B·ªô lu·∫≠t Lao ƒë·ªông
    },
    {
        "query": "265/2025/Nƒê-CP",
        "expected_metadata": {
            "document_number": "265/2025/Nƒê-CP"
        },
        "expected_top1_exact": True
    },
    {
        "query": "quy ƒë·ªãnh an to√†n h√†ng kh√¥ng do B·ªô GTVT ban h√†nh",
        "expected_metadata": {
            "category": "hang_khong",
            "organization": "B·ªô Giao th√¥ng V·∫≠n t·∫£i"
        }
    },
    # ... 17 queries n·ªØa covering c√°c scenarios
]
```

**Deliverable Giai ƒëo·∫°n 1:**
- ‚úÖ Audit report: bi·∫øt ch√≠nh x√°c % completeness m·ªói field
- ‚úÖ 42 documents c√≥ metadata clean (‚â•80% completeness)
- ‚úÖ Graph RAG c√≥ edges th·ª±c (300-500 edges)
- ‚úÖ Test dataset 20 queries ƒë·ªÉ ƒëo improvement

---

### GIAI ƒêO·∫†N 2: AUTO-ENRICHMENT PIPELINE (Tu·∫ßn 3)

**M·ª•c ti√™u:** Khi import document m·ªõi, metadata ƒë∆∞·ª£c t·ª± ƒë·ªông tr√≠ch xu·∫•t v√† b·ªï sung ‚Äî 
kh√¥ng c·∫ßn s·ª≠a tay n·ªØa.

#### Ki·∫øn tr√∫c: Metadata Enrichment Service

```
TR∆Ø·ªöC (hi·ªán t·∫°i):
FR-03.1 t·∫°o document.json (metadata s∆° s√†i)
    ‚Üí FR-03.3 import nguy√™n v√†o DB
    ‚Üí metadata tr·ªëng ‚Üí s·ª≠a tay

SAU (target):
FR-03.1 t·∫°o document.json (metadata s∆° s√†i)
    ‚Üí ‚≠ê MetadataEnricher (auto-extract t·ª´ content)
    ‚Üí ‚≠ê MetadataValidator (ki·ªÉm tra completeness)
    ‚Üí FR-03.3 import v√†o DB (metadata ƒë·∫ßy ƒë·ªß)
    ‚Üí Graph links t·ª± ƒë·ªông t·∫°o
```

#### V·ªã tr√≠ trong codebase

```
FR-03.3/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metadata/                    # ‚≠ê TH∆Ø M·ª§C M·ªöI
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ enricher.py              # Auto-extraction t·ª´ content
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ validator.py             # Ki·ªÉm tra completeness
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ legal_code_extractor.py  # Extract law_id patterns
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ keyword_extractor.py     # Vietnamese keyword extraction
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ search/                      # C√≥ s·∫µn
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ database/                    # C√≥ s·∫µn
‚îÇ   ‚îî‚îÄ‚îÄ ...
```

#### B∆∞·ªõc 2.1: Legal Code Extractor (Ng√†y 1-2)

ƒê√¢y l√† component c√≥ ROI cao nh·∫•t ‚Äî tr√≠ch xu·∫•t ch√≠nh x√°c m√£ s·ªë vƒÉn b·∫£n ph√°p lu·∫≠t.

```python
# src/core/metadata/legal_code_extractor.py
"""
Tr√≠ch xu·∫•t m√£ vƒÉn b·∫£n ph√°p lu·∫≠t t·ª´ n·ªôi dung Vietnamese legal documents.

Patterns recognized:
  - Lu·∫≠t s·ªë: 45/2019/QH14
  - Ngh·ªã ƒë·ªãnh: 265/2025/Nƒê-CP, 76/2018/Nƒê-CP
  - Th√¥ng t∆∞: 01/2024/TT-BTC
  - Quy·∫øt ƒë·ªãnh: 737/Qƒê-CQƒêHQ
  - C√¥ng vƒÉn: 1234/CV-VPCP
  - Ngh·ªã quy·∫øt: 01/NQ-CP
"""

import re
from typing import List, Dict, Optional
from dataclasses import dataclass


@dataclass
class LegalReference:
    """M·ªôt tham chi·∫øu vƒÉn b·∫£n ph√°p lu·∫≠t ƒë∆∞·ª£c tr√≠ch xu·∫•t."""
    law_id: str                    # "265/2025/Nƒê-CP"
    law_type: str                  # "nghi_dinh"
    issuing_body_code: str         # "CP" (Ch√≠nh ph·ªß)
    issuing_body_full: str         # "Ch√≠nh ph·ªß"
    year: Optional[int]            # 2025
    context: str                   # C√¢u ch·ª©a reference
    position: int                  # V·ªã tr√≠ trong text


# Mapping m√£ c∆° quan ‚Üí t√™n ƒë·∫ßy ƒë·ªß
ISSUING_BODY_MAP = {
    "QH": "Qu·ªëc h·ªôi",
    "CP": "Ch√≠nh ph·ªß",
    "TTg": "Th·ªß t∆∞·ªõng Ch√≠nh ph·ªß",
    "BTC": "B·ªô T√†i ch√≠nh",
    "BGTVT": "B·ªô Giao th√¥ng V·∫≠n t·∫£i",
    "BCA": "B·ªô C√¥ng an",
    "BQP": "B·ªô Qu·ªëc ph√≤ng",
    "BKHCN": "B·ªô Khoa h·ªçc v√† C√¥ng ngh·ªá",
    "BLƒêTBXH": "B·ªô Lao ƒë·ªông Th∆∞∆°ng binh v√† X√£ h·ªôi",
    "BTNMT": "B·ªô T√†i nguy√™n v√† M√¥i tr∆∞·ªùng",
    "BCT": "B·ªô C√¥ng Th∆∞∆°ng",
    "BXD": "B·ªô X√¢y d·ª±ng",
    "BNN": "B·ªô N√¥ng nghi·ªáp",
    "BYT": "B·ªô Y t·∫ø",
    "BGDƒêT": "B·ªô Gi√°o d·ª•c v√† ƒê√†o t·∫°o",
    "VPCP": "VƒÉn ph√≤ng Ch√≠nh ph·ªß",
    "CQƒêHQ": "C∆° quan ƒê·∫°i di·ªán H√†ng kh√¥ng",
    # ... th√™m theo nhu c·∫ßu ATTECH
}

# Mapping prefix ‚Üí lo·∫°i vƒÉn b·∫£n
LAW_TYPE_MAP = {
    "Nƒê": "nghi_dinh",
    "TT": "thong_tu",
    "Qƒê": "quyet_dinh",
    "NQ": "nghi_quyet",
    "CV": "cong_van",
    "CT": "chi_thi",
    "QH": "luat",
    "VBHN": "van_ban_hop_nhat",
}

# Regex patterns cho Vietnamese legal codes
# KH√îNG preprocess ‚Äî gi·ªØ nguy√™n format g·ªëc
LEGAL_CODE_PATTERNS = [
    # Pattern 1: S·ªë/NƒÉm/Lo·∫°i-C∆° quan (ph·ªï bi·∫øn nh·∫•t)
    # VD: 265/2025/Nƒê-CP, 01/2024/TT-BTC, 76/2018/Nƒê-CP
    re.compile(
        r'(\d{1,4})/(\d{4})/(Nƒê|TT|Qƒê|NQ|CT|VBHN)-([A-Zƒê√Ä-·ª∏a-zƒë√†-·ªπ&]+)',
        re.UNICODE
    ),
    
    # Pattern 2: S·ªë/Lo·∫°i-C∆° quan (kh√¥ng c√≥ nƒÉm)
    # VD: 737/Qƒê-CQƒêHQ, 1234/CV-VPCP
    re.compile(
        r'(\d{1,5})/(Qƒê|CV|CT|NQ|TB)-([A-Zƒê√Ä-·ª∏a-zƒë√†-·ªπ&]+)',
        re.UNICODE
    ),
    
    # Pattern 3: Lu·∫≠t s·ªë XX/YYYY/QHXX
    # VD: 45/2019/QH14
    re.compile(
        r'(\d{1,3})/(\d{4})/(QH\d{1,2})',
        re.UNICODE
    ),
    
    # Pattern 4: S·ªë hi·ªáu n·ªôi b·ªô ATTECH
    # VD: DTCT.2024.05, QT.ATTECH.001
    re.compile(
        r'([A-Zƒê]{2,6})\.(\d{4})\.(\d{2,3})',
        re.UNICODE
    ),
]


class LegalCodeExtractor:
    """Tr√≠ch xu·∫•t m√£ vƒÉn b·∫£n ph√°p lu·∫≠t t·ª´ text."""
    
    def extract_all_references(self, text: str) -> List[LegalReference]:
        """
        T√¨m t·∫•t c·∫£ m√£ vƒÉn b·∫£n trong text.
        
        ∆Øu ƒëi·ªÉm so v·ªõi approach c≈© (NER/regex ƒë∆°n gi·∫£n):
        - Kh√¥ng preprocess text (gi·ªØ nguy√™n d·∫•u, format g·ªëc)
        - Ph√¢n lo·∫°i ƒë∆∞·ª£c lo·∫°i vƒÉn b·∫£n v√† c∆° quan ban h√†nh
        - Tr·∫£ v·ªÅ context (c√¢u ch·ª©a reference) ƒë·ªÉ verify
        """
        references = []
        seen = set()  # Deduplicate
        
        for pattern in LEGAL_CODE_PATTERNS:
            for match in pattern.finditer(text):
                law_id = match.group(0)
                
                if law_id in seen:
                    continue
                seen.add(law_id)
                
                ref = self._parse_match(match, text)
                if ref:
                    references.append(ref)
        
        return references
    
    def extract_document_own_id(self, text: str, title: str) -> Optional[str]:
        """
        Tr√≠ch xu·∫•t m√£ c·ªßa ch√≠nh document n√†y (kh√¥ng ph·∫£i reference).
        Th∆∞·ªùng xu·∫•t hi·ªán trong ti√™u ƒë·ªÅ ho·∫∑c d√≤ng ƒë·∫ßu ti√™n.
        """
        # ∆Øu ti√™n t√¨m trong title
        refs = self.extract_all_references(title)
        if refs:
            return refs[0].law_id
        
        # Fallback: t√¨m trong 500 k√Ω t·ª± ƒë·∫ßu
        first_part = text[:500]
        refs = self.extract_all_references(first_part)
        if refs:
            return refs[0].law_id
        
        return None
    
    def extract_based_on_references(self, text: str) -> List[str]:
        """
        Tr√≠ch xu·∫•t danh s√°ch vƒÉn b·∫£n ƒë∆∞·ª£c vi·ªán d·∫´n (cƒÉn c·ª© ph√°p l√Ω).
        
        T√¨m trong c√°c ƒëo·∫°n b·∫Øt ƒë·∫ßu b·∫±ng:
        - "CƒÉn c·ª©..."
        - "Theo quy ƒë·ªãnh t·∫°i..."
        - "Th·ª±c hi·ªán theo..."
        """
        based_on = []
        
        # T√¨m ƒëo·∫°n "CƒÉn c·ª©" (th∆∞·ªùng ·ªü ƒë·∫ßu vƒÉn b·∫£n ph√°p lu·∫≠t)
        cancu_pattern = re.compile(
            r'[Cc]ƒÉn\s+c·ª©[^;.]*?(?:;|\.|\n)',
            re.UNICODE | re.DOTALL
        )
        
        for cancu_match in cancu_pattern.finditer(text[:3000]):  # Ch·ªâ x√©t 3000 k√Ω t·ª± ƒë·∫ßu
            cancu_text = cancu_match.group(0)
            refs = self.extract_all_references(cancu_text)
            for ref in refs:
                if ref.law_id not in based_on:
                    based_on.append(ref.law_id)
        
        return based_on
    
    def _parse_match(self, match, full_text: str) -> Optional[LegalReference]:
        """Parse regex match th√†nh LegalReference."""
        law_id = match.group(0)
        groups = match.groups()
        
        # Detect law_type v√† issuing_body
        law_type = "unknown"
        issuing_code = ""
        year = None
        
        # Pattern 1: s·ªë/nƒÉm/lo·∫°i-c∆° quan
        if len(groups) >= 4 and groups[1].isdigit():
            year = int(groups[1])
            type_code = groups[2]
            issuing_code = groups[3]
            law_type = LAW_TYPE_MAP.get(type_code, "unknown")
        
        # Pattern 2: s·ªë/lo·∫°i-c∆° quan
        elif len(groups) >= 3 and not groups[1].isdigit():
            type_code = groups[1]
            issuing_code = groups[2]
            law_type = LAW_TYPE_MAP.get(type_code, "unknown")
        
        # Pattern 3: QH
        elif len(groups) >= 3 and "QH" in groups[2]:
            year = int(groups[1])
            law_type = "luat"
            issuing_code = groups[2]
        
        # Extract context (c√¢u ch·ª©a reference)
        start = max(0, match.start() - 50)
        end = min(len(full_text), match.end() + 50)
        context = full_text[start:end].strip()
        
        issuing_full = ISSUING_BODY_MAP.get(issuing_code, issuing_code)
        
        return LegalReference(
            law_id=law_id,
            law_type=law_type,
            issuing_body_code=issuing_code,
            issuing_body_full=issuing_full,
            year=year,
            context=context,
            position=match.start()
        )
```

#### B∆∞·ªõc 2.2: Vietnamese Keyword Extractor (Ng√†y 2-3)

```python
# src/core/metadata/keyword_extractor.py
"""
Tr√≠ch xu·∫•t keywords t·ª´ Vietnamese legal text.
Thu·∫ßn Python ‚Äî KH√îNG d√πng LLM (ti·∫øt ki·ªám token).
D√πng underthesea cho word segmentation + POS tagging.
"""

from typing import List, Tuple
from collections import Counter
import re

# Import Vietnamese NLP
from underthesea import word_tokenize, pos_tag

# Vietnamese stopwords cho legal domain
LEGAL_STOPWORDS = {
    # Common Vietnamese stopwords
    "v√†", "c·ªßa", "c√°c", "c√≥", "ƒë∆∞·ª£c", "trong", "l√†", "cho", "v·ªõi",
    "n√†y", "ƒë√£", "t·ª´", "theo", "v·ªÅ", "kh√¥ng", "m·ªôt", "nh·ªØng",
    "khi", "ƒë·ªÉ", "ho·∫∑c", "nh∆∞ng", "n·∫øu", "tr√™n", "ƒë·∫øn", "do",
    "t·∫°i", "c≈©ng", "n√™n", "c√≤n", "s·∫Ω",
    
    # Legal boilerplate words (xu·∫•t hi·ªán ·ªü h·∫ßu h·∫øt m·ªçi vƒÉn b·∫£n)
    "quy_ƒë·ªãnh", "ƒëi·ªÅu", "kho·∫£n", "ƒëi·ªÉm", "m·ª•c", "ch∆∞∆°ng",
    "ban_h√†nh", "th·ª±c_hi·ªán", "√°p_d·ª•ng", "hi·ªáu_l·ª±c",
    "thi_h√†nh", "ch·ªãu_tr√°ch_nhi·ªám",
}

# Domain-specific compound nouns c·∫ßn gi·ªØ nguy√™n
COMPOUND_TERMS = {
    "h√†ng_kh√¥ng": "h√†ng kh√¥ng",
    "an_to√†n_bay": "an to√†n bay",
    "qu·∫£n_l√Ω_bay": "qu·∫£n l√Ω bay",
    "kh√¥ng_l∆∞u": "kh√¥ng l∆∞u",
    "CNS_ATM": "CNS/ATM",
    "s√¢n_bay": "s√¢n bay",
    "t·ªï_bay": "t·ªï bay",
    "lao_ƒë·ªông": "lao ƒë·ªông",
    "b·∫£o_hi·ªÉm_x√£_h·ªôi": "b·∫£o hi·ªÉm x√£ h·ªôi",
    "ngh·ªâ_ph√©p": "ngh·ªâ ph√©p",
    "h·ª£p_ƒë·ªìng_lao_ƒë·ªông": "h·ª£p ƒë·ªìng lao ƒë·ªông",
    "doanh_nghi·ªáp": "doanh nghi·ªáp",
    "thu·∫ø_thu_nh·∫≠p": "thu·∫ø thu nh·∫≠p",
    # Th√™m domain terms cho ATTECH
}


class VietnameseKeywordExtractor:
    """
    Tr√≠ch xu·∫•t keywords cho Vietnamese legal documents.
    Approach: TF-IDF ƒë∆°n gi·∫£n + POS filtering (ch·ªâ l·∫•y nouns).
    """
    
    def extract_keywords(
        self, 
        text: str, 
        title: str = "",
        max_keywords: int = 10,
        min_frequency: int = 2
    ) -> List[str]:
        """
        Extract top keywords t·ª´ document text.
        
        Args:
            text: N·ªôi dung document
            title: Ti√™u ƒë·ªÅ (keywords t·ª´ title ƒë∆∞·ª£c boost)
            max_keywords: S·ªë keywords t·ªëi ƒëa
            min_frequency: T·∫ßn su·∫•t t·ªëi thi·ªÉu
            
        Returns:
            List keywords ƒë√£ s·∫Øp x·∫øp theo relevance
        """
        # 1. Word segmentation
        tokens = word_tokenize(text, format="text").split()
        
        # 2. POS tagging ‚Äî ch·ªâ gi·ªØ nouns (N, Np, Nc, Nu)
        tagged = pos_tag(text)
        noun_tokens = [
            word.lower().replace(" ", "_")
            for word, tag in tagged
            if tag in ("N", "Np", "Nc", "Nu", "Ny")
            and len(word) > 1
        ]
        
        # 3. Filter stopwords
        filtered = [
            t for t in noun_tokens
            if t not in LEGAL_STOPWORDS
            and len(t) > 2
        ]
        
        # 4. Count frequency
        counter = Counter(filtered)
        
        # 5. Boost keywords from title
        if title:
            title_tokens = word_tokenize(title, format="text").split()
            for t in title_tokens:
                t_lower = t.lower().replace(" ", "_")
                if t_lower in counter:
                    counter[t_lower] *= 3  # Triple boost cho title keywords
        
        # 6. Boost compound terms
        for compound, display in COMPOUND_TERMS.items():
            if compound in counter:
                counter[compound] *= 2  # Double boost cho domain terms
        
        # 7. Filter by frequency and return
        keywords = [
            word.replace("_", " ")
            for word, count in counter.most_common(max_keywords * 2)
            if count >= min_frequency
        ][:max_keywords]
        
        return keywords
    
    def extract_category(self, text: str, keywords: List[str]) -> str:
        """
        Ph√¢n lo·∫°i document v√†o category d·ª±a tr√™n keywords.
        Rule-based ‚Äî kh√¥ng d√πng LLM.
        """
        text_lower = text.lower()
        keyword_set = set(k.lower() for k in keywords)
        
        # Category rules (ordered by specificity)
        CATEGORY_RULES = [
            ("hang_khong", [
                "h√†ng kh√¥ng", "bay", "s√¢n bay", "phi c√¥ng", "t·ªï bay",
                "kh√¥ng l∆∞u", "CNS", "ATM", "ICAO", "radar"
            ]),
            ("lao_dong", [
                "lao ƒë·ªông", "ngh·ªâ ph√©p", "h·ª£p ƒë·ªìng", "ti·ªÅn l∆∞∆°ng",
                "b·∫£o hi·ªÉm x√£ h·ªôi", "thai s·∫£n", "vi·ªác l√†m"
            ]),
            ("tai_chinh", [
                "t√†i ch√≠nh", "ng√¢n s√°ch", "thu·∫ø", "k·∫ø to√°n",
                "ki·ªÉm to√°n", "ƒë·∫ßu t∆∞", "v·ªën"
            ]),
            ("khoa_hoc_cong_nghe", [
                "khoa h·ªçc", "c√¥ng ngh·ªá", "nghi√™n c·ª©u", "s√°ng ch·∫ø",
                "ph√°t minh", "chuy·ªÉn giao"
            ]),
            ("hanh_chinh", [
                "h√†nh ch√≠nh", "t·ªï ch·ª©c", "nh√¢n s·ª±", "quy ch·∫ø",
                "n·ªôi quy", "ƒëi·ªÅu l·ªá"
            ]),
            ("an_toan", [
                "an to√†n", "b·∫£o h·ªô", "ph√≤ng ch√°y", "c·ª©u h·ªô",
                "m√¥i tr∆∞·ªùng"
            ]),
        ]
        
        scores = {}
        for category, terms in CATEGORY_RULES:
            score = 0
            for term in terms:
                if term in text_lower:
                    score += text_lower.count(term)
                if term in keyword_set:
                    score += 5  # Bonus n·∫øu xu·∫•t hi·ªán trong extracted keywords
            scores[category] = score
        
        if not scores or max(scores.values()) == 0:
            return "khac"  # fallback
        
        return max(scores, key=scores.get)
```

#### B∆∞·ªõc 2.3: Metadata Enricher ‚Äî Orchestrator (Ng√†y 3-4)

```python
# src/core/metadata/enricher.py
"""
Orchestrator: K·∫øt h·ª£p c√°c extractors ƒë·ªÉ enrich metadata t·ª± ƒë·ªông.
ƒê∆∞·ª£c g·ªçi TR∆Ø·ªöC khi import v√†o database.
"""

import json
import logging
from typing import Dict, Any, Optional
from .legal_code_extractor import LegalCodeExtractor
from .keyword_extractor import VietnameseKeywordExtractor

logger = logging.getLogger(__name__)


class MetadataEnricher:
    """
    T·ª± ƒë·ªông b·ªï sung metadata cho document tr∆∞·ªõc khi import.
    
    Nguy√™n t·∫Øc:
    - KH√îNG ghi ƒë√® metadata ƒë√£ c√≥ (ch·ªâ b·ªï sung fields tr·ªëng)
    - KH√îNG d√πng LLM (thu·∫ßn Python, deterministic)
    - Lu√¥n gi·ªØ nguy√™n legal codes (KH√îNG preprocess)
    """
    
    def __init__(self):
        self.legal_extractor = LegalCodeExtractor()
        self.keyword_extractor = VietnameseKeywordExtractor()
    
    def enrich(
        self,
        metadata: Dict[str, Any],
        full_content: str,
        title: str
    ) -> Dict[str, Any]:
        """
        Enrich metadata t·ª´ content analysis.
        
        Args:
            metadata: JSONB metadata hi·ªán c√≥ (c√≥ th·ªÉ thi·∫øu fields)
            full_content: N·ªôi dung ƒë·∫ßy ƒë·ªß document
            title: Ti√™u ƒë·ªÅ document
            
        Returns:
            metadata ƒë√£ ƒë∆∞·ª£c b·ªï sung
        """
        enriched = json.loads(json.dumps(metadata))  # Deep copy
        
        # ƒê·∫£m b·∫£o structure t·ªìn t·∫°i
        for section in ["identification", "authority", "domain", 
                        "relationships", "financial"]:
            if section not in enriched:
                enriched[section] = {}
        
        # 1. Enrich identification
        self._enrich_identification(enriched, full_content, title)
        
        # 2. Enrich domain (keywords + category)
        self._enrich_domain(enriched, full_content, title)
        
        # 3. Enrich relationships (based_on)
        self._enrich_relationships(enriched, full_content)
        
        # 4. Enrich authority (organization t·ª´ legal codes)
        self._enrich_authority(enriched, full_content, title)
        
        logger.info(
            f"Enriched metadata for '{title[:50]}': "
            f"identification={bool(enriched['identification'].get('document_number'))}, "
            f"keywords={len(enriched['domain'].get('keywords', []))}, "
            f"based_on={len(enriched['relationships'].get('based_on', []))}"
        )
        
        return enriched
    
    def _enrich_identification(self, meta, content, title):
        """B·ªï sung document_number, document_type, issue_date."""
        ident = meta["identification"]
        
        # Document number
        if not ident.get("document_number"):
            own_id = self.legal_extractor.extract_document_own_id(content, title)
            if own_id:
                ident["document_number"] = own_id
        
        # Document type (t·ª´ pattern trong document_number)
        if not ident.get("document_type") or ident["document_type"] == "unknown":
            refs = self.legal_extractor.extract_all_references(title)
            if refs:
                ident["document_type"] = refs[0].law_type
        
        # Issue date (t·ª´ year trong document_number)
        if not ident.get("issue_date"):
            refs = self.legal_extractor.extract_all_references(
                ident.get("document_number", "") or title
            )
            if refs and refs[0].year:
                ident["issue_date"] = f"{refs[0].year}-01-01"
    
    def _enrich_domain(self, meta, content, title):
        """B·ªï sung keywords v√† category."""
        domain = meta["domain"]
        
        # Keywords
        existing_keywords = domain.get("keywords", [])
        if not existing_keywords or len(existing_keywords) < 3:
            extracted = self.keyword_extractor.extract_keywords(
                content, title, max_keywords=10
            )
            # Merge: gi·ªØ existing + th√™m m·ªõi
            merged = list(set(existing_keywords + extracted))[:15]
            domain["keywords"] = merged
        
        # Category
        if not domain.get("category") or domain["category"] in ("unknown", "other", "khac"):
            domain["category"] = self.keyword_extractor.extract_category(
                content, domain.get("keywords", [])
            )
    
    def _enrich_relationships(self, meta, content):
        """B·ªï sung based_on (cƒÉn c·ª© ph√°p l√Ω)."""
        rels = meta["relationships"]
        
        existing_based_on = rels.get("based_on", [])
        if not existing_based_on:
            extracted = self.legal_extractor.extract_based_on_references(content)
            if extracted:
                rels["based_on"] = extracted
    
    def _enrich_authority(self, meta, content, title):
        """B·ªï sung organization t·ª´ legal code analysis."""
        auth = meta["authority"]
        
        if not auth.get("organization") or auth["organization"].lower() in ("general", "unknown", ""):
            refs = self.legal_extractor.extract_all_references(title)
            if refs and refs[0].issuing_body_full:
                auth["organization"] = refs[0].issuing_body_full
```

#### B∆∞·ªõc 2.4: Metadata Validator ‚Äî Quality Gate (Ng√†y 4-5)

```python
# src/core/metadata/validator.py
"""
Validation gate: Ki·ªÉm tra metadata ƒê·∫†T CHU·∫®N tr∆∞·ªõc khi import.
Thay th·∫ø cho FR-03.2 (mock service).
"""

from typing import Dict, Any, List, Tuple
from dataclasses import dataclass
from enum import Enum


class ValidationLevel(Enum):
    PASS = "pass"           # ƒê·ªß metadata, import ngay
    WARNING = "warning"     # Thi·∫øu m·ªôt s·ªë fields, import + flag
    FAIL = "fail"           # Thi·∫øu fields critical, KH√îNG import


@dataclass
class ValidationResult:
    level: ValidationLevel
    score: float                # 0-100
    missing_critical: List[str]
    missing_optional: List[str]
    warnings: List[str]
    
    @property
    def can_import(self) -> bool:
        return self.level in (ValidationLevel.PASS, ValidationLevel.WARNING)


class MetadataValidator:
    """
    Validate metadata quality tr∆∞·ªõc khi import.
    
    Thresholds:
    - PASS: score ‚â• 80 (t·∫•t c·∫£ critical fields c√≥)
    - WARNING: score 50-79 (thi·∫øu m·ªôt s·ªë, nh∆∞ng c√≥ basics)
    - FAIL: score < 50 (thi·∫øu qu√° nhi·ªÅu)
    """
    
    # Fields v√† weights
    CRITICAL_FIELDS = {
        # (section, field): weight
        ("identification", "document_number"): 15,
        ("identification", "document_type"): 10,
        ("identification", "issue_date"): 10,
        ("domain", "keywords"): 15,
        ("domain", "category"): 10,
    }
    
    HIGH_FIELDS = {
        ("authority", "organization"): 10,
        ("authority", "department"): 5,
        ("relationships", "based_on"): 15,
    }
    
    OPTIONAL_FIELDS = {
        ("relationships", "relates_to"): 5,
        ("financial", "budget"): 5,
    }
    
    def validate(self, metadata: Dict[str, Any]) -> ValidationResult:
        """Validate metadata completeness."""
        score = 0
        max_score = 0
        missing_critical = []
        missing_optional = []
        warnings = []
        
        # Check critical fields
        for (section, field), weight in self.CRITICAL_FIELDS.items():
            max_score += weight
            value = self._get_value(metadata, section, field)
            if self._is_valid(value, field):
                score += weight
            else:
                missing_critical.append(f"{section}.{field}")
        
        # Check high fields
        for (section, field), weight in self.HIGH_FIELDS.items():
            max_score += weight
            value = self._get_value(metadata, section, field)
            if self._is_valid(value, field):
                score += weight
            else:
                missing_optional.append(f"{section}.{field}")
        
        # Check optional fields
        for (section, field), weight in self.OPTIONAL_FIELDS.items():
            max_score += weight
            value = self._get_value(metadata, section, field)
            if self._is_valid(value, field):
                score += weight
        
        # Calculate percentage
        pct = (score / max_score * 100) if max_score > 0 else 0
        
        # Determine level
        if pct >= 80 and not missing_critical:
            level = ValidationLevel.PASS
        elif pct >= 50:
            level = ValidationLevel.WARNING
            warnings.append(
                f"Metadata completeness {pct:.0f}% (target: 80%). "
                f"Missing: {', '.join(missing_critical)}"
            )
        else:
            level = ValidationLevel.FAIL
            warnings.append(
                f"Metadata completeness {pct:.0f}% ‚Äî D∆Ø·ªöI NG∆Ø·ª†NG. "
                f"Cannot import without: {', '.join(missing_critical)}"
            )
        
        return ValidationResult(
            level=level,
            score=round(pct, 1),
            missing_critical=missing_critical,
            missing_optional=missing_optional,
            warnings=warnings
        )
    
    def _get_value(self, metadata, section, field):
        if section in metadata and isinstance(metadata[section], dict):
            return metadata[section].get(field)
        return None
    
    def _is_valid(self, value, field):
        if value is None or value == "" or value == []:
            return False
        if isinstance(value, str) and value.lower() in ("unknown", "other", "general", "null"):
            return False
        if isinstance(value, list) and len(value) == 0:
            return False
        if field == "keywords" and isinstance(value, list) and len(value) < 2:
            return False
        return True
```

#### B∆∞·ªõc 2.5: T√≠ch h·ª£p v√†o Import Pipeline (Ng√†y 5)

S·ª≠a `simple_import_processor.py`:

```python
# Th√™m v√†o import pipeline
from src.core.metadata.enricher import MetadataEnricher
from src.core.metadata.validator import MetadataValidator, ValidationLevel

enricher = MetadataEnricher()
validator = MetadataValidator()

async def process_document(doc_json_path: str):
    """Import pipeline C·∫¨P NH·∫¨T v·ªõi enrichment + validation."""
    
    # Step 1: Load document.json (c√≥ s·∫µn)
    with open(doc_json_path) as f:
        doc_data = json.load(f)
    
    metadata = doc_data.get("metadata", {})
    content = extract_full_content(doc_data)
    title = doc_data.get("title", "")
    
    # Step 2: ‚≠ê Auto-enrich metadata
    enriched_metadata = enricher.enrich(metadata, content, title)
    
    # Step 3: ‚≠ê Validate
    validation = validator.validate(enriched_metadata)
    
    if validation.level == ValidationLevel.FAIL:
        logger.warning(
            f"‚ö†Ô∏è SKIP '{title}': metadata score {validation.score}% "
            f"(missing: {validation.missing_critical})"
        )
        # V·∫´n import nh∆∞ng flag cho manual review
        enriched_metadata["_needs_review"] = True
        enriched_metadata["_validation_score"] = validation.score
    
    if validation.warnings:
        for w in validation.warnings:
            logger.info(f"  ‚Üí {w}")
    
    # Step 4: Import v√†o DB v·ªõi enriched metadata
    doc_data["metadata"] = enriched_metadata
    await import_to_database(doc_data)
    
    # Step 5: ‚≠ê Auto-sync Graph RAG
    if validation.can_import:
        await sync_to_graph(doc_data["document_id"])
    
    return validation
```

**Deliverable Giai ƒëo·∫°n 2:**
- ‚úÖ LegalCodeExtractor: tr√≠ch xu·∫•t ch√≠nh x√°c m√£ vƒÉn b·∫£n t·ª´ content
- ‚úÖ KeywordExtractor: tr√≠ch xu·∫•t 5-10 keywords thu·∫ßn Python
- ‚úÖ MetadataEnricher: orchestrator t·ª± ƒë·ªông b·ªï sung
- ‚úÖ MetadataValidator: quality gate tr∆∞·ªõc khi import
- ‚úÖ Import pipeline t√≠ch h·ª£p enrichment + validation
- ‚úÖ Documents m·ªõi t·ª± ƒë·ªông c√≥ metadata ‚â•80% completeness

---

### GIAI ƒêO·∫†N 3: T√çCH H·ª¢P METADATA V√ÄO SEARCH ALGORITHMS (Tu·∫ßn 4)

**M·ª•c ti√™u:** Metadata kh√¥ng ch·ªâ ƒë·ªÉ "trang tr√≠" m√† th·ª±c s·ª± c·∫£i thi·ªán search quality.

#### B∆∞·ªõc 3.1: Metadata-Boosted Hybrid Search (Ng√†y 1-2)

Hi·ªán t·∫°i HybridRanker d√πng: `0.7 * semantic + 0.3 * keyword`. 
Th√™m metadata signals v√†o scoring:

```python
# C·∫≠p nh·∫≠t hybrid_ranker.py

class MetadataAwareHybridRanker:
    """
    HybridRanker n√¢ng c·∫•p: k·∫øt h·ª£p metadata signals v√†o ranking.
    """
    
    async def combine_results(self, query, raw_results, top_k=20):
        """
        Score = 0.5 * semantic 
              + 0.2 * keyword 
              + 0.15 * metadata_match    # ‚≠ê M·ªöI
              + 0.1 * freshness          # ‚≠ê M·ªöI 
              + 0.05 * authority          # ‚≠ê M·ªöI
        """
        
        for result in raw_results:
            # Existing scores
            semantic_score = result.get("similarity_score", 0)
            keyword_score = normalize(result.get("bm25_score", 0))
            
            # ‚≠ê Metadata match score
            metadata_score = self._calc_metadata_match(query, result)
            
            # ‚≠ê Freshness score (docs m·ªõi h∆°n ƒë∆∞·ª£c boost)
            freshness_score = self._calc_freshness(result)
            
            # ‚≠ê Authority score (Lu·∫≠t > Nƒê > TT > Qƒê)
            authority_score = self._calc_authority(result)
            
            result["combined_score"] = (
                0.50 * semantic_score +
                0.20 * keyword_score +
                0.15 * metadata_score +
                0.10 * freshness_score +
                0.05 * authority_score
            )
        
        return sorted(raw_results, key=lambda x: x["combined_score"], reverse=True)[:top_k]
    
    def _calc_metadata_match(self, query: str, result: dict) -> float:
        """
        T√≠nh ƒëi·ªÉm d·ª±a tr√™n metadata match v·ªõi query.
        VD: query "ngh·ªã ƒë·ªãnh v·ªÅ thu·∫ø" ‚Üí boost docs c√≥ category=tai_chinh, type=nghi_dinh
        """
        score = 0.0
        metadata = result.get("metadata", {})
        query_lower = query.lower()
        
        # Category match
        category = metadata.get("domain", {}).get("category", "")
        category_keywords = {
            "tai_chinh": ["thu·∫ø", "t√†i ch√≠nh", "ng√¢n s√°ch", "k·∫ø to√°n"],
            "lao_dong": ["lao ƒë·ªông", "ngh·ªâ ph√©p", "l∆∞∆°ng", "b·∫£o hi·ªÉm"],
            "hang_khong": ["h√†ng kh√¥ng", "bay", "s√¢n bay", "phi c√¥ng"],
        }
        for cat, terms in category_keywords.items():
            if cat == category and any(t in query_lower for t in terms):
                score += 0.4
                break
        
        # Keyword overlap
        doc_keywords = metadata.get("domain", {}).get("keywords", [])
        if doc_keywords:
            query_words = set(query_lower.split())
            keyword_words = set(k.lower() for k in doc_keywords)
            overlap = query_words & keyword_words
            if overlap:
                score += min(len(overlap) * 0.15, 0.4)
        
        # Document type match
        doc_type = metadata.get("identification", {}).get("document_type", "")
        type_keywords = {
            "luat": ["lu·∫≠t", "b·ªô lu·∫≠t"],
            "nghi_dinh": ["ngh·ªã ƒë·ªãnh"],
            "thong_tu": ["th√¥ng t∆∞"],
            "quyet_dinh": ["quy·∫øt ƒë·ªãnh"],
        }
        for dtype, terms in type_keywords.items():
            if dtype == doc_type and any(t in query_lower for t in terms):
                score += 0.2
                break
        
        return min(score, 1.0)
    
    def _calc_freshness(self, result: dict) -> float:
        """
        Docs m·ªõi h∆°n ƒë∆∞·ª£c boost (quan tr·ªçng cho ph√°p lu·∫≠t: 
        Nƒê m·ªõi thay th·∫ø Nƒê c≈©).
        """
        from datetime import datetime, timedelta
        
        metadata = result.get("metadata", {})
        issue_date_str = metadata.get("identification", {}).get("issue_date", "")
        
        if not issue_date_str:
            return 0.3  # Default trung b√¨nh n·∫øu kh√¥ng c√≥ date
        
        try:
            issue_date = datetime.fromisoformat(issue_date_str[:10])
            now = datetime.now()
            age_days = (now - issue_date).days
            
            # Docs < 1 nƒÉm: score 1.0
            # Docs 1-3 nƒÉm: score 0.7
            # Docs 3-5 nƒÉm: score 0.5
            # Docs > 5 nƒÉm: score 0.3
            if age_days < 365:
                return 1.0
            elif age_days < 365 * 3:
                return 0.7
            elif age_days < 365 * 5:
                return 0.5
            else:
                return 0.3
        except:
            return 0.3
    
    def _calc_authority(self, result: dict) -> float:
        """
        Hierarchy ph√°p l√Ω: Lu·∫≠t > Ngh·ªã ƒë·ªãnh > Th√¥ng t∆∞ > Quy·∫øt ƒë·ªãnh.
        Docs c·∫•p cao h∆°n ƒë∆∞·ª£c boost khi relevance t∆∞∆°ng ƒë∆∞∆°ng.
        """
        metadata = result.get("metadata", {})
        doc_type = metadata.get("identification", {}).get("document_type", "")
        
        AUTHORITY_SCORES = {
            "luat": 1.0,
            "nghi_dinh": 0.9,
            "nghi_quyet": 0.85,
            "thong_tu": 0.8,
            "quyet_dinh": 0.7,
            "cong_van": 0.5,
            "chi_thi": 0.6,
        }
        
        return AUTHORITY_SCORES.get(doc_type, 0.4)
```

#### B∆∞·ªõc 3.2: Query Router s·ª≠ d·ª•ng metadata (Ng√†y 2-3)

```python
# src/core/search/query_router.py
"""
Intelligent query routing: ch·ªçn search strategy t·ªëi ∆∞u d·ª±a tr√™n query analysis.
Metadata-aware ‚Äî ch·ªâ d√πng Metadata Search khi metadata ƒë·ªß t·ªët.
"""

from enum import Enum
from typing import List
import re


class SearchStrategy(Enum):
    EXACT_LOOKUP = "exact_lookup"      # law_id ‚Üí Substring only
    SEMANTIC_HEAVY = "semantic_heavy"  # Conceptual ‚Üí 0.8 semantic + 0.2 keyword
    HYBRID_BALANCED = "hybrid"         # Mixed ‚Üí standard hybrid
    METADATA_FIRST = "metadata_first"  # Filtered ‚Üí metadata filter + semantic
    TEMPORAL = "temporal"              # Time-based ‚Üí freshness boost


class QueryRouter:
    """
    Ph√¢n lo·∫°i query v√† ch·ªçn strategy t·ªëi ∆∞u.
    Rule-based ‚Äî KH√îNG d√πng LLM.
    """
    
    # Patterns cho legal code detection
    LEGAL_CODE_PATTERN = re.compile(
        r'\d{1,4}/\d{4}/[A-Zƒê]{2,4}-[A-Zƒê]+|'  # 265/2025/Nƒê-CP
        r'\d{1,5}/[A-Zƒê]{2,4}-[A-Zƒê]+',           # 737/Qƒê-CQƒêHQ
        re.UNICODE
    )
    
    def route(self, query: str) -> SearchStrategy:
        """Ph√¢n lo·∫°i query v√† tr·∫£ v·ªÅ strategy t·ªëi ∆∞u."""
        query_lower = query.strip().lower()
        
        # 1. Exact lookup: query l√† legal code
        if self.LEGAL_CODE_PATTERN.search(query):
            return SearchStrategy.EXACT_LOOKUP
        
        # 2. Metadata-first: query ch·ª©a filter conditions
        if self._has_metadata_filters(query_lower):
            return SearchStrategy.METADATA_FIRST
        
        # 3. Temporal: query h·ªèi v·ªÅ th·ªùi gian/hi·ªáu l·ª±c
        if self._is_temporal_query(query_lower):
            return SearchStrategy.TEMPORAL
        
        # 4. Semantic-heavy: query conceptual, d√†i
        if len(query.split()) > 8:
            return SearchStrategy.SEMANTIC_HEAVY
        
        # 5. Default: hybrid balanced
        return SearchStrategy.HYBRID_BALANCED
    
    def _has_metadata_filters(self, query: str) -> bool:
        """Detect n·∫øu query ch·ª©a metadata filter hints."""
        filter_patterns = [
            r'do\s+(b·ªô|ch√≠nh ph·ªß|qu·ªëc h·ªôi|th·ªß t∆∞·ªõng)',  # "do B·ªô GTVT ban h√†nh"
            r'lo·∫°i\s+(lu·∫≠t|ngh·ªã ƒë·ªãnh|th√¥ng t∆∞)',          # "lo·∫°i ngh·ªã ƒë·ªãnh"
            r'nƒÉm\s+\d{4}',                               # "nƒÉm 2024"
            r'lƒ©nh v·ª±c\s+\w+',                            # "lƒ©nh v·ª±c t√†i ch√≠nh"
            r'(ban h√†nh|c·ªßa)\s+(b·ªô|s·ªü|c·ª•c)',              # "ban h√†nh b·ªüi B·ªô..."
        ]
        return any(re.search(p, query) for p in filter_patterns)
    
    def _is_temporal_query(self, query: str) -> bool:
        """Detect temporal queries."""
        temporal_keywords = [
            "hi·ªán h√†nh", "m·ªõi nh·∫•t", "c√≤n hi·ªáu l·ª±c",
            "thay th·∫ø", "h·∫øt hi·ªáu l·ª±c", "s·ª≠a ƒë·ªïi",
            "b·ªï sung", "b√£i b·ªè", "g·∫ßn ƒë√¢y"
        ]
        return any(kw in query for kw in temporal_keywords)
```

#### B∆∞·ªõc 3.3: T√≠ch h·ª£p Graph RAG v√†o Retrieval (Ng√†y 3-5)

```python
# Th√™m v√†o SearchOrchestrator

async def search_with_graph_expansion(self, query, top_k=10):
    """
    Enhanced search: k·∫øt qu·∫£ ch√≠nh + documents li√™n quan qua Graph.
    
    Flow:
    1. Standard search ‚Üí top results
    2. V·ªõi m·ªói top result, query Graph RAG ‚Üí related docs
    3. Merge v√† rerank to√†n b·ªô
    """
    # Step 1: Standard search
    primary_results = await self.search_documents(query, top_k=top_k)
    
    if not primary_results:
        return primary_results
    
    # Step 2: Graph expansion cho top 3 results
    expanded_doc_ids = set()
    for result in primary_results[:3]:
        doc_id = result.get("document_id")
        if not doc_id:
            continue
        
        # Query graph edges
        related = await self._get_graph_neighbors(doc_id, max_hops=1)
        expanded_doc_ids.update(related)
    
    # Step 3: Fetch expanded docs (n·∫øu ch∆∞a trong results)
    existing_ids = {r.get("document_id") for r in primary_results}
    new_ids = expanded_doc_ids - existing_ids
    
    if new_ids:
        # Fetch chunks t·ª´ related docs, score by graph relationship
        graph_results = await self._fetch_graph_related_chunks(
            new_ids, query, max_per_doc=2
        )
        
        # Tag graph results
        for r in graph_results:
            r["source"] = "graph_expansion"
            r["combined_score"] *= 0.8  # Slight discount vs primary
        
        primary_results.extend(graph_results)
    
    # Step 4: Rerank combined results
    primary_results.sort(key=lambda x: x["combined_score"], reverse=True)
    
    return primary_results[:top_k]

async def _get_graph_neighbors(self, document_id, max_hops=1):
    """Query graph_edges ƒë·ªÉ t√¨m documents li√™n quan."""
    query = """
        SELECT DISTINCT dm2.document_id
        FROM graph_documents gd1
        JOIN graph_edges e ON e.source_graph_doc_id = gd1.graph_doc_id
        JOIN graph_documents gd2 ON gd2.graph_doc_id = e.target_graph_doc_id
        JOIN documents_metadata_v2 dm2 ON dm2.document_id = gd2.source_document_id
        WHERE gd1.source_document_id = $1
          AND e.is_active = true
          AND e.confidence >= 0.6
        ORDER BY e.confidence DESC
        LIMIT 10
    """
    rows = await self.db_pool.fetch(query, document_id)
    return {row["document_id"] for row in rows}
```

**Deliverable Giai ƒëo·∫°n 3:**
- ‚úÖ HybridRanker s·ª≠ d·ª•ng metadata signals (category, freshness, authority)
- ‚úÖ QueryRouter ch·ªçn search strategy theo query type
- ‚úÖ Graph RAG expansion t√≠ch h·ª£p v√†o search pipeline
- ‚úÖ Benchmark: accuracy tƒÉng th√™m 5-10% nh·ªù metadata

---

### GIAI ƒêO·∫†N 4: GRAPH RAG LIVE + AUTO-SYNC (Tu·∫ßn 5-6)

**M·ª•c ti√™u:** Graph RAG ho·∫°t ƒë·ªông t·ª± ƒë·ªông, kh√¥ng c·∫ßn manual scripts.

#### B∆∞·ªõc 4.1: Database Trigger cho auto-sync (Ng√†y 1-2)

```sql
-- Trigger: Khi insert/update document ‚Üí t·ª± ƒë·ªông sync graph
CREATE OR REPLACE FUNCTION sync_document_to_graph_trigger()
RETURNS TRIGGER AS $$
BEGIN
    -- Sync document v√†o graph_documents
    INSERT INTO graph_documents (
        source_document_id, 
        document_title,
        document_metadata
    ) VALUES (
        NEW.document_id,
        NEW.title,
        NEW.metadata
    ) ON CONFLICT (source_document_id) DO UPDATE
    SET document_title = NEW.title,
        document_metadata = NEW.metadata,
        updated_at = NOW();
    
    -- Flag cho edge regeneration (async job s·∫Ω x·ª≠ l√Ω)
    INSERT INTO graph_sync_queue (document_id, action, created_at)
    VALUES (NEW.document_id, 'sync_edges', NOW())
    ON CONFLICT DO NOTHING;
    
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER trg_document_graph_sync
AFTER INSERT OR UPDATE ON documents_metadata_v2
FOR EACH ROW
EXECUTE FUNCTION sync_document_to_graph_trigger();

-- Queue table cho async edge creation
CREATE TABLE IF NOT EXISTS graph_sync_queue (
    id SERIAL PRIMARY KEY,
    document_id UUID NOT NULL,
    action VARCHAR(50) NOT NULL,
    status VARCHAR(20) DEFAULT 'pending',
    created_at TIMESTAMPTZ DEFAULT NOW(),
    processed_at TIMESTAMPTZ,
    error_message TEXT
);
```

#### B∆∞·ªõc 4.2: Background worker x·ª≠ l√Ω graph sync queue (Ng√†y 2-3)

```python
# scripts/graph_sync_worker.py
"""
Background worker: X·ª≠ l√Ω graph_sync_queue.
Ch·∫°y nh∆∞ cron job ho·∫∑c daemon.

Crontab entry:
*/5 * * * * cd /path/to/project && python graph_sync_worker.py
"""

async def process_sync_queue():
    """X·ª≠ l√Ω c√°c documents ch·ªù sync edges."""
    conn = await get_db_connection()
    
    # L·∫•y pending items
    pending = await conn.fetch("""
        SELECT id, document_id FROM graph_sync_queue
        WHERE status = 'pending'
        ORDER BY created_at
        LIMIT 50
    """)
    
    for item in pending:
        try:
            # T·∫°o edges cho document n√†y
            await create_edges_for_document(conn, item["document_id"])
            
            # Mark done
            await conn.execute("""
                UPDATE graph_sync_queue 
                SET status = 'completed', processed_at = NOW()
                WHERE id = $1
            """, item["id"])
            
        except Exception as e:
            await conn.execute("""
                UPDATE graph_sync_queue 
                SET status = 'error', error_message = $2, processed_at = NOW()
                WHERE id = $1
            """, item["id"], str(e))
    
    await conn.close()
```

#### B∆∞·ªõc 4.3: Cache invalidation khi metadata thay ƒë·ªïi (Ng√†y 3-4)

```python
# Th√™m v√†o import pipeline v√† metadata editor

async def invalidate_caches_for_document(document_id: str):
    """
    Khi metadata thay ƒë·ªïi ‚Üí invalidate cached search results 
    li√™n quan ƒë·∫øn document n√†y.
    """
    redis = await get_redis()
    
    # Pattern: t√¨m t·∫•t c·∫£ cache keys ch·ª©a document_id
    # Ho·∫∑c ƒë∆°n gi·∫£n h∆°n: flush to√†n b·ªô search cache
    await redis.delete("search_cache:*")
    
    # Log
    logger.info(f"Cache invalidated for document {document_id}")
```

#### B∆∞·ªõc 4.4: Monitoring metadata quality (Ng√†y 4-5)

```python
# Prometheus metrics cho metadata quality
from prometheus_client import Gauge, Counter

metadata_completeness = Gauge(
    'rag_metadata_completeness_percent',
    'Average metadata completeness score',
)

metadata_enrichment_count = Counter(
    'rag_metadata_enrichments_total',
    'Number of documents auto-enriched',
    ['field']
)

validation_results = Counter(
    'rag_metadata_validation_total',
    'Metadata validation results',
    ['level']  # pass, warning, fail
)
```

**Deliverable Giai ƒëo·∫°n 4:**
- ‚úÖ Database trigger t·ª± ƒë·ªông sync document ‚Üí graph
- ‚úÖ Background worker t·∫°o edges t·ª± ƒë·ªông
- ‚úÖ Cache invalidation khi metadata/document thay ƒë·ªïi
- ‚úÖ Prometheus metrics cho metadata quality
- ‚úÖ Zero manual intervention cho to√†n b·ªô pipeline

---

## 4. KPI TRACKING

| Metric | Hi·ªán t·∫°i | Sau Gƒê 1 | Sau Gƒê 2 | Sau Gƒê 3 | Sau Gƒê 4 |
|--------|----------|-----------|-----------|-----------|-----------|
| Metadata completeness | ~15% | ‚â•80% (42 docs) | ‚â•80% (new docs) | ‚â•80% | ‚â•85% |
| Graph RAG edges | 0 | 300-500 | +auto | used in search | auto-maintained |
| Metadata Search accuracy | ~20% | ~50% | ~70% | ~85% | ~85% |
| Overall RAG accuracy | ~75% | ~77% | ~80% | ~85% | ~87% |
| Manual intervention needed | Every import | Fix old docs only | Rare | None | None |

---

## 5. T√ìM T·∫ÆT

```
HI·ªÜN T·∫†I:
  Upload ‚Üí metadata tr·ªëng ‚Üí s·ª≠a tay (kh√¥ng k·ªãp) ‚Üí Graph RAG ch·∫øt
  ‚Üí Metadata Search v√¥ d·ª•ng ‚Üí accuracy b·ªã gi·ªõi h·∫°n

SAU L·ªò TR√åNH:
  Upload ‚Üí Auto-Enrich (Python, no LLM) ‚Üí Validate (quality gate)
  ‚Üí Import + Auto Graph Sync ‚Üí Metadata-Boosted Search
  ‚Üí Graph RAG Expansion ‚Üí accuracy ‚â•85%
```

**6 tu·∫ßn, 4 giai ƒëo·∫°n, t·ª´ manual ‚Üí semi-auto ‚Üí full-auto.**

Giai ƒëo·∫°n 1 (manual fix) c√≥ th·ªÉ b·∫Øt ƒë·∫ßu ngay ng√†y mai.
Giai ƒëo·∫°n 2 (auto enrichment) l√† investment quan tr·ªçng nh·∫•t ‚Äî 
gi·∫£i quy·∫øt g·ªëc r·ªÖ v·∫•n ƒë·ªÅ cho 100K docs t∆∞∆°ng lai.
