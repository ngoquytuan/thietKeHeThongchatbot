ƒê√∫ng r·ªìi! ƒê√¢y ch√≠nh l√† m·ªôt trong nh·ªØng ∆∞u ƒëi·ªÉm l·ªõn c·ªßa thi·∫øt k·∫ø hi·ªán t·∫°i. **File JSON v√† JSONL l√† ngu·ªìn d·ªØ li·ªáu master** c√≥ th·ªÉ d√πng ƒë·ªÉ kh√¥i ph·ª•c ho√†n to√†n d·ªØ li·ªáu trong SQL v√† Chroma.

## ‚úÖ Kh·∫£ nƒÉng Ph·ª•c H·ªìi

### 1. **PostgreSQL Recovery**

B·∫°n c√≥ th·ªÉ rebuild l·∫°i to√†n b·ªô database t·ª´ files:

```python
# repair_postgres.py
import json
import psycopg2
from psycopg2.extras import execute_values

def repair_document_in_postgres(passages_file, document_file):
    """
    Kh√¥i ph·ª•c ho·∫∑c s·ª≠a l·∫°i d·ªØ li·ªáu trong PostgreSQL
    """
    conn = psycopg2.connect("postgresql://...")
    cur = conn.cursor()
    
    # 1. Load files
    passages = [json.loads(line) for line in open(passages_file)]
    document = json.load(open(document_file))
    
    # 2. X√ìA d·ªØ li·ªáu c≈© (n·∫øu c·∫ßn)
    cur.execute("DELETE FROM document_chunks_enhanced WHERE document_id = %s", 
                (document['id'],))
    cur.execute("DELETE FROM documents_metadata_v2 WHERE document_id = %s", 
                (document['id'],))
    
    # 3. INSERT l·∫°i document metadata
    cur.execute("""
        INSERT INTO documents_metadata_v2 (
            document_id, title, description, document_type,
            department_owner, access_level, author,
            chunk_count, metadata
        ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
    """, (
        document['id'],
        document['title'],
        document['description'],
        document['type'],
        document['department'],
        document['access_level'],
        document['author']['name'],
        document['content_stats']['passages'],
        json.dumps(document)  # Full metadata as JSONB
    ))
    
    # 4. INSERT l·∫°i passages
    passage_data = [
        (p['id'], p['doc_id'], p['content'], p['position'], 
         p['tokens'], p.get('heading', ''), json.dumps(p['meta']))
        for p in passages
    ]
    
    execute_values(cur, """
        INSERT INTO document_chunks_enhanced (
            chunk_id, document_id, chunk_content, chunk_position,
            chunk_size_tokens, heading, metadata
        ) VALUES %s
    """, passage_data)
    
    # 5. Rebuild BM25 index
    cur.execute("""
        UPDATE document_chunks_enhanced
        SET bm25_tokens = to_tsvector('vietnamese', chunk_content)
        WHERE document_id = %s
    """, (document['id'],))
    
    conn.commit()
    print(f"‚úÖ ƒê√£ repair document {document['id']} trong PostgreSQL")
```

### 2. **ChromaDB Recovery**

```python
# repair_chroma.py
import json
import chromadb

def repair_document_in_chroma(passages_file, document_file):
    """
    Kh√¥i ph·ª•c ho·∫∑c s·ª≠a l·∫°i d·ªØ li·ªáu trong ChromaDB
    """
    client = chromadb.HttpClient(host='localhost', port=8000)
    
    # Load files
    passages = [json.loads(line) for line in open(passages_file)]
    document = json.load(open(document_file))
    
    # Get collection
    collection_name = document['collections']['chromadb_primary']
    collection = client.get_or_create_collection(collection_name)
    
    # X√ìA passages c≈© c·ªßa document n√†y
    doc_id = document['id']
    try:
        existing_ids = collection.get(
            where={"doc_id": doc_id}
        )['ids']
        if existing_ids:
            collection.delete(ids=existing_ids)
            print(f"ƒê√£ x√≥a {len(existing_ids)} passages c≈©")
    except:
        pass
    
    # ADD l·∫°i passages m·ªõi
    collection.add(
        ids=[p['id'] for p in passages],
        documents=[p['content'] for p in passages],
        metadatas=[{
            'doc_id': p['doc_id'],
            'position': p['position'],
            'heading': p.get('heading', ''),
            'dept': p['meta']['dept'],
            'type': p['meta']['type'],
            'access': p['meta']['access']
        } for p in passages]
    )
    
    print(f"‚úÖ ƒê√£ repair {len(passages)} passages trong ChromaDB")
```

## üîÑ C√°c T√¨nh Hu·ªëng S·ª≠ D·ª•ng

### **T√¨nh hu·ªëng 1: X√≥a nh·∫ßm document**
```bash
# Ch·ªâ c·∫ßn ch·∫°y l·∫°i import script v·ªõi files g·ªëc
python repair_postgres.py passages.jsonl document.json
python repair_chroma.py passages.jsonl document.json
```

### **T√¨nh hu·ªëng 2: S·ª≠a metadata sai**
```bash
# 1. S·ª≠a file document.json (edit b·∫±ng text editor)
# 2. Ch·∫°y l·∫°i import script
python repair_postgres.py passages.jsonl document.json
```

### **T√¨nh hu·ªëng 3: Database b·ªã h·ªèng ho√†n to√†n**
```bash
# Rebuild to√†n b·ªô t·ª´ t·∫•t c·∫£ files JSON/JSONL
for file in output/*.jsonl; do
    doc_file="${file%_passages.jsonl}_document.json"
    python repair_postgres.py "$file" "$doc_file"
    python repair_chroma.py "$file" "$doc_file"
done
```

### **T√¨nh hu·ªëng 4: Migration sang server m·ªõi**
```bash
# Copy t·∫•t c·∫£ files JSON/JSONL sang server m·ªõi
# Ch·∫°y import script tr√™n server m·ªõi
# D·ªØ li·ªáu ƒë∆∞·ª£c kh√¥i ph·ª•c 100%
```

## üí° Best Practices

### 1. **Backup Strategy**
```bash
# Lu√¥n gi·ªØ files JSON/JSONL sau khi import
output/
‚îú‚îÄ‚îÄ HR_POLICY_20251002_143022_passages.jsonl
‚îú‚îÄ‚îÄ HR_POLICY_20251002_143022_document.json
‚îú‚îÄ‚îÄ IT_MANUAL_20251003_091234_passages.jsonl
‚îî‚îÄ‚îÄ IT_MANUAL_20251003_091234_document.json

# T·ªët nh·∫•t l√† backup v√†o cloud storage ho·∫∑c NAS
```

### 2. **Version Control**
```bash
# N·∫øu ch·ªânh metadata, t·∫°o version m·ªõi
HR_POLICY_20251002_143022_v1_document.json  # Original
HR_POLICY_20251002_143022_v2_document.json  # After fix
```

### 3. **Validation Before Repair**
```python
def validate_before_repair(passages_file, document_file):
    """
    Ki·ªÉm tra files tr∆∞·ªõc khi repair
    """
    # Load files
    passages = [json.loads(line) for line in open(passages_file)]
    document = json.load(open(document_file))
    
    # Validate
    assert document['id']
    assert document['content_stats']['passages'] == len(passages)
    assert all(p['doc_id'] == document['id'] for p in passages)
    
    print("‚úÖ Files h·ª£p l·ªá, s·∫µn s√†ng repair")
```

## üìä So S√°nh: Files vs Database

| Ti√™u ch√≠ | JSON/JSONL Files | Database |
|----------|------------------|----------|
| **Portable** | ‚úÖ D·ªÖ copy, backup | ‚ö†Ô∏è C·∫ßn export |
| **Human-readable** | ‚úÖ ƒê·ªçc ƒë∆∞·ª£c b·∫±ng text editor | ‚ùå Binary format |
| **Recovery** | ‚úÖ Ch·ªâ c·∫ßn re-import | ‚ö†Ô∏è C·∫ßn backup/restore ph·ª©c t·∫°p |
| **Query performance** | ‚ùå Ph·∫£i load v√†o memory | ‚úÖ Index, fast |
| **Storage size** | ‚ö†Ô∏è L·ªõn h∆°n | ‚úÖ N√©n t·ªët h∆°n |

## üéØ K·∫øt Lu·∫≠n

**C√¢u tr·∫£ l·ªùi ng·∫Øn g·ªçn: ƒê√öNG!**

Files JSON/JSONL l√† **"source of truth"** v√† c√≥ th·ªÉ:
- ‚úÖ Kh√¥i ph·ª•c 100% d·ªØ li·ªáu trong PostgreSQL
- ‚úÖ Kh√¥i ph·ª•c 100% d·ªØ li·ªáu trong ChromaDB
- ‚úÖ S·ª≠a metadata v√† re-import
- ‚úÖ Migration sang h·ªá th·ªëng m·ªõi
- ‚úÖ Disaster recovery

**Khuy·∫øn ngh·ªã:**
1. **Lu√¥n gi·ªØ** files JSON/JSONL sau khi import
2. **Backup** files n√†y v√†o n∆°i an to√†n (cloud, NAS)
3. **Kh√¥ng x√≥a** files ngay c·∫£ khi ƒë√£ import th√†nh c√¥ng
4. **S·ª≠ d·ª•ng** files n√†y nh∆∞ checkpoint ƒë·ªÉ rollback

Thi·∫øt k·∫ø n√†y gi√∫p h·ªá th·ªëng **resilient** v√† **recoverable** - m·ªôt trong nh·ªØng best practices trong data engineering! üéâ
